{"path":"Extras/PDF/Computer Systems A Programmer’s Perspective, Third Edition (Global Edition) ( etc.) (Z-Library).pdf","text":"For these Global Editions, the editorial team at Pearson has collaborated with educators across the world to address a wide range of subjects and requirements, equipping students with the best possible learning tools. This Global Edition preserves the cutting-edge approach and pedagogy of the original, but also features alterations, customization, and adaptation from the North American version.Computer Systems A Programmer’s PerspectiveBryant • O’HallaronTHird EdiTiON GlOBAl EdiTiON This is a special edition of an established title widely used by colleges and universities throughout the world. Pearson published this exclusive edition for the benefit of students outside the United States and Canada. if you purchased this book within the United States or Canada, you should be aware that it has been imported without the approval of the Publisher or Author. Pearson Global Edition GlOBAl EdiTiON Computer Systems A Programmer’s Perspective THird EdiTiON Randal E. Bryant • David R. O’HallaronGlOBAl EdiTiON Bryant_1292101768_mech.indd 1 07/05/15 3:22 PM Computer Systems A Programmer’s Perspective Computer Systems A Programmer’s Perspective third edition global edition Randal E. Bryant Carnegie Mellon University David R. O’Hallaron Carnegie Mellon University Global Edition contributions by Manasa S. NMAM Institute of Technology Mohit Tahiliani National Institute of Technology Karnataka Boston Columbus Hoboken Indianapolis New York San Francisco Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo Vice President and Editorial Director: Marcia J. Horton Executive Editor: Matt Goldstein Editorial Assistant: Kelsey Loanes Acquisitions Editor, Global Editions: Karthik Subramanian VP of Marketing: Christy Lesko Director of Field Marketing: Tim Galligan Product Marketing Manager: Bram van Kempen Field Marketing Manager: Demetrius Hall Marketing Assistant: Jon Bryant Director of Product Management: Erin Gregg Team Lead Product Management: Scott Disanno Program Manager: Joanne Manning Project Editor, Global Editions: K.K. Neelakantan Senior Production Manufacturing Controller, Global Editions: Trudy Kimber Procurement Manager: Mary Fischer Senior Specialist, Program Planning and Support: Maura Zaldivar-Garcia Media Production Manager, Global Editions: Vikram Kumar Cover Designer: Lumina Datamatics Manager, Rights Management: Rachel Youdelman Associate Project Manager, Rights Management: William J. Opaluch Full-Service Project Management: Paul Anagnostopoulos, Windfall Software Pearson Education Limited Edinburgh Gate Harlow Essex CM20 2JE England and Associated Companies throughout the world Visit us on the World Wide Web at: www.pearsonglobaleditions.com © Pearson Education Limited 2016 The rights of Randal E. Bryant and David R. O’Hallaron to be identiﬁed as the authors of this work have been asserted by them in accordance with the Copyright, Designs and Patents Act 1988. Authorized adaptation from the United States edition, entitled Computer Systems: A Programmer’s Perspective, 3rd edition, ISBN 978-0-13-409266-9, by Randal E. Bryant and David R. O’Hallaron published by Pearson Education © 2016. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the publisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron House, 6-10 Kirby Street, London EC1N 8TS. All trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the author or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any afﬁliation with or endorsement of this book by such owners. British Library Cataloguing-in-Publication Data A catalogue record for this book is available from the British Library 10987654321 ISBN 10: 1-292-10176-8 ISBN 13: 978-1-292-10176-7 (Print) Typeset in 10/12 Times Ten, ITC Stone Sans by Windfall Software Printed in Malaysia ISBN 13: 978-1-488-67207-1 (PDF) To the students and instructors of the 15-213 course at Carnegie Mellon University, for inspiring us to develop and reﬁne the material for this book. MasteringEngineering ® For Computer Systems: A Programmer’s Perspective, Third Edition Mastering is Pearson’s proven online Tutorial Homework program, newly available with the third edition of Computer Systems: A Programmer’s Perspective. The Mastering platform allows you to integrate dynamic homework—with many problems taken directly from the Bryant/O’Hallaron textbook—with automatic grading. Mastering allows you to easily track the performance of your entire class on an assignment-by-assignment basis, or view the detailed work of an individual student. For more information or a demonstration of the course, visit www.MasteringEngineering.com Contents Preface 19 About the Authors 35 1 A Tour of Computer Systems 37 1.1 Information Is Bits + Context 39 1.2 Programs Are Translated by Other Programs into Different Forms 40 1.3 It Pays to Understand How Compilation Systems Work 42 1.4 Processors Read and Interpret Instructions Stored in Memory 43 1.4.1 Hardware Organization of a System 44 1.4.2 Running the ³−ÄÄÇ Program 46 1.5 Caches Matter 47 1.6 Storage Devices Form a Hierarchy 50 1.7 The Operating System Manages the Hardware 50 1.7.1 Processes 51 1.7.2 Threads 53 1.7.3 Virtual Memory 54 1.7.4 Files 55 1.8 Systems Communicate with Other Systems Using Networks 55 1.9 Important Themes 58 1.9.1 Amdahl’s Law 58 1.9.2 Concurrency and Parallelism 60 1.9.3 The Importance of Abstractions in Computer Systems 62 1.10 Summary 63 Bibliographic Notes 64 Solutions to Practice Problems 64 Part I Program Structure and Execution 2 Representing and Manipulating Information 67 2.1 Information Storage 70 2.1.1 Hexadecimal Notation 72 2.1.2 Data Sizes 75 7 8 Contents 2.1.3 Addressing and Byte Ordering 78 2.1.4 Representing Strings 85 2.1.5 Representing Code 85 2.1.6 Introduction to Boolean Algebra 86 2.1.7 Bit-Level Operations in C 90 2.1.8 Logical Operations in C 92 2.1.9 Shift Operations in C 93 2.2 Integer Representations 95 2.2.1 Integral Data Types 96 2.2.2 Unsigned Encodings 98 2.2.3 Two’s-Complement Encodings 100 2.2.4 Conversions between Signed and Unsigned 106 2.2.5 Signed versus Unsigned in C 110 2.2.6 Expanding the Bit Representation of a Number 112 2.2.7 Truncating Numbers 117 2.2.8 Advice on Signed versus Unsigned 119 2.3 Integer Arithmetic 120 2.3.1 Unsigned Addition 120 2.3.2 Two’s-Complement Addition 126 2.3.3 Two’s-Complement Negation 131 2.3.4 Unsigned Multiplication 132 2.3.5 Two’s-Complement Multiplication 133 2.3.6 Multiplying by Constants 137 2.3.7 Dividing by Powers of 2 139 2.3.8 Final Thoughts on Integer Arithmetic 143 2.4 Floating Point 144 2.4.1 Fractional Binary Numbers 145 2.4.2 IEEE Floating-Point Representation 148 2.4.3 Example Numbers 151 2.4.4 Rounding 156 2.4.5 Floating-Point Operations 158 2.4.6 Floating Point in C 160 2.5 Summary 162 Bibliographic Notes 163 Homework Problems 164 Solutions to Practice Problems 179 3 Machine-Level Representation of Programs 199 3.1 A Historical Perspective 202 Contents 9 3.2 Program Encodings 205 3.2.1 Machine-Level Code 206 3.2.2 Code Examples 208 3.2.3 Notes on Formatting 211 3.3 Data Formats 213 3.4 Accessing Information 215 3.4.1 Operand Speciﬁers 216 3.4.2 Data Movement Instructions 218 3.4.3 Data Movement Example 222 3.4.4 Pushing and Popping Stack Data 225 3.5 Arithmetic and Logical Operations 227 3.5.1 Load Effective Address 227 3.5.2 Unary and Binary Operations 230 3.5.3 Shift Operations 230 3.5.4 Discussion 232 3.5.5 Special Arithmetic Operations 233 3.6 Control 236 3.6.1 Condition Codes 237 3.6.2 Accessing the Condition Codes 238 3.6.3 Jump Instructions 241 3.6.4 Jump Instruction Encodings 243 3.6.5 Implementing Conditional Branches with Conditional Control 245 3.6.6 Implementing Conditional Branches with Conditional Moves 250 3.6.7 Loops 256 3.6.8 Switch Statements 268 3.7 Procedures 274 3.7.1 The Run-Time Stack 275 3.7.2 Control Transfer 277 3.7.3 Data Transfer 281 3.7.4 Local Storage on the Stack 284 3.7.5 Local Storage in Registers 287 3.7.6 Recursive Procedures 289 3.8 Array Allocation and Access 291 3.8.1 Basic Principles 291 3.8.2 Pointer Arithmetic 293 3.8.3 Nested Arrays 294 3.8.4 Fixed-Size Arrays 296 3.8.5 Variable-Size Arrays 298 10 Contents 3.9 Heterogeneous Data Structures 301 3.9.1 Structures 301 3.9.2 Unions 305 3.9.3 Data Alignment 309 3.10 Combining Control and Data in Machine-Level Programs 312 3.10.1 Understanding Pointers 313 3.10.2 Life in the Real World: Using the gdb Debugger 315 3.10.3 Out-of-Bounds Memory References and Buffer Overﬂow 315 3.10.4 Thwarting Buffer Overﬂow Attacks 320 3.10.5 Supporting Variable-Size Stack Frames 326 3.11 Floating-Point Code 329 3.11.1 Floating-Point Movement and Conversion Operations 332 3.11.2 Floating-Point Code in Procedures 337 3.11.3 Floating-Point Arithmetic Operations 338 3.11.4 Deﬁning and Using Floating-Point Constants 340 3.11.5 Using Bitwise Operations in Floating-Point Code 341 3.11.6 Floating-Point Comparison Operations 342 3.11.7 Observations about Floating-Point Code 345 3.12 Summary 345 Bibliographic Notes 346 Homework Problems 347 Solutions to Practice Problems 361 4 Processor Architecture 387 4.1 The Y86-64 Instruction Set Architecture 391 4.1.1 Programmer-Visible State 391 4.1.2 Y86-64 Instructions 392 4.1.3 Instruction Encoding 394 4.1.4 Y86-64 Exceptions 399 4.1.5 Y86-64 Programs 400 4.1.6 Some Y86-64 Instruction Details 406 4.2 Logic Design and the Hardware Control Language HCL 408 4.2.1 Logic Gates 409 4.2.2 Combinational Circuits and HCL Boolean Expressions 410 4.2.3 Word-Level Combinational Circuits and HCL Integer Expressions 412 4.2.4 Set Membership 416 4.2.5 Memory and Clocking 417 4.3 Sequential Y86-64 Implementations 420 4.3.1 Organizing Processing into Stages 420 Contents 11 4.3.2 SEQ Hardware Structure 432 4.3.3 SEQ Timing 436 4.3.4 SEQ Stage Implementations 440 4.4 General Principles of Pipelining 448 4.4.1 Computational Pipelines 448 4.4.2 A Detailed Look at Pipeline Operation 450 4.4.3 Limitations of Pipelining 452 4.4.4 Pipelining a System with Feedback 455 4.5 Pipelined Y86-64 Implementations 457 4.5.1 SEQ+: Rearranging the Computation Stages 457 4.5.2 Inserting Pipeline Registers 458 4.5.3 Rearranging and Relabeling Signals 462 4.5.4 Next PC Prediction 463 4.5.5 Pipeline Hazards 465 4.5.6 Exception Handling 480 4.5.7 PIPE Stage Implementations 483 4.5.8 Pipeline Control Logic 491 4.5.9 Performance Analysis 500 4.5.10 Unﬁnished Business 504 4.6 Summary 506 4.6.1 Y86-64 Simulators 508 Bibliographic Notes 509 Homework Problems 509 Solutions to Practice Problems 516 5 Optimizing Program Performance 531 5.1 Capabilities and Limitations of Optimizing Compilers 534 5.2 Expressing Program Performance 538 5.3 Program Example 540 5.4 Eliminating Loop Inefﬁciencies 544 5.5 Reducing Procedure Calls 548 5.6 Eliminating Unneeded Memory References 550 5.7 Understanding Modern Processors 553 5.7.1 Overall Operation 554 5.7.2 Functional Unit Performance 559 5.7.3 An Abstract Model of Processor Operation 561 5.8 Loop Unrolling 567 5.9 Enhancing Parallelism 572 5.9.1 Multiple Accumulators 572 5.9.2 Reassociation Transformation 577 12 Contents 5.10 Summary of Results for Optimizing Combining Code 583 5.11 Some Limiting Factors 584 5.11.1 Register Spilling 584 5.11.2 Branch Prediction and Misprediction Penalties 585 5.12 Understanding Memory Performance 589 5.12.1 Load Performance 590 5.12.2 Store Performance 591 5.13 Life in the Real World: Performance Improvement Techniques 597 5.14 Identifying and Eliminating Performance Bottlenecks 598 5.14.1 Program Proﬁling 598 5.14.2 Using a Proﬁler to Guide Optimization 601 5.15 Summary 604 Bibliographic Notes 605 Homework Problems 606 Solutions to Practice Problems 609 6 The Memory Hierarchy 615 6.1 Storage Technologies 617 6.1.1 Random Access Memory 617 6.1.2 Disk Storage 625 6.1.3 Solid State Disks 636 6.1.4 Storage Technology Trends 638 6.2 Locality 640 6.2.1 Locality of References to Program Data 642 6.2.2 Locality of Instruction Fetches 643 6.2.3 Summary of Locality 644 6.3 The Memory Hierarchy 645 6.3.1 Caching in the Memory Hierarchy 646 6.3.2 Summary of Memory Hierarchy Concepts 650 6.4 Cache Memories 650 6.4.1 Generic Cache Memory Organization 651 6.4.2 Direct-Mapped Caches 653 6.4.3 Set Associative Caches 660 6.4.4 Fully Associative Caches 662 6.4.5 Issues with Writes 666 6.4.6 Anatomy of a Real Cache Hierarchy 667 6.4.7 Performance Impact of Cache Parameters 667 6.5 Writing Cache-Friendly Code 669 6.6 Putting It Together: The Impact of Caches on Program Performance 675 Contents 13 6.6.1 The Memory Mountain 675 6.6.2 Rearranging Loops to Increase Spatial Locality 679 6.6.3 Exploiting Locality in Your Programs 683 6.7 Summary 684 Bibliographic Notes 684 Homework Problems 685 Solutions to Practice Problems 696 Part II Running Programs on a System 7 Linking 705 7.1 Compiler Drivers 707 7.2 Static Linking 708 7.3 Object Files 709 7.4 Relocatable Object Files 710 7.5 Symbols and Symbol Tables 711 7.6 Symbol Resolution 715 7.6.1 How Linkers Resolve Duplicate Symbol Names 716 7.6.2 Linking with Static Libraries 720 7.6.3 How Linkers Use Static Libraries to Resolve References 724 7.7 Relocation 725 7.7.1 Relocation Entries 726 7.7.2 Relocating Symbol References 727 7.8 Executable Object Files 731 7.9 Loading Executable Object Files 733 7.10 Dynamic Linking with Shared Libraries 734 7.11 Loading and Linking Shared Libraries from Applications 737 7.12 Position-Independent Code (PIC) 740 7.13 Library Interpositioning 743 7.13.1 Compile-Time Interpositioning 744 7.13.2 Link-Time Interpositioning 744 7.13.3 Run-Time Interpositioning 746 7.14 Tools for Manipulating Object Files 749 7.15 Summary 749 Bibliographic Notes 750 Homework Problems 750 Solutions to Practice Problems 753 14 Contents 8 Exceptional Control Flow 757 8.1 Exceptions 759 8.1.1 Exception Handling 760 8.1.2 Classes of Exceptions 762 8.1.3 Exceptions in Linux/x86-64 Systems 765 8.2 Processes 768 8.2.1 Logical Control Flow 768 8.2.2 Concurrent Flows 769 8.2.3 Private Address Space 770 8.2.4 User and Kernel Modes 770 8.2.5 Context Switches 772 8.3 System Call Error Handling 773 8.4 Process Control 774 8.4.1 Obtaining Process IDs 775 8.4.2 Creating and Terminating Processes 775 8.4.3 Reaping Child Processes 779 8.4.4 Putting Processes to Sleep 785 8.4.5 Loading and Running Programs 786 8.4.6 Using ðÇËÂ and −Ó−²Ì− to Run Programs 789 8.5 Signals 792 8.5.1 Signal Terminology 794 8.5.2 Sending Signals 795 8.5.3 Receiving Signals 798 8.5.4 Blocking and Unblocking Signals 800 8.5.5 Writing Signal Handlers 802 8.5.6 Synchronizing Flows to Avoid Nasty Concurrency Bugs 812 8.5.7 Explicitly Waiting for Signals 814 8.6 Nonlocal Jumps 817 8.7 Tools for Manipulating Processes 822 8.8 Summary 823 Bibliographic Notes 823 Homework Problems 824 Solutions to Practice Problems 831 9 Virtual Memory 837 9.1 Physical and Virtual Addressing 839 9.2 Address Spaces 840 Contents 15 9.3 VM as a Tool for Caching 841 9.3.1 DRAM Cache Organization 842 9.3.2 Page Tables 842 9.3.3 Page Hits 844 9.3.4 Page Faults 844 9.3.5 Allocating Pages 846 9.3.6 Locality to the Rescue Again 846 9.4 VM as a Tool for Memory Management 847 9.5 VM as a Tool for Memory Protection 848 9.6 Address Translation 849 9.6.1 Integrating Caches and VM 853 9.6.2 Speeding Up Address Translation with a TLB 853 9.6.3 Multi-Level Page Tables 855 9.6.4 Putting It Together: End-to-End Address Translation 857 9.7 Case Study: The Intel Core i7/Linux Memory System 861 9.7.1 Core i7 Address Translation 862 9.7.2 Linux Virtual Memory System 864 9.8 Memory Mapping 869 9.8.1 Shared Objects Revisited 869 9.8.2 The ðÇËÂ Function Revisited 872 9.8.3 The −Ó−²Ì− Function Revisited 872 9.8.4 User-Level Memory Mapping with the ÀÀþÉ Function 873 9.9 Dynamic Memory Allocation 875 9.9.1 The ÀþÄÄÇ² and ðË−− Functions 876 9.9.2 Why Dynamic Memory Allocation? 879 9.9.3 Allocator Requirements and Goals 880 9.9.4 Fragmentation 882 9.9.5 Implementation Issues 882 9.9.6 Implicit Free Lists 883 9.9.7 Placing Allocated Blocks 885 9.9.8 Splitting Free Blocks 885 9.9.9 Getting Additional Heap Memory 886 9.9.10 Coalescing Free Blocks 886 9.9.11 Coalescing with Boundary Tags 887 9.9.12 Putting It Together: Implementing a Simple Allocator 890 9.9.13 Explicit Free Lists 898 9.9.14 Segregated Free Lists 899 9.10 Garbage Collection 901 9.10.1 Garbage Collector Basics 902 9.10.2 Mark&Sweep Garbage Collectors 903 9.10.3 Conservative Mark&Sweep for C Programs 905 16 Contents 9.11 Common Memory-Related Bugs in C Programs 906 9.11.1 Dereferencing Bad Pointers 906 9.11.2 Reading Uninitialized Memory 907 9.11.3 Allowing Stack Buffer Overﬂows 907 9.11.4 Assuming That Pointers and the Objects They Point to Are the Same Size 908 9.11.5 Making Off-by-One Errors 908 9.11.6 Referencing a Pointer Instead of the Object It Points To 909 9.11.7 Misunderstanding Pointer Arithmetic 909 9.11.8 Referencing Nonexistent Variables 910 9.11.9 Referencing Data in Free Heap Blocks 910 9.11.10 Introducing Memory Leaks 911 9.12 Summary 911 Bibliographic Notes 912 Homework Problems 912 Solutions to Practice Problems 916 Part III Interaction and Communication between Programs 10 System-Level I/O 925 10.1 Unix I/O 926 10.2 Files 927 10.3 Opening and Closing Files 929 10.4 Reading and Writing Files 931 10.5 Robust Reading and Writing with the Rio Package 933 10.5.1 Rio Unbuffered Input and Output Functions 933 10.5.2 Rio Buffered Input Functions 934 10.6 Reading File Metadata 939 10.7 Reading Directory Contents 941 10.8 Sharing Files 942 10.9 I/O Redirection 945 10.10 Standard I/O 947 10.11 Putting It Together: Which I/O Functions Should I Use? 947 10.12 Summary 949 Bibliographic Notes 950 Homework Problems 950 Solutions to Practice Problems 951 Contents 17 11 Network Programming 953 11.1 The Client-Server Programming Model 954 11.2 Networks 955 11.3 The Global IP Internet 960 11.3.1 IP Addresses 961 11.3.2 Internet Domain Names 963 11.3.3 Internet Connections 965 11.4 The Sockets Interface 968 11.4.1 Socket Address Structures 969 11.4.2 The ÍÇ²Â−Î Function 970 11.4.3 The ²ÇÅÅ−²Î Function 970 11.4.4 The ¾©Å® Function 971 11.4.5 The Ä©ÍÎ−Å Function 971 11.4.6 The þ²²−ÉÎ Function 972 11.4.7 Host and Service Conversion 973 11.4.8 Helper Functions for the Sockets Interface 978 11.4.9 Example Echo Client and Server 980 11.5 Web Servers 984 11.5.1 Web Basics 984 11.5.2 Web Content 985 11.5.3 HTTP Transactions 986 11.5.4 Serving Dynamic Content 989 11.6 Putting It Together: The Tiny Web Server 992 11.7 Summary 1000 Bibliographic Notes 1001 Homework Problems 1001 Solutions to Practice Problems 1002 12 Concurrent Programming 1007 12.1 Concurrent Programming with Processes 1009 12.1.1 A Concurrent Server Based on Processes 1010 12.1.2 Pros and Cons of Processes 1011 12.2 Concurrent Programming with I/O Multiplexing 1013 12.2.1 A Concurrent Event-Driven Server Based on I/O Multiplexing 1016 12.2.2 Pros and Cons of I/O Multiplexing 1021 12.3 Concurrent Programming with Threads 1021 12.3.1 Thread Execution Model 1022 18 Contents 12.3.2 Posix Threads 1023 12.3.3 Creating Threads 1024 12.3.4 Terminating Threads 1024 12.3.5 Reaping Terminated Threads 1025 12.3.6 Detaching Threads 1025 12.3.7 Initializing Threads 1026 12.3.8 A Concurrent Server Based on Threads 1027 12.4 Shared Variables in Threaded Programs 1028 12.4.1 Threads Memory Model 1029 12.4.2 Mapping Variables to Memory 1030 12.4.3 Shared Variables 1031 12.5 Synchronizing Threads with Semaphores 1031 12.5.1 Progress Graphs 1035 12.5.2 Semaphores 1037 12.5.3 Using Semaphores for Mutual Exclusion 1038 12.5.4 Using Semaphores to Schedule Shared Resources 1040 12.5.5 Putting It Together: A Concurrent Server Based on Prethreading 1044 12.6 Using Threads for Parallelism 1049 12.7 Other Concurrency Issues 1056 12.7.1 Thread Safety 1056 12.7.2 Reentrancy 1059 12.7.3 Using Existing Library Functions in Threaded Programs 1060 12.7.4 Races 1061 12.7.5 Deadlocks 1063 12.8 Summary 1066 Bibliographic Notes 1066 Homework Problems 1067 Solutions to Practice Problems 1072 A Error Handling 1077 A.1 Error Handling in Unix Systems 1078 A.2 Error-Handling Wrappers 1079 References 1083 Index 1089 Preface This book (known as CS:APP) is for computer scientists, computer engineers, and others who want to be able to write better programs by learning what is going on “under the hood” of a computer system. Our aim is to explain the enduring concepts underlying all computer systems, and to show you the concrete ways that these ideas affect the correctness, perfor- mance, and utility of your application programs. Many systems books are written from a builder’s perspective, describing how to implement the hardware or the sys- tems software, including the operating system, compiler, and network interface. This book is written from a programmer’s perspective, describing how application programmers can use their knowledge of a system to write better programs. Of course, learning what a system is supposed to do provides a good ﬁrst step in learn- ing how to build one, so this book also serves as a valuable introduction to those who go on to implement systems hardware and software. Most systems books also tend to focus on just one aspect of the system, for example, the hardware archi- tecture, the operating system, the compiler, or the network. This book spans all of these aspects, with the unifying theme of a programmer’s perspective. If you study and learn the concepts in this book, you will be on your way to becoming the rare power programmer who knows how things work and how to ﬁx them when they break. You will be able to write programs that make better use of the capabilities provided by the operating system and systems software, that operate correctly across a wide range of operating conditions and run-time parameters, that run faster, and that avoid the ﬂaws that make programs vulner- able to cyberattack. You will be prepared to delve deeper into advanced topics such as compilers, computer architecture, operating systems, embedded systems, networking, and cybersecurity. Assumptions about the Reader’s Background This book focuses on systems that execute x86-64 machine code. x86-64 is the latest in an evolutionary path followed by Intel and its competitors that started with the 8086 microprocessor in 1978. Due to the naming conventions used by Intel for its microprocessor line, this class of microprocessors is referred to colloquially as “x86.” As semiconductor technology has evolved to allow more transistors to be integrated onto a single chip, these processors have progressed greatly in their computing power and their memory capacity. As part of this progression, they have gone from operating on 16-bit words, to 32-bit words with the introduction of IA32 processors, and most recently to 64-bit words with x86-64. We consider how these machines execute C programs on Linux. Linux is one of a number of operating systems having their heritage in the Unix operating system developed originally by Bell Laboratories. Other members of this class 19 20 Preface New to C? Advice on the C programming language To help readers whose background in C programming is weak (or nonexistent), we have also included these special notes to highlight features that are especially important in C. We assume you are familiar with C++ or Java. of operating systems include Solaris, FreeBSD, and MacOS X. In recent years, these operating systems have maintained a high level of compatibility through the efforts of the Posix and Standard Unix Speciﬁcation standardization efforts. Thus, the material in this book applies almost directly to these “Unix-like” operating systems. The text contains numerous programming examples that have been compiled and run on Linux systems. We assume that you have access to such a machine, and are able to log in and do simple things such as listing ﬁles and changing directo- ries. If your computer runs Microsoft Windows, we recommend that you install one of the many different virtual machine environments (such as VirtualBox or VMWare) that allow programs written for one operating system (the guest OS) to run under another (the host OS). We also assume that you have some familiarity with C or C++. If your only prior experience is with Java, the transition will require more effort on your part, but we will help you. Java and C share similar syntax and control statements. However, there are aspects of C (particularly pointers, explicit dynamic memory allocation, and formatted I/O) that do not exist in Java. Fortunately, C is a small language, and it is clearly and beautifully described in the classic “K&R” text by Brian Kernighan and Dennis Ritchie [61]. Regardless of your programming background, consider K&R an essential part of your personal systems library. If your prior experience is with an interpreted language, such as Python, Ruby, or Perl, you will deﬁnitely want to devote some time to learning C before you attempt to use this book. Several of the early chapters in the book explore the interactions between C programs and their machine-language counterparts. The machine-language exam- ples were all generated by the GNU gcc compiler running on x86-64 processors. We do not assume any prior experience with hardware, machine language, or assembly-language programming. How to Read the Book Learning how computer systems work from a programmer’s perspective is great fun, mainly because you can do it actively. Whenever you learn something new, you can try it out right away and see the result ﬁrsthand. In fact, we believe that the only way to learn systems is to do systems, either working concrete problems or writing and running programs on real systems. This theme pervades the entire book. When a new concept is introduced, it is followed in the text by one or more practice problems that you should work Preface 21 code/intro/hello.c 1 a©Å²ÄÏ®− zÍÎ®©Çl³| 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ÉË©ÅÎðf‘³−ÄÄÇj ÑÇËÄ®¿Å‘gy 6 Ë−ÎÏËÅ ny 7 Û code/intro/hello.c Figure 1 A typical code example. immediately to test your understanding. Solutions to the practice problems are at the end of each chapter. As you read, try to solve each problem on your own and then check the solution to make sure you are on the right track. Each chapter is followed by a set of homework problems of varying difﬁculty. Your instructor has the solutions to the homework problems in an instructor’s manual. For each homework problem, we show a rating of the amount of effort we feel it will require: ◆ Should require just a few minutes. Little or no programming required. ◆◆ Might require up to 20 minutes. Often involves writing and testing some code. (Many of these are derived from problems we have given on exams.) ◆◆◆ Requires a signiﬁcant effort, perhaps 1–2 hours. Generally involves writ- ing and testing a signiﬁcant amount of code. ◆◆◆◆ A lab assignment, requiring up to 10 hours of effort. Each code example in the text was formatted directly, without any manual intervention, from a C program compiled with gcc and tested on a Linux system. Of course, your system may have a different version of gcc, or a different compiler altogether, so your compiler might generate different machine code; but the overall behavior should be the same. All of the source code is available from the CS:APP Web page (“CS:APP” being our shorthand for the book’s title) at csapp .cs.cmu.edu. In the text, the ﬁlenames of the source programs are documented in horizontal bars that surround the formatted code. For example, the program in Figure 1 can be found in the ﬁle ³−ÄÄÇl² in directory ²Ç®−m©ÅÎËÇm. We encourage you to try running the example programs on your system as you encounter them. To avoid having a book that is overwhelming, both in bulk and in content, we have created a number of Web asides containing material that supplements the main presentation of the book. These asides are referenced within the book with a notation of the form chap:top, where chap is a short encoding of the chapter sub- ject, and top is a short code for the topic that is covered. For example, Web Aside data:bool contains supplementary material on Boolean algebra for the presenta- tion on data representations in Chapter 2, while Web Aside arch:vlog contains 22 Preface material describing processor designs using the Verilog hardware description lan- guage, supplementing the presentation of processor design in Chapter 4. All of these Web asides are available from the CS:APP Web page. Book Overview The CS:APP book consists of 12 chapters designed to capture the core ideas in computer systems. Here is an overview. Chapter 1: A Tour of Computer Systems. This chapter introduces the major ideas and themes in computer systems by tracing the life cycle of a simple “hello, world” program. Chapter 2: Representing and Manipulating Information. We cover computer arith- metic, emphasizing the properties of unsigned and two’s-complement num- ber representations that affect programmers. We consider how numbers are represented and therefore what range of values can be encoded for a given word size. We consider the effect of casting between signed and unsigned numbers. We cover the mathematical properties of arithmetic op- erations. Novice programmers are often surprised to learn that the (two’s- complement) sum or product of two positive numbers can be negative. On the other hand, two’s-complement arithmetic satisﬁes many of the algebraic properties of integer arithmetic, and hence a compiler can safely transform multiplication by a constant into a sequence of shifts and adds. We use the bit-level operations of C to demonstrate the principles and applications of Boolean algebra. We cover the IEEE ﬂoating-point format in terms of how it represents values and the mathematical properties of ﬂoating-point oper- ations. Having a solid understanding of computer arithmetic is critical to writ- ing reliable programs. For example, programmers and compilers cannot re- place the expression fÓzÔg with fÓkÔ z ng, due to the possibility of overﬂow. They cannot even replace it with the expression fkÔ z kÓg, due to the asym- metric range of negative and positive numbers in the two’s-complement representation. Arithmetic overﬂow is a common source of programming errors and security vulnerabilities, yet few other books cover the properties of computer arithmetic from a programmer’s perspective. Chapter 3: Machine-Level Representation of Programs. We teach you how to read the x86-64 machine code generated by a C compiler. We cover the ba- sic instruction patterns generated for different control constructs, such as conditionals, loops, and ÍÑ©Î²³ statements. We cover the implementation of procedures, including stack allocation, register usage conventions, and parameter passing. We cover the way different data structures such as struc- tures, unions, and arrays are allocated and accessed. We cover the instruc- tions that implement both integer and ﬂoating-point arithmetic. We also use the machine-level view of programs as a way to understand common code security vulnerabilities, such as buffer overﬂow, and steps that the pro- Preface 23 Aside What is an aside? You will encounter asides of this form throughout the text. Asides are parenthetical remarks that give you some additional insight into the current topic. Asides serve a number of purposes. Some are little history lessons. For example, where did C, Linux, and the Internet come from? Other asides are meant to clarify ideas that students often ﬁnd confusing. For example, what is the difference between a cache line, set, and block? Other asides give real-world examples, such as how a ﬂoating-point error crashed a French rocket or the geometric and operational parameters of a commercial disk drive. Finally, some asides are just fun stuff. For example, what is a “hoinky”? grammer, the compiler, and the operating system can take to reduce these threats. Learning the concepts in this chapter helps you become a better programmer, because you will understand how programs are represented on a machine. One certain beneﬁt is that you will develop a thorough and concrete understanding of pointers. Chapter 4: Processor Architecture. This chapter covers basic combinational and sequential logic elements, and then shows how these elements can be com- bined in a datapath that executes a simpliﬁed subset of the x86-64 instruction set called “Y86-64.” We begin with the design of a single-cycle datapath. This design is conceptually very simple, but it would not be very fast. We then introduce pipelining, where the different steps required to process an instruction are implemented as separate stages. At any given time, each stage can work on a different instruction. Our ﬁve-stage processor pipeline is much more realistic. The control logic for the processor designs is described using a simple hardware description language called HCL. Hardware de- signs written in HCL can be compiled and linked into simulators provided with the textbook, and they can be used to generate Verilog descriptions suitable for synthesis into working hardware. Chapter 5: Optimizing Program Performance. This chapter introduces a number of techniques for improving code performance, with the idea being that pro- grammers learn to write their C code in such a way that a compiler can then generate efﬁcient machine code. We start with transformations that reduce the work to be done by a program and hence should be standard practice when writing any program for any machine. We then progress to trans- formations that enhance the degree of instruction-level parallelism in the generated machine code, thereby improving their performance on modern “superscalar” processors. To motivate these transformations, we introduce a simple operational model of how modern out-of-order processors work, and show how to measure the potential performance of a program in terms of the critical paths through a graphical representation of a program. You will be surprised how much you can speed up a program by simple transfor- mations of the C code. 24 Preface Chapter 6: The Memory Hierarchy. The memory system is one of the most visible parts of a computer system to application programmers. To this point, you have relied on a conceptual model of the memory system as a linear array with uniform access times. In practice, a memory system is a hierarchy of storage devices with different capacities, costs, and access times. We cover the different types of RAM and ROM memories and the geometry and organization of magnetic-disk and solid state drives. We describe how these storage devices are arranged in a hierarchy. We show how this hierarchy is made possible by locality of reference. We make these ideas concrete by introducing a unique view of a memory system as a “memory mountain” with ridges of temporal locality and slopes of spatial locality. Finally, we show you how to improve the performance of application programs by improving their temporal and spatial locality. Chapter 7: Linking. This chapter covers both static and dynamic linking, including the ideas of relocatable and executable object ﬁles, symbol resolution, re- location, static libraries, shared object libraries, position-independent code, and library interpositioning. Linking is not covered in most systems texts, but we cover it for two reasons. First, some of the most confusing errors that programmers can encounter are related to glitches during linking, especially for large software packages. Second, the object ﬁles produced by linkers are tied to concepts such as loading, virtual memory, and memory mapping. Chapter 8: Exceptional Control Flow. In this part of the presentation, we step beyond the single-program model by introducing the general concept of exceptional control ﬂow (i.e., changes in control ﬂow that are outside the normal branches and procedure calls). We cover examples of exceptional control ﬂow that exist at all levels of the system, from low-level hardware ex- ceptions and interrupts, to context switches between concurrent processes, to abrupt changes in control ﬂow caused by the receipt of Linux signals, to the nonlocal jumps in C that break the stack discipline. This is the part of the book where we introduce the fundamental idea of a process, an abstraction of an executing program. You will learn how processes work and how they can be created and manipulated from appli- cation programs. We show how application programmers can make use of multiple processes via Linux system calls. When you ﬁnish this chapter, you will be able to write a simple Linux shell with job control. It is also your ﬁrst introduction to the nondeterministic behavior that arises with concurrent program execution. Chapter 9: Virtual Memory. Our presentation of the virtual memory system seeks to give some understanding of how it works and its characteristics. We want you to know how it is that the different simultaneous processes can each use an identical range of addresses, sharing some pages but having individual copies of others. We also cover issues involved in managing and manip- ulating virtual memory. In particular, we cover the operation of storage allocators such as the standard-library ÀþÄÄÇ² and ðË−− operations. Cov- Preface 25 ering this material serves several purposes. It reinforces the concept that the virtual memory space is just an array of bytes that the program can subdivide into different storage units. It helps you understand the effects of programs containing memory referencing errors such as storage leaks and invalid pointer references. Finally, many application programmers write their own storage allocators optimized toward the needs and characteris- tics of the application. This chapter, more than any other, demonstrates the beneﬁt of covering both the hardware and the software aspects of computer systems in a uniﬁed way. Traditional computer architecture and operating systems texts present only part of the virtual memory story. Chapter 10: System-Level I/O. We cover the basic concepts of Unix I/O such as ﬁles and descriptors. We describe how ﬁles are shared, how I/O redirection works, and how to access ﬁle metadata. We also develop a robust buffered I/O package that deals correctly with a curious behavior known as short counts, where the library function reads only part of the input data. We cover the C standard I/O library and its relationship to Linux I/O, focusing on limitations of standard I/O that make it unsuitable for network program- ming. In general, the topics covered in this chapter are building blocks for the next two chapters on network and concurrent programming. Chapter 11: Network Programming. Networks are interesting I/O devices to pro- gram, tying together many of the ideas that we study earlier in the text, such as processes, signals, byte ordering, memory mapping, and dynamic storage allocation. Network programs also provide a compelling context for con- currency, which is the topic of the next chapter. This chapter is a thin slice through network programming that gets you to the point where you can write a simple Web server. We cover the client-server model that underlies all network applications. We present a programmer’s view of the Internet and show how to write Internet clients and servers using the sockets inter- face. Finally, we introduce HTTP and develop a simple iterative Web server. Chapter 12: Concurrent Programming. This chapter introduces concurrent pro- gramming using Internet server design as the running motivational example. We compare and contrast the three basic mechanisms for writing concur- rent programs—processes, I/O multiplexing, and threads—and show how to use them to build concurrent Internet servers. We cover basic principles of synchronization using P and V semaphore operations, thread safety and reentrancy, race conditions, and deadlocks. Writing concurrent code is es- sential for most server applications. We also describe the use of thread-level programming to express parallelism in an application program, enabling faster execution on multi-core processors. Getting all of the cores working on a single computational problem requires a careful coordination of the concurrent threads, both for correctness and to achieve high performance. 26 Preface New to This Edition The ﬁrst edition of this book was published with a copyright of 2003, while the second had a copyright of 2011. Considering the rapid evolution of computer technology, the book content has held up surprisingly well. Intel x86 machines running C programs under Linux (and related operating systems) has proved to be a combination that continues to encompass many systems today. However, changes in hardware technology, compilers, program library interfaces, and the experience of many instructors teaching the material have prompted a substantial revision. The biggest overall change from the second edition is that we have switched our presentation from one based on a mix of IA32 and x86-64 to one based exclusively on x86-64. This shift in focus affected the contents of many of the chapters. Here is a summary of the signiﬁcant changes. Chapter 1: A Tour of Computer Systems We have moved the discussion of Am- dahl’s Law from Chapter 5 into this chapter. Chapter 2: Representing and Manipulating Information. A consistent bit of feed- back from readers and reviewers is that some of the material in this chapter can be a bit overwhelming. So we have tried to make the material more ac- cessible by clarifying the points at which we delve into a more mathematical style of presentation. This enables readers to ﬁrst skim over mathematical details to get a high-level overview and then return for a more thorough reading. Chapter 3: Machine-Level Representation of Programs. We have converted from the earlier presentation based on a mix of IA32 and x86-64 to one based entirely on x86-64. We have also updated for the style of code generated by more recent versions of gcc. The result is a substantial rewriting, including changing the order in which some of the concepts are presented. We also have included, for the ﬁrst time, a presentation of the machine-level support for programs operating on ﬂoating-point data. We have created a Web aside describing IA32 machine code for legacy reasons. Chapter 4: Processor Architecture. We have revised the earlier processor design, based on a 32-bit architecture, to one that supports 64-bit words and oper- ations. Chapter 5: Optimizing Program Performance. We have updated the material to reﬂect the performance capabilities of recent generations of x86-64 proces- sors. With the introduction of more functional units and more sophisticated control logic, the model of program performance we developed based on a data-ﬂow representation of programs has become a more reliable predictor of performance than it was before. Chapter 6: The Memory Hierarchy. We have updated the material to reﬂect more recent technology. Preface 27 Chapter 7: Linking. We have rewritten this chapter for x86-64, expanded the discussion of using the GOT and PLT to create position-independent code, and added a new section on a powerful linking technique known as library interpositioning. Chapter 8: Exceptional Control Flow. We have added a more rigorous treatment of signal handlers, including async-signal-safe functions, speciﬁc guidelines for writing signal handlers, and using Í©×ÍÏÍÉ−Å® to wait for handlers. Chapter 9: Virtual Memory. This chapter has changed only slightly. Chapter 10: System-Level I/O. We have added a new section on ﬁles and the ﬁle hierarchy, but otherwise, this chapter has changed only slightly. Chapter 11: Network Programming. We have introduced techniques for protocol- independent and thread-safe network programming using the modern ×−Îþ®®Ë©ÅðÇ and ×−ÎÅþÀ−©ÅðÇ functions, which replace the obsolete and non-reentrant ×−Î³ÇÍÎ¾ÔÅþÀ− and ×−Î³ÇÍÎ¾Ôþ®®Ë functions. Chapter 12: Concurrent Programming. We have increased our coverage of using thread-level parallelism to make programs run faster on multi-core ma- chines. In addition, we have added and revised a number of practice and homework problems throughout the text. Origins of the Book This book stems from an introductory course that we developed at Carnegie Mel- lon University in the fall of 1998, called 15-213: Introduction to Computer Systems (ICS) [14]. The ICS course has been taught every semester since then. Over 400 students take the course each semester. The students range from sophomores to graduate students in a wide variety of majors. It is a required core course for all undergraduates in the CS and ECE departments at Carnegie Mellon, and it has become a prerequisite for most upper-level systems courses in CS and ECE. The idea with ICS was to introduce students to computers in a different way. Few of our students would have the opportunity to build a computer system. On the other hand, most students, including all computer scientists and computer engineers, would be required to use and program computers on a daily basis. So we decided to teach about systems from the point of view of the programmer, using the following ﬁlter: we would cover a topic only if it affected the performance, correctness, or utility of user-level C programs. For example, topics such as hardware adder and bus designs were out. Top- ics such as machine language were in; but instead of focusing on how to write assembly language by hand, we would look at how a C compiler translates C con- structs into machine code, including pointers, loops, procedure calls, and switch statements. Further, we would take a broader and more holistic view of the system as both hardware and systems software, covering such topics as linking, loading, 28 Preface processes, signals, performance optimization, virtual memory, I/O, and network and concurrent programming. This approach allowed us to teach the ICS course in a way that is practical, concrete, hands-on, and exciting for the students. The response from our students and faculty colleagues was immediate and overwhelmingly positive, and we real- ized that others outside of CMU might beneﬁt from using our approach. Hence this book, which we developed from the ICS lecture notes, and which we have now revised to reﬂect changes in technology and in how computer systems are implemented. Via the multiple editions and multiple translations of this book, ICS and many variants have become part of the computer science and computer engineering curricula at hundreds of colleges and universities worldwide. For Instructors: Courses Based on the Book Instructors can use the CS:APP book to teach a number of different types of systems courses. Five categories of these courses are illustrated in Figure 2. The particular course depends on curriculum requirements, personal taste, and the backgrounds and abilities of the students. From left to right in the ﬁgure, the courses are characterized by an increasing emphasis on the programmer’s perspective of a system. Here is a brief description. ORG. A computer organization course with traditional topics covered in an un- traditional style. Traditional topics such as logic design, processor architec- ture, assembly language, and memory systems are covered. However, there is more emphasis on the impact for the programmer. For example, data rep- resentations are related back to the data types and operations of C programs, and the presentation on assembly code is based on machine code generated by a C compiler rather than handwritten assembly code. ORG+. The ORG course with additional emphasis on the impact of hardware on the performance of application programs. Compared to ORG, students learn more about code optimization and about improving the memory per- formance of their C programs. ICS. The baseline ICS course, designed to produce enlightened programmers who understand the impact of the hardware, operating system, and compilation system on the performance and correctness of their application programs. A signiﬁcant difference from ORG+ is that low-level processor architecture is not covered. Instead, programmers work with a higher-level model of a modern out-of-order processor. The ICS course ﬁts nicely into a 10-week quarter, and can also be stretched to a 15-week semester if covered at a more leisurely pace. ICS+. The baseline ICS course with additional coverage of systems programming topics such as system-level I/O, network programming, and concurrent pro- gramming. This is the semester-long Carnegie Mellon course, which covers every chapter in CS:APP except low-level processor architecture. Preface 29 Course Chapter Topic ORG ORG+ ICS ICS+ SP 1 Tour of systems • • • • • 2 Data representation • • • • ⊙ (d) 3 Machine language • • • • • 4 Processor architecture • • 5 Code optimization • • • 6 Memory hierarchy ⊙ (a) •• • ⊙ (a) 7 Linking ⊙ (c) ⊙ (c) • 8 Exceptional control ﬂow • • • 9 Virtual memory ⊙ (b) •• • • 10 System-level I/O • • 11 Network programming • • 12 Concurrent programming • • Figure 2 Five systems courses based on the CS:APP book. ICS+ is the 15-213 course from Carnegie Mellon. Notes: The ⊙ symbol denotes partial coverage of a chapter, as follows: (a) hardware only; (b) no dynamic storage allocation; (c) no dynamic linking; (d) no ﬂoating point. SP. A systems programming course. This course is similar to ICS+, but it drops ﬂoating point and performance optimization, and it places more empha- sis on systems programming, including process control, dynamic linking, system-level I/O, network programming, and concurrent programming. In- structors might want to supplement from other sources for advanced topics such as daemons, terminal control, and Unix IPC. The main message of Figure 2 is that the CS:APP book gives a lot of options to students and instructors. If you want your students to be exposed to lower- level processor architecture, then that option is available via the ORG and ORG+ courses. On the other hand, if you want to switch from your current computer organization course to an ICS or ICS+ course, but are wary of making such a drastic change all at once, then you can move toward ICS incrementally. You can start with ORG, which teaches the traditional topics in a nontraditional way. Once you are comfortable with that material, then you can move to ORG+, and eventually to ICS. If students have no experience in C (e.g., they have only programmed in Java), you could spend several weeks on C and then cover the material of ORG or ICS. Finally, we note that the ORG+ and SP courses would make a nice two-term sequence (either quarters or semesters). Or you might consider offering ICS+ as one term of ICS and one term of SP. 30 Preface For Instructors: Classroom-Tested Laboratory Exercises The ICS+ course at Carnegie Mellon receives very high evaluations from students. Median scores of 5.0/5.0 and means of 4.6/5.0 are typical for the student course evaluations. Students cite the fun, exciting, and relevant laboratory exercises as the primary reason. The labs are available from the CS:APP Web page. Here are examples of the labs that are provided with the book. Data Lab. This lab requires students to implement simple logical and arithmetic functions, but using a highly restricted subset of C. For example, they must compute the absolute value of a number using only bit-level operations. This lab helps students understand the bit-level representations of C data types and the bit-level behavior of the operations on data. Binary Bomb Lab. A binary bomb is a program provided to students as an object- code ﬁle. When run, it prompts the user to type in six different strings. If any of these are incorrect, the bomb “explodes,” printing an error message and logging the event on a grading server. Students must “defuse” their own unique bombs by disassembling and reverse engineering the programs to determine what the six strings should be. The lab teaches students to understand assembly language and also forces them to learn how to use a debugger. Buffer Overﬂow Lab. Students are required to modify the run-time behavior of a binary executable by exploiting a buffer overﬂow vulnerability. This lab teaches the students about the stack discipline and about the danger of writing code that is vulnerable to buffer overﬂow attacks. Architecture Lab. Several of the homework problems of Chapter 4 can be com- bined into a lab assignment, where students modify the HCL description of a processor to add new instructions, change the branch prediction policy, or add or remove bypassing paths and register ports. The resulting processors can be simulated and run through automated tests that will detect most of the possible bugs. This lab lets students experience the exciting parts of pro- cessor design without requiring a complete background in logic design and hardware description languages. Performance Lab. Students must optimize the performance of an application ker- nel function such as convolution or matrix transposition. This lab provides a very clear demonstration of the properties of cache memories and gives students experience with low-level program optimization. Cache Lab. In this alternative to the performance lab, students write a general- purpose cache simulator, and then optimize a small matrix transpose kernel to minimize the number of misses on a simulated cache. We use the Valgrind tool to generate real address traces for the matrix transpose kernel. Shell Lab. Students implement their own Unix shell program with job control, including the Ctrl+C and Ctrl+Z keystrokes and the ð×, ¾×, and ÁÇ¾Í com- Preface 31 mands. This is the student’s ﬁrst introduction to concurrency, and it gives them a clear idea of Unix process control, signals, and signal handling. Malloc Lab. Students implement their own versions of ÀþÄÄÇ², ðË−−, and (op- tionally) Ë−þÄÄÇ². This lab gives students a clear understanding of data layout and organization, and requires them to evaluate different trade-offs between space and time efﬁciency. Proxy Lab. Students implement a concurrent Web proxy that sits between their browsers and the rest of the World Wide Web. This lab exposes the students to such topics as Web clients and servers, and ties together many of the con- cepts from the course, such as byte ordering, ﬁle I/O, process control, signals, signal handling, memory mapping, sockets, and concurrency. Students like being able to see their programs in action with real Web browsers and Web servers. The CS:APP instructor’s manual has a detailed discussion of the labs, as well as directions for downloading the support software. Acknowledgments for the Third Edition It is a pleasure to acknowledge and thank those who have helped us produce this third edition of the CS:APP text. We would like to thank our Carnegie Mellon colleagues who have taught the ICS course over the years and who have provided so much insightful feedback and encouragement: Guy Blelloch, Roger Dannenberg, David Eckhardt, Franz Franchetti, Greg Ganger, Seth Goldstein, Khaled Harras, Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, Markus Pueschel, and Anthony Rowe. David Winters was very helpful in installing and conﬁguring the reference Linux box. Jason Fritts (St. Louis University) and Cindy Norris (Appalachian State) provided us with detailed and thoughtful reviews of the second edition. Yili Gong (Wuhan University) wrote the Chinese translation, maintained the errata page for the Chinese version, and contributed many bug reports. Godmar Back (Virginia Tech) helped us improve the text signiﬁcantly by introducing us to the notions of async-signal safety and protocol-independent network programming. Many thanks to our eagle-eyed readers who reported bugs in the second edi- tion: Rami Ammari, Paul Anagnostopoulos, Lucas B ¨arenf ¨anger, Godmar Back, Ji Bin, Sharbel Bousemaan, Richard Callahan, Seth Chaiken, Cheng Chen, Libo Chen, Tao Du, Pascal Garcia, Yili Gong, Ronald Greenberg, Dorukhan G ¨ul ¨oz, Dong Han, Dominik Helm, Ronald Jones, Mustafa Kazdagli, Gordon Kindlmann, Sankar Krishnan, Kanak Kshetri, Junlin Lu, Qiangqiang Luo, Sebastian Luy, Lei Ma, Ashwin Nanjappa, Gregoire Paradis, Jonas Pfenninger, Karl Pichotta, David Ramsey, Kaustabh Roy, David Selvaraj, Sankar Shanmugam, Dominique Smulkowska, Dag Sørbø, Michael Spear, Yu Tanaka, Steven Tricanowicz, Scott Wright, Waiki Wright, Han Xu, Zhengshan Yan, Firo Yang, Shuang Yang, John Ye, Taketo Yoshida, Yan Zhu, and Michael Zink. 32 Preface Thanks also to our readers who have contributed to the labs, including God- mar Back (Virginia Tech), Taymon Beal (Worcester Polytechnic Institute), Aran Clauson (Western Washington University), Cary Gray (Wheaton College), Paul Haiduk (West Texas A&M University), Len Hamey (Macquarie University), Ed- die Kohler (Harvard), Hugh Lauer (Worcester Polytechnic Institute), Robert Marmorstein (Longwood University), and James Riely (DePaul University). Once again, Paul Anagnostopoulos of Windfall Software did a masterful job of typesetting the book and leading the production process. Many thanks to Paul and his stellar team: Richard Camp (copyediting), Jennifer McClain (proofread- ing), Laurel Muller (art production), and Ted Laux (indexing). Paul even spotted a bug in our description of the origins of the acronym BSS that had persisted undetected since the ﬁrst edition! Finally, we would like to thank our friends at Prentice Hall. Marcia Horton and our editor, Matt Goldstein, have been unﬂagging in their support and encour- agement, and we are deeply grateful to them. Acknowledgments from the Second Edition We are deeply grateful to the many people who have helped us produce this second edition of the CS:APP text. First and foremost, we would like to recognize our colleagues who have taught the ICS course at Carnegie Mellon for their insightful feedback and encourage- ment: Guy Blelloch, Roger Dannenberg, David Eckhardt, Greg Ganger, Seth Goldstein, Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, and Markus Pueschel. Thanks also to our sharp-eyed readers who contributed reports to the errata page for the ﬁrst edition: Daniel Amelang, Rui Baptista, Quarup Barreirinhas, Michael Bombyk, J ¨org Brauer, Jordan Brough, Yixin Cao, James Caroll, Rui Car- valho, Hyoung-Kee Choi, Al Davis, Grant Davis, Christian Dufour, Mao Fan, Tim Freeman, Inge Frick, Max Gebhardt, Jeff Goldblat, Thomas Gross, Anita Gupta, John Hampton, Hiep Hong, Greg Israelsen, Ronald Jones, Haudy Kazemi, Brian Kell, Constantine Kousoulis, Sacha Krakowiak, Arun Krishnaswamy, Mar- tin Kulas, Michael Li, Zeyang Li, Ricky Liu, Mario Lo Conte, Dirk Maas, Devon Macey, Carl Marcinik, Will Marrero, Simone Martins, Tao Men, Mark Morris- sey, Venkata Naidu, Bhas Nalabothula, Thomas Niemann, Eric Peskin, David Po, Anne Rogers, John Ross, Michael Scott, Seiki, Ray Shih, Darren Shultz, Erik Silkensen, Suryanto, Emil Tarazi, Nawanan Theera-Ampornpunt, Joe Trdinich, Michael Trigoboff, James Troup, Martin Vopatek, Alan West, Betsy Wolff, Tim Wong, James Woodruff, Scott Wright, Jackie Xiao, Guanpeng Xu, Qing Xu, Caren Yang, Yin Yongsheng, Wang Yuanxuan, Steven Zhang, and Day Zhong. Special thanks to Inge Frick, who identiﬁed a subtle deep copy bug in our lock-and-copy example, and to Ricky Liu for his amazing proofreading skills. Our Intel Labs colleagues Andrew Chien and Limor Fix were exceptionally supportive throughout the writing of the text. Steve Schlosser graciously provided some disk drive characterizations. Casey Helfrich and Michael Ryan installed Preface 33 and maintained our new Core i7 box. Michael Kozuch, Babu Pillai, and Jason Campbell provided valuable insight on memory system performance, multi-core systems, and the power wall. Phil Gibbons and Shimin Chen shared their consid- erable expertise on solid state disk designs. We have been able to call on the talents of many, including Wen-Mei Hwu, Markus Pueschel, and Jiri Simsa, to provide both detailed comments and high- level advice. James Hoe helped us create a Verilog version of the Y86 processor and did all of the work needed to synthesize working hardware. Many thanks to our colleagues who provided reviews of the draft manu- script: James Archibald (Brigham Young University), Richard Carver (George Mason University), Mirela Damian (Villanova University), Peter Dinda (North- western University), John Fiore (Temple University), Jason Fritts (St. Louis Uni- versity), John Greiner (Rice University), Brian Harvey (University of California, Berkeley), Don Heller (Penn State University), Wei Chung Hsu (University of Minnesota), Michelle Hugue (University of Maryland), Jeremy Johnson (Drexel University), Geoff Kuenning (Harvey Mudd College), Ricky Liu, Sam Mad- den (MIT), Fred Martin (University of Massachusetts, Lowell), Abraham Matta (Boston University), Markus Pueschel (Carnegie Mellon University), Norman Ramsey (Tufts University), Glenn Reinmann (UCLA), Michela Taufer (Univer- sity of Delaware), and Craig Zilles (UIUC). Paul Anagnostopoulos of Windfall Software did an outstanding job of type- setting the book and leading the production team. Many thanks to Paul and his superb team: Rick Camp (copyeditor), Joe Snowden (compositor), MaryEllen N. Oliver (proofreader), Laurel Muller (artist), and Ted Laux (indexer). Finally, we would like to thank our friends at Prentice Hall. Marcia Horton has always been there for us. Our editor, Matt Goldstein, provided stellar leadership from beginning to end. We are profoundly grateful for their help, encouragement, and insights. Acknowledgments from the First Edition We are deeply indebted to many friends and colleagues for their thoughtful crit- icisms and encouragement. A special thanks to our 15-213 students, whose infec- tious energy and enthusiasm spurred us on. Nick Carter and Vinny Furia gener- ously provided their malloc package. Guy Blelloch, Greg Kesden, Bruce Maggs, and Todd Mowry taught the course over multiple semesters, gave us encouragement, and helped improve the course material. Herb Derby provided early spiritual guidance and encouragement. Al- lan Fisher, Garth Gibson, Thomas Gross, Satya, Peter Steenkiste, and Hui Zhang encouraged us to develop the course from the start. A suggestion from Garth early on got the whole ball rolling, and this was picked up and reﬁned with the help of a group led by Allan Fisher. Mark Stehlik and Peter Lee have been very supportive about building this material into the undergraduate curriculum. Greg Kesden provided helpful feedback on the impact of ICS on the OS course. Greg Ganger and Jiri Schindler graciously provided some disk drive characterizations 34 Preface and answered our questions on modern disks. Tom Stricker showed us the mem- ory mountain. James Hoe provided useful ideas and feedback on how to present processor architecture. A special group of students—Khalil Amiri, Angela Demke Brown, Chris Colohan, Jason Crawford, Peter Dinda, Julio Lopez, Bruce Lowekamp, Jeff Pierce, Sanjay Rao, Balaji Sarpeshkar, Blake Scholl, Sanjit Seshia, Greg Stef- fan, Tiankai Tu, Kip Walker, and Yinglian Xie—were instrumental in helping us develop the content of the course. In particular, Chris Colohan established a fun (and funny) tone that persists to this day, and invented the legendary “binary bomb” that has proven to be a great tool for teaching machine code and debugging concepts. Chris Bauer, Alan Cox, Peter Dinda, Sandhya Dwarkadas, John Greiner, Don Heller, Bruce Jacob, Barry Johnson, Bruce Lowekamp, Greg Morrisett, Brian Noble, Bobbie Othmer, Bill Pugh, Michael Scott, Mark Smotherman, Greg Steffan, and Bob Wier took time that they did not have to read and advise us on early drafts of the book. A very special thanks to Al Davis (University of Utah), Peter Dinda (Northwestern University), John Greiner (Rice University), Wei Hsu (University of Minnesota), Bruce Lowekamp (College of William & Mary), Bobbie Othmer (University of Minnesota), Michael Scott (University of Rochester), and Bob Wier (Rocky Mountain College) for class testing the beta version. A special thanks to their students as well! We would also like to thank our colleagues at Prentice Hall. Marcia Horton, Eric Frank, and Harold Stone have been unﬂagging in their support and vision. Harold also helped us present an accurate historical perspective on RISC and CISC processor architectures. Jerry Ralya provided sharp insights and taught us a lot about good writing. Finally, we would like to acknowledge the great technical writers Brian Kernighan and the late W. Richard Stevens, for showing us that technical books can be beautiful. Thank you all. Randy Bryant Dave O’Hallaron Pittsburgh, Pennsylvania Pearson would like to thank and acknowledge Chetan Venkatesh, MS Ramaiah Institute of Technology, Desny Antony, Don Bosco College, and Chitra Dhawale, SP College, for reviewing the Global Edition. About the Authors Randal E. Bryant received his bachelor’s degree from the University of Michigan in 1973 and then attended graduate school at the Massachusetts Institute of Technology, receiving his PhD degree in computer science in 1981. He spent three years as an assistant professor at the California Institute of Technology, and has been on the faculty at Carnegie Mellon since 1984. For ﬁve of those years he served as head of the Computer Science Department, and for ten of them he served as Dean of the School of Computer Science. He is currently a university professor of computer sci- ence. He also holds a courtesy appointment with the Department of Electrical and Computer Engineering. Professor Bryant has taught courses in computer systems at both the under- graduate and graduate level for around 40 years. Over many years of teaching computer architecture courses, he began shifting the focus from how computers are designed to how programmers can write more efﬁcient and reliable programs if they understand the system better. Together with Professor O’Hallaron, he de- veloped the course 15-213, Introduction to Computer Systems, at Carnegie Mellon that is the basis for this book. He has also taught courses in algorithms, program- ming, computer networking, distributed systems, and VLSI design. Most of Professor Bryant’s research concerns the design of software tools to help software and hardware designers verify the correctness of their systems. These include several types of simulators, as well as formal veriﬁcation tools that prove the correctness of a design using mathematical methods. He has published over 150 technical papers. His research results are used by major computer manu- facturers, including Intel, IBM, Fujitsu, and Microsoft. He has won several major awards for his research. These include two inventor recognition awards and a technical achievement award from the Semiconductor Research Corporation, the Kanellakis Theory and Practice Award from the Association for Computer Ma- chinery (ACM), and the W. R. G. Baker Award, the Emmanuel Piore Award, the Phil Kaufman Award, and the A. Richard Newton Award from the Institute of Electrical and Electronics Engineers (IEEE). He is a fellow of both the ACM and the IEEE and a member of both the US National Academy of Engineering and the American Academy of Arts and Sciences. 35 36 About the Authors David R. O’Hallaron is a professor of computer science and electrical and computer engineering at Carnegie Mellon University. He received his PhD from the Uni- versity of Virginia. He served as the director of Intel Labs, Pittsburgh, from 2007 to 2010. He has taught computer systems courses at the un- dergraduate and graduate levels for 20 years on such topics as computer architecture, introductory com- puter systems, parallel processor design, and Internet services. Together with Professor Bryant, he developed the course at Carnegie Mellon that led to this book. In 2004, he was awarded the Herbert Simon Award for Teaching Excellence by the CMU School of Computer Science, an award for which the winner is chosen based on a poll of the students. Professor O’Hallaron works in the area of computer systems, with speciﬁc in- terests in software systems for scientiﬁc computing, data-intensive computing, and virtualization. The best-known example of his work is the Quake project, an en- deavor involving a group of computer scientists, civil engineers, and seismologists who have developed the ability to predict the motion of the ground during strong earthquakes. In 2003, Professor O’Hallaron and the other members of the Quake team won the Gordon Bell Prize, the top international prize in high-performance computing. His current work focuses on the notion of autograding, that is, pro- grams that evaluate the quality of other programs. CHAPTER 1 A Tour of Computer Systems 1.1 Information Is Bits + Context 39 1.2 Programs Are Translated by Other Programs into Different Forms 40 1.3 It Pays to Understand How Compilation Systems Work 42 1.4 Processors Read and Interpret Instructions Stored in Memory 43 1.5 Caches Matter 47 1.6 Storage Devices Form a Hierarchy 50 1.7 The Operating System Manages the Hardware 50 1.8 Systems Communicate with Other Systems Using Networks 55 1.9 Important Themes 58 1.10 Summary 63 Bibliographic Notes 64 Solutions to Practice Problems 64 37 38 Chapter 1 A Tour of Computer Systems A computer system consists of hardware and systems software that work to- gether to run application programs. Speciﬁc implementations of systems change over time, but the underlying concepts do not. All computer systems have similar hardware and software components that perform similar functions. This book is written for programmers who want to get better at their craft by under- standing how these components work and how they affect the correctness and performance of their programs. You are poised for an exciting journey. If you dedicate yourself to learning the concepts in this book, then you will be on your way to becoming a rare “power pro- grammer,” enlightened by an understanding of the underlying computer system and its impact on your application programs. You are going to learn practical skills such as how to avoid strange numerical errors caused by the way that computers represent numbers. You will learn how to optimize your C code by using clever tricks that exploit the designs of modern processors and memory systems. You will learn how the compiler implements procedure calls and how to use this knowledge to avoid the security holes from buffer overﬂow vulnerabilities that plague network and Internet software. You will learn how to recognize and avoid the nasty errors during linking that confound the average programmer. You will learn how to write your own Unix shell, your own dynamic storage allocation package, and even your own Web server. You will learn the promises and pitfalls of concurrency, a topic of increasing importance as multiple processor cores are integrated onto single chips. In their classic text on the C programming language [61], Kernighan and Ritchie introduce readers to C using the ³−ÄÄÇ program shown in Figure 1.1. Although ³−ÄÄÇ is a very simple program, every major part of the system must work in concert in order for it to run to completion. In a sense, the goal of this book is to help you understand what happens and why when you run ³−ÄÄÇ on your system. We begin our study of systems by tracing the lifetime of the ³−ÄÄÇ program, from the time it is created by a programmer, until it runs on a system, prints its simple message, and terminates. As we follow the lifetime of the program, we will brieﬂy introduce the key concepts, terminology, and components that come into play. Later chapters will expand on these ideas. code/intro/hello.c 1 a©Å²ÄÏ®− zÍÎ®©Çl³| 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ÉË©ÅÎðf‘³−ÄÄÇj ÑÇËÄ®¿Å‘gy 6 Ë−ÎÏËÅ ny 7 Û code/intro/hello.c Figure 1.1 The ³−ÄÄÇ program. (Source: [60]) Section 1.1 Information Is Bits + Context 39 a©Å²ÄÏ®− SP zÍÎ®©Çl qs ons oon ww onv oou onn ono qp tn oos oot onn ons ooo rt ³|¿Å ¿Å ©ÅÎ SP Àþ©Åfg¿Å Õ onr tp on on ons oon oot qp onw wu ons oon rn ro on opq ¿Å SP SP SP SP ÉË©ÅÎðf‘³−Ä on qp qp qp qp oop oor ons oon oot onp rn qr onr ono onv ÄÇj SP ÑÇËÄ®¿Å‘gy¿Å SP onv ooo rr qp oow ooo oor onv onn wp oon qr ro sw on qp SP SP SP Ë−ÎÏËÅ SP ny¿Å Û¿Å qp qp qp oor ono oot oou oor oon qp rv sw on ops on Figure 1.2 The ASCII text representation of ³−ÄÄÇl². 1.1 Information Is Bits + Context Our ³−ÄÄÇ program begins life as a source program (or source ﬁle) that the programmer creates with an editor and saves in a text ﬁle called ³−ÄÄÇl².The source program is a sequence of bits, each with a value of 0 or 1, organized in 8-bit chunks called bytes. Each byte represents some text character in the program. Most computer systems represent text characters using the ASCII standard that represents each character with a unique byte-size integer value.1 For example, Figure 1.2 shows the ASCII representation of the ³−ÄÄÇl² program. The ³−ÄÄÇl² program is stored in a ﬁle as a sequence of bytes. Each byte has an integer value that corresponds to some character. For example, the ﬁrst byte has the integer value 35, which corresponds to the character ‘a’. The second byte has the integer value 105, which corresponds to the character ‘©’, and so on. Notice that each text line is terminated by the invisible newline character ‘¿Å’, which is represented by the integer value 10. Files such as ³−ÄÄÇl² that consist exclusively of ASCII characters are known as text ﬁles. All other ﬁles are known as binary ﬁles. The representation of ³−ÄÄÇl² illustrates a fundamental idea: All information in a system—including disk ﬁles, programs stored in memory, user data stored in memory, and data transferred across a network—is represented as a bunch of bits. The only thing that distinguishes different data objects is the context in which we view them. For example, in different contexts, the same sequence of bytes might represent an integer, ﬂoating-point number, character string, or machine instruction. As programmers, we need to understand machine representations of numbers because they are not the same as integers and real numbers. They are ﬁnite 1. Other encoding methods are used to represent text in non-English languages. See the aside on page 86 for a discussion on this. 40 Chapter 1 A Tour of Computer Systems Aside Origins of the C programming language C was developed from 1969 to 1973 by Dennis Ritchie of Bell Laboratories. The American National Standards Institute (ANSI) ratiﬁed the ANSI C standard in 1989, and this standardization later became the responsibility of the International Standards Organization (ISO). The standards deﬁne the C language and a set of library functions known as the C standard library. Kernighan and Ritchie describe ANSI C in their classic book, which is known affectionately as “K&R” [61]. In Ritchie’s words [92], C is “quirky, ﬂawed, and an enormous success.” So why the success? . C was closely tied with the Unix operating system. C was developed from the beginning as the system programming language for Unix. Most of the Unix kernel (the core part of the operating system), and all of its supporting tools and libraries, were written in C. As Unix became popular in universities in the late 1970s and early 1980s, many people were exposed to C and found that they liked it. Since Unix was written almost entirely in C, it could be easily ported to new machines, which created an even wider audience for both C and Unix. . C is a small, simple language.The design was controlled by a single person, rather than a committee, and the result was a clean, consistent design with little baggage. The K&R book describes the complete language and standard library, with numerous examples and exercises, in only 261 pages. The simplicity of C made it relatively easy to learn and to port to different computers. . C was designed for a practical purpose. C was designed to implement the Unix operating system. Later, other people found that they could write the programs they wanted, without the language getting in the way. C is the language of choice for system-level programming, and there is a huge installed base of application-level programs as well. However, it is not perfect for all programmers and all situations. C pointers are a common source of confusion and programming errors. C also lacks explicit support for useful abstractions such as classes, objects, and exceptions. Newer languages such as C++ and Java address these issues for application-level programs. approximations that can behave in unexpected ways. This fundamental idea is explored in detail in Chapter 2. 1.2 Programs Are Translated by Other Programs into Different Forms The ³−ÄÄÇ program begins life as a high-level C program because it can be read and understood by human beings in that form. However, in order to run ³−ÄÄÇl² on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions. These instructions are then packaged in a form called an executable object program and stored as a binary disk ﬁle. Object programs are also referred to as executable object ﬁles. On a Unix system, the translation from source ﬁle to object ﬁle is performed by a compiler driver: Section 1.2 Programs Are Translated by Other Programs into Different Forms 41 Pre- processor (cpp) Compiler (cc1) Assembler (as) Linker (ld) hello.c hello.i hello.s hello.o printf.o hello Source program (text) Modified source program (text) Assembly program (text) Relocatable object programs (binary) Executable object program (binary) Figure 1.3 The compilation system. Ä©ÅÏÓ| gcc -o hello hello.c Here, the gcc compiler driver reads the source ﬁle ³−ÄÄÇl² and translates it into an executable object ﬁle ³−ÄÄÇ. The translation is performed in the sequence of four phases shown in Figure 1.3. The programs that perform the four phases (preprocessor, compiler, assembler, and linker) are known collectively as the compilation system. . Preprocessing phase. The preprocessor (²ÉÉ) modiﬁes the original C program according to directives that begin with the ‘a’ character. For example, the a©Å²ÄÏ®− zÍÎ®©Çl³| command in line 1 of ³−ÄÄÇl² tells the preprocessor to read the contents of the system header ﬁle ÍÎ®©Çl³ and insert it directly into the program text. The result is another C program, typically with the l© sufﬁx. . Compilation phase. The compiler (²²o) translates the text ﬁle ³−ÄÄÇl© into the text ﬁle ³−ÄÄÇlÍ, which contains an assembly-language program. This program includes the following deﬁnition of function Àþ©Å: 1 Àþ©Åx 2 ÍÏ¾Ê bvj cËÍÉ 3 ÀÇÌÄ bl‹£nj c−®© 4 ²þÄÄ ÉÏÎÍ 5 ÀÇÌÄ bnj c−þÓ 6 þ®®Ê bvj cËÍÉ 7 Ë−Î Each of lines 2–7 in this deﬁnition describes one low-level machine- language instruction in a textual form. Assembly language is useful because it provides a common output language for different compilers for different high-level languages. For example, C compilers and Fortran compilers both generate output ﬁles in the same assembly language. . Assembly phase. Next, the assembler (þÍ) translates ³−ÄÄÇlÍ into machine- language instructions, packages them in a form known as a relocatable object program, and stores the result in the object ﬁle ³−ÄÄÇlÇ. This ﬁle is a binary ﬁle containing 17 bytes to encode the instructions for function Àþ©Å.Ifwe were to view ³−ÄÄÇlÇ with a text editor, it would appear to be gibberish. 42 Chapter 1 A Tour of Computer Systems Aside The GNU project Gcc is one of many useful tools developed by the GNU (short for GNU’s Not Unix) project. The GNU project is a tax-exempt charity started by Richard Stallman in 1984, with the ambitious goal of developing a complete Unix-like system whose source code is unencumbered by restrictions on how it can be modiﬁed or distributed. The GNU project has developed an environment with all the major components of a Unix operating system, except for the kernel, which was developed separately by the Linux project. The GNU environment includes the emacs editor, gcc compiler, gdb debugger, assembler, linker, utilities for manipulating binaries, and other components. The gcc compiler has grown to support many different languages, with the ability to generate code for many different machines. Supported languages include C, C++, Fortran, Java, Pascal, Objective-C, and Ada. The GNU project is a remarkable achievement, and yet it is often overlooked. The modern open- source movement (commonly associated with Linux) owes its intellectual origins to the GNU project’s notion of free software (“free” as in “free speech,” not “free beer”). Further, Linux owes much of its popularity to the GNU tools, which provide the environment for the Linux kernel. . Linking phase.Notice that our ³−ÄÄÇ program calls the ÉË©ÅÎð function, which is part of the standard C library provided by every C compiler. The ÉË©ÅÎð function resides in a separate precompiled object ﬁle called ÉË©ÅÎðlÇ, which must somehow be merged with our ³−ÄÄÇlÇ program. The linker (Ä®) handles this merging. The result is the ³−ÄÄÇ ﬁle, which is an executable object ﬁle (or simply executable) that is ready to be loaded into memory and executed by the system. 1.3 It Pays to Understand How Compilation Systems Work For simple programs such as ³−ÄÄÇl², we can rely on the compilation system to produce correct and efﬁcient machine code. However, there are some important reasons why programmers need to understand how compilation systems work: . Optimizing program performance. Modern compilers are sophisticated tools that usually produce good code. As programmers, we do not need to know the inner workings of the compiler in order to write efﬁcient code. However, in order to make good coding decisions in our C programs, we do need a basic understanding of machine-level code and how the compiler translates different C statements into machine code. For example, is a ÍÑ©Î²³ statement always more efﬁcient than a sequence of ©ð-−ÄÍ− statements? How much overhead is incurred by a function call? Is a Ñ³©Ä− loop more efﬁcient than a ðÇË loop? Are pointer references more efﬁcient than array indexes? Why does our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? How can a function run faster when we simply rearrange the parentheses in an arithmetic expression? Section 1.4 Processors Read and Interpret Instructions Stored in Memory 43 In Chapter 3, we introduce x86-64, the machine language of recent gen- erations of Linux, Macintosh, and Windows computers. We describe how compilers translate different C constructs into this language. In Chapter 5, you will learn how to tune the performance of your C programs by making simple transformations to the C code that help the compiler do its job better. In Chapter 6, you will learn about the hierarchical nature of the memory sys- tem, how C compilers store data arrays in memory, and how your C programs can exploit this knowledge to run more efﬁciently. . Understanding link-time errors. In our experience, some of the most perplex- ing programming errors are related to the operation of the linker, especially when you are trying to build large software systems. For example, what does it mean when the linker reports that it cannot resolve a reference? What is the difference between a static variable and a global variable? What happens if you deﬁne two global variables in different C ﬁles with the same name? What is the difference between a static library and a dynamic library? Why does it matter what order we list libraries on the command line? And scariest of all, why do some linker-related errors not appear until run time? You will learn the answers to these kinds of questions in Chapter 7. . Avoiding security holes. For many years, buffer overﬂow vulnerabilities have accounted for many of the security holes in network and Internet servers. These vulnerabilities exist because too few programmers understand the need to carefully restrict the quantity and forms of data they accept from untrusted sources. A ﬁrst step in learning secure programming is to understand the con- sequences of the way data and control information are stored on the program stack. We cover the stack discipline and buffer overﬂow vulnerabilities in Chapter 3 as part of our study of assembly language. We will also learn about methods that can be used by the programmer, compiler, and operating system to reduce the threat of attack. 1.4 Processors Read and Interpret Instructions Stored in Memory At this point, our ³−ÄÄÇl² source program has been translated by the compilation system into an executable object ﬁle called ³−ÄÄÇ that is stored on disk. To run the executable ﬁle on a Unix system, we type its name to an application program known as a shell: Ä©ÅÏÓ| ./hello ³−ÄÄÇj ÑÇËÄ® Ä©ÅÏÓ| The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then performs the command. If the ﬁrst word of the command line does not correspond to a built-in shell command, then the shell 44 Chapter 1 A Tour of Computer Systems Figure 1.4 Hardware organization of a typical system. CPU: central processing unit, ALU: arithmetic/logic unit, PC: program counter, USB: Universal Serial Bus. CPU Register file PC ALU Bus interface I/O bridge System bus Memory bus Main memory I/O bus Expansion slots for other devices such as network adaptersDisk controller Graphics adapter DisplayMouse Keyboard USB controller Disk hello executable stored on disk assumes that it is the name of an executable ﬁle that it should load and run. So in this case, the shell loads and runs the ³−ÄÄÇ program and then waits for it to terminate. The ³−ÄÄÇ program prints its message to the screen and then terminates. The shell then prints a prompt and waits for the next input command line. 1.4.1 Hardware Organization of a System To understand what happens to our ³−ÄÄÇ program when we run it, we need to understand the hardware organization of a typical system, which is shown in Figure 1.4. This particular picture is modeled after the family of recent Intel systems, but all systems have a similar look and feel. Don’t worry about the complexity of this ﬁgure just now. We will get to its various details in stages throughout the course of the book. Buses Running throughout the system is a collection of electrical conduits called buses that carry bytes of information back and forth between the components. Buses are typically designed to transfer ﬁxed-size chunks of bytes known as words.The number of bytes in a word (the word size) is a fundamental system parameter that varies across systems. Most machines today have word sizes of either 4 bytes (32 bits) or 8 bytes (64 bits). In this book, we do not assume any ﬁxed deﬁnition of word size. Instead, we will specify what we mean by a “word” in any context that requires this to be deﬁned. Section 1.4 Processors Read and Interpret Instructions Stored in Memory 45 I/O Devices Input/output (I/O) devices are the system’s connection to the external world. Our example system has four I/O devices: a keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term storage of data and programs. Initially, the executable ³−ÄÄÇ program resides on the disk. Each I/O device is connected to the I/O bus by either a controller or an adapter. The distinction between the two is mainly one of packaging. Controllers are chip sets in the device itself or on the system’s main printed circuit board (often called the motherboard). An adapter is a card that plugs into a slot on the motherboard. Regardless, the purpose of each is to transfer information back and forth between the I/O bus and an I/O device. Chapter 6 has more to say about how I/O devices such as disks work. In Chapter 10, you will learn how to use the Unix I/O interface to access devices from your application programs. We focus on the especially interesting class of devices known as networks, but the techniques generalize to other kinds of devices as well. Main Memory The main memory is a temporary storage device that holds both a program and the data it manipulates while the processor is executing the program. Physically, main memory consists of a collection of dynamic random access memory (DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address (array index) starting at zero. In general, each of the machine instructions that constitute a program can consist of a variable number of bytes. The sizes of data items that correspond to C program variables vary according to type. For example, on an x86-64 machine running Linux, data of type Í³ÇËÎ require 2 bytes, types ©ÅÎ and ðÄÇþÎ 4 bytes, and types ÄÇÅ× and ®ÇÏ¾Ä− 8 bytes. Chapter 6 has more to say about how memory technologies such as DRAM chips work, and how they are combined to form main memory. Processor The central processing unit (CPU), or simply processor, is the engine that inter- prets (or executes) instructions stored in main memory. At its core is a word-size storage device (or register) called the program counter (PC). At any point in time, the PC points at (contains the address of) some machine-language instruction in main memory.2 From the time that power is applied to the system until the time that the power is shut off, a processor repeatedly executes the instruction pointed at by the program counter and updates the program counter to point to the next instruction. A processor appears to operate according to a very simple instruction execution model, deﬁned by its instruction set architecture. In this model, instructions execute 2. PC is also a commonly used acronym for “personal computer.” However, the distinction between the two should be clear from the context. 46 Chapter 1 A Tour of Computer Systems in strict sequence, and executing a single instruction involves performing a series of steps. The processor reads the instruction from memory pointed at by the program counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, and then updates the PC to point to the next instruction, which may or may not be contiguous in memory to the instruction that was just executed. There are only a few of these simple operations, and they revolve around main memory, the register ﬁle, and the arithmetic/logic unit (ALU). The register ﬁle is a small storage device that consists of a collection of word-size registers, each with its own unique name. The ALU computes new data and address values. Here are some examples of the simple operations that the CPU might carry out at the request of an instruction: . Load: Copy a byte or a word from main memory into a register, overwriting the previous contents of the register. . Store: Copy a byte or a word from a register to a location in main memory, overwriting the previous contents of that location. . Operate: Copy the contents of two registers to the ALU, perform an arithmetic operation on the two words, and store the result in a register, overwriting the previous contents of that register. . Jump: Extract a word from the instruction itself and copy that word into the program counter (PC), overwriting the previous value of the PC. We say that a processor appears to be a simple implementation of its in- struction set architecture, but in fact modern processors use far more complex mechanisms to speed up program execution. Thus, we can distinguish the pro- cessor’s instruction set architecture, describing the effect of each machine-code instruction, from its microarchitecture, describing how the processor is actually implemented. When we study machine code in Chapter 3, we will consider the abstraction provided by the machine’s instruction set architecture. Chapter 4 has more to say about how processors are actually implemented. Chapter 5 describes a model of how modern processors work that enables predicting and optimizing the performance of machine-language programs. 1.4.2 Running the ³−ÄÄÇ Program Given this simple view of a system’s hardware organization and operation, we can begin to understand what happens when we run our example program. We must omit a lot of details here that will be ﬁlled in later, but for now we will be content with the big picture. Initially, the shell program is executing its instructions, waiting for us to type a command. As we type the characters lm³−ÄÄÇ at the keyboard, the shell program reads each one into a register and then stores it in memory, as shown in Figure 1.5. When we hit the −ÅÎ−Ë key on the keyboard, the shell knows that we have ﬁnished typing the command. The shell then loads the executable ³−ÄÄÇ ﬁle by executing a sequence of instructions that copies the code and data in the ³−ÄÄÇ Section 1.5 Caches Matter 47 Figure 1.5 Reading the ³−ÄÄÇ command from the keyboard. CPU Register file PC ALU Bus interface I/O bridge System bus Memory bus Main memory I/O bus Expansion slots for other devices such as network adaptersDisk controller Graphics adapter DisplayMouse Keyboard USB controller Disk “hello” User types “hello” object ﬁle from disk to main memory. The data includes the string of characters ³−ÄÄÇj ÑÇËÄ®¿Å that will eventually be printed out. Using a technique known as direct memory access (DMA, discussed in Chap- ter 6), the data travel directly from disk to main memory, without passing through the processor. This step is shown in Figure 1.6. Once the code and data in the ³−ÄÄÇ object ﬁle are loaded into memory, the processor begins executing the machine-language instructions in the ³−ÄÄÇ program’s Àþ©Å routine. These instructions copy the bytes in the ³−ÄÄÇj ÑÇËÄ®¿Å string from memory to the register ﬁle, and from there to the display device, where they are displayed on the screen. This step is shown in Figure 1.7. 1.5 Caches Matter An important lesson from this simple example is that a system spends a lot of time moving information from one place to another. The machine instructions in the ³−ÄÄÇ program are originally stored on disk. When the program is loaded, they are copied to main memory. As the processor runs the program, instruc- tions are copied from main memory into the processor. Similarly, the data string ³−ÄÄÇjÑÇËÄ®¿Å, originally on disk, is copied to main memory and then copied from main memory to the display device. From a programmer’s perspective, much of this copying is overhead that slows down the “real work” of the program. Thus, a major goal for system designers is to make these copy operations run as fast as possible. Because of physical laws, larger storage devices are slower than smaller stor- age devices. And faster devices are more expensive to build than their slower 48 Chapter 1 A Tour of Computer Systems Disk CPU Register file PC ALU Bus interface I/O bridge System bus Memory bus Main memory I/O bus Expansion slots for other devices such as network adaptersDisk controller Graphics adapter DisplayMouse Keyboard USB controller “hello, world\\n” hello code hello executable stored on disk Figure 1.6 Loading the executable from disk into main memory. CPU Register file PC ALU Bus interface I/O bridge System bus Memory bus Main memory I/O bus Expansion slots for other devices such as network adaptersDisk controller Graphics adapter DisplayMouse Keyboard USB controller Disk“hello, world\\n” “hello, world\\n” hello code hello executable stored on disk Figure 1.7 Writing the output string from memory to the display. Section 1.5 Caches Matter 49 Figure 1.8 Cache memories. I/O bridge CPU chip Cache memories Register file System bus Memory bus Bus interface Main memory ALU counterparts. For example, the disk drive on a typical system might be 1,000 times larger than the main memory, but it might take the processor 10,000,000 times longer to read a word from disk than from memory. Similarly, a typical register ﬁle stores only a few hundred bytes of information, as opposed to billions of bytes in the main memory. However, the processor can read data from the register ﬁle almost 100 times faster than from memory. Even more troublesome, as semiconductor technology progresses over the years, this processor–memory gap continues to increase. It is easier and cheaper to make processors run faster than it is to make main memory run faster. To deal with the processor–memory gap, system designers include smaller, faster storage devices called cache memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to need in the near future. Figure 1.8 shows the cache memories in a typical system. An L1 cache on the processor chip holds tens of thousands of bytes and can be accessed nearly as fast as the register ﬁle. A larger L2 cache with hundreds of thousands to millions of bytes is connected to the processor by a special bus. It might take 5 times longer for the processor to access the L2 cache than the L1 cache, but this is still 5 to 10 times faster than accessing the main memory. The L1 and L2 caches are implemented with a hardware technology known as static random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. The idea behind caching is that a system can get the effect of both a very large memory and a very fast one by exploiting locality, the tendency for programs to access data and code in localized regions. By setting up caches to hold data that are likely to be accessed often, we can perform most memory operations using the fast caches. One of the most important lessons in this book is that application program- mers who are aware of cache memories can exploit them to improve the perfor- mance of their programs by an order of magnitude. You will learn more about these important devices and how to exploit them in Chapter 6. 50 Chapter 1 A Tour of Computer Systems CPU registers hold words retrieved from cache memory. L1 cache holds cache lines retrieved from L2 cache. L2 cache holds cache lines retrieved from L3 cache. Main memory holds disk blocks retrieved from local disks. Local disks hold files retrieved from disks on remote network server. Regs L3 cache (SRAM) L2 cache (SRAM) L1 cache (SRAM) Main memory (DRAM) Local secondary storage (local disks) Remote secondary storage (distributed file systems, Web servers) Smaller, faster, and costlier (per byte) storage devices Larger, slower, and cheaper (per byte) storage devices L0: L1: L2: L3: L4: L5: L6: L3 cache holds cache lines retrieved from memory. Figure 1.9 An example of a memory hierarchy. 1.6 Storage Devices Form a Hierarchy This notion of inserting a smaller, faster storage device (e.g., cache memory) between the processor and a larger, slower device (e.g., main memory) turns out to be a general idea. In fact, the storage devices in every computer system are organized as a memory hierarchy similar to Figure 1.9. As we move from the top of the hierarchy to the bottom, the devices become slower, larger, and less costly per byte. The register ﬁle occupies the top level in the hierarchy, which is known as level 0 or L0. We show three levels of caching L1 to L3, occupying memory hierarchy levels 1 to 3. Main memory occupies level 4, and so on. The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower level. Thus, the register ﬁle is a cache for the L1 cache. Caches L1 and L2 are caches for L2 and L3, respectively. The L3 cache is a cache for the main memory, which is a cache for the disk. On some networked systems with distributed ﬁle systems, the local disk serves as a cache for data stored on the disks of other systems. Just as programmers can exploit knowledge of the different caches to improve performance, programmers can exploit their understanding of the entire memory hierarchy. Chapter 6 will have much more to say about this. 1.7 The Operating System Manages the Hardware Back to our ³−ÄÄÇ example. When the shell loaded and ran the ³−ÄÄÇ program, and when the ³−ÄÄÇ program printed its message, neither program accessed the Section 1.7 The Operating System Manages the Hardware 51 Figure 1.10 Layered view of a computer system. Application programs Operating system Main memory I/O devicesProcessor Software Hardware Figure 1.11 Abstractions provided by an operating system. Main memory I/O devicesProcessor Processes Virtual memory Files keyboard, display, disk, or main memory directly. Rather, they relied on the services provided by the operating system. We can think of the operating system as a layer of software interposed between the application program and the hardware, as shown in Figure 1.10. All attempts by an application program to manipulate the hardware must go through the operating system. The operating system has two primary purposes: (1) to protect the hardware from misuse by runaway applications and (2) to provide applications with simple and uniform mechanisms for manipulating complicated and often wildly different low-level hardware devices. The operating system achieves both goals via the fundamental abstractions shown in Figure 1.11: processes, virtual memory, and ﬁles. As this ﬁgure suggests, ﬁles are abstractions for I/O devices, virtual memory is an abstraction for both the main memory and disk I/O devices, and processes are abstractions for the processor, main memory, and I/O devices. We will discuss each in turn. 1.7.1 Processes When a program such as ³−ÄÄÇ runs on a modern system, the operating system provides the illusion that the program is the only one running on the system. The program appears to have exclusive use of both the processor, main memory, and I/O devices. The processor appears to execute the instructions in the program, one after the other, without interruption. And the code and data of the program appear to be the only objects in the system’s memory. These illusions are provided by the notion of a process, one of the most important and successful ideas in computer science. A process is the operating system’s abstraction for a running program. Multi- ple processes can run concurrently on the same system, and each process appears to have exclusive use of the hardware. By concurrently, we mean that the instruc- tions of one process are interleaved with the instructions of another process. In most systems, there are more processes to run than there are CPUs to run them. 52 Chapter 1 A Tour of Computer Systems Aside Unix, Posix, and the Standard Unix Speciﬁcation The 1960s was an era of huge, complex operating systems, such as IBM’s OS/360 and Honeywell’s Multics systems. While OS/360 was one of the most successful software projects in history, Multics dragged on for years and never achieved wide-scale use. Bell Laboratories was an original partner in the Multics project but dropped out in 1969 because of concern over the complexity of the project and the lack of progress. In reaction to their unpleasant Multics experience, a group of Bell Labs researchers—Ken Thompson, Dennis Ritchie, Doug McIlroy, and Joe Ossanna—began work in 1969 on a simpler operating system for a Digital Equipment Corporation PDP-7 computer, written entirely in machine language. Many of the ideas in the new system, such as the hierarchical ﬁle system and the notion of a shell as a user-level process, were borrowed from Multics but implemented in a smaller, simpler package. In 1970, Brian Kernighan dubbed the new system “Unix” as a pun on the complexity of “Multics.” The kernel was rewritten in C in 1973, and Unix was announced to the outside world in 1974 [93]. Because Bell Labs made the source code available to schools with generous terms, Unix developed a large following at universities. The most inﬂuential work was done at the University of California at Berkeley in the late 1970s and early 1980s, with Berkeley researchers adding virtual memory and the Internet protocols in a series of releases called Unix 4.xBSD (Berkeley Software Distribution). Concurrently, Bell Labs was releasing their own versions, which became known as System V Unix. Versions from other vendors, such as the Sun Microsystems Solaris system, were derived from these original BSD and System V versions. Trouble arose in the mid 1980s as Unix vendors tried to differentiate themselves by adding new and often incompatible features. To combat this trend, IEEE (Institute for Electrical and Electron- ics Engineers) sponsored an effort to standardize Unix, later dubbed “Posix” by Richard Stallman. The result was a family of standards, known as the Posix standards, that cover such issues as the C language interface for Unix system calls, shell programs and utilities, threads, and network program- ming. More recently, a separate standardization effort, known as the “Standard Unix Speciﬁcation,” has joined forces with Posix to create a single, uniﬁed standard for Unix systems. As a result of these standardization efforts, the differences between Unix versions have largely disappeared. Traditional systems could only execute one program at a time, while newer multi- core processors can execute several programs simultaneously. In either case, a single CPU can appear to execute multiple processes concurrently by having the processor switch among them. The operating system performs this interleaving with a mechanism known as context switching. To simplify the rest of this discus- sion, we consider only a uniprocessor system containing a single CPU. We will return to the discussion of multiprocessor systems in Section 1.9.2. The operating system keeps track of all the state information that the process needs in order to run. This state, which is known as the context, includes informa- tion such as the current values of the PC, the register ﬁle, and the contents of main memory. At any point in time, a uniprocessor system can only execute the code for a single process. When the operating system decides to transfer control from the current process to some new process, it performs a context switch by saving the context of the current process, restoring the context of the new process, and Section 1.7 The Operating System Manages the Hardware 53 Figure 1.12 Process context switching. Process A read Process B User code Kernel code Kernel code User code User code Context switch Context switch Time Disk interrupt Return from read then passing control to the new process. The new process picks up exactly where it left off. Figure 1.12 shows the basic idea for our example ³−ÄÄÇ scenario. There are two concurrent processes in our example scenario: the shell process and the ³−ÄÄÇ process. Initially, the shell process is running alone, waiting for input on the command line. When we ask it to run the ³−ÄÄÇ program, the shell carries out our request by invoking a special function known as a system call that passes control to the operating system. The operating system saves the shell’s context, creates a new ³−ÄÄÇ process and its context, and then passes control to the new ³−ÄÄÇ process. After ³−ÄÄÇ terminates, the operating system restores the context of the shell process and passes control back to it, where it waits for the next command-line input. As Figure 1.12 indicates, the transition from one process to another is man- aged by the operating system kernel. The kernel is the portion of the operating system code that is always resident in memory. When an application program requires some action by the operating system, such as to read or write a ﬁle, it executes a special system call instruction, transferring control to the kernel. The kernel then performs the requested operation and returns back to the application program. Note that the kernel is not a separate process. Instead, it is a collection of code and data structures that the system uses to manage all the processes. Implementing the process abstraction requires close cooperation between both the low-level hardware and the operating system software. We will explore how this works, and how applications can create and control their own processes, in Chapter 8. 1.7.2 Threads Although we normally think of a process as having a single control ﬂow, in modern systems a process can actually consist of multiple execution units, called threads, each running in the context of the process and sharing the same code and global data. Threads are an increasingly important programming model because of the requirement for concurrency in network servers, because it is easier to share data between multiple threads than between multiple processes, and because threads are typically more efﬁcient than processes. Multi-threading is also one way to make programs run faster when multiple processors are available, as we will discuss in 54 Chapter 1 A Tour of Computer Systems Figure 1.13 Process virtual address space. (The regions are not drawn to scale.) 0 Memory invisible to user code printf function Loaded from the hello executable file Program start User stack (created at run time) Memory-mapped region for shared libraries Run-time heap (created by malloc) Read/write data Read-only code and data Kernel virtual memory Section 1.9.2. You will learn the basic concepts of concurrency, including how to write threaded programs, in Chapter 12. 1.7.3 Virtual Memory Virtual memory is an abstraction that provides each process with the illusion that it has exclusive use of the main memory. Each process has the same uniform view of memory, which is known as its virtual address space. The virtual address space for Linux processes is shown in Figure 1.13. (Other Unix systems use a similar layout.) In Linux, the topmost region of the address space is reserved for code and data in the operating system that is common to all processes. The lower region of the address space holds the code and data deﬁned by the user’s process. Note that addresses in the ﬁgure increase from the bottom to the top. The virtual address space seen by each process consists of a number of well- deﬁned areas, each with a speciﬁc purpose. You will learn more about these areas later in the book, but it will be helpful to look brieﬂy at each, starting with the lowest addresses and working our way up: . Program code and data.Code begins at the same ﬁxed address for all processes, followed by data locations that correspond to global C variables. The code and data areas are initialized directly from the contents of an executable object ﬁle—in our case, the ³−ÄÄÇ executable. You will learn more about this part of the address space when we study linking and loading in Chapter 7. . Heap.The code and data areas are followed immediately by the run-time heap. Unlike the code and data areas, which are ﬁxed in size once the process begins Section 1.8 Systems Communicate with Other Systems Using Networks 55 running, the heap expands and contracts dynamically at run time as a result of calls to C standard library routines such as ÀþÄÄÇ² and ðË−−. We will study heaps in detail when we learn about managing virtual memory in Chapter 9. . Shared libraries. Near the middle of the address space is an area that holds the code and data for shared libraries such as the C standard library and the math library. The notion of a shared library is a powerful but somewhat difﬁcult concept. You will learn how they work when we study dynamic linking in Chapter 7. . Stack. At the top of the user’s virtual address space is the user stack that the compiler uses to implement function calls. Like the heap, the user stack expands and contracts dynamically during the execution of the program. In particular, each time we call a function, the stack grows. Each time we return from a function, it contracts. You will learn how the compiler uses the stack in Chapter 3. . Kernel virtual memory. The top region of the address space is reserved for the kernel. Application programs are not allowed to read or write the contents of this area or to directly call functions deﬁned in the kernel code. Instead, they must invoke the kernel to perform these operations. For virtual memory to work, a sophisticated interaction is required between the hardware and the operating system software, including a hardware translation of every address generated by the processor. The basic idea is to store the contents of a process’s virtual memory on disk and then use the main memory as a cache for the disk. Chapter 9 explains how this works and why it is so important to the operation of modern systems. 1.7.4 Files A ﬁle is a sequence of bytes, nothing more and nothing less. Every I/O device, including disks, keyboards, displays, and even networks, is modeled as a ﬁle. All input and output in the system is performed by reading and writing ﬁles, using a small set of system calls known as Unix I/O. This simple and elegant notion of a ﬁle is nonetheless very powerful because it provides applications with a uniform view of all the varied I/O devices that might be contained in the system. For example, application programmers who manipulate the contents of a disk ﬁle are blissfully unaware of the speciﬁc disk technology. Further, the same program will run on different systems that use different disk technologies. You will learn about Unix I/O in Chapter 10. 1.8 Systems Communicate with Other Systems Using Networks Up to this point in our tour of systems, we have treated a system as an isolated collection of hardware and software. In practice, modern systems are often linked to other systems by networks. From the point of view of an individual system, the 56 Chapter 1 A Tour of Computer Systems Aside The Linux project In August 1991, a Finnish graduate student named Linus Torvalds modestly announced a new Unix-like operating system kernel: ƒËÇÀx ÎÇËÌþÄ®Í~ÂÄþþÌþl¤−ÄÍ©ÅÂ©lƒ' f‹©ÅÏÍ ¢−Å−®©²Î ¶ÇËÌþÄ®Íg ﬁ−ÑÍ×ËÇÏÉÍx ²ÇÀÉlÇÍlÀ©Å©Ó ·Ï¾Á−²Îx „³þÎ ÑÇÏÄ® ÔÇÏ Ä©Â− ÎÇ Í−− ÀÇÍÎ ©Å À©Å©Ó} ·ÏÀÀþËÔx ÍÀþÄÄ ÉÇÄÄ ðÇË ÀÔ Å−Ñ ÇÉ−ËþÎ©Å× ÍÔÍÎ−À ⁄þÎ−x ps ¡Ï× wo pnxsuxnv §›¶ ¤−ÄÄÇ −Ì−ËÔ¾Ç®Ô ÇÏÎ Î³−Ë− ÏÍ©Å× À©Å©Ó k '’À ®Ç©Å× þ fðË−−g ÇÉ−ËþÎ©Å× ÍÔÍÎ−À fÁÏÍÎ þ ³Ç¾¾Ôj ÑÇÅ’Î ¾− ¾©× þÅ® ÉËÇð−ÍÍ©ÇÅþÄ Ä©Â− ×ÅÏg ðÇË qvtfrvtg ¡¶ ²ÄÇÅ−Íl ¶³©Í ³þÍ ¾−−Å ¾Ë−Ñ©Å× Í©Å²− ¡ÉË©Äj þÅ® ©Í ÍÎþËÎ©Å× ÎÇ ×−Î Ë−þ®Ôl '’® Ä©Â− þÅÔ ð−−®¾þ²Â ÇÅ Î³©Å×Í É−ÇÉÄ− Ä©Â−m®©ÍÄ©Â− ©Å À©Å©Ój þÍ ÀÔ ﬂ· Ë−Í−À¾Ä−Í ©Î ÍÇÀ−Ñ³þÎ fÍþÀ− É³ÔÍ©²þÄ ÄþÔÇÏÎ Çð Î³− ð©Ä−kÍÔÍÎ−À f®Ï− ÎÇ ÉËþ²Î©²þÄ Ë−þÍÇÅÍg þÀÇÅ× ÇÎ³−Ë Î³©Å×Ígl '’Ì− ²ÏËË−ÅÎÄÔ ÉÇËÎ−® ¾þÍ³folnvg þÅ® ×²²folrngj þÅ® Î³©Å×Í Í−−À ÎÇ ÑÇËÂl ¶³©Í ©ÀÉÄ©−Í Î³þÎ '’ÄÄ ×−Î ÍÇÀ−Î³©Å× ÉËþ²Î©²þÄ Ñ©Î³©Å þ ð−Ñ ÀÇÅÎ³Íj þÅ® '’® Ä©Â− ÎÇ ÂÅÇÑ Ñ³þÎ ð−þÎÏË−Í ÀÇÍÎ É−ÇÉÄ− ÑÇÏÄ® ÑþÅÎl ¡ÅÔ ÍÏ××−ÍÎ©ÇÅÍ þË− Ñ−Ä²ÇÀ−j ¾ÏÎ ' ÑÇÅ’Î ÉËÇÀ©Í− '’ÄÄ ©ÀÉÄ−À−ÅÎ Î³−À xkg ‹©ÅÏÍ fÎÇËÌþÄ®Í~ÂËÏÏÅþl³−ÄÍ©ÅÂ©lð©g As Torvalds indicates, his starting point for creating Linux was Minix, an operating system devel- oped by Andrew S. Tanenbaum for educational purposes [113]. The rest, as they say, is history. Linux has evolved into a technical and cultural phenomenon. By combining forces with the GNU project, the Linux project has developed a complete, Posix-compliant version of the Unix operating system, including the kernel and all of the supporting infrastructure. Linux is available on a wide array of computers, from handheld devices to mainframe computers. A group at IBM has even ported Linux to a wristwatch! network can be viewed as just another I/O device, as shown in Figure 1.14. When the system copies a sequence of bytes from main memory to the network adapter, the data ﬂow across the network to another machine, instead of, say, to a local disk drive. Similarly, the system can read data sent from other machines and copy these data to its main memory. With the advent of global networks such as the Internet, copying information from one machine to another has become one of the most important uses of computer systems. For example, applications such as email, instant messaging, the World Wide Web, FTP, and telnet are all based on the ability to copy information over a network. Section 1.8 Systems Communicate with Other Systems Using Networks 57 Figure 1.14 A network is another I/O device. CPU chip Register file PC ALU Bus interface I/O bridge System bus Memory bus Main memory I/O bus Expansion slots Disk controller Network adapter Network Graphics adapter MonitorMouse Keyboard USB controller Disk 1.User types “hello” at the keyboard 5. Client prints “hello, world\\n” string on display 2. Client sends “hello” string to telnet server 4. Telnet server sends “hello, world\\n” string to client 3. Server sends “hello” string to the shell, which runs the hello program and passes the output to the telnet server Local telnet client Remote telnet server Figure 1.15 Using telnet to run ³−ÄÄÇ remotely over a network. Returning to our ³−ÄÄÇ example, we could use the familiar telnet application to run ³−ÄÄÇ on a remote machine. Suppose we use a telnet client running on our local machine to connect to a telnet server on a remote machine. After we log in to the remote machine and run a shell, the remote shell is waiting to receive an input command. From this point, running the ³−ÄÄÇ program remotely involves the ﬁve basic steps shown in Figure 1.15. After we type in the ³−ÄÄÇ string to the telnet client and hit the −ÅÎ−Ë key, the client sends the string to the telnet server. After the telnet server receives the string from the network, it passes it along to the remote shell program. Next, the remote shell runs the ³−ÄÄÇ program and passes the output line back to the telnet server. Finally, the telnet server forwards the output string across the network to the telnet client, which prints the output string on our local terminal. This type of exchange between clients and servers is typical of all network applications. In Chapter 11 you will learn how to build network applications and apply this knowledge to build a simple Web server. 58 Chapter 1 A Tour of Computer Systems 1.9 Important Themes This concludes our initial whirlwind tour of systems. An important idea to take away from this discussion is that a system is more than just hardware. It is a collection of intertwined hardware and systems software that must cooperate in order to achieve the ultimate goal of running application programs. The rest of this book will ﬁll in some details about the hardware and the software, and it will show how, by knowing these details, you can write programs that are faster, more reliable, and more secure. To close out this chapter, we highlight several important concepts that cut across all aspects of computer systems. We will discuss the importance of these concepts at multiple places within the book. 1.9.1 Amdahl’s Law Gene Amdahl, one of the early pioneers in computing, made a simple but insight- ful observation about the effectiveness of improving the performance of one part of a system. This observation has come to be known as Amdahl’s law. The main idea is that when we speed up one part of a system, the effect on the overall sys- tem performance depends on both how signiﬁcant this part was and how much it sped up. Consider a system in which executing some application requires time Told. Suppose some part of the system requires a fraction α of this time, and that we improve its performance by a factor of k. That is, the component originally re- quired time αTold, and it now requires time (αTold)/k. The overall execution time would thus be Tnew = (1 − α)Told + (αTold)/k = Told[(1 − α) + α/k] From this, we can compute the speedup S = Told/Tnew as S = 1 (1 − α) + α/k (1.1) As an example, consider the case where a part of the system that initially consumed 60% of the time (α = 0.6) is sped up by a factor of 3 (k = 3). Then we get a speedup of 1/[0.4 + 0.6/3] = 1.67×. Even though we made a substantial improvement to a major part of the system, our net speedup was signiﬁcantly less than the speedup for the one part. This is the major insight of Amdahl’s law— to signiﬁcantly speed up the entire system, we must improve the speed of a very large fraction of the overall system. Practice Problem 1.1 (solution page 64) Suppose you work as a truck driver, and you have been hired to carry a load of potatoes from Boise, Idaho, to Minneapolis, Minnesota, a total distance of 2,500 kilometers. You estimate you can average 100 km/hr driving within the speed limits, requiring a total of 25 hours for the trip. Section 1.9 Important Themes 59 Aside Expressing relative performance The best way to express a performance improvement is as a ratio of the form Told/Tnew, where Told is the time required for the original version and Tnew is the time required by the modiﬁed version. This will be a number greater than 1.0 if any real improvement occurred. We use the sufﬁx ‘×’ to indicate such a ratio, where the factor “2.2×” is expressed verbally as “2.2 times.” The more traditional way of expressing relative change as a percentage works well when the change is small, but its deﬁnition is ambiguous. Should it be 100 . (Told − Tnew)/Tnew, or possibly 100 . (Told − Tnew)/Told, or something else? In addition, it is less instructive for large changes. Saying that “performance improved by 120%” is more difﬁcult to comprehend than simply saying that the performance improved by 2.2×. A. You hear on the news that Montana has just abolished its speed limit, which constitutes 1,500 km of the trip. Your truck can travel at 150 km/hr. What will be your speedup for the trip? B. You can buy a new turbocharger for your truck at www.fasttrucks.com. They stock a variety of models, but the faster you want to go, the more it will cost. How fast must you travel through Montana to get an overall speedup for your trip of 1.67×? Practice Problem 1.2 (solution page 64) A car manufacturing company has promised their customers that the next release of a new engine will show a 4× performance improvement. You have been as- signed the task of delivering on that promise. You have determined that only 90% of the engine can be improved. How much (i.e., what value of k) would you need to improve this part to meet the overall performance target of the engine? One interesting special case of Amdahl’s law is to consider the effect of setting k to ∞. That is, we are able to take some part of the system and speed it up to the point at which it takes a negligible amount of time. We then get S∞ = 1 (1 − α) (1.2) So, for example, if we can speed up 60% of the system to the point where it requires close to no time, our net speedup will still only be 1/0.4 = 2.5×. Amdahl’s law describes a general principle for improving any process. In addition to its application to speeding up computer systems, it can guide a company trying to reduce the cost of manufacturing razor blades, or a student trying to improve his or her grade point average. Perhaps it is most meaningful in the world 60 Chapter 1 A Tour of Computer Systems of computers, where we routinely improve performance by factors of 2 or more. Such high factors can only be achieved by optimizing large parts of a system. 1.9.2 Concurrency and Parallelism Throughout the history of digital computers, two demands have been constant forces in driving improvements: we want them to do more, and we want them to run faster. Both of these factors improve when the processor does more things at once. We use the term concurrency to refer to the general concept of a system with multiple, simultaneous activities, and the term parallelism to refer to the use of concurrency to make a system run faster. Parallelism can be exploited at multiple levels of abstraction in a computer system. We highlight three levels here, working from the highest to the lowest level in the system hierarchy. Thread-Level Concurrency Building on the process abstraction, we are able to devise systems where multiple programs execute at the same time, leading to concurrency. With threads, we can even have multiple control ﬂows executing within a single process. Support for concurrent execution has been found in computer systems since the advent of time-sharing in the early 1960s. Traditionally, this concurrent execution was only simulated, by having a single computer rapidly switch among its executing processes, much as a juggler keeps multiple balls ﬂying through the air. This form of concurrency allows multiple users to interact with a system at the same time, such as when many people want to get pages from a single Web server. It also allows a single user to engage in multiple tasks concurrently, such as having a Web browser in one window, a word processor in another, and streaming music playing at the same time. Until recently, most actual computing was done by a single processor, even if that processor had to switch among multiple tasks. This conﬁguration is known as a uniprocessor system. When we construct a system consisting of multiple processors all under the control of a single operating system kernel, we have a multiprocessor system. Such systems have been available for large-scale computing since the 1980s, but they have more recently become commonplace with the advent of multi-core processors and hyperthreading. Figure 1.16 shows a taxonomy of these different processor types. Multi-core processors have several CPUs (referred to as “cores”) integrated onto a single integrated-circuit chip. Figure 1.17 illustrates the organization of a Figure 1.16 Categorizing different processor conﬁgurations. Multiprocessors are becoming prevalent with the advent of multi- core processors and hyperthreading. All processors Uniprocessors Multiprocessors Multi- core Hyper- threaded Section 1.9 Important Themes 61 Figure 1.17 Multi-core processor organization. Four processor cores are integrated onto a single chip. Processor package Core 0 Core 3 . . . Regs L1 d-cache L2 unified cache L3 unified cache (shared by all cores) Main memory L1 i-cache Regs L1 d-cache L2 unified cache L1 i-cache typical multi-core processor, where the chip has four CPU cores, each with its own L1 and L2 caches, and with each L1 cache split into two parts—one to hold recently fetched instructions and one to hold data. The cores share higher levels of cache as well as the interface to main memory. Industry experts predict that they will be able to have dozens, and ultimately hundreds, of cores on a single chip. Hyperthreading, sometimes called simultaneous multi-threading, is a tech- nique that allows a single CPU to execute multiple ﬂows of control. It involves having multiple copies of some of the CPU hardware, such as program counters and register ﬁles, while having only single copies of other parts of the hardware, such as the units that perform ﬂoating-point arithmetic. Whereas a conventional processor requires around 20,000 clock cycles to shift between different threads, a hyperthreaded processor decides which of its threads to execute on a cycle-by- cycle basis. It enables the CPU to take better advantage of its processing resources. For example, if one thread must wait for some data to be loaded into a cache, the CPU can proceed with the execution of a different thread. As an example, the In- tel Core i7 processor can have each core executing two threads, and so a four-core system can actually execute eight threads in parallel. The use of multiprocessing can improve system performance in two ways. First, it reduces the need to simulate concurrency when performing multiple tasks. As mentioned, even a personal computer being used by a single person is expected to perform many activities concurrently. Second, it can run a single application program faster, but only if that program is expressed in terms of multiple threads that can effectively execute in parallel. Thus, although the principles of concur- rency have been formulated and studied for over 50 years, the advent of multi-core and hyperthreaded systems has greatly increased the desire to ﬁnd ways to write application programs that can exploit the thread-level parallelism available with 62 Chapter 1 A Tour of Computer Systems the hardware. Chapter 12 will look much more deeply into concurrency and its use to provide a sharing of processing resources and to enable more parallelism in program execution. Instruction-Level Parallelism At a much lower level of abstraction, modern processors can execute multiple instructions at one time, a property known as instruction-level parallelism.For example, early microprocessors, such as the 1978-vintage Intel 8086, required multiple (typically 3–10) clock cycles to execute a single instruction. More recent processors can sustain execution rates of 2–4 instructions per clock cycle. Any given instruction requires much longer from start to ﬁnish, perhaps 20 cycles or more, but the processor uses a number of clever tricks to process as many as 100 instructions at a time. In Chapter 4, we will explore the use of pipelining, where the actions required to execute an instruction are partitioned into different steps and the processor hardware is organized as a series of stages, each performing one of these steps. The stages can operate in parallel, working on different parts of different instructions. We will see that a fairly simple hardware design can sustain an execution rate close to 1 instruction per clock cycle. Processors that can sustain execution rates faster than 1 instruction per cycle are known as superscalar processors. Most modern processors support superscalar operation. In Chapter 5, we will describe a high-level model of such processors. We will see that application programmers can use this model to understand the performance of their programs. They can then write programs such that the gen- erated code achieves higher degrees of instruction-level parallelism and therefore runs faster. Single-Instruction, Multiple-Data (SIMD) Parallelism At the lowest level, many modern processors have special hardware that allows a single instruction to cause multiple operations to be performed in parallel, a mode known as single-instruction, multiple-data (SIMD) parallelism. For example, recent generations of Intel and AMD processors have instructions that can add 8 pairs of single-precision ﬂoating-point numbers (C data type ðÄÇþÎ) in parallel. These SIMD instructions are provided mostly to speed up applications that process image, sound, and video data. Although some compilers attempt to auto- matically extract SIMD parallelism from C programs, a more reliable method is to write programs using special vector data types supported in compilers such as gcc. We describe this style of programming in Web Aside opt:simd, as a supplement to the more general presentation on program optimization found in Chapter 5. 1.9.3 The Importance of Abstractions in Computer Systems The use of abstractions is one of the most important concepts in computer science. For example, one aspect of good programming practice is to formulate a simple application program interface (API) for a set of functions that allow programmers to use the code without having to delve into its inner workings. Different program- Section 1.10 Summary 63 Figure 1.18 Some abstractions provided by a computer system. A major theme in computer systems is to provide abstract representations at different levels to hide the complexity of the actual implementations. Main memory I/O devicesProcessorOperating system Processes Virtual memory Files Virtual machine Instruction set architecture ming languages provide different forms and levels of support for abstraction, such as Java class declarations and C function prototypes. We have already been introduced to several of the abstractions seen in com- puter systems, as indicated in Figure 1.18. On the processor side, the instruction set architecture provides an abstraction of the actual processor hardware. With this abstraction, a machine-code program behaves as if it were executed on a proces- sor that performs just one instruction at a time. The underlying hardware is far more elaborate, executing multiple instructions in parallel, but always in a way that is consistent with the simple, sequential model. By keeping the same execu- tion model, different processor implementations can execute the same machine code while offering a range of cost and performance. On the operating system side, we have introduced three abstractions: ﬁles as an abstraction of I/O devices, virtual memory as an abstraction of program mem- ory, and processes as an abstraction of a running program. To these abstractions we add a new one: the virtual machine, providing an abstraction of the entire computer, including the operating system, the processor, and the programs. The idea of a virtual machine was introduced by IBM in the 1960s, but it has become more prominent recently as a way to manage computers that must be able to run programs designed for multiple operating systems (such as Microsoft Windows, Mac OS X, and Linux) or different versions of the same operating system. We will return to these abstractions in subsequent sections of the book. 1.10 Summary A computer system consists of hardware and systems software that cooperate to run application programs. Information inside the computer is represented as groups of bits that are interpreted in different ways, depending on the context. Programs are translated by other programs into different forms, beginning as ASCII text and then translated by compilers and linkers into binary executable ﬁles. Processors read and interpret binary instructions that are stored in main mem- ory. Since computers spend most of their time copying data between memory, I/O devices, and the CPU registers, the storage devices in a system are arranged in a hi- erarchy, with the CPU registers at the top, followed by multiple levels of hardware cache memories, DRAM main memory, and disk storage. Storage devices that are higher in the hierarchy are faster and more costly per bit than those lower in the 64 Chapter 1 A Tour of Computer Systems hierarchy. Storage devices that are higher in the hierarchy serve as caches for de- vices that are lower in the hierarchy. Programmers can optimize the performance of their C programs by understanding and exploiting the memory hierarchy. The operating system kernel serves as an intermediary between the applica- tion and the hardware. It provides three fundamental abstractions: (1) Files are abstractions for I/O devices. (2) Virtual memory is an abstraction for both main memory and disks. (3) Processes are abstractions for the processor, main memory, and I/O devices. Finally, networks provide ways for computer systems to communicate with one another. From the viewpoint of a particular system, the network is just another I/O device. Bibliographic Notes Ritchie has written interesting ﬁrsthand accounts of the early days of C and Unix [91, 92]. Ritchie and Thompson presented the ﬁrst published account of Unix [93]. Silberschatz, Galvin, and Gagne [102] provide a comprehensive history of the different ﬂavors of Unix. The GNU (www.gnu.org) and Linux (www.linux .org) Web pages have loads of current and historical information. The Posix standards are available online at (www.unix.org). Solutions to Practice Problems Solution to Problem 1.1 (page 58) This problem illustrates that Amdahl’s law applies to more than just computer systems. A. In terms of Equation 1.1, we have α = 0.6 and k = 1.5. More directly, travel- ing the 1,500 kilometers through Montana will require 10 hours, and the rest of the trip also requires 10 hours. This will give a speedup of 25/(10 + 10) = 1.25×. B. In terms of Equation 1.1, we have α = 0.6, and we require S = 1.67, from which we can solve for k. More directly, to speed up the trip by 1.67×,we must decrease the overall time to 15 hours. The parts outside of Montana will still require 10 hours, so we must drive through Montana in 5 hours. This requires traveling at 300 km/hr, which is pretty fast for a truck! Solution to Problem 1.2 (page 59) Amdahl’s law is best understood by working through some examples. This one requires you to look at Equation 1.1 from an unusual perspective. This problem is a simple application of the equation. You are given S = 4 and α = 0.9, and you must then solve for k: 4 = 1/(1 − 0.9) + 0.9/k 0.4 + 3.6/k = 1.0 k = 6.0 Part I Program Structure and Execution O ur exploration of computer systems starts by studying the com- puter itself, comprising a processor and a memory subsystem. At the core, we require ways to represent basic data types, such as approximations to integer and real arithmetic. From there, we can con- sider how machine-level instructions manipulate data and how a com- piler translates C programs into these instructions. Next, we study several methods of implementing a processor to gain a better understanding of how hardware resources are used to execute instructions. Once we under- stand compilers and machine-level code, we can examine how to maxi- mize program performance by writing C programs that, when compiled, achieve the maximum possible performance. We conclude with the de- sign of the memory subsystem, one of the most complex components of a modern computer system. This part of the book will give you a deep understanding of how application programs are represented and executed. You will gain skills that help you write programs that are secure, reliable, and make the best use of the computing resources. 65 This page is intentionally left blank. CHAPTER 2 Representing and Manipulating Information 2.1 Information Storage 70 2.2 Integer Representations 95 2.3 Integer Arithmetic 120 2.4 Floating Point 144 2.5 Summary 162 Bibliographic Notes 163 Homework Problems 164 Solutions to Practice Problems 179 67 68 Chapter 2 Representing and Manipulating Information M odern computers store and process information represented as two-valued signals. These lowly binary digits, or bits, form the basis of the digital revo- lution. The familiar decimal, or base-10, representation has been in use for over 1,000 years, having been developed in India, improved by Arab mathematicians in the 12th century, and brought to the West in the 13th century by the Italian mathe- matician Leonardo Pisano (ca. 1170 to ca. 1250), better known as Fibonacci. Using decimal notation is natural for 10-ﬁngered humans, but binary values work better when building machines that store and process information. Two-valued signals can readily be represented, stored, and transmitted—for example, as the presence or absence of a hole in a punched card, as a high or low voltage on a wire, or as a magnetic domain oriented clockwise or counterclockwise. The electronic circuitry for storing and performing computations on two-valued signals is very simple and reliable, enabling manufacturers to integrate millions, or even billions, of such circuits on a single silicon chip. In isolation, a single bit is not very useful. When we group bits together and apply some interpretation that gives meaning to the different possible bit patterns, however, we can represent the elements of any ﬁnite set. For example, using a binary number system, we can use groups of bits to encode nonnegative numbers. By using a standard character code, we can encode the letters and symbols in a document. We cover both of these encodings in this chapter, as well as encodings to represent negative numbers and to approximate real numbers. We consider the three most important representations of numbers. Unsigned encodings are based on traditional binary notation, representing numbers greater than or equal to 0. Two’s-complement encodings are the most common way to represent signed integers, that is, numbers that may be either positive or negative. Floating-point encodings are a base-2 version of scientiﬁc notation for represent- ing real numbers. Computers implement arithmetic operations, such as addition and multiplication, with these different representations, similar to the correspond- ing operations on integers and real numbers. Computer representations use a limited number of bits to encode a number, and hence some operations can overﬂow when the results are too large to be rep- resented. This can lead to some surprising results. For example, on most of today’s computers (those using a 32-bit representation for data type ©ÅÎ), computing the expression pnn h qnn h rnn h snn yields −884,901,888. This runs counter to the properties of integer arithmetic— computing the product of a set of positive numbers has yielded a negative result. On the other hand, integer computer arithmetic satisﬁes many of the familiar properties of true integer arithmetic. For example, multiplication is associative and commutative, so that computing any of the following C expressions yields −884,901,888: fsnn h rnng h fqnn h pnng ffsnn h rnng h qnng h pnn ffpnn h snng h qnng h rnn rnn h fpnn h fqnn h snngg Chapter 2 Representing and Manipulating Information 69 The computer might not generate the expected result, but at least it is con- sistent! Floating-point arithmetic has altogether different mathematical properties. The product of a set of positive numbers will always be positive, although over- ﬂow will yield the special value +∞. Floating-point arithmetic is not associative due to the ﬁnite precision of the representation. For example, the C expression fqlorio−pngko−pn will evaluate to 0.0 on most machines, while qlorifo−pnk o−png will evaluate to 3.14. The different mathematical properties of integer versus ﬂoating-point arithmetic stem from the difference in how they handle the ﬁniteness of their representations—integer representations can encode a compar- atively small range of values, but do so precisely, while ﬂoating-point representa- tions can encode a wide range of values, but only approximately. By studying the actual number representations, we can understand the ranges of values that can be represented and the properties of the different arithmetic operations. This understanding is critical to writing programs that work correctly over the full range of numeric values and that are portable across different combi- nations of machine, operating system, and compiler. As we will describe, a number of computer security vulnerabilities have arisen due to some of the subtleties of computer arithmetic. Whereas in an earlier era program bugs would only incon- venience people when they happened to be triggered, there are now legions of hackers who try to exploit any bug they can ﬁnd to obtain unauthorized access to other people’s systems. This puts a higher level of obligation on programmers to understand how their programs work and how they can be made to behave in undesirable ways. Computers use several different binary representations to encode numeric values. You will need to be familiar with these representations as you progress into machine-level programming in Chapter 3. We describe these encodings in this chapter and show you how to reason about number representations. We derive several ways to perform arithmetic operations by directly ma- nipulating the bit-level representations of numbers. Understanding these tech- niques will be important for understanding the machine-level code generated by compilers in their attempt to optimize the performance of arithmetic expression evaluation. Our treatment of this material is based on a core set of mathematical prin- ciples. We start with the basic deﬁnitions of the encodings and then derive such properties as the range of representable numbers, their bit-level representations, and the properties of the arithmetic operations. We believe it is important for you to examine the material from this abstract viewpoint, because programmers need to have a clear understanding of how computer arithmetic relates to the more familiar integer and real arithmetic. The C++ programming language is built upon C, using the exact same numeric representations and operations. Everything said in this chapter about C also holds for C++. The Java language deﬁnition, on the other hand, created a new set of standards for numeric representations and operations. Whereas the C standards are designed to allow a wide range of implementations, the Java standard is quite speciﬁc on the formats and encodings of data. We highlight the representations and operations supported by Java at several places in the chapter. 70 Chapter 2 Representing and Manipulating Information Aside How to read this chapter In this chapter, we examine the fundamental properties of how numbers and other forms of data are represented on a computer and the properties of the operations that computers perform on these data. This requires us to delve into the language of mathematics, writing formulas and equations and showing derivations of important properties. To help you navigate this exposition, we have structured the presentation to ﬁrst state a property as a principle in mathematical notation. We then illustrate this principle with examples and an informal discussion. We recommend that you go back and forth between the statement of the principle and the examples and discussion until you have a solid intuition for what is being said and what is important about the property. For more complex properties, we also provide a derivation, structured much like a mathematical proof. You should try to understand these derivations eventually, but you could skip over them on ﬁrst reading. We also encourage you to work on the practice problems as you proceed through the presentation. The practice problems engage you in active learning, helping you put thoughts into action. With these as background, you will ﬁnd it much easier to go back and follow the derivations. Be assured, as well, that the mathematical skills required to understand this material are within reach of someone with a good grasp of high school algebra. 2.1 Information Storage Rather than accessing individual bits in memory, most computers use blocks of 8 bits, or bytes, as the smallest addressable unit of memory. A machine-level program views memory as a very large array of bytes, referred to as virtual memory. Every byte of memory is identiﬁed by a unique number, known as its address, and the set of all possible addresses is known as the virtual address space. As indicated by its name, this virtual address space is just a conceptual image presented to the machine-level program. The actual implementation (presented in Chapter 9) uses a combination of dynamic random access memory (DRAM), ﬂash memory, disk storage, special hardware, and operating system software to provide the program with what appears to be a monolithic byte array. In subsequent chapters, we will cover how the compiler and run-time system partitions this memory space into more manageable units to store the different program objects, that is, program data, instructions, and control information. Various mechanisms are used to allocate and manage the storage for different parts of the program. This management is all performed within the virtual address space. For example, the value of a pointer in C—whether it points to an integer, a structure, or some other program object—is the virtual address of the ﬁrst byte of some block of storage. The C compiler also associates type information with each pointer, so that it can generate different machine-level code to access the value stored at the location designated by the pointer depending on the type of that value. Although the C compiler maintains this type information, the actual machine-level program it generates has no information about data types. It simply treats each program object as a block of bytes and the program itself as a sequence of bytes. Section 2.1 Information Storage 71 Aside The evolution of the C programming language As was described in an aside on page 40, the C programming language was ﬁrst developed by Dennis Ritchie of Bell Laboratories for use with the Unix operating system (also developed at Bell Labs). At the time, most system programs, such as operating systems, had to be written largely in assembly code in order to have access to the low-level representations of different data types. For example, it was not feasible to write a memory allocator, such as is provided by the ÀþÄÄÇ² library function, in other high-level languages of that era. The original Bell Labs version of C was documented in the ﬁrst edition of the book by Brian Kernighan and Dennis Ritchie [60]. Over time, C has evolved through the efforts of several standard- ization groups. The ﬁrst major revision of the original Bell Labs C led to the ANSI C standard in 1989, by a group working under the auspices of the American National Standards Institute. ANSI C was a major departure from Bell Labs C, especially in the way functions are declared. ANSI C is described in the second edition of Kernighan and Ritchie’s book [61], which is still considered one of the best references on C. The International Standards Organization took over responsibility for standardizing the C lan- guage, adopting a version that was substantially the same as ANSI C in 1990 and hence is referred to as “ISO C90.” This same organization sponsored an updating of the language in 1999, yielding “ISO C99.” Among other things, this version introduced some new data types and provided support for text strings requiring characters not found in the English language. A more recent standard was approved in 2011, and hence is named “ISO C11,” again adding more data types and features. Most of these recent additions have been backward compatible, meaning that programs written according to the earlier standard (at least as far back as ISO C90) will have the same behavior when compiled according to the newer standards. The GNU Compiler Collection (gcc) can compile programs according to the conventions of several different versions of the C language, based on different command-line options, as shown in Figure 2.1. For example, to compile program ÉËÇ×l² according to ISO C11, we could give the command line Ä©ÅÏÓ| gcc -std=c11 prog.c The options kþÅÍ© and kÍÎ®{²vw have identical effect—the code is compiled according to the ANSI or ISO C90 standard. (C90 is sometimes referred to as “C89,” since its standardization effort began in 1989.) The option kÍÎ®{²ww causes the compiler to follow the ISO C99 convention. As of the writing of this book, when no option is speciﬁed, the program will be compiled according to a version of C based on ISO C90, but including some features of C99, some of C11, some of C++, and others speciﬁc to gcc. The GNU project is developing a version that combines ISO C11, plus other features, that can be speciﬁed with the command-line option kÍÎ®{×ÅÏoo. (Currently, this implementation is incomplete.) This will become the default version. C version gcc command-line option GNU 89 none, kÍÎ®{×ÅÏvw ANSI, ISO C90 kþÅÍ©, kÍÎ®{²vw ISO C99 kÍÎ®{²ww ISO C11 kÍÎ®{²oo Figure 2.1 Specifying different versions of C to gcc. 72 Chapter 2 Representing and Manipulating Information New to C? The role of pointers in C Pointers are a central feature of C. They provide the mechanism for referencing elements of data structures, including arrays. Just like a variable, a pointer has two aspects: its value and its type.The value indicates the location of some object, while its type indicates what kind of object (e.g., integer or ﬂoating-point number) is stored at that location. Truly understanding pointers requires examining their representation and implementation at the machine level. This will be a major focus in Chapter 3, culminating in an in-depth presentation in Section 3.10.1. 2.1.1 Hexadecimal Notation A single byte consists of 8 bits. In binary notation, its value ranges from 000000002 to 111111112. When viewed as a decimal integer, its value ranges from 010 to 25510. Neither notation is very convenient for describing bit patterns. Binary notation is too verbose, while with decimal notation it is tedious to convert to and from bit patterns. Instead, we write bit patterns as base-16, or hexadecimal numbers. Hexadecimal (or simply “hex”) uses digits ‘0’ through ‘9’ along with characters ‘A’ through ‘F’ to represent 16 possible values. Figure 2.2 shows the decimal and binary values associated with the 16 hexadecimal digits. Written in hexadecimal, the value of a single byte can range from 0016 to FF16. In C, numeric constants starting with nÓ or n” are interpreted as being in hexadecimal. The characters ‘A’ through ‘F’ may be written in either upper- or lowercase. For example, we could write the number FA1D37B16 as nÓƒ¡o⁄qu¢,as nÓðþo®qu¾, or even mixing upper- and lowercase (e.g., nÓƒþo⁄qu¾). We will use the C notation for representing hexadecimal values in this book. A common task in working with machine-level programs is to manually con- vert between decimal, binary, and hexadecimal representations of bit patterns. Converting between binary and hexadecimal is straightforward, since it can be performed one hexadecimal digit at a time. Digits can be converted by referring to a chart such as that shown in Figure 2.2. One simple trick for doing the conver- sion in your head is to memorize the decimal equivalents of hex digits ¡, £, and ƒ. Hex digit 01234567 Decimal value 01234567 Binary value 0000 0001 0010 0011 0100 0101 0110 0111 Hex digit 8 9 A B C D E F Decimal value 8 9 10 11 12 13 14 15 Binary value 1000 1001 1010 1011 1100 1101 1110 1111 Figure 2.2 Hexadecimal notation. Each hex digit encodes one of 16 values. Section 2.1 Information Storage 73 The hex values ¢, ⁄, and ¥ can be translated to decimal by computing their values relative to the ﬁrst three. For example, suppose you are given the number nÓouq¡r£. You can convert this to binary format by expanding each hexadecimal digit, as follows: Hexadecimal o u q¡r£ Binary 0001 0111 0011 1010 0100 1100 This gives the binary representation 000101110011101001001100. Conversely, given a binary number 1111001010110110110011, you convert it to hexadecimal by ﬁrst splitting it into groups of 4 bits each. Note, however, that if the total number of bits is not a multiple of 4, you should make the leftmost group be the one with fewer than 4 bits, effectively padding the number with leading zeros. Then you translate each group of bits into the corresponding hexadecimal digit: Binary 11 1100 1010 1101 1011 0011 Hexadecimal q £ ¡⁄¢q Practice Problem 2.1 (solution page 179) Perform the following number conversions: A. nÓps¢w⁄p to binary B. binary 1010111001001001 to hexadecimal C. nÓ¡v¢q⁄ to binary D. binary 1100100010110110010110 to hexadecimal When a value x is a power of 2, that is, x = 2n for some nonnegative integer n, we can readily write x in hexadecimal form by remembering that the binary representation of x is simply 1 followed by n zeros. The hexadecimal digit n represents 4 binary zeros. So, for n written in the form i + 4j , where 0 ≤ i ≤ 3, we can write x with a leading hex digit of o (i = 0), p (i = 1), r (i = 2), or v (i = 3), followed by j hexadecimal ns. As an example, for x = 2,048 = 211,we have n = 11 = 3 + 4 . 2, giving hexadecimal representation nÓvnn. Practice Problem 2.2 (solution page 179) Fill in the blank entries in the following table, giving the decimal and hexadecimal representations of different powers of 2: 74 Chapter 2 Representing and Manipulating Information n 2n (decimal) 2n (hexadecimal) 532 nÓpn 23 32,768 nÓpnnn 12 64 nÓonn Converting between decimal and hexadecimal representations requires using multiplication or division to handle the general case. To convert a decimal num- ber x to hexadecimal, we can repeatedly divide x by 16, giving a quotient q and a remainder r, such that x = q . 16 + r. We then use the hexadecimal digit represent- ing r as the least signiﬁcant digit and generate the remaining digits by repeating the process on q. As an example, consider the conversion of decimal 314,156: 314,156 = 19,634 . 16 + 12 (£) 19,634 = 1,227 . 16 + 2 (p) 1,227 = 76 . 16 + 11 (¢) 76 = 4 . 16 + 12 (£) 4 = 0 . 16 + 4 (r) From this we can read off the hexadecimal representation as nÓr£¢p£. Conversely, to convert a hexadecimal number to decimal, we can multiply each of the hexadecimal digits by the appropriate power of 16. For example, given the number nÓu¡ƒ, we compute its decimal equivalent as 7 . 162 + 10 . 16 + 15 = 7 . 256 + 10 . 16 + 15 = 1,792 + 160 + 15 = 1,967. Practice Problem 2.3 (solution page 180) A single byte can be represented by 2 hexadecimal digits. Fill in the missing entries in the following table, giving the decimal, binary, and hexadecimal values of different byte patterns: Decimal Binary Hexadecimal 0 0000 0000 nÓnn 158 76 145 1010 1110 0011 1100 1111 0001 Section 2.1 Information Storage 75 Aside Converting between decimal and hexadecimal For converting larger values between decimal and hexadecimal, it is best to let a computer or calculator do the work. There are numerous tools that can do this. One simple way is to use any of the standard search engines, with queries such as Convert 0xabcd to decimal or 123 in hex Decimal Binary Hexadecimal nÓus nÓ¢⁄ nÓƒs Practice Problem 2.4 (solution page 180) Without converting the numbers to decimal or binary, try to solve the following arithmetic problems, giving the answers in hexadecimal. Hint: Just modify the methods you use for performing decimal addition and subtraction to use base 16. A. nÓtns² + nÓs = B. nÓtns² − nÓpn = C. nÓtns² + 32 = D. nÓtnðþ − nÓtns² = 2.1.2 Data Sizes Every computer has a word size, indicating the nominal size of pointer data. Since a virtual address is encoded by such a word, the most important system parameter determined by the word size is the maximum size of the virtual address space. That is, for a machine with a w-bit word size, the virtual addresses can range from 0 to 2w − 1, giving the program access to at most 2w bytes. In recent years, there has been a widespread shift from machines with 32- bit word sizes to those with word sizes of 64 bits. This occurred ﬁrst for high-end machines designed for large-scale scientiﬁc and database applications, followed by desktop and laptop machines, and most recently for the processors found in smartphones. A 32-bit word size limits the virtual address space to 4 gigabytes (written 4 GB), that is, just over 4 × 109 bytes. Scaling up to a 64-bit word size leads to a virtual address space of 16 exabytes, or around 1.84 × 1019 bytes. 76 Chapter 2 Representing and Manipulating Information Most 64-bit machines can also run programs compiled for use on 32-bit ma- chines, a form of backward compatibility. So, for example, when a program ÉËÇ×l² is compiled with the directive Ä©ÅÏÓ| gcc -m32 prog.c then this program will run correctly on either a 32-bit or a 64-bit machine. On the other hand, a program compiled with the directive Ä©ÅÏÓ| gcc -m64 prog.c will only run on a 64-bit machine. We will therefore refer to programs as being either “32-bit programs” or “64-bit programs,” since the distinction lies in how a program is compiled, rather than the type of machine on which it runs. Computers and compilers support multiple data formats using different ways to encode data, such as integers and ﬂoating point, as well as different lengths. For example, many machines have instructions for manipulating single bytes, as well as integers represented as 2-, 4-, and 8-byte quantities. They also support ﬂoating-point numbers represented as 4- and 8-byte quantities. The C language supports multiple data formats for both integer and ﬂoating- point data. Figure 2.3 shows the number of bytes typically allocated for different C data types. (We discuss the relation between what is guaranteed by the C standard versus what is typical in Section 2.2.) The exact numbers of bytes for some data types depends on how the program is compiled. We show sizes for typical 32-bit and 64-bit programs. Integer data can be either signed, able to represent negative, zero, and positive values, or unsigned, only allowing nonnegative values. Data type ²³þË represents a single byte. Although the name ²³þË derives from the fact that it is used to store a single character in a text string, it can also be used to store integer values. Data types Í³ÇËÎ, ©ÅÎ, and ÄÇÅ× are intended to provide a range of C declaration Bytes Signed Unsigned 32-bit 64-bit [Í©×Å−®] ²³þË ÏÅÍ©×Å−® ²³þË 11 Í³ÇËÎ ÏÅÍ©×Å−® Í³ÇËÎ 22 ©ÅÎ ÏÅÍ©×Å−® 44 ÄÇÅ× ÏÅÍ©×Å−® ÄÇÅ× 48 ©ÅÎqpˆÎ Ï©ÅÎqpˆÎ 44 ©ÅÎtrˆÎ Ï©ÅÎtrˆÎ 88 ²³þË h 48 ðÄÇþÎ 44 ®ÇÏ¾Ä− 88 Figure 2.3 Typical sizes (in bytes) of basic C data types. The number of bytes allocated varies with how the program is compiled. This chart shows the values typical of 32-bit and 64-bit programs. Section 2.1 Information Storage 77 New to C? Declaring pointers For any data type T , the declaration T hÉy indicates that É is a pointer variable, pointing to an object of type T . For example, ²³þË hÉy is the declaration of a pointer to an object of type ²³þË. sizes. Even when compiled for 64-bit systems, data type ©ÅÎ is usually just 4 bytes. Data type ÄÇÅ× commonly has 4 bytes in 32-bit programs and 8 bytes in 64-bit programs. To avoid the vagaries of relying on “typical” sizes and different compiler set- tings, ISO C99 introduced a class of data types where the data sizes are ﬁxed regardless of compiler and machine settings. Among these are data types ©ÅÎqpˆÎ and ©ÅÎtrˆÎ, having exactly 4 and 8 bytes, respectively. Using ﬁxed-size integer types is the best way for programmers to have close control over data represen- tations. Most of the data types encode signed values, unless preﬁxed by the keyword ÏÅÍ©×Å−® or using the speciﬁc unsigned declaration for ﬁxed-size data types. The exception to this is data type ²³þË. Although most compilers and machines treat these as signed data, the C standard does not guarantee this. Instead, as indicated by the square brackets, the programmer should use the declaration Í©×Å−® ²³þË to guarantee a 1-byte signed value. In many contexts, however, the program’s behavior is insensitive to whether data type ²³þË is signed or unsigned. The C language allows a variety of ways to order the keywords and to include or omit optional keywords. As examples, all of the following declarations have identical meaning: ÏÅÍ©×Å−® ÄÇÅ× ÏÅÍ©×Å−® ÄÇÅ× ©ÅÎ ÄÇÅ× ÏÅÍ©×Å−® ÄÇÅ× ÏÅÍ©×Å−® ©ÅÎ We will consistently use the forms found in Figure 2.3. Figure 2.3 also shows that a pointer (e.g., a variable declared as being of type ²³þË h) uses the full word size of the program. Most machines also support two different ﬂoating-point formats: single precision, declared in C as ðÄÇþÎ, and double precision, declared in C as ®ÇÏ¾Ä−. These formats use 4 and 8 bytes, respectively. Programmers should strive to make their programs portable across different machines and compilers. One aspect of portability is to make the program insensi- tive to the exact sizes of the different data types. The C standards set lower bounds 78 Chapter 2 Representing and Manipulating Information on the numeric ranges of the different data types, as will be covered later, but there are no upper bounds (except with the ﬁxed-size types). With 32-bit machines and 32-bit programs being the dominant combination from around 1980 until around 2010, many programs have been written assuming the allocations listed for 32- bit programs in Figure 2.3. With the transition to 64-bit machines, many hidden word size dependencies have arisen as bugs in migrating these programs to new machines. For example, many programmers historically assumed that an object declared as type ©ÅÎ could be used to store a pointer. This works ﬁne for most 32-bit programs, but it leads to problems for 64-bit programs. 2.1.3 Addressing and Byte Ordering For program objects that span multiple bytes, we must establish two conventions: what the address of the object will be, and how we will order the bytes in memory. In virtually all machines, a multi-byte object is stored as a contiguous sequence of bytes, with the address of the object given by the smallest address of the bytes used. For example, suppose a variable Ó of type ©ÅÎ has address nÓonn; that is, the value of the address expression dÓ is nÓonn. Then (assuming data type ©ÅÎ has a 32-bit representation) the 4 bytes of Ó would be stored in memory locations nÓonn, nÓono, nÓonp, and nÓonq. For ordering the bytes representing an object, there are two common conven- tions. Consider a w-bit integer having a bit representation [xw−1,xw−2,. . .,x1,x0], where xw−1 is the most signiﬁcant bit and x0 is the least. Assuming w is a multiple of 8, these bits can be grouped as bytes, with the most signiﬁcant byte having bits [xw−1,xw−2,...,xw−8], the least signiﬁcant byte having bits [x7,x6,...,x0], and the other bytes having bits from the middle. Some machines choose to store the ob- ject in memory ordered from least signiﬁcant byte to most, while other machines store them from most to least. The former convention—where the least signiﬁcant byte comes ﬁrst—is referred to as little endian. The latter convention—where the most signiﬁcant byte comes ﬁrst—is referred to as big endian. Suppose the variable Ó of type ©ÅÎ and at address nÓonn has a hexadecimal value of nÓnopqrstu. The ordering of the bytes within the address range nÓonn through nÓonq depends on the type of machine: 01 0x100 23 0x101 45 0x102 67 . . .. . . 0x103 Big endian 67 0x100 45 0x101 23 0x102 01 . . .. . . 0x103 Little endian Note that in the word nÓnopqrstu the high-order byte has hexadecimal value nÓno, while the low-order byte has value nÓtu. Most Intel-compatible machines operate exclusively in little-endian mode. On the other hand, most machines from IBM and Oracle (arising from their acquisi- Section 2.1 Information Storage 79 Aside Origin of “endian” Here is how Jonathan Swift, writing in 1726, described the history of the controversy between big and little endians: . . . Lilliput and Blefuscu... have, as I was going to tell you, been engaged in a most obstinate war for six-and-thirty moons past. It began upon the following occasion. It is allowed on all hands, that the primitive way of breaking eggs, before we eat them, was upon the larger end; but his present majesty’s grandfather, while he was a boy, going to eat an egg, and breaking it according to the ancient practice, happened to cut one of his ﬁngers. Whereupon the emperor his father published an edict, commanding all his subjects, upon great penalties, to break the smaller end of their eggs. The people so highly resented this law, that our histories tell us, there have been six rebellions raised on that account; wherein one emperor lost his life, and another his crown. These civil commotions were constantly fomented by the monarchs of Blefuscu; and when they were quelled, the exiles always ﬂed for refuge to that empire. It is computed that eleven thousand persons have at several times suffered death, rather than submit to break their eggs at the smaller end. Many hundred large volumes have been published upon this controversy: but the books of the Big-endians have been long forbidden, and the whole party rendered incapable by law of holding employments. (Jonathan Swift. Gulliver’s Travels, Benjamin Motte, 1726.) In his day, Swift was satirizing the continued conﬂicts between England (Lilliput) and France (Blefuscu). Danny Cohen, an early pioneer in networking protocols, ﬁrst applied these terms to refer to byte ordering [24], and the terminology has been widely adopted. tion of Sun Microsystems in 2010) operate in big-endian mode. Note that we said “most.” The conventions do not split precisely along corporate boundaries. For example, both IBM and Oracle manufacture machines that use Intel-compatible processors and hence are little endian. Many recent microprocessor chips are bi-endian, meaning that they can be conﬁgured to operate as either little- or big-endian machines. In practice, however, byte ordering becomes ﬁxed once a particular operating system is chosen. For example, ARM microprocessors, used in many cell phones, have hardware that can operate in either little- or big-endian mode, but the two most common operating systems for these chips—Android (from Google) and IOS (from Apple)—operate only in little-endian mode. People get surprisingly emotional about which byte ordering is the proper one. In fact, the terms “little endian” and “big endian” come from the book Gulliver’s Travels by Jonathan Swift, where two warring factions could not agree as to how a soft-boiled egg should be opened—by the little end or by the big. Just like the egg issue, there is no technological reason to choose one byte ordering convention over the other, and hence the arguments degenerate into bickering about sociopolitical issues. As long as one of the conventions is selected and adhered to consistently, the choice is arbitrary. For most application programmers, the byte orderings used by their machines are totally invisible; programs compiled for either class of machine give identi- cal results. At times, however, byte ordering becomes an issue. The ﬁrst is when 80 Chapter 2 Representing and Manipulating Information binary data are communicated over a network between different machines. A common problem is for data produced by a little-endian machine to be sent to a big-endian machine, or vice versa, leading to the bytes within the words being in reverse order for the receiving program. To avoid such problems, code written for networking applications must follow established conventions for byte order- ing to make sure the sending machine converts its internal representation to the network standard, while the receiving machine converts the network standard to its internal representation. We will see examples of these conversions in Chap- ter 11. A second case where byte ordering becomes important is when looking at the byte sequences representing integer data. This occurs often when inspecting machine-level programs. As an example, the following line occurs in a ﬁle that gives a text representation of the machine-level code for an Intel x86-64 processor: rnnr®qx no ns rq n¾ pn nn þ®® c−þÓjnÓpnn¾rqfcË©Ég This line was generated by a disassembler, a tool that determines the instruction sequence represented by an executable program ﬁle. We will learn more about disassemblers and how to interpret lines such as this in Chapter 3. For now, we simply note that this line states that the hexadecimal byte sequence no ns rq n¾ pn nn is the byte-level representation of an instruction that adds a word of data to the value stored at an address computed by adding nÓpnn¾rq to the current value of the program counter, the address of the next instruction to be executed. If we take the ﬁnal 4 bytes of the sequence rq n¾ pn nn and write them in reverse order, we have nn pn n¾ rq. Dropping the leading 0, we have the value nÓpnn¾rq, the numeric value written on the right. Having bytes appear in reverse order is a common occurrence when reading machine-level program representations generated for little-endian machines such as this one. The natural way to write a byte sequence is to have the lowest-numbered byte on the left and the highest on the right, but this is contrary to the normal way of writing numbers with the most signiﬁcant digit on the left and the least on the right. A third case where byte ordering becomes visible is when programs are written that circumvent the normal type system. In the C language, this can be done using a cast or a union to allow an object to be referenced according to a different data type from which it was created. Such coding tricks are strongly discouraged for most application programming, but they can be quite useful and even necessary for system-level programming. Figure 2.4 shows C code that uses casting to access and print the byte rep- resentations of different program objects. We use ÎÔÉ−®−ð to deﬁne data type ¾ÔÎ−ˆÉÇ©ÅÎ−Ë as a pointer to an object of type ÏÅÍ©×Å−® ²³þË. Such a byte pointer references a sequence of bytes where each byte is considered to be a nonnega- tive integer. The ﬁrst routine Í³ÇÑˆ¾ÔÎ−Í is given the address of a sequence of bytes, indicated by a byte pointer, and a byte count. The byte count is speciﬁed as having data type Í©Ö−ˆÎ, the preferred data type for expressing the sizes of data structures. It prints the individual bytes in hexadecimal. The C formatting direc- tive clpÓ indicates that an integer should be printed in hexadecimal with at least 2 digits. Section 2.1 Information Storage 81 1 a©Å²ÄÏ®− zÍÎ®©Çl³| 2 3 ÎÔÉ−®−ð ÏÅÍ©×Å−® ²³þË h¾ÔÎ−ˆÉÇ©ÅÎ−Ëy 4 5 ÌÇ©® Í³ÇÑˆ¾ÔÎ−Íf¾ÔÎ−ˆÉÇ©ÅÎ−Ë ÍÎþËÎj Í©Ö−ˆÎ Ä−Åg Õ 6 ©ÅÎ ©y 7 ðÇË f© { ny © z Ä−Åy ©iig 8 ÉË©ÅÎðf‘ clpÓ‘j ÍÎþËÎ‰©`gy 9 ÉË©ÅÎðf‘¿Å‘gy 10 Û 11 12 ÌÇ©® Í³ÇÑˆ©ÅÎf©ÅÎ Óg Õ 13 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÓj Í©Ö−Çðf©ÅÎggy 14 Û 15 16 ÌÇ©® Í³ÇÑˆðÄÇþÎfðÄÇþÎ Óg Õ 17 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÓj Í©Ö−ÇðfðÄÇþÎggy 18 Û 19 20 ÌÇ©® Í³ÇÑˆÉÇ©ÅÎ−ËfÌÇ©® hÓg Õ 21 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÓj Í©Ö−ÇðfÌÇ©® hggy 22 Û Figure 2.4 Code to print the byte representation of program objects. This code uses casting to circumvent the type system. Similar functions are easily deﬁned for other data types. Procedures Í³ÇÑˆ©ÅÎ, Í³ÇÑˆðÄÇþÎ, and Í³ÇÑˆÉÇ©ÅÎ−Ë demonstrate how to use procedure Í³ÇÑˆ¾ÔÎ−Í to print the byte representations of C program objects of type ©ÅÎ, ðÄÇþÎ, and ÌÇ©® h, respectively. Observe that they simply pass Í³ÇÑˆ ¾ÔÎ−Í a pointer dÓ to their argument Ó, casting the pointer to be of type ÏÅÍ©×Å−® ²³þË h. This cast indicates to the compiler that the program should consider the pointer to be to a sequence of bytes rather than to an object of the original data type. This pointer will then be to the lowest byte address occupied by the object. These procedures use the C Í©Ö−Çð operator to determine the number of bytes used by the object. In general, the expression Í©Ö−ÇðfT g returns the number of bytes required to store an object of type T . Using Í©Ö−Çð rather than a ﬁxed value is one step toward writing code that is portable across different machine types. We ran the code shown in Figure 2.5 on several different machines, giving the results shown in Figure 2.6. The following machines were used: Linux 32 Intel IA32 processor running Linux. Windows Intel IA32 processor running Windows. Sun Sun Microsystems SPARC processor running Solaris. (These machines are now produced by Oracle.) Linux 64 Intel x86-64 processor running Linux. 82 Chapter 2 Representing and Manipulating Information code/data/show-bytes.c 1 ÌÇ©® Î−ÍÎˆÍ³ÇÑˆ¾ÔÎ−Íf©ÅÎ ÌþÄg Õ 2 ©ÅÎ ©ÌþÄ { ÌþÄy 3 ðÄÇþÎ ðÌþÄ { fðÄÇþÎg ©ÌþÄy 4 ©ÅÎ hÉÌþÄ { d©ÌþÄy 5 Í³ÇÑˆ©ÅÎf©ÌþÄgy 6 Í³ÇÑˆðÄÇþÎfðÌþÄgy 7 Í³ÇÑˆÉÇ©ÅÎ−ËfÉÌþÄgy 8 Û code/data/show-bytes.c Figure 2.5 Byte representation examples. This code prints the byte representations of sample data objects. Machine Value Type Bytes (hex) Linux 32 12,345 ©ÅÎ qw qn nn nn Windows 12,345 ©ÅÎ qw qn nn nn Sun 12,345 ©ÅÎ nn nn qn qw Linux 64 12,345 ©ÅÎ qw qn nn nn Linux 32 12,345.0 ðÄÇþÎ nn −r rn rt Windows 12,345.0 ðÄÇþÎ nn −r rn rt Sun 12,345.0 ðÄÇþÎ rt rn −r nn Linux 64 12,345.0 ðÄÇþÎ nn −r rn rt Linux 32 d©ÌþÄ ©ÅÎ h −r ðw ðð ¾ð Windows d©ÌþÄ ©ÅÎ h ¾r ²² pp nn Sun d©ÌþÄ ©ÅÎ h −ð ðð ðþ n² Linux 64 d©ÌþÄ ©ÅÎ h ¾v oo −s ðð ðð uð nn nn Figure 2.6 Byte representations of different data values. Results for ©ÅÎ and ðÄÇþÎ are identical, except for byte ordering. Pointer values are machine dependent. Our argument 12,345 has hexadecimal representation nÓnnnnqnqw. For the ©ÅÎ data, we get identical results for all machines, except for the byte ordering. In particular, we can see that the least signiﬁcant byte value of nÓqw is printed ﬁrst for Linux 32, Windows, and Linux 64, indicating little-endian machines, and last for Sun, indicating a big-endian machine. Similarly, the bytes of the ðÄÇþÎ data are identical, except for the byte ordering. On the other hand, the pointer values are completely different. The different machine/operating system conﬁgurations use different conventions for storage allocation. One feature to note is that the Linux 32, Windows, and Sun machines use 4-byte addresses, while the Linux 64 machine uses 8-byte addresses. Section 2.1 Information Storage 83 New to C? Naming data types with ÎÔÉ−®−ð The ÎÔÉ−®−ð declaration in C provides a way of giving a name to a data type. This can be a great help in improving code readability, since deeply nested type declarations can be difﬁcult to decipher. The syntax for ÎÔÉ−®−ð is exactly like that of declaring a variable, except that it uses a type name rather than a variable name. Thus, the declaration of ¾ÔÎ−ˆÉÇ©ÅÎ−Ë in Figure 2.4 has the same form as the declaration of a variable of type ÏÅÍ©×Å−® ²³þË h. For example, the declaration ÎÔÉ−®−ð ©ÅÎ h©ÅÎˆÉÇ©ÅÎ−Ëy ©ÅÎˆÉÇ©ÅÎ−Ë ©Éy deﬁnes type ©ÅÎˆÉÇ©ÅÎ−Ë to be a pointer to an ©ÅÎ, and declares a variable ©É of this type. Alternatively, we could declare this variable directly as ©ÅÎ h©Éy New to C? Formatted printing with ÉË©ÅÎð The ÉË©ÅÎð function (along with its cousins ðÉË©ÅÎð and ÍÉË©ÅÎð) provides a way to print information with considerable control over the formatting details. The ﬁrst argument is a format string, while any remaining arguments are values to be printed. Within the format string, each character sequence starting with ‘c’ indicates how to format the next argument. Typical examples include c® to print a decimal integer, cð to print a ﬂoating-point number, and c² to print a character having the character code given by the argument. Specifying the formatting of ﬁxed-size data types, such as ©ÅÎˆqpÎ, is a bit more involved, as is described in the aside on page 103. Observe that although the ﬂoating-point and the integer data both encode the numeric value 12,345, they have very different byte patterns: nÓnnnnqnqw for the integer and nÓrtrn¥rnn for ﬂoating point. In general, these two formats use different encoding schemes. If we expand these hexadecimal patterns into binary form and shift them appropriately, we ﬁnd a sequence of 13 matching bits, indicated by a sequence of asterisks, as follows: nnnnqnqw nnnnnnnnnnnnnnnnnnoonnnnnnooonno hhhhhhhhhhhhh rtrn¥rnn nonnnoonnonnnnnnooonnonnnnnnnnnn This is not coincidental. We will return to this example when we study ﬂoating- point formats. 84 Chapter 2 Representing and Manipulating Information New to C? Pointers and arrays In function Í³ÇÑˆ¾ÔÎ−Í (Figure 2.4), we see the close connection between pointers and arrays, as will be discussed in detail in Section 3.8. We see that this function has an argument ÍÎþËÎ of type ¾ÔÎ−ˆ ÉÇ©ÅÎ−Ë (which has been deﬁned to be a pointer to ÏÅÍ©×Å−® ²³þË), but we see the array reference ÍÎþËÎ‰©` on line 8. In C, we can dereference a pointer with array notation, and we can reference array elements with pointer notation. In this example, the reference ÍÎþËÎ‰©` indicates that we want to read the byte that is © positions beyond the location pointed to by ÍÎþËÎ. New to C? Pointer creation and dereferencing In lines 13, 17, and 21 of Figure 2.4 we see uses of two operations that give C (and therefore C++) its distinctive character. The C “address of” operator ‘d’ creates a pointer. On all three lines, the expression dÓ creates a pointer to the location holding the object indicated by variable Ó. The type of this pointer depends on the type of Ó, and hence these three pointers are of type ©ÅÎ h, ðÄÇþÎ h, and ÌÇ©® hh, respectively. (Data type ÌÇ©® h is a special kind of pointer with no associated type information.) The cast operator converts from one data type to another. Thus, the cast f¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÓ indicates that whatever type the pointer dÓ had before, the program will now reference a pointer to data of type ÏÅÍ©×Å−® ²³þË. The casts shown here do not change the actual pointer; they simply direct the compiler to refer to the data being pointed to according to the new data type. Aside Generating an ASCII table You can display a table showing the ASCII character code by executing the command ÀþÅ þÍ²©©. Practice Problem 2.5 (solution page 180) Consider the following three calls to Í³ÇÑˆ¾ÔÎ−Í: ©ÅÎ þ { nÓopqrstuvy ¾ÔÎ−ˆÉÇ©ÅÎ−Ë þÉ { f¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dþy Í³ÇÑˆ¾ÔÎ−ÍfþÉj ogy mh ¡l hm Í³ÇÑˆ¾ÔÎ−ÍfþÉj pgy mh ¢l hm Í³ÇÑˆ¾ÔÎ−ÍfþÉj qgy mh £l hm Indicate the values that will be printed by each call on a little-endian machine and on a big-endian machine: A. Little endian: Big endian: B. Little endian: Big endian: C. Little endian: Big endian: Section 2.1 Information Storage 85 Practice Problem 2.6 (solution page 181) Using Í³ÇÑˆ©ÅÎ and Í³ÇÑˆðÄÇþÎ, we determine that the integer 2607352 has hexa- decimal representation nÓnnpu£vƒv, while the ﬂoating-point number 3510593.0 has hexadecimal representation nÓr¡oƒpq¥n. A. Write the binary representations of these two hexadecimal values. B. Shift these two strings relative to one another to maximize the number of matching bits. How many bits match? C. What parts of the strings do not match? 2.1.4 Representing Strings A string in C is encoded by an array of characters terminated by the null (having value 0) character. Each character is represented by some standard encoding, with the most common being the ASCII character code. Thus, if we run our routine Í³ÇÑˆ¾ÔÎ−Í with arguments ‘opqrs‘ and t (to include the terminating character), we get the result qo qp qq qr qs nn. Observe that the ASCII code for decimal digit x happens to be nÓqx, and that the terminating byte has the hex representation nÓnn. This same result would be obtained on any system using ASCII as its character code, independent of the byte ordering and word size conventions. As a consequence, text data are more platform independent than binary data. Practice Problem 2.7 (solution page 181) What would be printed as a result of the following call to Í³ÇÑˆ¾ÔÎ−Í? ²ÇÅÍÎ ²³þË hÀ { ‘ÀÅÇÉÊË‘y Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg Àj ÍÎËÄ−ÅfÀggy Note that letters ‘þ’ through ‘Ö’ have ASCII codes nÓto through nÓu¡. 2.1.5 Representing Code Consider the following C function: 1 ©ÅÎ ÍÏÀf©ÅÎ Ój ©ÅÎ Ôg Õ 2 Ë−ÎÏËÅÓiÔy 3 Û When compiled on our sample machines, we generate machine code having the following byte representations: Linux 32 ss vw −s v¾ rs n² nq rs nv ²w ²q Windows ss vw −s v¾ rs n² nq rs nv s® ²q Sun vo ²q −n nv wn np nn nw Linux 64 ss rv vw −s vw u® ð² vw us ðv nq rs ð² ²w ²q 86 Chapter 2 Representing and Manipulating Information Aside The Unicode standard for text encoding The ASCII character set is suitable for encoding English-language documents, but it does not have much in the way of special characters, such as the French ‘¸c’. It is wholly unsuited for encoding documents in languages such as Greek, Russian, and Chinese. Over the years, a variety of methods have been developed to encode text for different languages. The Unicode Consortium has devised the most comprehensive and widely accepted standard for encoding text. The current Unicode standard (version 7.0) has a repertoire of over 100,000 characters supporting a wide range of languages, including the ancient languages of Egypt and Babylon. To their credit, the Unicode Technical Committee rejected a proposal to include a standard writing for Klingon, a ﬁctional civilization from the television series Star Trek. The base encoding, known as the “Universal Character Set” of Unicode, uses a 32-bit representa- tion of characters. This would seem to require every string of text to consist of 4 bytes per character. However, alternative codings are possible where common characters require just 1 or 2 bytes, while less common ones require more. In particular, the UTF-8 representation encodes each character as a sequence of bytes, such that the standard ASCII characters use the same single-byte encodings as they have in ASCII, implying that all ASCII byte sequences have the same meaning in UTF-8 as they do in ASCII. The Java programming language uses Unicode in its representations of strings. Program libraries are also available for C to support Unicode. Here we ﬁnd that the instruction codings are different. Different machine types use different and incompatible instructions and encodings. Even identical proces- sors running different operating systems have differences in their coding conven- tions and hence are not binary compatible. Binary code is seldom portable across different combinations of machine and operating system. A fundamental concept of computer systems is that a program, from the perspective of the machine, is simply a sequence of bytes. The machine has no information about the original source program, except perhaps some auxiliary tables maintained to aid in debugging. We will see this more clearly when we study machine-level programming in Chapter 3. 2.1.6 Introduction to Boolean Algebra Since binary values are at the core of how computers encode, store, and manipu- late information, a rich body of mathematical knowledge has evolved around the study of the values 0 and 1. This started with the work of George Boole (1815– 1864) around 1850 and thus is known as Boolean algebra. Boole observed that by encoding logic values true and false as binary values 1 and 0, he could formulate an algebra that captures the basic principles of logical reasoning. The simplest Boolean algebra is deﬁned over the two-element set {0, 1}. Figure 2.7 deﬁnes several operations in this algebra. Our symbols for representing these operations are chosen to match those used by the C bit-level operations, Section 2.1 Information Storage 87 Üd 01 Ú 01 ´ 01 01 00 0 00 1 00 1 10 10 1 11 1 11 0 Figure 2.7 Operations of Boolean algebra. Binary values 1 and 0 encode logic values true and false, while operations Ü, d, Ú, and ´ encode logical operations not, and, or, and exclusive-or, respectively. as will be discussed later. The Boolean operation Ü corresponds to the logical operation not, denoted by the symbol ¬. That is, we say that ¬P is true when P is not true, and vice versa. Correspondingly, Üp equals 1 when p equals 0, and vice versa. Boolean operation d corresponds to the logical operation and, denoted by the symbol ∧. We say that P ∧ Q holds when both P is true and Q is true. Correspondingly, p d q equals 1 only when p = 1 and q = 1. Boolean operation Ú corresponds to the logical operation or, denoted by the symbol ∨. We say that P ∨ Q holds when either P is true or Q is true. Correspondingly, p Ú q equals 1 when either p = 1or q = 1. Boolean operation ´ corresponds to the logical operation exclusive-or, denoted by the symbol ⊕. We say that P ⊕ Q holds when either P is true or Q is true, but not both. Correspondingly, p ´ q equals 1 when either p = 1 and q = 0, or p = 0 and q = 1. Claude Shannon (1916–2001), who later founded the ﬁeld of information theory, ﬁrst made the connection between Boolean algebra and digital logic. In his 1937 master’s thesis, he showed that Boolean algebra could be applied to the design and analysis of networks of electromechanical relays. Although computer technology has advanced considerably since, Boolean algebra still plays a central role in the design and analysis of digital systems. We can extend the four Boolean operations to also operate on bit vectors, strings of zeros and ones of some ﬁxed length w. We deﬁne the operations over bit vectors according to their applications to the matching elements of the arguments. Let a and b denote the bit vectors [aw−1,aw−2,...,a0] and [bw−1,bw−2,...,b0], respectively. We deﬁne a d b to also be a bit vector of length w, where the ith element equals ai d bi, for 0 ≤ i< w. The operations Ú, ´, and Ü are extended to bit vectors in a similar fashion. As examples, consider the case where w = 4, and with arguments a = [0110] and b = [1100]. Then the four operations a d b, a Ú b, a ´ b, and Üb yield 0110 0110 0110 d 1100 Ú 1100 ´ 1100 Ü 1100 0100 1110 1010 0011 Practice Problem 2.8 (solution page 181) Fill in the following table showing the results of evaluating Boolean operations on bit vectors. 88 Chapter 2 Representing and Manipulating Information Web Aside DATA:BOOL More on Boolean algebra and Boolean rings The Boolean operations Ú, d, and Ü operating on bit vectors of length w form a Boolean algebra, for any integer w> 0. The simplest is the case where w = 1 and there are just two elements, but for the more general case there are 2w bit vectors of length w. Boolean algebra has many of the same properties as arithmetic over integers. For example, just as multiplication distributes over addition, written a . (b + c) = (a . b) + (a . c), Boolean operation d distributes over Ú, written a d (b Ú c) = (a d b) Ú (a d c). In addition, however. Boolean operation Ú distributes over d, and so we can write a Ú (b d c) = (a Ú b) d (a Ú c), whereas we cannot say that a + (b . c) = (a + b) . (a + c) holds for all integers. When we consider operations ´, d, and Ü operating on bit vectors of length w, we get a different mathematical form, known as a Boolean ring. Boolean rings have many properties in common with integer arithmetic. For example, one property of integer arithmetic is that every value x has an additive inverse −x, such that x +−x = 0. A similar property holds for Boolean rings, where ´ is the “addition” operation, but in this case each element is its own additive inverse. That is, a ´ a = 0 for any value a, where we use 0 here to represent a bit vector of all zeros. We can see this holds for single bits, since 0 ´ 0 = 1 ´ 1 = 0, and it extends to bit vectors as well. This property holds even when we rearrange terms and combine them in a different order, and so (a ´ b) ´ a = b. This property leads to some interesting results and clever tricks, as we will explore in Problem 2.10. Operation Result a [01001110] b [11100001] Üa Üb a d b a Ú b a ´ b One useful application of bit vectors is to represent ﬁnite sets. We can encode any subset A ⊆{0, 1, ... , w − 1} with a bit vector [aw−1,. . .,a1,a0], where ai = 1if and only if i ∈ A. For example, recalling that we write aw−1 on the left and a0 on the right, bit vector a = [01101001] encodes the set A ={0, 3, 5, 6}, while bit vector b = [01010101] encodes the set B ={0, 2, 4, 6}. With this way of encoding sets, Boolean operations Ú and d correspond to set union and intersection, respectively, and Ü corresponds to set complement. Continuing our earlier example, the operation a d b yields bit vector [01000001], while A ∩ B ={0, 6}. We will see the encoding of sets by bit vectors in a number of practical applications. For example, in Chapter 8, we will see that there are a number of different signals that can interrupt the execution of a program. We can selectively enable or disable different signals by specifying a bit-vector mask, wherea1in bit position i indicates that signal i is enabled and a 0 indicates that it is disabled. Thus, the mask represents the set of enabled signals. Section 2.1 Information Storage 89 Practice Problem 2.9 (solution page 182) Computers generate color pictures on a video screen or liquid crystal display by mixing three different colors of light: red, green, and blue. Imagine a simple scheme, with three different lights, each of which can be turned on or off, project- ing onto a glass screen: Light sources Glass screen Observer Red Green Blue We can then create eight different colors based on the absence (0) or presence (1) of light sources R, G, and B: RG B Color 0 0 0 Black 0 0 1 Blue 0 1 0 Green 0 1 1 Cyan 1 0 0 Red 1 0 1 Magenta 1 1 0 Yellow 1 1 1 White Each of these colors can be represented as a bit vector of length 3, and we can apply Boolean operations to them. A. The complement of a color is formed by turning off the lights that are on and turning on the lights that are off. What would be the complement of each of the eight colors listed above? B. Describe the effect of applying Boolean operations on the following colors: Blue Ú Green = Yellow d Cyan = Red ´ Magenta = 90 Chapter 2 Representing and Manipulating Information 2.1.7 Bit-Level Operations in C One useful feature of C is that it supports bitwise Boolean operations. In fact, the symbols we have used for the Boolean operations are exactly those used by C: Ú for or, d for and, Ü for not, and ´ for exclusive-or. These can be applied to any “integral” data type, including all of those listed in Figure 2.3. Here are some examples of expression evaluation for data type ²³þË: C expression Binary expression Binary result Hexadecimal result ÜnÓro Ü[0100 0001] [1011 1110] nÓ¢¥ ÜnÓnn Ü[0000 0000] [1111 1111] nÓƒƒ nÓtw d nÓss [0110 1001] d [0101 0101] [0100 0001] nÓro nÓtw Ú nÓss [0110 1001] Ú [0101 0101] [0111 1101] nÓu⁄ As our examples show, the best way to determine the effect of a bit-level ex- pression is to expand the hexadecimal arguments to their binary representations, perform the operations in binary, and then convert back to hexadecimal. Practice Problem 2.10 (solution page 182) As an application of the property that a ´ a = 0 for any bit vector a, consider the following program: 1 ÌÇ©® ©ÅÉÄþ²−ˆÍÑþÉf©ÅÎ hÓj ©ÅÎ hÔg Õ 2 hÔ { hÓ ´ hÔy mh ·Î−É o hm 3 hÓ { hÓ ´ hÔy mh ·Î−É p hm 4 hÔ { hÓ ´ hÔy mh ·Î−É q hm 5 Û As the name implies, we claim that the effect of this procedure is to swap the values stored at the locations denoted by pointer variables Ó and Ô. Note that unlike the usual technique for swapping two values, we do not need a third location to temporarily store one value while we are moving the other. There is no performance advantage to this way of swapping; it is merely an intellectual amusement. Starting with values a and b in the locations pointed to by Ó and Ô, respectively, ﬁll in the table that follows, giving the values stored at the two locations after each step of the procedure. Use the properties of ´ to show that the desired effect is achieved. Recall that every element is its own additive inverse (that is, a ´ a = 0). Step hÓ hÔ Initially ab Step 1 Step 2 Step 3 Section 2.1 Information Storage 91 Practice Problem 2.11 (solution page 182) Armed with the function ©ÅÉÄþ²−ˆÍÑþÉ from Problem 2.10, you decide to write code that will reverse the elements of an array by swapping elements from opposite ends of the array, working toward the middle. You arrive at the following function: 1 ÌÇ©® Ë−Ì−ËÍ−ˆþËËþÔf©ÅÎ þ‰`j ©ÅÎ ²ÅÎg Õ 2 ©ÅÎ ð©ËÍÎj ÄþÍÎy 3 ðÇË fð©ËÍÎ { nj ÄþÍÎ { ²ÅÎkoy 4 ð©ËÍÎ z{ ÄþÍÎy 5 ð©ËÍÎiijÄþÍÎkkg 6 ©ÅÉÄþ²−ˆÍÑþÉfdþ‰ð©ËÍÎ`j dþ‰ÄþÍÎ`gy 7 Û When you apply your function to an array containing elements 1, 2, 3, and 4, you ﬁnd the array now has, as expected, elements 4, 3, 2, and 1. When you try it on an array with elements 1, 2, 3, 4, and 5, however, you are surprised to see that the array now has elements 5, 4, 0, 2, and 1. In fact, you discover that the code always works correctly on arrays of even length, but it sets the middle element to 0 whenever the array has odd length. A. For an array of odd length ²ÅÎ = 2k + 1, what are the values of variables ð©ËÍÎ and ÄþÍÎ in the ﬁnal iteration of function Ë−Ì−ËÍ−ˆþËËþÔ? B. Why does this call to function ©ÅÉÄþ²−ˆÍÑþÉ set the array element to 0? C. What simple modiﬁcation to the code for Ë−Ì−ËÍ−ˆþËËþÔ would eliminate this problem? One common use of bit-level operations is to implement masking operations, where a mask is a bit pattern that indicates a selected set of bits within a word. As an example, the mask nÓƒƒ (having ones for the least signiﬁcant 8 bits) indicates the low-order byte of a word. The bit-level operation Ó d nÓƒƒ yields a value consisting of the least signiﬁcant byte of Ó, but with all other bytes set to 0. For example, with Ó = nÓvw¡¢£⁄¥ƒ, the expression would yield nÓnnnnnn¥ƒ.The expression Ün will yield a mask of all ones, regardless of the size of the data representation. The same mask can be written nÓƒƒƒƒƒƒƒƒ when data type ©ÅÎ is 32 bits, but it would not be as portable. Practice Problem 2.12 (solution page 182) Write C expressions, in terms of variable Ó, for the following values. Your code should work for any word size w ≥ 8. For reference, we show the result of evalu- ating the expressions for Ó = nÓvutsrqpo, with w = 32. A. The least signiﬁcant byte of Ó, with all other bits set to 0. [nÓnnnnnnpo] B. All but the least signiﬁcant byte of Ó complemented, with the least signiﬁcant byte left unchanged. [nÓuvw¡¢£po] 92 Chapter 2 Representing and Manipulating Information C. The least signiﬁcant byte set to all ones, and all other bytes of Ó left un- changed. [nÓvutsrqƒƒ] Practice Problem 2.13 (solution page 183) The Digital Equipment VAX computer was a very popular machine from the late 1970s until the late 1980s. Rather than instructions for Boolean operations and and or, it had instructions ¾©Í (bit set) and ¾©² (bit clear). Both instructions take a data word Ó and a mask word À. They generate a result Ö consisting of the bits of Ó modiﬁed according to the bits of À. With ¾©Í, the modiﬁcation involves setting Ö to 1 at each bit position where À is 1. With ¾©², the modiﬁcation involves setting Ö to 0 at each bit position where À is 1. To see how these operations relate to the C bit-level operations, assume we have functions ¾©Í and ¾©² implementing the bit set and bit clear operations, and that we want to use these to implement functions computing bitwise operations Ú and ´, without using any other C operations. Fill in the missing code below. Hint: Write C expressions for the operations ¾©Í and ¾©². mh ⁄−²ÄþËþÎ©ÇÅÍ Çð ðÏÅ²Î©ÇÅÍ ©ÀÉÄ−À−ÅÎ©Å× ÇÉ−ËþÎ©ÇÅÍ ¾©Í þÅ® ¾©² hm ©ÅÎ ¾©Íf©ÅÎ Ój ©ÅÎ Àgy ©ÅÎ ¾©²f©ÅÎ Ój ©ÅÎ Àgy mh £ÇÀÉÏÎ− ÓÚÔ ÏÍ©Å× ÇÅÄÔ ²þÄÄÍ ÎÇ ðÏÅ²Î©ÇÅÍ ¾©Í þÅ® ¾©² hm ©ÅÎ ¾ÇÇÄˆÇËf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ Ë−ÍÏÄÎ { y Ë−ÎÏËÅ Ë−ÍÏÄÎy Û mh £ÇÀÉÏÎ− Ó´Ô ÏÍ©Å× ÇÅÄÔ ²þÄÄÍ ÎÇ ðÏÅ²Î©ÇÅÍ ¾©Í þÅ® ¾©² hm ©ÅÎ ¾ÇÇÄˆÓÇËf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ Ë−ÍÏÄÎ { y Ë−ÎÏËÅ Ë−ÍÏÄÎy Û 2.1.8 Logical Operations in C C also provides a set of logical operators ÚÚ, dd, and _, which correspond to the or, and, and not operations of logic. These can easily be confused with the bit- level operations, but their behavior is quite different. The logical operations treat any nonzero argument as representing true and argument 0 as representing false. They return either 1 or 0, indicating a result of either true or false, respectively. Here are some examples of expression evaluation: Section 2.1 Information Storage 93 Expression Result _nÓro nÓnn _nÓnn nÓno __nÓro nÓno nÓtw dd nÓss nÓno nÓtw ÚÚ nÓss nÓno Observe that a bitwise operation will have behavior matching that of its logical counterpart only in the special case in which the arguments are restricted to 0 or 1. A second important distinction between the logical operators ‘dd’ and ‘ÚÚ’ versus their bit-level counterparts ‘d’ and ‘Ú’ is that the logical operators do not evaluate their second argument if the result of the expression can be determined by evaluating the ﬁrst argument. Thus, for example, the expression þddsmþ will never cause a division by zero, and the expression É dd hÉii will never cause the dereferencing of a null pointer. Practice Problem 2.14 (solution page 183) Suppose that þ and ¾ have byte values nÓss and nÓrt, respectively. Fill in the following table indicating the byte values of the different C expressions: Expression Value Expression Value þd¾ þdd¾ þÚ¾ þÚÚ¾ ÜþÚÜ¾ _þ ÚÚ _¾ þd_¾ þddÜ¾ Practice Problem 2.15 (solution page 184) Using only bit-level and logical operations, write a C expression that is equivalent to Ó{{Ô. In other words, it will return 1 when Ó and Ô are equal and 0 otherwise. 2.1.9 Shift Operations in C C also provides a set of shift operations for shifting bit patterns to the left and to the right. For an operand Ó having bit representation [xw−1,xw−2,...,x0], the C expression ÓzzÂ yields a value with bit representation [xw−k−1,xw−k−2,...,x0, 0,..., 0]. That is, Ó is shifted k bits to the left, dropping off the k most signiﬁcant bits and ﬁlling the right end with k zeros. The shift amount should be a value between 0 and w − 1. Shift operations associate from left to right, so ÓzzÁzzÂ is equivalent to fÓ zz Ág zz Â. There is a corresponding right shift operation, written in C as Ó||Â, but it has a slightly subtle behavior. Generally, machines support two forms of right shift: 94 Chapter 2 Representing and Manipulating Information Logical. A logical right shift ﬁlls the left end with k zeros, giving a result [0,..., 0,xw−1,xw−2,...xk]. Arithmetic. An arithmetic right shift ﬁlls the left end with k repetitions of the most signiﬁcant bit, giving a result [xw−1,. . .,xw−1,xw−1,xw−2, ... xk]. This convention might seem peculiar, but as we will see, it is useful for operating on signed integer data. As examples, the following table shows the effect of applying the different shift operations to two different values of an 8-bit argument x: Operation Value 1 Value 2 Argument Ó [01100011] [10010101] Ózzr [00110000] [01010000] Ó||r (logical) [00000110] [00001001] Ó||r (arithmetic) [00000110] [11111001] The italicized digits indicate the values that ﬁll the right (left shift) or left (right shift) ends. Observe that all but one entry involves ﬁlling with zeros. The exception is the case of shifting [10010101] right arithmetically. Since its most signiﬁcant bit is 1, this will be used as the ﬁll value. The C standards do not precisely deﬁne which type of right shift should be used with signed numbers—either arithmetic or logical shifts may be used. This unfortunately means that any code assuming one form or the other will potentially encounter portability problems. In practice, however, almost all compiler/machine combinations use arithmetic right shifts for signed data, and many programmers assume this to be the case. For unsigned data, on the other hand, right shifts must be logical. In contrast to C, Java has a precise deﬁnition of how right shifts should be performed. The expression Ó||Â shifts Ó arithmetically by Â positions, while Ó ||| Â shifts it logically. Practice Problem 2.16 (solution page 184) Fill in the table below showing the effects of the different shift operations on single- byte quantities. The best way to think about shift operations is to work with binary representations. Convert the initial values to binary, perform the shifts, and then convert back to hexadecimal. Each of the answers should be 8 binary digits or 2 hexadecimal digits. Logical Arithmetic þ þ zz p þ || q þ || q Hex Binary Binary Hex Binary Hex Binary Hex nÓ⁄r nÓtr nÓup nÓrr Section 2.2 Integer Representations 95 Aside Shifting by k, for large values of k For a data type consisting of w bits, what should be the effect of shifting by some value k ≥ w?For example, what should be the effect of computing the following expressions, assuming data type ©ÅÎ has w = 32: ©ÅÎ ÄÌþÄ { nÓƒ¥⁄£¢¡wv zz qpy ©ÅÎ þÌþÄ { nÓƒ¥⁄£¢¡wv || qty ÏÅÍ©×Å−® ÏÌþÄ { nÓƒ¥⁄£¢¡wvÏ || rny The C standards carefully avoid stating what should be done in such a case. On many machines, the shift instructions consider only the lower log2 w bits of the shift amount when shifting a w-bit value, and so the shift amount is computed as k mod w. For example, with w = 32, the above three shifts would be computed as if they were by amounts 0, 4, and 8, respectively, giving results ÄÌþÄ nÓƒ¥⁄£¢¡wv þÌþÄ nÓƒƒ¥⁄£¢¡w ÏÌþÄ nÓnnƒ¥⁄£¢¡ This behavior is not guaranteed for C programs, however, and so shift amounts should be kept less than the word size. Java, on the other hand, speciﬁcally requires that shift amounts should be computed in the modular fashion we have shown. Aside Operator precedence issues with shift operations It might be tempting to write the expression ozzp i qzzr, intending it to mean fozzpg i fqzzrg. How- ever, in C the former expression is equivalent to o zz fpiqg zz r, since addition (and subtraction) have higher precedence than shifts. The left-to-right associativity rule then causes this to be parenthesized as fo zz fpiqgg zz r, giving value 512, rather than the intended 52. Getting the precedence wrong in C expressions is a common source of program errors, and often these are difﬁcult to spot by inspection. When in doubt, put in parentheses! 2.2 Integer Representations In this section, we describe two different ways bits can be used to encode integers— one that can only represent nonnegative numbers, and one that can represent negative, zero, and positive numbers. We will see later that they are strongly related both in their mathematical properties and their machine-level implemen- tations. We also investigate the effect of expanding or shrinking an encoded integer to ﬁt a representation with a different length. Figure 2.8 lists the mathematical terminology we introduce to precisely de- ﬁne and characterize how computers encode and operate on integer data. This 96 Chapter 2 Representing and Manipulating Information Symbol Type Meaning Page B2T w Function Binary to two’s complement 100 B2U w Function Binary to unsigned 98 U2Bw Function Unsigned to binary 100 U2T w Function Unsigned to two’s complement 107 T2Bw Function Two’s complement to binary 101 T2U w Function Two’s complement to unsigned 107 TMinw Constant Minimum two’s-complement value 101 TMaxw Constant Maximum two’s-complement value 101 UMaxw Constant Maximum unsigned value 99 it w Operation Two’s-complement addition 126 iu w Operation Unsigned addition 121 ht w Operation Two’s-complement multiplication 133 hu w Operation Unsigned multiplication 132 kt w Operation Two’s-complement negation 131 ku w Operation Unsigned negation 125 Figure 2.8 Terminology for integer data and arithmetic operations. The subscript w denotes the number of bits in the data representation. The “Page” column indicates the page on which the term is deﬁned. terminology will be introduced over the course of the presentation. The ﬁgure is included here as a reference. 2.2.1 Integral Data Types C supports a variety of integral data types—ones that represent ﬁnite ranges of integers. These are shown in Figures 2.9 and 2.10, along with the ranges of values they can have for “typical” 32- and 64-bit programs. Each type can specify a size with keyword ²³þË, Í³ÇËÎ, ÄÇÅ×, as well as an indication of whether the represented numbers are all nonnegative (declared as ÏÅÍ©×Å−®), or possibly negative (the default.) As we saw in Figure 2.3, the number of bytes allocated for the different sizes varies according to whether the program is compiled for 32 or 64 bits. Based on the byte allocations, the different sizes allow different ranges of values to be represented. The only machine-dependent range indicated is for size designator ÄÇÅ×. Most 64-bit programs use an 8-byte representation, giving a much wider range of values than the 4-byte representation used with 32-bit programs. One important feature to note in Figures 2.9 and 2.10 is that the ranges are not symmetric—the range of negative numbers extends one further than the range of positive numbers. We will see why this happens when we consider how negative numbers are represented. Section 2.2 Integer Representations 97 C data type Minimum Maximum [Í©×Å−®] ²³þË −128 127 ÏÅÍ©×Å−® ²³þË 0 255 Í³ÇËÎ −32,768 32,767 ÏÅÍ©×Å−® Í³ÇËÎ 0 65,535 ©ÅÎ −2,147,483,648 2,147,483,647 ÏÅÍ©×Å−® 0 4,294,967,295 ÄÇÅ× −2,147,483,648 2,147,483,647 ÏÅÍ©×Å−® ÄÇÅ× 0 4,294,967,295 ©ÅÎqpˆÎ −2,147,483,648 2,147,483,647 Ï©ÅÎqpˆÎ 0 4,294,967,295 ©ÅÎtrˆÎ −9,223,372,036,854,775,808 9,223,372,036,854,775,807 Ï©ÅÎtrˆÎ 0 18,446,744,073,709,551,615 Figure 2.9 Typical ranges for C integral data types for 32-bit programs. C data type Minimum Maximum [Í©×Å−®] ²³þË −128 127 ÏÅÍ©×Å−® ²³þË 0 255 Í³ÇËÎ −32,768 32,767 ÏÅÍ©×Å−® Í³ÇËÎ 0 65,535 ©ÅÎ −2,147,483,648 2,147,483,647 ÏÅÍ©×Å−® 0 4,294,967,295 ÄÇÅ× −9,223,372,036,854,775,808 9,223,372,036,854,775,807 ÏÅÍ©×Å−® ÄÇÅ× 0 18,446,744,073,709,551,615 ©ÅÎqpˆÎ −2,147,483,648 2,147,483,647 Ï©ÅÎqpˆÎ 0 4,294,967,295 ©ÅÎtrˆÎ −9,223,372,036,854,775,808 9,223,372,036,854,775,807 Ï©ÅÎtrˆÎ 0 18,446,744,073,709,551,615 Figure 2.10 Typical ranges for C integral data types for 64-bit programs. The C standards deﬁne minimum ranges of values that each data type must be able to represent. As shown in Figure 2.11, their ranges are the same or smaller than the typical implementations shown in Figures 2.9 and 2.10. In particular, with the exception of the ﬁxed-size data types, we see that they require only a 98 Chapter 2 Representing and Manipulating Information New to C? Signed and unsigned numbers in C, C++, and Java Both C and C++ support signed (the default) and unsigned numbers. Java supports only signed numbers. C data type Minimum Maximum [Í©×Å−®] ²³þË −127 127 ÏÅÍ©×Å−® ²³þË 0 255 Í³ÇËÎ −32,767 32,767 ÏÅÍ©×Å−® Í³ÇËÎ 0 65,535 ©ÅÎ −32,767 32,767 ÏÅÍ©×Å−® 0 65,535 ÄÇÅ× −2,147,483,647 2,147,483,647 ÏÅÍ©×Å−® ÄÇÅ× 0 4,294,967,295 ©ÅÎqpˆÎ −2,147,483,648 2,147,483,647 Ï©ÅÎqpˆÎ 0 4,294,967,295 ©ÅÎtrˆÎ −9,223,372,036,854,775,808 9,223,372,036,854,775,807 Ï©ÅÎtrˆÎ 0 18,446,744,073,709,551,615 Figure 2.11 Guaranteed ranges for C integral data types. The C standards require that the data types have at least these ranges of values. symmetric range of positive and negative numbers. We also see that data type ©ÅÎ could be implemented with 2-byte numbers, although this is mostly a throwback to the days of 16-bit machines. We also see that size ÄÇÅ× can be implemented with 4-byte numbers, and it typically is for 32-bit programs. The ﬁxed-size data types guarantee that the ranges of values will be exactly those given by the typical numbers of Figure 2.9, including the asymmetry between negative and positive. 2.2.2 Unsigned Encodings Let us consider an integer data type of w bits. We write a bit vector as either ⃗x,to denote the entire vector, or as [xw−1,xw−2,...,x0] to denote the individual bits within the vector. Treating ⃗x as a number written in binary notation, we obtain the unsigned interpretation of ⃗x. In this encoding, each bit xi has value 0 or 1, with the latter case indicating that value 2i should be included as part of the numeric value. We can express this interpretation as a function B2U w (for “binary to unsigned,” length w): Section 2.2 Integer Representations 99 Figure 2.12 Unsigned number examples for w = 4. When bit i in the binary representation has value 1, it contributes 2i to the value. 161514131211109876543210 20 = 1 2 1 = 2 2 2 = 4 2 3 = 8 [0001] [0101] [1011] [1111] principle: Deﬁnition of unsigned encoding For vector ⃗x = [xw−1,xw−2,...,x0]: B2U w(⃗x) .= w−1∑ i=0 xi2i (2.1) In this equation, the notation .= means that the left-hand side is deﬁned to be equal to the right-hand side. The function B2U w maps strings of zeros and ones of length w to nonnegative integers. As examples, Figure 2.12 shows the mapping, given by B2U, from bit vectors to integers for the following cases: B2U 4([0001]) = 0 . 23 + 0 . 22 + 0 . 21 + 1 . 20 = 0 + 0 + 0 + 1 = 1 B2U 4([0101]) = 0 . 23 + 1 . 22 + 0 . 21 + 1 . 20 = 0 + 4 + 0 + 1 = 5 B2U 4([1011]) = 1 . 23 + 0 . 22 + 1 . 21 + 1 . 20 = 8 + 0 + 2 + 1 = 11 B2U 4([1111]) = 1 . 23 + 1 . 22 + 1 . 21 + 1 . 20 = 8 + 4 + 2 + 1 = 15 (2.2) In the ﬁgure, we represent each bit position i by a rightward-pointing blue bar of length 2i. The numeric value associated with a bit vector then equals the sum of the lengths of the bars for which the corresponding bit values are 1. Let us consider the range of values that can be represented using w bits. The least value is given by bit vector [00 ... 0] having integer value 0, and the greatest value is given by bit vector [11 ... 1] having integer value UMaxw .= ∑w−1 i=0 2i = 2w − 1. Using the 4-bit case as an example, we have UMax4 = B2U 4([1111]) = 24 − 1 = 15. Thus, the function B2U w can be deﬁned as a mapping B2U w: {0, 1}w → {0,..., UMaxw}. The unsigned binary representation has the important property that every number between 0 and 2w − 1 has a unique encoding as a w-bit value. For example, 100 Chapter 2 Representing and Manipulating Information there is only one representation of decimal value 11 as an unsigned 4-bit number— namely, [1011]. We highlight this as a mathematical principle, which we ﬁrst state and then explain. principle: Uniqueness of unsigned encoding Function B2U w is a bijection. The mathematical term bijection refers to a function f that goes two ways: it maps a value x to a value y where y = f(x), but it can also operate in reverse, since for every y, there is a unique value x such that f(x) = y. This is given by the inverse function f −1, where, for our example, x = f −1(y). The function B2U w maps each bit vector of length w to a unique number between 0 and 2w − 1, and it has an inverse, which we call U2Bw (for “unsigned to binary”), that maps each number in the range 0 to 2w − 1 to a unique pattern of w bits. 2.2.3 Two’s-Complement Encodings For many applications, we wish to represent negative values as well. The most com- mon computer representation of signed numbers is known as two’s-complement form. This is deﬁned by interpreting the most signiﬁcant bit of the word to have negative weight. We express this interpretation as a function B2T w (for “binary to two’s complement” length w): principle: Deﬁnition of two’s-complement encoding For vector ⃗x = [xw−1,xw−2,...,x0]: B2T w(⃗x) .= −xw−12w−1 + w−2∑ i=0 xi2i (2.3) The most signiﬁcant bit xw−1 is also called the sign bit. Its “weight” is −2w−1, the negation of its weight in an unsigned representation. When the sign bit is set to 1, the represented value is negative, and when set to 0, the value is nonnegative. As examples, Figure 2.13 shows the mapping, given by B2T, from bit vectors to integers for the following cases: B2T 4([0001]) =−0 . 23 + 0 . 22 + 0 . 21 + 1 . 20 = 0 + 0 + 0 + 1 = 1 B2T 4([0101]) =−0 . 23 + 1 . 22 + 0 . 21 + 1 . 20 = 0 + 4 + 0 + 1 = 5 B2T 4([1011]) =−1 . 23 + 0 . 22 + 1 . 21 + 1 . 20 =−8 + 0 + 2 + 1 =−5 B2T 4([1111]) =−1 . 23 + 1 . 22 + 1 . 21 + 1 . 20 =−8 + 4 + 2 + 1 =−1 (2.4) In the ﬁgure, we indicate that the sign bit has negative weight by showing it as a leftward-pointing gray bar. The numeric value associated with a bit vector is then given by the combination of the possible leftward-pointing gray bar and the rightward-pointing blue bars. Section 2.2 Integer Representations 101 Figure 2.13 Two’s-complement number examples for w = 4. Bit 3 serves as a sign bit; when set to 1, it contributes −23 =−8 to the value. This weighting is shown as a leftward- pointing gray bar. 876543210–1–2–3–4–5–6–7–8 2 0 = 1 2 1 = 2 2 2 = 4 –23 = –8 [0001] [0101] [1011] [1111] We see that the bit patterns are identical for Figures 2.12 and 2.13 (as well as for Equations 2.2 and 2.4), but the values differ when the most signiﬁcant bit is 1, since in one case it has weight +8, and in the other case it has weight −8. Let us consider the range of values that can be represented as a w-bit two’s- complement number. The least representable value is given by bit vector [10 ... 0] (set the bit with negative weight but clear all others), having integer value TMinw .= −2w−1. The greatest value is given by bit vector [01 ... 1] (clear the bit with negative weight but set all others), having integer value TMaxw .= ∑w−2 i=0 2i = 2w−1 − 1. Using the 4-bit case as an example, we have TMin4 = B2T 4([1000]) = −23 =−8 and TMax4 = B2T 4([0111]) = 22 + 21 + 20 = 4 + 2 + 1 = 7. We can see that B2T w is a mapping of bit patterns of length w to numbers be- tween TMinw and TMaxw, written as B2T w: {0, 1}w →{TMinw,..., TMaxw}.As we saw with the unsigned representation, every number within the representable range has a unique encoding as a w-bit two’s-complement number. This leads to a principle for two’s-complement numbers similar to that for unsigned numbers: principle: Uniqueness of two’s-complement encoding Function B2T w is a bijection. We deﬁne function T2Bw (for “two’s complement to binary”) to be the inverse of B2T w. That is, for a number x, such that TMinw ≤ x ≤ TMaxw, T2Bw(x) is the (unique) w-bit pattern that encodes x. Practice Problem 2.17 (solution page 184) Assuming w = 4, we can assign a numeric value to each possible hexadecimal digit, assuming either an unsigned or a two’s-complement interpretation. Fill in the following table according to these interpretations by writing out the nonzero powers of 2 in the summations shown in Equations 2.1 and 2.3: 102 Chapter 2 Representing and Manipulating Information ⃗x Hexadecimal Binary B2U 4(⃗x) B2T 4(⃗x) nÓ¡ [1010] 23 + 21 = 10 −23 + 21 =−6 nÓo nÓ¢ nÓp nÓu nÓ£ Figure 2.14 shows the bit patterns and numeric values for several important numbers for different word sizes. The ﬁrst three give the ranges of representable integers in terms of the values of UMaxw, TMinw, and TMaxw. We will refer to these three special values often in the ensuing discussion. We will drop the subscript w and refer to the values UMax, TMin, and TMax when w can be inferred from context or is not central to the discussion. A few points are worth highlighting about these numbers. First, as observed in Figures 2.9 and 2.10, the two’s-complement range is asymmetric: |TMin|= |TMax|+ 1; that is, there is no positive counterpart to TMin. As we shall see, this leads to some peculiar properties of two’s-complement arithmetic and can be the source of subtle program bugs. This asymmetry arises because half the bit patterns (those with the sign bit set to 1) represent negative numbers, while half (those with the sign bit set to 0) represent nonnegative numbers. Since 0 is nonnegative, this means that it can represent one less positive number than negative. Second, the maximum unsigned value is just over twice the maximum two’s-complement value: UMax = 2TMax + 1. All of the bit patterns that denote negative numbers in two’s-complement notation become positive values in an unsigned representation. Word size w Value 8 16 32 64 UMaxw nÓƒƒ nÓƒƒƒƒ nÓƒƒƒƒƒƒƒƒ nÓƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ 255 65,535 4,294,967,295 18,446,744,073,709,551,615 TMinw nÓvn nÓvnnn nÓvnnnnnnn nÓvnnnnnnnnnnnnnnn −128 −32,768 −2,147,483,648 −9,223,372,036,854,775,808 TMaxw nÓuƒ nÓuƒƒƒ nÓuƒƒƒƒƒƒƒ nÓuƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ 127 32,767 2,147,483,647 9,223,372,036,854,775,807 −1 nÓƒƒ nÓƒƒƒƒ nÓƒƒƒƒƒƒƒƒ nÓƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ 0 nÓnn nÓnnnn nÓnnnnnnnn nÓnnnnnnnnnnnnnnnn Figure 2.14 Important numbers. Both numeric values and hexadecimal representa- tions are shown. Section 2.2 Integer Representations 103 Aside More on ﬁxed-size integer types For some programs, it is essential that data types be encoded using representations with speciﬁc sizes. For example, when writing programs to enable a machine to communicate over the Internet according to a standard protocol, it is important to have data types compatible with those speciﬁed by the protocol. We have seen that some C data types, especially ÄÇÅ×, have different ranges on different machines, and in fact the C standards only specify the minimum ranges for any data type, not the exact ranges. Although we can choose data types that will be compatible with standard representations on most machines, there is no guarantee of portability. We have already encountered the 32- and 64-bit versions of ﬁxed-size integer types (Figure 2.3); they are part of a larger class of data types. The ISO C99 standard introduces this class of integer types in the ﬁle ÍÎ®©ÅÎl³. This ﬁle deﬁnes a set of data types with declarations of the form ©ÅÎN ˆÎ and Ï©ÅÎNˆÎ, specifying N -bit signed and unsigned integers, for different values of N . The exact values of N are implementation dependent, but most compilers allow values of 8, 16, 32, and 64. Thus, we can unambiguously declare an unsigned 16-bit variable by giving it type Ï©ÅÎotˆÎ, and a signed variable of 32 bits as ©ÅÎqpˆÎ. Along with these data types are a set of macros deﬁning the minimum and maximum values for each value of N . These have names of the form 'ﬁ¶N ˆ›'ﬁ, 'ﬁ¶N ˆ›¡”, and •'ﬁ¶Nˆ›¡”. Formatted printing with ﬁxed-width types requires use of macros that expand into format strings in a system-dependent manner. So, for example, the values of variables Ó and Ô of type ©ÅÎqpˆÎ and Ï©ÅÎtrˆÎ can be printed by the following call to ÉË©ÅÎð: ÉË©ÅÎðf‘Ó { c‘ –‡'®qp ‘jÔ{c‘ –‡'Ïtr ‘¿Å‘j Ój Ôgy When compiled as a 64-bit program, macro –‡'®qp expands to the string ‘®‘, while –‡'Ïtr expands to the pair of strings ‘Ä‘ ‘Ï‘. When the C preprocessor encounters a sequence of string constants separated only by spaces (or other whitespace characters), it concatenates them together. Thus, the above call to ÉË©ÅÎð becomes ÉË©ÅÎðf‘Ó { c®j Ô { cÄÏ¿Å‘j Ój Ôgy Using the macros ensures that a correct format string will be generated regardless of how the code is compiled. Figure 2.14 also shows the representations of constants −1 and 0. Note that −1 has the same bit representation as UMax—a string of all ones. Numeric value 0 is represented as a string of all zeros in both representations. The C standards do not require signed integers to be represented in two’s- complement form, but nearly all machines do so. Programmers who are concerned with maximizing portability across all possible machines should not assume any particular range of representable values, beyond the ranges indicated in Figure 2.11, nor should they assume any particular representation of signed numbers. On the other hand, many programs are written assuming a two’s-complement representation of signed numbers, and the “typical” ranges shown in Figures 2.9 and 2.10, and these programs are portable across a broad range of machines and compilers. The ﬁle zÄ©À©ÎÍl³| in the C library deﬁnes a set of constants 104 Chapter 2 Representing and Manipulating Information Aside Alternative representations of signed numbers There are two other standard representations for signed numbers: Ones’ complement. This is the same as two’s complement, except that the most signiﬁcant bit has weight −(2w−1 − 1) rather than −2w−1: B2Ow(⃗x) .= −xw−1(2w−1 − 1) + w−2∑ i=0 xi2i Sign magnitude. The most signiﬁcant bit is a sign bit that determines whether the remaining bits should be given negative or positive weight: B2Sw(⃗x) .= (−1)xw−1 . (w−2∑ i=0 xi2i) Both of these representations have the curious property that there are two different encodings of the number 0. For both representations, [00 ... 0] is interpreted as +0. The value −0 can be represented in sign-magnitude form as [10 ... 0] and in ones’ complement as [11 ... 1]. Although machines based on ones’-complement representations were built in the past, almost all modern machines use two’s complement. We will see that sign-magnitude encoding is used with ﬂoating-point numbers. Note the different position of apostrophes: two’s complement versus ones’ complement. The term “two’s complement” arises from the fact that for nonnegative x we compute a w-bit representation of −x as 2w − x (a single two.) The term “ones’ complement” comes from the property that we can compute −x in this notation as [111 ... 1] − x (multiple ones). delimiting the ranges of the different integer data types for the particular machine on which the compiler is running. For example, it deﬁnes constants 'ﬁ¶ˆ›¡”, 'ﬁ¶ˆ ›'ﬁ, and •'ﬁ¶ˆ›¡” describing the ranges of signed and unsigned integers. For a two’s-complement machine in which data type ©ÅÎ has w bits, these constants correspond to the values of TMaxw, TMinw, and UMaxw. The Java standard is quite speciﬁc about integer data type ranges and repre- sentations. It requires a two’s-complement representation with the exact ranges shown for the 64-bit case (Figure 2.10). In Java, the single-byte data type is called ¾ÔÎ− instead of ²³þË. These detailed requirements are intended to enable Java programs to behave identically regardless of the machines or operating systems running them. To get a better understanding of the two’s-complement representation, con- sider the following code example: 1 Í³ÇËÎ Ó { opqrsy 2 Í³ÇËÎ ÀÓ { kÓy 3 4 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÓj Í©Ö−ÇðfÍ³ÇËÎggy 5 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÀÓj Í©Ö−ÇðfÍ³ÇËÎggy Section 2.2 Integer Representations 105 12,345 −12,345 53,191 Weight Bit Value Bit Value Bit Value 1 o 1 o 1 o 1 2 n 0 o 2 o 2 4 n 0 o 4 o 4 8 o 8 n 0 n 0 16 o 16 n 0 n 0 32 o 32 n 0 n 0 64 n 0 o 64 o 64 128 n 0 o 128 o 128 256 n 0 o 256 o 256 512 n 0 o 512 o 512 1,024 n 0 o 1,024 o 1,024 2,048 n 0 o 2,048 o 2,048 4,096 o 4,096 n 0 n 0 8,192 o 8,192 n 0 n 0 16,384 n 0 o 16,384 o 16,384 ±32,768 n 0 o −32,768 o 32,768 Total 12,345 −12,345 53,191 Figure 2.15 Two’s-complement representations of 12,345 and −12,345, and unsigned representation of 53,191. Note that the latter two have identical bit representations. When run on a big-endian machine, this code prints qn qw and ²ð ²u, indi- cating that Ó has hexadecimal representation nÓqnqw, while ÀÓ has hexadeci- mal representation nÓ£ƒ£u. Expanding these into binary, we get bit patterns [0011000000111001] for Ó and [1100111111000111] for ÀÓ. As Figure 2.15 shows, Equation 2.3 yields values 12,345 and −12,345 for these two bit patterns. Practice Problem 2.18 (solution page 185) In Chapter 3, we will look at listings generated by a disassembler, a program that converts an executable program ﬁle back to a more readable ASCII form. These ﬁles contain many hexadecimal numbers, typically representing values in two’s- complement form. Being able to recognize these numbers and understand their signiﬁcance (for example, whether they are negative or positive) is an important skill. For the lines labeled A–I (on the right) in the following listing, convert the hexadecimal values (in 32-bit two’s-complement form) shown to the right of the instruction names (ÍÏ¾, ÀÇÌ, and þ®®) into their decimal equivalents: 106 Chapter 2 Representing and Manipulating Information rnnr®nx rv vo −² −n np nn nn ÍÏ¾ bnÓp−njcËÍÉ A. rnnr®ux rv v¾ rr pr þv ÀÇÌ knÓsvfcËÍÉgjcËþÓ B. rnnr®²x rv nq ru pv þ®® nÓpvfcË®©gjcËþÓ C. rnnr−nx rv vw rr pr ®n ÀÇÌ cËþÓjknÓqnfcËÍÉg D. rnnr−sx rv v¾ rr pr uv ÀÇÌ nÓuvfcËÍÉgjcËþÓ E. rnnr−þx rv vw vu vv nn nn nn ÀÇÌ cËþÓjnÓvvfcË®©g F. rnnrðox rv v¾ vr pr ðv no nn ÀÇÌ nÓoðvfcËÍÉgjcËþÓ G. rnnrðvx nn rnnrðwx rv nq rr pr nv þ®® nÓvfcËÍÉgjcËþÓ rnnrð−x rv vw vr pr ²n nn nn ÀÇÌ cËþÓjnÓ²nfcËÍÉg H. rnnsnsx nn rnnsntx rv v¾ rr ®r ¾v ÀÇÌ knÓrvfcËÍÉjcË®ÓjvgjcËþÓ I. 2.2.4 Conversions between Signed and Unsigned C allows casting between different numeric data types. For example, suppose variable Ó is declared as ©ÅÎ and Ï as ÏÅÍ©×Å−®. The expression fÏÅÍ©×Å−®g Ó converts the value of Ó to an unsigned value, and f©ÅÎg Ï converts the value of Ï to a signed integer. What should be the effect of casting signed value to unsigned, or vice versa? From a mathematical perspective, one can imagine several different conventions. Clearly, we want to preserve any value that can be represented in both forms. On the other hand, converting a negative value to unsigned might yield zero. Converting an unsigned value that is too large to be represented in two’s- complement form might yield TMax. For most implementations of C, however, the answer to this question is based on a bit-level perspective, rather than on a numeric one. For example, consider the following code: 1 Í³ÇËÎ ©ÅÎ Ì { kopqrsy 2 ÏÅÍ©×Å−® Í³ÇËÎ ÏÌ { fÏÅÍ©×Å−® Í³ÇËÎg Ìy 3 ÉË©ÅÎðf‘Ì { c®j ÏÌ { cÏ¿Å‘j Ìj ÏÌgy When run on a two’s-complement machine, it generates the following output: Ì { kopqrsj ÏÌ { sqowo What we see here is that the effect of casting is to keep the bit values identical but change how these bits are interpreted. We saw in Figure 2.15 that the 16-bit two’s-complement representation of −12,345 is identical to the 16-bit unsigned representation of 53,191. Casting from Í³ÇËÎ to ÏÅÍ©×Å−® Í³ÇËÎ changed the numeric value, but not the bit representation. Similarly, consider the following code: 1 ÏÅÍ©×Å−® Ï { rpwrwtupwsÏy mh •›þÓ hm 2 ©ÅÎ ÎÏ { f©ÅÎg Ïy Section 2.2 Integer Representations 107 3 ÉË©ÅÎðf‘Ï { cÏj ÎÏ { c®¿Å‘j Ïj ÎÏgy When run on a two’s-complement machine, it generates the following output: Ï { rpwrwtupwsj ÎÏ { ko We can see from Figure 2.14 that, for a 32-bit word size, the bit patterns represent- ing 4,294,967,295 (UMax32) in unsigned form and −1 in two’s-complement form are identical. In casting from ÏÅÍ©×Å−® to ©ÅÎ, the underlying bit representation stays the same. This is a general rule for how most C implementations handle conversions between signed and unsigned numbers with the same word size—the numeric values might change, but the bit patterns do not. Let us capture this idea in a more mathematical form. We deﬁned functions U2Bw and T2Bw that map numbers to their bit representations in either unsigned or two’s-complement form. That is, given an integer x in the range 0 ≤ x< UMaxw, the function U2Bw(x) gives the unique w-bit unsigned representation of x. Similarly, when x is in the range TMinw ≤ x ≤ TMaxw, the function T2Bw(x) gives the unique w-bit two’s- complement representation of x. Now deﬁne the function T2U w as T2U w(x) .= B2U w(T2Bw(x)). This function takes a number between TMinw and TMaxw and yields a number between 0 and UMaxw, where the two numbers have identical bit representations, except that the argument has a two’s-complement representation while the result is unsigned. Similarly, for x between 0 and UMaxw, the function U2T w, deﬁned as U2T w(x) .= B2T w(U2Bw(x)), yields the number having the same two’s-complement represen- tation as the unsigned representation of x. Pursuing our earlier examples, we see from Figure 2.15 that T2U 16(−12,345) = 53,191, and that U2T 16(53,191) =−12,345. That is, the 16-bit pattern written in hexadecimal as nÓ£ƒ£u is both the two’s-complement representation of −12,345 and the unsigned representation of 53,191. Note also that 12,345 + 53,191 = 65,536 = 216. This property generalizes to a relationship between the two nu- meric values (two’s complement and unsigned) represented by a given bit pat- tern. Similarly, from Figure 2.14, we see that T2U 32(−1) = 4,294,967,295, and U2T 32(4,294,967,295) =−1. That is, UMax has the same bit representation in un- signed form as does −1 in two’s-complement form. We can also see the relationship between these two numbers: 1 + UMaxw = 2w. We see, then, that function T2U describes the conversion of a two’s- complement number to its unsigned counterpart, while U2T converts in the op- posite direction. These describe the effect of casting between these data types in most C implementations. Practice Problem 2.19 (solution page 185) Using the table you ﬁlled in when solving Problem 2.17, ﬁll in the following table describing the function T2U 4: 108 Chapter 2 Representing and Manipulating Information x T2U 4(x) −1 −5 −6 −4 1 8 The relationship we have seen, via several examples, between the two’s- complement and unsigned values for a given bit pattern can be expressed as a property of the function T2U: principle: Conversion from two’s complement to unsigned For x such that TMinw ≤ x ≤ TMaxw: T2U w(x) = { x + 2w,x < 0 x, x ≥ 0 (2.5) For example, we saw that T2U 16(−12,345) =−12,345 + 216 = 53,191, and also that T2U w(−1) =−1 + 2w = UMaxw. This property can be derived by comparing Equations 2.1 and 2.3. derivation: Conversion from two’s complement to unsigned Comparing Equations 2.1 and 2.3, we can see that for bit pattern ⃗x, if we compute the difference B2U w(⃗x) − B2T w(⃗x), the weighted sums for bits from 0 to w − 2 will cancel each other, leaving a value B2U w(⃗x) − B2T w(⃗x) = xw−1(2w−1 −−2w−1) = xw−12w. This gives a relationship B2U w(⃗x) = B2T w(⃗x) + xw−12w. We therefore have B2U w(T2Bw(x)) = T2U w(x) = x + xw−12w (2.6) In a two’s-complement representation of x, bit xw−1 determines whether or not x is negative, giving the two cases of Equation 2.5. As examples, Figure 2.16 compares how functions B2U and B2T assign values to bit patterns for w = 4. For the two’s-complement case, the most signiﬁcant bit serves as the sign bit, which we diagram as a leftward-pointing gray bar. For the unsigned case, this bit has positive weight, which we show as a rightward-pointing black bar. In going from two’s complement to unsigned, the most signiﬁcant bit changes its weight from −8to +8. As a consequence, the values that are nega- tive in a two’s-complement representation increase by 24 = 16 with an unsigned representation. Thus, −5 becomes +11, and −1 becomes +15. Section 2.2 Integer Representations 109 Figure 2.16 Comparing unsigned and two’s-complement representations for w = 4. The weight of the most signiﬁcant bit is −8 for two’s complement and +8 for unsigned, yielding a net difference of 16. 876543211615141312111090–1–2–3–4–5–6–7–8 2 0 = 1 2 1 = 2 2 2 = 4 –23 = –8 [1011] [1111] 23 = 8 +16 +16 Figure 2.17 Conversion from two’s complement to unsigned. Function T2U converts negative numbers to large positive numbers. +2 w–1 0 –2 w–1 2 w 0 2 w–1 Two’s complement Unsigned Figure 2.17 illustrates the general behavior of function T2U. As it shows, when mapping a signed number to its unsigned counterpart, negative numbers are con- verted to large positive numbers, while nonnegative numbers remain unchanged. Practice Problem 2.20 (solution page 185) Explain how Equation 2.5 applies to the entries in the table you generated when solving Problem 2.19. Going in the other direction, we can state the relationship between an un- signed number u and its signed counterpart U2T w(u): principle: Unsigned to two’s-complement conversion For u such that 0 ≤ u ≤ UMaxw: U2T w(u) = { u, u ≤ TMaxw u − 2w,u > TMaxw (2.7) 110 Chapter 2 Representing and Manipulating Information Figure 2.18 Conversion from unsigned to two’s complement. Function U2T converts numbers greater than 2w−1 − 1 to negative values. +2 w–1 0 –2 w–1 2 w 0 2 w–1 Two’s complement Unsigned This principle can be justiﬁed as follows: derivation: Unsigned to two’s-complement conversion Let ⃗u = U2Bw(u). This bit vector will also be the two’s-complement representation of U2T w(u). Equations 2.1 and 2.3 can be combined to give U2T w(u) =−uw−12w + u (2.8) In the unsigned representation of u, bit uw−1 determines whether or not u is greater than TMaxw = 2w−1 − 1, giving the two cases of Equation 2.7. The behavior of function U2T is illustrated in Figure 2.18. For small (≤ TMaxw) numbers, the conversion from unsigned to signed preserves the nu- meric value. Large (> TMaxw) numbers are converted to negative values. To summarize, we considered the effects of converting in both directions between unsigned and two’s-complement representations. For values x in the range 0 ≤ x ≤ TMaxw, we have T2U w(x) = x and U2T w(x) = x. That is, num- bers in this range have identical unsigned and two’s-complement representations. For values outside of this range, the conversions either add or subtract 2w.For example, we have T2U w(−1) =−1 + 2w = UMaxw—the negative number clos- est to zero maps to the largest unsigned number. At the other extreme, one can see that T2U w(TMinw) =−2w−1 + 2w = 2w−1 = TMaxw + 1—the most neg- ative number maps to an unsigned number just outside the range of positive two’s-complement numbers. Using the example of Figure 2.15, we can see that T2U 16(−12,345) = 65,536 +−12,345 = 53,191. 2.2.5 Signed versus Unsigned in C As indicated in Figures 2.9 and 2.10, C supports both signed and unsigned arith- metic for all of its integer data types. Although the C standard does not spec- ify a particular representation of signed numbers, almost all machines use two’s complement. Generally, most numbers are signed by default. For example, when declaring a constant such as opqrs or nÓo¡p¢, the value is considered signed. Adding character ‘•’or‘Ï’ as a sufﬁx creates an unsigned constant; for example, opqrs• or nÓo¡p¢Ï. Section 2.2 Integer Representations 111 C allows conversion between unsigned and signed. Although the C standard does not specify precisely how this conversion should be made, most systems follow the rule that the underlying bit representation does not change. This rule has the effect of applying the function U2T w when converting from unsigned to signed, and T2U w when converting from signed to unsigned, where w is the number of bits for the data type. Conversions can happen due to explicit casting, such as in the following code: 1 ©ÅÎ ÎÓj ÎÔy 2 ÏÅÍ©×Å−® ÏÓj ÏÔy 3 4 ÎÓ { f©ÅÎg ÏÓy 5 ÏÔ { fÏÅÍ©×Å−®g ÎÔy Alternatively, they can happen implicitly when an expression of one type is as- signed to a variable of another, as in the following code: 1 ©ÅÎ ÎÓj ÎÔy 2 ÏÅÍ©×Å−® ÏÓj ÏÔy 3 4 ÎÓ { ÏÓy mh £þÍÎ ÎÇ Í©×Å−® hm 5 ÏÔ { ÎÔy mh £þÍÎ ÎÇ ÏÅÍ©×Å−® hm When printing numeric values with ÉË©ÅÎð, the directives c®, cÏ, and cÓ are used to print a number as a signed decimal, an unsigned decimal, and in hexadecimal format, respectively. Note that ÉË©ÅÎð does not make use of any type information, and so it is possible to print a value of type ©ÅÎ with directive cÏ and a value of type ÏÅÍ©×Å−® with directive c®. For example, consider the following code: 1 ©ÅÎ Ó { koy 2 ÏÅÍ©×Å−® Ï { porurvqtrvy mh p ÎÇ Î³− qoÍÎ hm 3 4 ÉË©ÅÎðf‘Ó { cÏ { c®¿Å‘j Ój Ógy 5 ÉË©ÅÎðf‘Ï { cÏ { c®¿Å‘j Ïj Ïgy When compiled as a 32-bit program, it prints the following: Ó { rpwrwtupws { ko Ï { porurvqtrv { kporurvqtrv In both cases, ÉË©ÅÎð prints the word ﬁrst as if it represented an unsigned number and second as if it represented a signed number. We can see the conversion routines in action: T2U 32(−1) = UMax32 = 232 − 1 and U2T 32(231) = 231 − 232 = −231 = TMin32. Some possibly nonintuitive behavior arises due to C’s handling of expres- sions containing combinations of signed and unsigned quantities. When an op- eration is performed where one operand is signed and the other is unsigned, C implicitly casts the signed argument to unsigned and performs the operations 112 Chapter 2 Representing and Manipulating Information Expression Type Evaluation n{{ n• Unsigned o ko z n Signed o ko z n• Unsigned n * porurvqtru | kporurvqtruko Signed o porurvqtru• | kporurvqtruko Unsigned n * porurvqtru | f©ÅÎg porurvqtrv• Signed o * ko | kp Signed o fÏÅÍ©×Å−®g ko | kp Unsigned o Figure 2.19 Effects of C promotion rules. Nonintuitive cases are marked by ‘*’. When either operand of a comparison is unsigned, the other operand is implicitly cast to unsigned. See Web Aside data:tmin for why we write TMin32 as kpjorujrvqjtruko. assuming the numbers are nonnegative. As we will see, this convention makes little difference for standard arithmetic operations, but it leads to nonintuitive results for relational operators such as z and |. Figure 2.19 shows some sample relational expressions and their resulting evaluations, when data type ©ÅÎ has a 32-bit two’s-complement representation. Consider the comparison kozn•. Since the second operand is unsigned, the ﬁrst one is implicitly cast to unsigned, and hence the expression is equivalent to the comparison rpwrwtupws• z n• (recall that T2U w(−1) = UMaxw), which of course is false. The other cases can be under- stood by similar analyses. Practice Problem 2.21 (solution page 185) Assuming the expressions are evaluated when executing a 32-bit program on a ma- chine that uses two’s-complement arithmetic, ﬁll in the following table describing the effect of casting and relational operations, in the style of Figure 2.19: Expression Type Evaluation kporurvqtruko {{ porurvqtrv• kporurvqtruko z porurvqtru kporurvqtruko• z porurvqtru kporurvqtruko z kporurvqtru kporurvqtruko• z kporurvqtru 2.2.6 Expanding the Bit Representation of a Number One common operation is to convert between integers having different word sizes while retaining the same numeric value. Of course, this may not be possible when the destination data type is too small to represent the desired value. Converting from a smaller to a larger data type, however, should always be possible. Section 2.2 Integer Representations 113 Web Aside DATA:TMIN Writing TMin in C In Figure 2.19 and in Problem 2.21, we carefully wrote the value of TMin32 as kpjorujrvqjtruko.Why not simply write it as either kpjorujrvqjtrv or nÓvnnnnnnn? Looking at the C header ﬁle Ä©À©ÎÍl³, we see that they use a similar method as we have to write TMin32 and TMax32: mh ›©Å©ÀÏÀ þÅ® ÀþÓ©ÀÏÀ ÌþÄÏ−Í þ ‘Í©×Å−® ©ÅÎ’ ²þÅ ³ÇÄ®l hm a®−ð©Å− 'ﬁ¶ˆ›¡” porurvqtru a®−ð©Å− 'ﬁ¶ˆ›'ﬁ fk'ﬁ¶ˆ›¡” k og Unfortunately, a curious interaction between the asymmetry of the two’s-complement representa- tion and the conversion rules of C forces us to write TMin32 in this unusual way. Although understanding this issue requires us to delve into one of the murkier corners of the C language standards, it will help us appreciate some of the subtleties of integer data types and representations. To convert an unsigned number to a larger data type, we can simply add leading zeros to the representation; this operation is known as zero extension, expressed by the following principle: principle: Expansion of an unsigned number by zero extension Deﬁne bit vectors ⃗u = [uw−1,uw−2,...,u0] of width w and ⃗u′ = [0,..., 0,uw−1, uw−2,...,u0] of width w′, where w′ >w. Then B2U w(⃗u) = B2U w′(⃗u′). This principle can be seen to follow directly from the deﬁnition of the unsigned encoding, given by Equation 2.1. For converting a two’s-complement number to a larger data type, the rule is to perform a sign extension, adding copies of the most signiﬁcant bit to the representation, expressed by the following principle. We show the sign bit xw−1 in blue to highlight its role in sign extension. principle: Expansion of a two’s-complement number by sign extension Deﬁne bit vectors ⃗x = [xw−1,xw−2,...,x0] of width w and ⃗x′ = [xw−1,..., xw−1, xw−1,xw−2,...,x0] of width w′, where w′ >w. Then B2T w(⃗x) = B2T w′(⃗x′). As an example, consider the following code: 1 Í³ÇËÎ ÍÓ { kopqrsy mh kopqrs hm 2 ÏÅÍ©×Å−® Í³ÇËÎ ÏÍÓ { ÍÓy mh sqowo hm 3 ©ÅÎ Ó { ÍÓy mh kopqrs hm 4 ÏÅÍ©×Å−® ÏÓ { ÏÍÓy mh sqowo hm 5 6 ÉË©ÅÎðf‘ÍÓ { c®x¿Î‘j ÍÓgy 7 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÍÓj Í©Ö−ÇðfÍ³ÇËÎggy 8 ÉË©ÅÎðf‘ÏÍÓ { cÏx¿Î‘j ÏÍÓgy 9 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÏÍÓj Í©Ö−ÇðfÏÅÍ©×Å−® Í³ÇËÎggy 10 ÉË©ÅÎðf‘Ó { c®x¿Î‘j Ógy 114 Chapter 2 Representing and Manipulating Information 11 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÓj Í©Ö−Çðf©ÅÎggy 12 ÉË©ÅÎðf‘ÏÓ { cÏx¿Î‘j ÏÓgy 13 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÏÓj Í©Ö−ÇðfÏÅÍ©×Å−®ggy When run as a 32-bit program on a big-endian machine that uses a two’s- complement representation, this code prints the output ÍÓ { kopqrsx ²ð ²u ÏÍÓ { sqowox ²ð ²u Ó { kopqrsx ðð ðð ²ð ²u ÏÓ { sqowox nn nn ²ð ²u We see that, although the two’s-complement representation of −12,345 and the unsigned representation of 53,191 are identical for a 16-bit word size, they dif- fer for a 32-bit word size. In particular, −12,345 has hexadecimal representation nÓƒƒƒƒ£ƒ£u, while 53,191 has hexadecimal representation nÓnnnn£ƒ£u. The for- mer has been sign extended—16 copies of the most signiﬁcant bit 1, having hexa- decimal representation nÓƒƒƒƒ, have been added as leading bits. The latter has been extended with 16 leading zeros, having hexadecimal representation nÓnnnn. As an illustration, Figure 2.20 shows the result of expanding from word size w = 3to w = 4 by sign extension. Bit vector [101]represents the value −4 + 1 =−3. Applying sign extension gives bit vector [1101] representing the value −8 + 4 + 1 =−3. We can see that, for w = 4, the combined value of the two most signiﬁcant bits, −8 + 4 =−4, matches the value of the sign bit for w = 3. Similarly, bit vectors [111] and [1111] both represent the value −1. With this as intuition, we can now show that sign extension preserves the value of a two’s-complement number. Figure 2.20 Examples of sign extension from w = 3 to w = 4. For w = 4, the combined weight of the upper 2 bits is −8 + 4 =−4, matching that of the sign bit for w = 3. 876543210–1–2–3–4–5–6–7–8 2 0 = 1 2 1 = 2 2 2 = 4 –23 = –8 [101] [1101] [111] [1111] –22 = –4 Section 2.2 Integer Representations 115 derivation: Expansion of a two’s-complement number by sign extension Let w′ = w + k. What we want to prove is that B2T w+k([xw−1,..., xw−1︸ ︷︷ ︸ k times , xw−1,xw−2,...,x0]) = B2T w([xw−1,xw−2,...,x0]) The proof follows by induction on k. That is, if we can prove that sign extending by 1 bit preserves the numeric value, then this property will hold when sign extending by an arbitrary number of bits. Thus, the task reduces to proving that B2T w+1([xw−1, xw−1,xw−2,...,x0]) = B2T w([xw−1,xw−2,...,x0]) Expanding the left-hand expression with Equation 2.3 gives the following: B2T w+1([xw−1, xw−1,xw−2,...,x0]) =−xw−12w + w−1∑ i=0 xi2i =−xw−12w + xw−12w−1 + w−2∑ i=0 xi2i =−xw−1 (2w − 2w−1 ) + w−2∑ i=0 xi2i =−xw−12w−1 + w−2∑ i=0 xi2i = B2T w([xw−1,xw−2,...,x0]) The key property we exploit is that 2w − 2w−1 = 2w−1. Thus, the combined effect of adding a bit of weight −2w and of converting the bit having weight −2w−1 to be one with weight 2w−1 is to preserve the original numeric value. Practice Problem 2.22 (solution page 186) Show that each of the following bit vectors is a two’s-complement representation of −4 by applying Equation 2.3: A. [1100] B. [11100] C. [111100] Observe that the second and third bit vectors can be derived from the ﬁrst by sign extension. 116 Chapter 2 Representing and Manipulating Information One point worth making is that the relative order of conversion from one data size to another and between unsigned and signed can affect the behavior of a program. Consider the following code: 1 Í³ÇËÎ ÍÓ { kopqrsy mh kopqrs hm 2 ÏÅÍ©×Å−® ÏÔ { ÍÓy mh ›ÔÍÎ−ËÔ_ hm 3 4 ÉË©ÅÎðf‘ÏÔ { cÏx¿Î‘j ÏÔgy 5 Í³ÇÑˆ¾ÔÎ−Íff¾ÔÎ−ˆÉÇ©ÅÎ−Ëg dÏÔj Í©Ö−ÇðfÏÅÍ©×Å−®ggy When run on a big-endian machine, this code causes the following output to be printed: ÏÔ { rpwrwsrwsox ðð ðð ²ð ²u This shows that, when converting from Í³ÇËÎ to ÏÅÍ©×Å−®, the program ﬁrst changes the size and then the type. That is, fÏÅÍ©×Å−®g ÍÓ is equivalent to fÏÅÍ©×Å−®g f©ÅÎg ÍÓ, evaluating to 4,294,954,951, not fÏÅÍ©×Å−®g fÏÅÍ©×Å−® Í³ÇËÎg ÍÓ, which evaluates to 53,191. Indeed, this convention is required by the C standards. Practice Problem 2.23 (solution page 186) Consider the following C functions: ©ÅÎ ðÏÅofÏÅÍ©×Å−® ÑÇË®g Õ Ë−ÎÏËÅ f©ÅÎg ffÑÇË® zz prg || prgy Û ©ÅÎ ðÏÅpfÏÅÍ©×Å−® ÑÇË®g Õ Ë−ÎÏËÅ ff©ÅÎg ÑÇË® zz prg || pry Û Assume these are executed as a 32-bit program on a machine that uses two’s- complement arithmetic. Assume also that right shifts of signed values are per- formed arithmetically, while right shifts of unsigned values are performed logically. A. Fill in the following table showing the effect of these functions for several example arguments. You will ﬁnd it more convenient to work with a hexa- decimal representation. Just remember that hex digits v through ƒ have their most signiﬁcant bits equal to 1. Ñ ðÏÅofÑg ðÏÅpfÑg nÓnnnnnnut nÓvutsrqpo nÓnnnnnn£w nÓ¥⁄£¢¡wvu B. Describe in words the useful computation each of these functions performs. Section 2.2 Integer Representations 117 2.2.7 Truncating Numbers Suppose that, rather than extending a value with extra bits, we reduce the number of bits representing a number. This occurs, for example, in the following code: 1 ©ÅÎ Ó { sqowoy 2 Í³ÇËÎ ÍÓ { fÍ³ÇËÎg Óy mh kopqrs hm 3 ©ÅÎ Ô { ÍÓy mh kopqrs hm Casting Ó to be Í³ÇËÎ will truncate a 32-bit ©ÅÎ to a 16-bit Í³ÇËÎ.Aswesaw before, this 16-bit pattern is the two’s-complement representation of −12,345. When casting this back to ©ÅÎ, sign extension will set the high-order 16 bits to ones, yielding the 32-bit two’s-complement representation of −12,345. When truncating a w-bit number ⃗x = [xw−1,xw−2,...,x0]to a k-bit number, we drop the high-order w − k bits, giving a bit vector ⃗x′ = [xk−1,xk−2,. . .,x0]. Truncating a number can alter its value—a form of overﬂow. For an unsigned number, we can readily characterize the numeric value that will result. principle: Truncation of an unsigned number Let ⃗x be the bit vector [xw−1,xw−2,...,x0], and let ⃗x′ be the result of truncating it to k bits: ⃗x′ = [xk−1,xk−2, ... , x0]. Let x = B2U w(⃗x) and x′ = B2U k(⃗x′). Then x′ = x mod 2k. The intuition behind this principle is simply that all of the bits that were truncated have weights of the form 2i, where i ≥ k, and therefore each of these weights reduces to zero under the modulus operation. This is formalized by the following derivation: derivation: Truncation of an unsigned number Applying the modulus operation to Equation 2.1 yields B2U w([xw−1,xw−2,...,x0]) mod 2k = [w−1∑ i=0 xi2i] mod 2k = [k−1∑ i=0 xi2i] mod 2k = k−1∑ i=0 xi2i = B2U k([xk−1,xk−2,...,x0]) In this derivation, we make use of the property that 2i mod 2k = 0 for any i ≥ k. A similar property holds for truncating a two’s-complement number, except that it then converts the most signiﬁcant bit into a sign bit: 118 Chapter 2 Representing and Manipulating Information principle: Truncation of a two’s-complement number Let ⃗x be the bit vector [xw−1,xw−2,...,x0], and let ⃗′x be the result of truncating it to k bits: ⃗x′ = [xk−1,xk−2,. . .,x0]. Let x = B2T w(⃗x) and x′ = B2T k(⃗x′). Then x′ = U2T k(x mod 2k). In this formulation, x mod 2k will be a number between 0 and 2k − 1. Applying function U2T k to it will have the effect of converting the most signiﬁcant bit xk−1 from having weight 2k−1 to having weight −2k−1. We can see this with the example of converting value x = 53,191 from ©ÅÎ to Í³ÇËÎ. Since 216 = 65,536 ≥ x, we have x mod 216 = x. But when we convert this number to a 16-bit two’s-complement number, we get x′ = 53,191 − 65,536 =−12,345. derivation: Truncation of a two’s-complement number Using a similar argument to the one we used for truncation of an unsigned number shows that B2T w([xw−1,xw−2,...,x0]) mod 2k = B2U k([xk−1,xk−2,...,x0]) That is, x mod 2k can be represented by an unsigned number having bit-level rep- resentation [xk−1,xk−2,. . .,x0]. Converting this to a two’s-complement number gives x′ = U2T k(x mod 2k). Summarizing, the effect of truncation for unsigned numbers is B2U k([xk−1,xk−2,...,x0]) = B2U w([xw−1,xw−2,...,x0]) mod 2k (2.9) while the effect for two’s-complement numbers is B2T k([xk−1,xk−2,...,x0]) = U2T k(B2U w([xw−1,xw−2,...,x0]) mod 2k) (2.10) Practice Problem 2.24 (solution page 186) Suppose we truncate a 4-bit value (represented by hex digits n through ƒ)toa3- bit value (represented as hex digits n through u.) Fill in the table below showing the effect of this truncation for some cases, in terms of the unsigned and two’s- complement interpretations of those bit patterns. Hex Unsigned Two’s complement Original Truncated Original Truncated Original Truncated oo 1 1 qq 3 3 ss 5 5 £r 12 −4 ¥t 14 −2 Explain how Equations 2.9 and 2.10 apply to these cases. Section 2.2 Integer Representations 119 2.2.8 Advice on Signed versus Unsigned As we have seen, the implicit casting of signed to unsigned leads to some non- intuitive behavior. Nonintuitive features often lead to program bugs, and ones involving the nuances of implicit casting can be especially difﬁcult to see. Since the casting takes place without any clear indication in the code, programmers often overlook its effects. The following two practice problems illustrate some of the subtle errors that can arise due to implicit casting and the unsigned data type. Practice Problem 2.25 (solution page 187) Consider the following code that attempts to sum the elements of an array þ, where the number of elements is given by parameter Ä−Å×Î³: 1 mh „¡‡ﬁ'ﬁ§x ¶³©Í ©Í ¾Ï××Ô ²Ç®− hm 2 ðÄÇþÎ ÍÏÀˆ−Ä−À−ÅÎÍfðÄÇþÎ þ‰`j ÏÅÍ©×Å−® Ä−Å×Î³g Õ 3 ©ÅÎ ©y 4 ðÄÇþÎ Ë−ÍÏÄÎ { ny 5 6 ðÇË f© { ny © z{ Ä−Å×Î³koy ©iig 7 Ë−ÍÏÄÎ i{ þ‰©`y 8 Ë−ÎÏËÅ Ë−ÍÏÄÎy 9 Û When run with argument Ä−Å×Î³ equal to 0, this code should return 0.0. Instead, it encounters a memory error. Explain why this happens. Show how this code can be corrected. Practice Problem 2.26 (solution page 187) You are given the assignment of writing a function that determines whether one string is longer than another. You decide to make use of the string library function ÍÎËÄ−Å having the following declaration: mh –ËÇÎÇÎÔÉ− ðÇË Ä©¾ËþËÔ ðÏÅ²Î©ÇÅ ÍÎËÄ−Å hm Í©Ö−ˆÎ ÍÎËÄ−Åf²ÇÅÍÎ ²³þË hÍgy Here is your ﬁrst attempt at the function: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë ÍÎË©Å× Í ©Í ÄÇÅ×−Ë Î³þÅ ÍÎË©Å× Î hm mh „¡‡ﬁ'ﬁ§x ¶³©Í ðÏÅ²Î©ÇÅ ©Í ¾Ï××Ô hm ©ÅÎ ÍÎËÄÇÅ×−Ëf²³þË hÍj ²³þË hÎg Õ Ë−ÎÏËÅ ÍÎËÄ−ÅfÍg k ÍÎËÄ−ÅfÎg | ny Û When you test this on some sample data, things do not seem to work quite right. You investigate further and determine that, when compiled as a 32-bit 120 Chapter 2 Representing and Manipulating Information program, data type Í©Ö−ˆÎ is deﬁned (via ÎÔÉ−®−ð) in header ﬁle ÍÎ®©Çl³ to be ÏÅÍ©×Å−®. A. For what cases will this function produce an incorrect result? B. Explain how this incorrect result comes about. C. Show how to ﬁx the code so that it will work reliably. We have seen multiple ways in which the subtle features of unsigned arith- metic, and especially the implicit conversion of signed to unsigned, can lead to errors or vulnerabilities. One way to avoid such bugs is to never use unsigned numbers. In fact, few languages other than C support unsigned integers. Appar- ently, these other language designers viewed them as more trouble than they are worth. For example, Java supports only signed integers, and it requires that they be implemented with two’s-complement arithmetic. The normal right shift oper- ator || is guaranteed to perform an arithmetic shift. The special operator ||| is deﬁned to perform a logical right shift. Unsigned values are very useful when we want to think of words as just col- lections of bits with no numeric interpretation. This occurs, for example, when packing a word with ﬂags describing various Boolean conditions. Addresses are naturally unsigned, so systems programmers ﬁnd unsigned types to be helpful. Unsigned values are also useful when implementing mathematical packages for modular arithmetic and for multiprecision arithmetic, in which numbers are rep- resented by arrays of words. 2.3 Integer Arithmetic Many beginning programmers are surprised to ﬁnd that adding two positive num- bers can yield a negative result, and that the comparison ÓzÔ can yield a different result than the comparison ÓkÔzn. These properties are artifacts of the ﬁnite na- ture of computer arithmetic. Understanding the nuances of computer arithmetic can help programmers write more reliable code. 2.3.1 Unsigned Addition Consider two nonnegative integers x and y, such that 0 ≤ x, y < 2w. Each of these values can be represented by a w-bit unsigned number. If we compute their sum, however, we have a possible range 0 ≤ x + y ≤ 2w+1 − 2. Representing this sum could require w + 1 bits. For example, Figure 2.21 shows a plot of the func- tion x + y when x and y have 4-bit representations. The arguments (shown on the horizontal axes) range from 0 to 15, but the sum ranges from 0 to 30. The shape of the function is a sloping plane (the function is linear in both dimen- sions). If we were to maintain the sum as a (w + 1)-bit number and add it to another value, we may require w + 2 bits, and so on. This continued “word size Section 2.3 Integer Arithmetic 121 32 28 24 20 16 12 8 4 0 2 0 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Figure 2.21 Integer addition. With a 4-bit word size, the sum could require 5 bits. inﬂation” means we cannot place any bound on the word size required to fully rep- resent the results of arithmetic operations. Some programming languages, such as Lisp, actually support arbitrary size arithmetic to allow integers of any size (within the memory limits of the computer, of course.) More commonly, pro- gramming languages support ﬁxed-size arithmetic, and hence operations such as “addition” and “multiplication” differ from their counterpart operations over integers. Let us deﬁne the operation iu w for arguments x and y, where 0 ≤ x, y < 2w, as the result of truncating the integer sum x + y to be w bits long and then viewing the result as an unsigned number. This can be characterized as a form of modular arithmetic, computing the sum modulo 2w by simply discarding any bits with weight greater than 2w−1 in the bit-level representation of x + y.For example, consider a 4-bit number representation with x = 9 and y = 12, having bit representations [1001] and [1100], respectively. Their sum is 21, having a 5-bit representation [10101]. But if we discard the high-order bit, we get [0101], that is, decimal value 5. This matches the value 21 mod 16 = 5. 122 Chapter 2 Representing and Manipulating Information Aside Security vulnerability in ×−ÎÉ−−ËÅþÀ− In 2002, programmers involved in the FreeBSD open-source operating systems project realized that their implementation of the ×−ÎÉ−−ËÅþÀ− library function had a security vulnerability. A simpliﬁed version of their code went something like this: 1 mh 2 h 'ÄÄÏÍÎËþÎ©ÇÅ Çð ²Ç®− ÌÏÄÅ−Ëþ¾©Ä©ÎÔ Í©À©ÄþË ÎÇ Î³þÎ ðÇÏÅ® ©Å 3 h ƒË−−¢·⁄’Í ©ÀÉÄ−À−ÅÎþÎ©ÇÅ Çð ×−ÎÉ−−ËÅþÀ−fg 4 hm 5 6 mh ⁄−²ÄþËþÎ©ÇÅ Çð Ä©¾ËþËÔ ðÏÅ²Î©ÇÅ À−À²ÉÔ hm 7 ÌÇ©® hÀ−À²ÉÔfÌÇ©® h®−ÍÎj ÌÇ©® hÍË²j Í©Ö−ˆÎ Ågy 8 9 mh «−ËÅ−Ä À−ÀÇËÔ Ë−×©ÇÅ ³ÇÄ®©Å× ÏÍ−Ëkþ²²−ÍÍ©¾Ä− ®þÎþ hm 10 a®−ð©Å− «·'…¥ onpr 11 ²³þË Â¾Ïð‰«·'…¥`y 12 13 mh £ÇÉÔ þÎ ÀÇÍÎ ÀþÓÄ−Å ¾ÔÎ−Í ðËÇÀ Â−ËÅ−Ä Ë−×©ÇÅ ÎÇ ÏÍ−Ë ¾Ïðð−Ë hm 14 ©ÅÎ ²ÇÉÔˆðËÇÀˆÂ−ËÅ−ÄfÌÇ©® hÏÍ−Ëˆ®−ÍÎj ©ÅÎ ÀþÓÄ−Åg Õ 15 mh ¢ÔÎ− ²ÇÏÅÎ Ä−Å ©Í À©Å©ÀÏÀ Çð ¾Ïðð−Ë Í©Ö− þÅ® ÀþÓÄ−Å hm 16 ©ÅÎ Ä−Å { «·'…¥ z ÀþÓÄ−Å } «·'…¥ x ÀþÓÄ−Åy 17 À−À²ÉÔfÏÍ−Ëˆ®−ÍÎj Â¾Ïðj Ä−Ågy 18 Ë−ÎÏËÅ Ä−Åy 19 Û In this code, we show the prototype for library function À−À²ÉÔ on line 7, which is designed to copy a speciﬁed number of bytes Å from one region of memory to another. The function ²ÇÉÔˆðËÇÀˆÂ−ËÅ−Ä, starting at line 14, is designed to copy some of the data main- tained by the operating system kernel to a designated region of memory accessible to the user. Most of the data structures maintained by the kernel should not be readable by a user, since they may con- tain sensitive information about other users and about other jobs running on the system, but the region shown as Â¾Ïð was intended to be one that the user could read. The parameter ÀþÓÄ−Å is intended to be the length of the buffer allocated by the user and indicated by argument ÏÍ−Ëˆ®−ÍÎ. The computation at line 16 then makes sure that no more bytes are copied than are available in either the source or the destination buffer. Suppose, however, that some malicious programmer writes code that calls ²ÇÉÔˆðËÇÀˆÂ−ËÅ−Ä with a negative value of ÀþÓÄ−Å. Then the minimum computation on line 16 will compute this value for Ä−Å, which will then be passed as the parameter Å to À−À²ÉÔ. Note, however, that parameter Å is declared as having data type Í©Ö−ˆÎ. This data type is declared (via ÎÔÉ−®−ð) in the library ﬁle ÍÎ®©Çl³. Typically, it is deﬁned to be ÏÅÍ©×Å−® for 32-bit programs and ÏÅÍ©×Å−® ÄÇÅ× for 64-bit programs. Since argument Å is unsigned, À−À²ÉÔ will treat it as a very large positive number and attempt to copy that many bytes from the kernel region to the user’s buffer. Copying that many bytes (at least 231) will not actually work, because the program will encounter invalid addresses in the process, but the program could read regions of the kernel memory for which it is not authorized. Section 2.3 Integer Arithmetic 123 Aside Security vulnerability in ×−ÎÉ−−ËÅþÀ− (continued) We can see that this problem arises due to the mismatch between data types: in one place the length parameter is signed; in another place it is unsigned. Such mismatches can be a source of bugs and, as this example shows, can even lead to security vulnerabilities. Fortunately, there were no reported cases where a programmer had exploited the vulnerability in FreeBSD. They issued a security advisory “FreeBSD-SA-02:38.signed-error” advising system administrators on how to apply a patch that would remove the vulnerability. The bug can be ﬁxed by declaring parameter ÀþÓÄ−Å to ²ÇÉÔˆðËÇÀˆÂ−ËÅ−Ä to be of type Í©Ö−ˆÎ, to be consistent with parameter Å of À−À²ÉÔ. We should also declare local variable Ä−Å and the return value to be of type Í©Ö−ˆÎ. We can characterize operation iu w as follows: principle: Unsigned addition For x and y such that 0 ≤ x, y < 2w: x iu w y = { x + y, x + y< 2w Normal x + y − 2w, 2w ≤ x + y< 2w+1 Overﬂow (2.11) The two cases of Equation 2.11 are illustrated in Figure 2.22, showing the sum x + y on the left mapping to the unsigned w-bit sum x iu w y on the right. The normal case preserves the value of x + y, while the overﬂow case has the effect of decrementing this sum by 2w. derivation: Unsigned addition In general, we can see that if x + y< 2w, the leading bit in the (w + 1)-bit represen- tation of the sum will equal 0, and hence discarding it will not change the numeric value. On the other hand, if 2w ≤ x + y< 2w+1, the leading bit in the (w + 1)-bit representation of the sum will equal 1, and hence discarding it is equivalent to subtracting 2w from the sum. An arithmetic operation is said to overﬂow when the full integer result cannot ﬁt within the word size limits of the data type. As Equation 2.11 indicates, overﬂow 2w 0 2 w+1 Overflow Normal x +uy x + y Figure 2.22 Relation between integer addition and unsigned addition. When x + y is greater than 2w − 1, the sum overﬂows. 124 Chapter 2 Representing and Manipulating Information 16 14 12 10 8 6 4 2 0 Overflow Normal 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Figure 2.23 Unsigned addition. With a 4-bit word size, addition is performed modulo 16. occurs when the two operands sum to 2w or more. Figure 2.23 shows a plot of the unsigned addition function for word size w = 4. The sum is computed modulo 24 = 16. When x + y< 16, there is no overﬂow, and x i u 4 y is simply x + y. This is shown as the region forming a sloping plane labeled “Normal.” When x + y ≥ 16, the addition overﬂows, having the effect of decrementing the sum by 16. This is shown as the region forming a sloping plane labeled “Overﬂow.” When executing C programs, overﬂows are not signaled as errors. At times, however, we might wish to determine whether or not overﬂow has occurred. principle: Detecting overﬂow of unsigned addition For x and y in the range 0 ≤ x, y ≤ UMaxw, let s .= x iu w y. Then the computation of s overﬂowed if and only if s< x (or equivalently, s< y). As an illustration, in our earlier example, we saw that 9 i u 4 12 = 5. We can see that overﬂow occurred, since 5 < 9. Section 2.3 Integer Arithmetic 125 derivation: Detecting overﬂow of unsigned addition Observe that x + y ≥ x, and hence if s did not overﬂow, we will surely have s ≥ x. On the other hand, if s did overﬂow, we have s = x + y − 2w. Given that y< 2w, we have y − 2w < 0, and hence s = x + (y − 2w)<x. Practice Problem 2.27 (solution page 188) Write a function with the following prototype: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− þ®®−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm ©ÅÎ Ïþ®®ˆÇÂfÏÅÍ©×Å−® Ój ÏÅÍ©×Å−® Ôgy This function should return 1 if arguments Ó and Ô can be added without causing overﬂow. Modular addition forms a mathematical structure known as an abelian group, named after the Norwegian mathematician Niels Henrik Abel (1802–1829). That is, it is commutative (that’s where the “abelian” part comes in) and associative; it has an identity element 0, and every element has an additive inverse. Let us consider the set of w-bit unsigned numbers with addition operation i u w. For every value x, there must be some value k u w x such that ku w x iu w x = 0. This additive inverse operation can be characterized as follows: principle: Unsigned negation For any number x such that 0 ≤ x< 2w, its w-bit unsigned negation k u w x is given by the following: k u w x = { x, x = 0 2w − x, x > 0 (2.12) This result can readily be derived by case analysis: derivation: Unsigned negation When x = 0, the additive inverse is clearly 0. For x> 0, consider the value 2w − x. Observe that this number is in the range 0 < 2w − x< 2w. We can also see that (x + 2w − x) mod 2w = 2w mod 2w = 0. Hence it is the inverse of x under iu w. Practice Problem 2.28 (solution page 188) We can represent a bit pattern of length w = 4 with a single hex digit. For an unsigned interpretation of these digits, use Equation 2.12 to ﬁll in the following table giving the values and the bit representations (in hex) of the unsigned additive inverses of the digits shown. 126 Chapter 2 Representing and Manipulating Information x ku 4 x Hex Decimal Decimal Hex o r u ¡ ¥ 2.3.2 Two’s-Complement Addition With two’s-complement addition, we must decide what to do when the result is either too large (positive) or too small (negative) to represent. Given integer values x and y in the range −2w−1 ≤ x, y ≤ 2w−1 − 1, their sum is in the range −2w ≤ x + y ≤ 2w − 2, potentially requiring w + 1 bits to represent exactly. As before, we avoid ever-expanding data sizes by truncating the representation to w bits. The result is not as familiar mathematically as modular addition, however. Let us deﬁne x i t w y to be the result of truncating the integer sum x + y to be w bits long and then viewing the result as a two’s-complement number. principle: Two’s-complement addition For integer values x and y in the range −2w−1 ≤ x, y ≤ 2w−1 − 1: x i t w y = ⎧ ⎪⎨ ⎪⎩ x + y − 2w, 2w−1 ≤ x + y Positive overﬂow x + y, −2w−1 ≤ x + y< 2w−1 Normal x + y + 2w,x + y< −2w−1 Negative overﬂow (2.13) This principle is illustrated in Figure 2.24, where the sum x + y is shown on the left, having a value in the range −2w ≤ x + y ≤ 2w − 2, and the result of truncating the sum to a w-bit two’s-complement number is shown on the right. (The labels “Case 1” to “Case 4” in this ﬁgure are for the case analysis of the formal derivation of the principle.) When the sum x + y exceeds TMaxw (case 4), we say that positive overﬂow has occurred. In this case, the effect of truncation is to subtract 2w from the sum. When the sum x + y is less than TMinw (case 1), we say that negative overﬂow has occurred. In this case, the effect of truncation is to add 2w to the sum. The w-bit two’s-complement sum of two numbers has the exact same bit-level representation as the unsigned sum. In fact, most computers use the same machine instruction to perform either unsigned or signed addition. derivation: Two’s-complement addition Since two’s-complement addition has the exact same bit-level representation as unsigned addition, we can characterize the operation it w as one of converting its arguments to unsigned, performing unsigned addition, and then converting back to two’s complement: Section 2.3 Integer Arithmetic 127 Figure 2.24 Relation between integer and two’s-complement addition. When x + y is less than −2w−1, there is a negative overﬂow. When it is greater than or equal to 2w−1, there is a positive overﬂow. +2 w –2 w 00 +2 w–1 +2 w–1 –2 w–1 –2w–1 Negative overflow Positive overflow Case 4 Case 3 Case 2 Case 1 Normal x + ty x + y x it w y = U2T w(T2U w(x) iu w T2U w(y)) (2.14) By Equation 2.6, we can write T2U w(x) as xw−12w + x and T2U w(y) as yw−12w + y. Using the property that iu w is simply addition modulo 2w, along with the properties of modular addition, we then have x i t w y = U2T w(T2U w(x) iu w T2U w(y)) = U2T w[(xw−12w + x + yw−12w + y) mod 2w] = U2T w[(x + y) mod 2w] The terms xw−12w and yw−12w drop out since they equal 0 modulo 2w. To better understand this quantity, let us deﬁne z as the integer sum z .= x + y, z′ as z′ .= z mod 2w, and z′′ as z′′ .= U2T w(z′). The value z′′ is equal to x i t w y.We can divide the analysis into four cases as illustrated in Figure 2.24: 1. −2w ≤ z< −2w−1. Then we will have z′ = z + 2w. This gives 0 ≤ z′ < −2w−1 + 2w = 2w−1. Examining Equation 2.7, we see that z′ is in the range such that z′′ = z′. This is the case of negative overﬂow. We have added two negative numbers x and y (that’s the only way we can have z< −2w−1) and obtained a nonnegative result z′′ = x + y + 2w. 2. −2w−1 ≤ z< 0. Then we will again have z′ = z + 2w, giving −2w−1 + 2w = 2w−1 ≤ z′ < 2w. Examining Equation 2.7, we see that z′ is in such a range that z′′ = z′ − 2w, and therefore z′′ = z′ − 2w = z + 2w − 2w = z. That is, our two’s- complement sum z′′ equals the integer sum x + y. 3. 0 ≤ z< 2w−1. Then we will have z′ = z, giving 0 ≤ z′ < 2w−1, and hence z′′ = z′ = z. Again, the two’s-complement sum z′′ equals the integer sum x + y. 4. 2w−1 ≤ z< 2w. We will again have z′ = z, giving 2w−1 ≤ z′ < 2w. But in this range we have z′′ = z′ − 2w, giving z′′ = x + y − 2w. This is the case of positive overﬂow. We have added two positive numbers x and y (that’s the only way we can have z ≥ 2w−1) and obtained a negative result z′′ = x + y − 2w. 128 Chapter 2 Representing and Manipulating Information xy x + yx it 4 y Case −8 −5 −13 3 1 [1000] [1011] [10011] [0011] −8 −8 −16 0 1 [1000] [1000] [10000] [0000] −85 −3 −32 [1000] [0101] [11101] [1101] 25 77 3 [0010] [0101] [00111] [0111] 55 10 −64 [0101] [0101] [01010] [1010] Figure 2.25 Two’s-complement addition examples. The bit-level representation of the 4-bit two’s-complement sum can be obtained by performing binary addition of the operands and truncating the result to 4 bits. As illustrations of two’s-complement addition, Figure 2.25 shows some exam- ples when w = 4. Each example is labeled by the case to which it corresponds in the derivation of Equation 2.13. Note that 24 = 16, and hence negative overﬂow yields a result 16 more than the integer sum, and positive overﬂow yields a result 16 less. We include bit-level representations of the operands and the result. Observe that the result can be obtained by performing binary addition of the operands and truncating the result to 4 bits. Figure 2.26 illustrates two’s-complement addition for word size w = 4. The operands range between −8 and 7. When x + y< −8, two’s-complement addition has a negative overﬂow, causing the sum to be incremented by 16. When −8 ≤ x + y< 8, the addition yields x + y. When x + y ≥ 8, the addition has a positive overﬂow, causing the sum to be decremented by 16. Each of these three ranges forms a sloping plane in the ﬁgure. Equation 2.13 also lets us identify the cases where overﬂow has occurred: principle: Detecting overﬂow in two’s-complement addition For x and y in the range TMinw ≤ x, y ≤ TMaxw, let s .= x it w y. Then the compu- tation of s has had positive overﬂow if and only if x> 0 and y> 0 but s ≤ 0. The computation has had negative overﬂow if and only if x< 0 and y< 0 but s ≥ 0. Figure 2.25 shows several illustrations of this principle for w = 4. The ﬁrst entry shows a case of negative overﬂow, where two negative numbers sum to a positive one. The ﬁnal entry shows a case of positive overﬂow, where two positive numbers sum to a negative one. Section 2.3 Integer Arithmetic 129 Normal Negative overflow Positive overflow8 6 4 2 0 22 24 26 28 28 28 26 22 24 0 2 4 6 26 24 22 0 2 4 6 Figure 2.26 Two’s-complement addition. With a 4-bit word size, addition can have a negative overﬂow when x + y< −8 and a positive overﬂow when x + y ≥ 8. derivation: Detecting overﬂow of two’s-complement addition Let us ﬁrst do the analysis for positive overﬂow. If both x> 0 and y> 0 but s ≤ 0, then clearly positive overﬂow has occurred. Conversely, positive overﬂow requires (1) that x> 0 and y> 0 (otherwise, x + y< TMaxw) and (2) that s ≤ 0 (from Equation 2.13). A similar set of arguments holds for negative overﬂow. Practice Problem 2.29 (solution page 188) Fill in the following table in the style of Figure 2.25. Give the integer values of the 5-bit arguments, the values of both their integer and two’s-complement sums, the bit-level representation of the two’s-complement sum, and the case from the derivation of Equation 2.13. xy x + yx i t 5 y Case [10100] [10001] 130 Chapter 2 Representing and Manipulating Information xy x + yx i t 5 y Case [11000] [11000] [10111] [01000] [00010] [00101] [01100] [00100] Practice Problem 2.30 (solution page 189) Write a function with the following prototype: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− þ®®−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm ©ÅÎ Îþ®®ˆÇÂf©ÅÎ Ój ©ÅÎ Ôgy This function should return 1 if arguments Ó and Ô can be added without causing overﬂow. Practice Problem 2.31 (solution page 189) Your coworker gets impatient with your analysis of the overﬂow conditions for two’s-complement addition and presents you with the following implementation of Îþ®®ˆÇÂ: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− þ®®−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ôl hm ©ÅÎ Îþ®®ˆÇÂf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ ÍÏÀ { ÓiÔy Ë−ÎÏËÅ fÍÏÀkÓ {{ Ôg dd fÍÏÀkÔ {{ Ógy Û You look at the code and laugh. Explain why. Practice Problem 2.32 (solution page 189) You are assigned the task of writing code for a function ÎÍÏ¾ˆÇÂ, with arguments Ó and Ô, that will return 1 if computing ÓkÔ does not cause overﬂow. Having just written the code for Problem 2.30, you write the following: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− ÍÏ¾ÎËþ²Î−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ôl hm ©ÅÎ ÎÍÏ¾ˆÇÂf©ÅÎ Ój ©ÅÎ Ôg Õ Section 2.3 Integer Arithmetic 131 Ë−ÎÏËÅ Îþ®®ˆÇÂfÓj kÔgy Û For what values of Ó and Ô will this function give incorrect results? Writing a correct version of this function is left as an exercise (Problem 2.74). 2.3.3 Two’s-Complement Negation We can see that every number x in the range TMinw ≤ x ≤ TMaxw has an additive inverse under i t w, which we denote kt w x as follows: principle: Two’s-complement negation For x in the range TMinw ≤ x ≤ TMaxw, its two’s-complement negation kt w x is given by the formula k t w x = { TMinw,x = TMinw −x, x > TMinw (2.15) That is, for w-bit two’s-complement addition, TMinw is its own additive in- verse, while any other value x has −x as its additive inverse. derivation: Two’s-complement negation Observe that TMinw + TMinw =−2w−1 +−2w−1 =−2w. This would cause nega- tive overﬂow, and hence TMinw it w TMinw =−2w + 2w = 0. For values of x such that x> TMinw, the value −x can also be represented as a w-bit two’s-complement number, and their sum will be −x + x = 0. Practice Problem 2.33 (solution page 189) We can represent a bit pattern of length w = 4 with a single hex digit. For a two’s- complement interpretation of these digits, ﬁll in the following table to determine the additive inverses of the digits shown: x kt 4 x Hex Decimal Decimal Hex p q w ¢ £ What do you observe about the bit patterns generated by two’s-complement and unsigned (Problem 2.28) negation? 132 Chapter 2 Representing and Manipulating Information Web Aside DATA:TNEG Bit-level representation of two’s-complement negation There are several clever ways to determine the two’s-complement negation of a value represented at the bit level. The following two techniques are both useful, such as when one encounters the value nÓðððððððþ when debugging a program, and they lend insight into the nature of the two’s-complement representation. One technique for performing two’s-complement negation at the bit level is to complement the bits and then increment the result. In C, we can state that for any integer value Ó, computing the expressions kÓ and ÜÓio will give identical results. Here are some examples with a 4-bit word size: ⃗x Ü⃗x incr(Ü⃗x) [0101] 5 [1010] −6 [1011] −5 [0111] 7 [1000] −8 [1001] −7 [1100] −4 [0011] 3 [0100] 4 [0000] 0 [1111] −1 [0000] 0 [1000] −8 [0111] 7 [1000] −8 For our earlier example, we know that the complement of nÓð is nÓn and the complement of nÓþ is nÓs, and so nÓðððððððþ is the two’s-complement representation of −6. A second way to perform two’s-complement negation of a number x is based on splitting the bit vector into two parts. Let k be the position of the rightmost 1, so the bit-level representation of x has the form [xw−1,xw−2,. . .,xk+1, 1, 0, ... 0]. (This is possible as long as x ̸= 0.) The negation is then written in binary form as [Üxw−1, Üxw−2,... Ü xk+1, 1, 0,..., 0]. That is, we complement each bit to the left of bit position k. We illustrate this idea with some 4-bit numbers, where we highlight the rightmost pattern 1, 0, ... , 0 in italics: x −x [1100] −4[0100]4 [1000] −8[1000] −8 [0101] 5 [1011] −5 [0111] 7 [1001] −7 2.3.4 Unsigned Multiplication Integers x and y in the range 0 ≤ x, y ≤ 2w − 1 can be represented as w-bit un- signed numbers, but their product x . y can range between 0 and (2w − 1)2 = 22w − 2w+1 + 1. This could require as many as 2w bits to represent. Instead, un- signed multiplication in C is deﬁned to yield the w-bit value given by the low-order w bits of the 2w-bit integer product. Let us denote this value as x hu w y. Truncating an unsigned number to w bits is equivalent to computing its value modulo 2w, giving the following: Section 2.3 Integer Arithmetic 133 principle: Unsigned multiplication For x and y such that 0 ≤ x, y ≤ UMaxw: x hu w y = (x . y) mod 2w (2.16) 2.3.5 Two’s-Complement Multiplication Integers x and y in the range −2w−1 ≤ x, y ≤ 2w−1 − 1 can be represented as w-bit two’s-complement numbers, but their product x . y can range between −2w−1 . (2w−1 − 1) =−22w−2 + 2w−1 and −2w−1 . −2w−1 = 22w−2. This could require as many as 2w bits to represent in two’s-complement form. Instead, signed multi- plication in C generally is performed by truncating the 2w-bit product to w bits. We denote this value as x h t w y. Truncating a two’s-complement number to w bits is equivalent to ﬁrst computing its value modulo 2w and then converting from unsigned to two’s complement, giving the following: principle: Two’s-complement multiplication For x and y such that TMinw ≤ x, y ≤ TMaxw: x h t w y = U2T w((x . y) mod 2w) (2.17) We claim that the bit-level representation of the product operation is identical for both unsigned and two’s-complement multiplication, as stated by the following principle: principle: Bit-level equivalence of unsigned and two’s-complement multipli- cation Let ⃗x and ⃗y be bit vectors of length w. Deﬁne integers x and y as the values repre- sented by these bits in two’s-complement form: x = B2T w(⃗x) and y = B2T w(⃗y). Deﬁne nonnegative integers x′ and y′ as the values represented by these bits in unsigned form: x′ = B2U w(⃗x) and y′ = B2U w(⃗y). Then T2Bw(x ht w y) = U2Bw(x′ hu w y′) As illustrations, Figure 2.27 shows the results of multiplying different 3-bit numbers. For each pair of bit-level operands, we perform both unsigned and two’s-complement multiplication, yielding 6-bit products, and then truncate these to 3 bits. The unsigned truncated product always equals x . y mod 8. The bit- level representations of both truncated products are identical for both unsigned and two’s-complement multiplication, even though the full 6-bit representations differ. 134 Chapter 2 Representing and Manipulating Information Mode xy x . y Truncated x . y Unsigned 5 [101] 3 [011] 15 [001111] 7 [111] Two’s complement −3 [101] 3 [011] −9 [110111] −1 [111] Unsigned 4 [100] 7 [111] 28 [011100] 4 [100] Two’s complement −4 [100] −1 [111] 4 [000100] −4 [100] Unsigned 3 [011] 3 [011] 9 [001001] 1 [001] Two’s complement 3 [011] 3 [011] 9 [001001] 1 [001] Figure 2.27 Three-bit unsigned and two’s-complement multiplication examples. Although the bit-level representations of the full products may differ, those of the truncated products are identical. derivation: Bit-level equivalence of unsigned and two’s-complement multipli- cation From Equation 2.6, we have x′ = x + xw−12w and y′ = y + yw−12w. Computing the product of these values modulo 2w gives the following: (x′ . y′) mod 2w = [(x + xw−12w) . (y + yw−12w)] mod 2w (2.18) = [x . y + (xw−1y + yw−1x)2w + xw−1yw−122w] mod 2w = (x . y) mod 2w The terms with weight 2w and 22w drop out due to the modulus operator. By Equa- tion 2.17, we have x ht w y = U2T w((x . y) mod 2w). We can apply the operation T2U w to both sides to get T2U w(x ht w y) = T2U w(U2T w((x . y) mod 2w)) = (x . y) mod 2w Combining this result with Equations 2.16 and 2.18 shows that T2U w(x h t w y) = (x′ . y′) mod 2w = x′ hu w y′. We can then apply U2Bw to both sides to get U2Bw(T2U w(x ht w y)) = T2Bw(x ht w y) = U2Bw(x′ h u w y′) Practice Problem 2.34 (solution page 189) Fill in the following table showing the results of multiplying different 3-bit num- bers, in the style of Figure 2.27: Mode xy x . y Truncated x . y Unsigned [100] [101] Two’s complement [100] [101] Unsigned [010] [111] Two’s complement [010] [111] Section 2.3 Integer Arithmetic 135 Mode xy x . y Truncated x . y Unsigned [110] [110] Two’s complement [110] [110] Practice Problem 2.35 (solution page 190) You are given the assignment to develop code for a function ÎÀÏÄÎˆÇÂ that will determine whether two arguments can be multiplied without causing overﬂow. Here is your solution: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− ÀÏÄÎ©ÉÄ©−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm ©ÅÎ ÎÀÏÄÎˆÇÂf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ É { ÓhÔy mh ¥©Î³−Ë Ó ©Í Ö−ËÇj ÇË ®©Ì©®©Å× É ¾Ô Ó ×©Ì−Í Ô hm Ë−ÎÏËÅ _Ó ÚÚ ÉmÓ {{ Ôy Û You test this code for a number of values of Ó and Ô, and it seems to work properly. Your coworker challenges you, saying, “If I can’t use subtraction to test whether addition has overﬂowed (see Problem 2.31), then how can you use division to test whether multiplication has overﬂowed?” Devise a mathematical justiﬁcation of your approach, along the following lines. First, argue that the case x = 0 is handled correctly. Otherwise, consider w-bit numbers x (x ̸= 0), y, p, and q, where p is the result of performing two’s- complement multiplication on x and y, and q is the result of dividing p by x. 1. Show that x . y, the integer product of x and y, can be written in the form x . y = p + t2w, where t ̸= 0 if and only if the computation of p overﬂows. 2. Show that p can be written in the form p = x . q + r, where |r| < |x|. 3. Show that q = y if and only if r = t = 0. Practice Problem 2.36 (solution page 190) For the case where data type ©ÅÎ has 32 bits, devise a version of ÎÀÏÄÎˆÇÂ (Prob- lem 2.35) that uses the 64-bit precision of data type ©ÅÎtrˆÎ, without using division. Practice Problem 2.37 (solution page 191) You are given the task of patching the vulnerability in the XDR code shown in the aside on page 136 for the case where both data types ©ÅÎ and Í©Ö−ˆÎ are 32 bits. You decide to eliminate the possibility of the multiplication overﬂowing by computing the number of bytes to allocate using data type Ï©ÅÎtrˆÎ. You replace 136 Chapter 2 Representing and Manipulating Information Aside Security vulnerability in the XDR library In 2002, it was discovered that code supplied by Sun Microsystems to implement the XDR library, a widely used facility for sharing data structures between programs, had a security vulnerability arising from the fact that multiplication can overﬂow without any notice being given to the program. Code similar to that containing the vulnerability is shown below: 1 mh 'ÄÄÏÍÎËþÎ©ÇÅ Çð ²Ç®− ÌÏÄÅ−Ëþ¾©Ä©ÎÔ Í©À©ÄþË ÎÇ Î³þÎ ðÇÏÅ® ©Å 2 h ·ÏÅ’Í ”⁄‡ Ä©¾ËþËÔl 3 hm 4 ÌÇ©®h ²ÇÉÔˆ−Ä−À−ÅÎÍfÌÇ©® h−Ä−ˆÍË²‰`j ©ÅÎ −Ä−ˆ²ÅÎj Í©Ö−ˆÎ −Ä−ˆÍ©Ö−g Õ 5 mh 6 h ¡ÄÄÇ²þÎ− ¾Ïðð−Ë ðÇË −Ä−ˆ²ÅÎ Ç¾Á−²ÎÍj −þ²³ Çð −Ä−ˆÍ©Ö− ¾ÔÎ−Í 7 h þÅ® ²ÇÉÔ ðËÇÀ ÄÇ²þÎ©ÇÅÍ ®−Í©×ÅþÎ−® ¾Ô −Ä−ˆÍË² 8 hm 9 ÌÇ©® hË−ÍÏÄÎ { ÀþÄÄÇ²f−Ä−ˆ²ÅÎ h −Ä−ˆÍ©Ö−gy 10 ©ð fË−ÍÏÄÎ {{ ﬁ•‹‹g 11 mh ÀþÄÄÇ² ðþ©Ä−® hm 12 Ë−ÎÏËÅ ﬁ•‹‹y 13 ÌÇ©® hÅ−ÓÎ { Ë−ÍÏÄÎy 14 ©ÅÎ ©y 15 ðÇË f© { ny © z −Ä−ˆ²ÅÎy ©iig Õ 16 mh £ÇÉÔ Ç¾Á−²Î © ÎÇ ®−ÍÎ©ÅþÎ©ÇÅ hm 17 À−À²ÉÔfÅ−ÓÎj −Ä−ˆÍË²‰©`j −Ä−ˆÍ©Ö−gy 18 mh ›ÇÌ− ÉÇ©ÅÎ−Ë ÎÇ Å−ÓÎ À−ÀÇËÔ Ë−×©ÇÅ hm 19 Å−ÓÎ i{ −Ä−ˆÍ©Ö−y 20 Û 21 Ë−ÎÏËÅ Ë−ÍÏÄÎy 22 Û The function ²ÇÉÔˆ−Ä−À−ÅÎÍ is designed to copy −Ä−ˆ²ÅÎ data structures, each consisting of −Ä−ˆ Í©Ö− bytes into a buffer allocated by the function on line 9. The number of bytes required is computed as −Ä−ˆ²ÅÎ h −Ä−ˆÍ©Ö−. Imagine, however, that a malicious programmer calls this function with −Ä−ˆ²ÅÎ being 1,048,577 (220 + 1) and −Ä−ˆÍ©Ö− being 4,096 (212) with the program compiled for 32 bits. Then the multiplication on line 9 will overﬂow, causing only 4,096 bytes to be allocated, rather than the 4,294,971,392 bytes required to hold that much data. The loop starting at line 15 will attempt to copy all of those bytes, overrunning the end of the allocated buffer, and therefore corrupting other data structures. This could cause the program to crash or otherwise misbehave. The Sun code was used by almost every operating system and in such widely used programs as Internet Explorer and the Kerberos authentication system. The Computer Emergency Response Team (CERT), an organization run by the Carnegie Mellon Software Engineering Institute to track security vulnerabilities and breaches, issued advisory “CA-2002-25,” and many companies rushed to patch their code. Fortunately, there were no reported security breaches caused by this vulnerability. A similar vulnerability existed in many implementations of the library function ²þÄÄÇ². These have since been patched. Unfortunately, many programmers call allocation functions, such as ÀþÄÄÇ², using arithmetic expressions as arguments, without checking these expressions for overﬂow. Writing a reliable version of ²þÄÄÇ² is left as an exercise (Problem 2.76). Section 2.3 Integer Arithmetic 137 the original call to ÀþÄÄÇ² (line 9) as follows: Ï©ÅÎtrˆÎ þÍ©Ö− { −Ä−ˆ²ÅÎ h fÏ©ÅÎtrˆÎg −Ä−ˆÍ©Ö−y ÌÇ©® hË−ÍÏÄÎ { ÀþÄÄÇ²fþÍ©Ö−gy Recall that the argument to ÀþÄÄÇ² has type Í©Ö−ˆÎ. A. Does your code provide any improvement over the original? B. How would you change the code to eliminate the vulnerability? 2.3.6 Multiplying by Constants Historically, the integer multiply instruction on many machines was fairly slow, requiring 10 or more clock cycles, whereas other integer operations—such as addition, subtraction, bit-level operations, and shifting—required only 1 clock cycle. Even on the Intel Core i7 Haswell we use as our reference machine, integer multiply requires 3 clock cycles. As a consequence, one important optimization used by compilers is to attempt to replace multiplications by constant factors with combinations of shift and addition operations. We will ﬁrst consider the case of multiplying by a power of 2, and then we will generalize this to arbitrary constants. principle: Multiplication by a power of 2 Let x be the unsigned integer represented by bit pattern [xw−1,xw−2,. . .,x0]. Then for any k ≥ 0, the w + k-bit unsigned representation of x2k is given by [xw−1,xw−2,...,x0, 0,..., 0], where k zeros have been added to the right. So, for example, 11 can be represented for w = 4 as [1011]. Shifting this left by k = 2 yields the 6-bit vector [101100], which encodes the unsigned number 11 . 4 = 44. derivation: Multiplication by a power of 2 This property can be derived using Equation 2.1: B2U w+k([xw−1,xw−2,...,x0, 0,..., 0]) = w−1∑ i=0 xi2i+k = [w−1∑ i=0 xi2i] . 2k = x2k When shifting left by k for a ﬁxed word size, the high-order k bits are discarded, yielding [xw−k−1,xw−k−2,...,x0, 0,..., 0] 138 Chapter 2 Representing and Manipulating Information but this is also the case when performing multiplication on ﬁxed-size words. We can therefore see that shifting a value left is equivalent to performing unsigned multiplication by a power of 2: principle: Unsigned multiplication by a power of 2 For C variables Ó and Â with unsigned values x and k, such that 0 ≤ k< w, the C expression ÓzzÂ yields the value x h u w 2k. Since the bit-level operation of ﬁxed-size two’s-complement arithmetic is equivalent to that for unsigned arithmetic, we can make a similar statement about the relationship between left shifts and multiplication by a power of 2 for two’s- complement arithmetic: principle: Two’s-complement multiplication by a power of 2 For C variables Ó and Â with two’s-complement value x and unsigned value k, such that 0 ≤ k< w, the C expression ÓzzÂ yields the value x h t w 2k. Note that multiplying by a power of 2 can cause overﬂow with either unsigned or two’s-complement arithmetic. Our result shows that even then we will get the same effect by shifting. Returning to our earlier example, we shifted the 4-bit pattern [1011] (numeric value 11) left by two positions to get [101100] (numeric value 44). Truncating this to 4 bits gives [1100] (numeric value 12 = 44 mod 16). Given that integer multiplication is more costly than shifting and adding, many C compilers try to remove many cases where an integer is being multiplied by a constant with combinations of shifting, adding, and subtracting. For example, sup- pose a program contains the expression Óhor. Recognizing that 14 = 23 + 22 + 21, the compiler can rewrite the multiplication as fÓzzqg i fÓzzpg i fÓzzog, replac- ing one multiplication with three shifts and two additions. The two computations will yield the same result, regardless of whether Ó is unsigned or two’s comple- ment, and even if the multiplication would cause an overﬂow. Even better, the compiler can also use the property 14 = 24 − 21 to rewrite the multiplication as fÓzzrg k fÓzzog, requiring only two shifts and a subtraction. Practice Problem 2.38 (solution page 191) As we will see in Chapter 3, the lea instruction can perform computations of the form fþzzÂg i ¾, where Â is either 0, 1, 2, or 3, and ¾ is either 0 or some program value. The compiler often uses this instruction to perform multiplications by constant factors. For example, we can compute qhþ as fþzzog i þ. Considering cases where ¾ is either 0 or equal to þ, and all possible values of Â, what multiples of þ can be computed with a single lea instruction? Generalizing from our example, consider the task of generating code for the expression Óh K, for some constant K. The compiler can express the binary representation of K as an alternating sequence of zeros and ones: Section 2.3 Integer Arithmetic 139 [(0 ... 0)(1 ... 1)(0 ... 0) ... (1 ... 1)] For example, 14 can be written as [(0 ... 0)(111)(0)]. Consider a run of ones from bit position n down to bit position m (n ≥ m). (For the case of 14, we have n = 3 and m = 1.) We can compute the effect of these bits on the product using either of two different forms: Form A: fÓzzng i fÓzz(n − 1)gi ... i fÓzzmg Form B: fÓzz(n + 1)g k fÓzzmg By adding together the results for each run, we are able to compute Óh K with- out any multiplications. Of course, the trade-off between using combinations of shifting, adding, and subtracting versus a single multiplication instruction depends on the relative speeds of these instructions, and these can be highly machine de- pendent. Most compilers only perform this optimization when a small number of shifts, adds, and subtractions sufﬁce. Practice Problem 2.39 (solution page 192) How could we modify the expression for form B for the case where bit position n is the most signiﬁcant bit? Practice Problem 2.40 (solution page 192) For each of the following values of K, ﬁnd ways to express Óh K using only the speciﬁed number of operations, where we consider both additions and subtrac- tions to have comparable cost. You may need to use some tricks beyond the simple form A and B rules we have considered so far. K Shifts Add/Subs Expression 71 1 30 4 3 28 2 1 55 2 2 Practice Problem 2.41 (solution page 192) For a run of ones starting at bit position n down to bit position m (n ≥ m), we saw that we can generate two forms of code, A and B. How should the compiler decide which form to use? 2.3.7 Dividing by Powers of 2 Integer division on most machines is even slower than integer multiplication— requiring 30 or more clock cycles. Dividing by a power of 2 can also be performed 140 Chapter 2 Representing and Manipulating Information Â|| Â (binary) Decimal 12,340/2Â 0 nnoonnnnnnoononn 12,340 12,340.0 1 0nnoonnnnnnoonon 6,170 6,170.0 4 0000nnoonnnnnnoo 771 771.25 8 00000000nnoonnnn 48 48.203125 Figure 2.28 Dividing unsigned numbers by powers of 2. The examples illustrate how performing a logical right shift by Â has the same effect as dividing by 2Â and then rounding toward zero. using shift operations, but we use a right shift rather than a left shift. The two different right shifts—logical and arithmetic—serve this purpose for unsigned and two’s-complement numbers, respectively. Integer division always rounds toward zero. To deﬁne this precisely, let us introduce some notation. For any real number a, deﬁne ⌊a⌋ to be the unique integer a′ such that a′ ≤ a< a′ + 1. As examples, ⌊3.14⌋= 3, ⌊−3.14⌋=−4, and ⌊3⌋= 3. Similarly, deﬁne ⌈a⌉ to be the unique integer a′ such that a′ − 1 <a ≤ a′. As examples, ⌈3.14⌉= 4, ⌈−3.14⌉=−3, and ⌈3⌉= 3. For x ≥ 0 and y> 0, integer division should yield ⌊x/y⌋, while for x< 0 and y> 0, it should yield ⌈x/y⌉. That is, it should round down a positive result but round up a negative one. The case for using shifts with unsigned arithmetic is straightforward, in part because right shifting is guaranteed to be performed logically for unsigned values. principle: Unsigned division by a power of 2 For C variables Ó and Â with unsigned values x and k, such that 0 ≤ k< w, the C expression Ó||Â yields the value ⌊x/2k⌋. As examples, Figure 2.28 shows the effects of performing logical right shifts on a 16-bit representation of 12,340 to perform division by 1, 2, 16, and 256. The zeros shifted in from the left are shown in italics. We also show the result we would obtain if we did these divisions with real arithmetic. These examples show that the result of shifting consistently rounds toward zero, as is the convention for integer division. derivation: Unsigned division by a power of 2 Let x be the unsigned integer represented by bit pattern [xw−1,xw−2, ... , x0], and let k be in the range 0 ≤ k< w. Let x′ be the unsigned number with w − k-bit representation [xw−1,xw−2, ... , xk], and let x′′ be the unsigned number with k-bit representation [xk−1,. . .,x0]. We can therefore see that x = 2kx′ + x′′, and that 0 ≤ x′′ < 2k. It therefore follows that ⌊x/2k⌋= x′. Performing a logical right shift of bit vector [xw−1,xw−2,...,x0]by k yields the bit vector [0,..., 0,xw−1,xw−2,...,xk] Section 2.3 Integer Arithmetic 141 Â|| Â (binary) Decimal −12,340/2Â 0 oonnoooooonnoonn −12,340 −12,340.0 1 1oonnoooooonnoon −6,170 −6,170.0 4 1111oonnoooooonn −772 −771.25 8 11111111oonnoooo −49 −48.203125 Figure 2.29 Applying arithmetic right shift. The examples illustrate that arithmetic right shift is similar to division by a power of 2, except that it rounds down rather than toward zero. This bit vector has numeric value x′, which we have seen is the value that would result by computing the expression Ó||Â. The case for dividing by a power of 2 with two’s-complement arithmetic is slightly more complex. First, the shifting should be performed using an arithmetic right shift, to ensure that negative values remain negative. Let us investigate what value such a right shift would produce. principle: Two’s-complement division by a power of 2, rounding down Let C variables Ó and Â have two’s-complement value x and unsigned value k, respectively, such that 0 ≤ k< w. The C expression Ó||Â, when the shift is performed arithmetically, yields the value ⌊x/2k⌋. For x ≥ 0, variable Ó has 0 as the most signiﬁcant bit, and so the effect of an arithmetic shift is the same as for a logical right shift. Thus, an arithmetic right shift by k is the same as division by 2k for a nonnegative number. As an example of a negative number, Figure 2.29 shows the effect of applying arithmetic right shift to a 16-bit representation of −12,340 for different shift amounts. For the case when no rounding is required (k = 1), the result will be x/2k. When rounding is required, shifting causes the result to be rounded downward. For example, the shifting right by four has the effect of rounding −771.25 down to −772. We will need to adjust our strategy to handle division for negative values of x. derivation: Two’s-complement division by a power of 2, rounding down Let x be the two’s-complement integer represented by bit pattern [xw−1,xw−2, . . .,x0], and let k be in the range 0 ≤ k< w. Let x′ be the two’s-complement number represented by the w − k bits [xw−1,xw−2, ... , xk], and let x′′ be the unsigned number represented by the low-order k bits [xk−1,...,x0]. By a similar analysis as the unsigned case, we have x = 2kx′ + x′′ and 0 ≤ x′′ < 2k, giving x′ = ⌊x/2k⌋. Furthermore, observe that shifting bit vector [xw−1,xw−2,. . .,x0] right arithmetically by k yields the bit vector [xw−1,..., xw−1, xw−1,xw−2,...,xk] which is the sign extension from w − k bits to w bits of [xw−1,xw−2,. . .,xk]. Thus, this shifted bit vector is the two’s-complement representation of ⌊x/2k⌋. 142 Chapter 2 Representing and Manipulating Information Â Bias −12,340 + bias (binary) || Â (binary) Decimal −12,340/2Â 00 oonnoooooonnoonn oonnoooooonnoonn −12,340 −12,340.0 11 oonnoooooonnoon11oonnoooooonnoon −6,170 −6,170.0 415 oonnoooooono1011 1111oonnoooooono −771 −771.25 8 255 oononnnn11001011 11111111oononnnn −48 −48.203125 Figure 2.30 Dividing two’s-complement numbers by powers of 2. By adding a bias before the right shift, the result is rounded toward zero. We can correct for the improper rounding that occurs when a negative number is shifted right by “biasing” the value before shifting. principle: Two’s-complement division by a power of 2, rounding up Let C variables Ó and Â have two’s-complement value x and unsigned value k, respectively, such that 0 ≤ k< w. The C expression fÓifozzÂgkog||Â, when the shift is performed arithmetically, yields the value ⌈x/2k⌉. Figure 2.30 demonstrates how adding the appropriate bias before performing the arithmetic right shift causes the result to be correctly rounded. In the third column, we show the result of adding the bias value to −12,340, with the lower k bits (those that will be shifted off to the right) shown in italics. We can see that the bits to the left of these may or may not be incremented. For the case where no rounding is required (k = 1), adding the bias only affects bits that are shifted off. For the cases where rounding is required, adding the bias causes the upper bits to be incremented, so that the result will be rounded toward zero. The biasing technique exploits the property that ⌈x/y⌉=⌊(x + y − 1)/y⌋ for integers x and y such that y> 0. As examples, when x =−30 and y = 4, we have x + y − 1 =−27 and ⌈−30/4⌉=−7 =⌊−27/4⌋. When x =−32 and y = 4, we have x + y − 1 =−29 and ⌈−32/4⌉=−8 =⌊−29/4⌋. derivation: Two’s-complement division by a power of 2, rounding up To see that ⌈x/y⌉=⌊(x + y − 1)/y⌋, suppose that x = qy + r, where 0 ≤ r< y, giving (x + y − 1)/y = q + (r + y − 1)/y, and so ⌊(x + y − 1)/y⌋= q +⌊(r + y − 1)/y⌋. The latter term will equal 0 when r = 0 and 1 when r> 0. That is, by adding a bias of y − 1to x and then rounding the division downward, we will get q when y divides x and q + 1 otherwise. Returning to the case where y = 2k, the C expression ÓifozzÂgko yields the value x + 2k − 1. Shifting this right arithmetically by k therefore yields ⌈x/2k⌉. These analyses show that for a two’s-complement machine using arithmetic right shifts, the C expression fÓzn } ÓifozzÂgko x Óg || Â will compute the value x/2k. Section 2.3 Integer Arithmetic 143 Practice Problem 2.42 (solution page 192) Write a function ®©Ìot that returns the value Ómot for integer argument Ó. Your function should not use division, modulus, multiplication, any conditionals (©ð or }x), any comparison operators (e.g., z, |,or {{), or any loops. You may assume that data type ©ÅÎ is 32 bits long and uses a two’s-complement representation, and that right shifts are performed arithmetically. We now see that division by a power of 2 can be implemented using logical or arithmetic right shifts. This is precisely the reason the two types of right shifts are available on most machines. Unfortunately, this approach does not generalize to division by arbitrary constants. Unlike multiplication, we cannot express division by arbitrary constants K in terms of division by powers of 2. Practice Problem 2.43 (solution page 193) In the following code, we have omitted the deﬁnitions of constants › and ﬁ: a®−ð©Å− › mh ›ÔÍÎ−ËÔ ÅÏÀ¾−Ë o hm a®−ð©Å− ﬁ mh ›ÔÍÎ−ËÔ ÅÏÀ¾−Ë p hm ©ÅÎ þË©Î³f©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ Ë−ÍÏÄÎ { ny Ë−ÍÏÄÎ { Óh› i Ômﬁy mh › þÅ® ﬁ þË− ÀÔÍÎ−ËÔ ÅÏÀ¾−ËÍl hm Ë−ÎÏËÅ Ë−ÍÏÄÎy Û We compiled this code for particular values of › and ﬁ. The compiler opti- mized the multiplication and division using the methods we have discussed. The following is a translation of the generated machine code back into C: mh ¶ËþÅÍÄþÎ©ÇÅ Çð þÍÍ−À¾ÄÔ ²Ç®− ðÇË þË©Î³ hm ©ÅÎ ÇÉÎþË©Î³f©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎÎ{Óy Ó zz{ sy Ók{Îy ©ðfÔzngÔi{uy Ô ||{ qy mh ¡Ë©Î³À−Î©² Í³©ðÎ hm Ë−ÎÏËÅ ÓiÔy Û What are the values of › and ﬁ? 2.3.8 Final Thoughts on Integer Arithmetic As we have seen, the “integer” arithmetic performed by computers is really a form of modular arithmetic. The ﬁnite word size used to represent numbers 144 Chapter 2 Representing and Manipulating Information limits the range of possible values, and the resulting operations can overﬂow. We have also seen that the two’s-complement representation provides a clever way to represent both negative and positive values, while using the same bit-level implementations as are used to perform unsigned arithmetic—operations such as addition, subtraction, multiplication, and even division have either identical or very similar bit-level behaviors, whether the operands are in unsigned or two’s- complement form. We have seen that some of the conventions in the C language can yield some surprising results, and these can be sources of bugs that are hard to recognize or understand. We have especially seen that the ÏÅÍ©×Å−® data type, while conceptu- ally straightforward, can lead to behaviors that even experienced programmers do not expect. We have also seen that this data type can arise in unexpected ways—for example, when writing integer constants and when invoking library routines. Practice Problem 2.44 (solution page 193) Assume data type ©ÅÎ is 32 bits long and uses a two’s-complement representation for signed values. Right shifts are performed arithmetically for signed values and logically for unsigned values. The variables are declared and initialized as follows: ©ÅÎ Ó { ðÇÇfgy mh ¡Ë¾©ÎËþËÔ ÌþÄÏ− hm ©ÅÎ Ô { ¾þËfgy mh ¡Ë¾©ÎËþËÔ ÌþÄÏ− hm ÏÅÍ©×Å−® ÏÓ { Óy ÏÅÍ©×Å−® ÏÔ { Ôy For each of the following C expressions, either (1) argue that it is true (evalu- ates to 1) for all values of Ó and Ô, or (2) give values of Ó and Ô for which it is false (evaluates to 0): A. fÓ|ngÚÚfÓkozng B. fÓ d ug _{ u ÚÚ fÓzzpw z ng C. fÓhÓg|{n D. ÓznÚÚkÓz{n E. Ó|nÚÚkÓ|{n F. ÓiÔ {{ ÏÔiÏÓ G. ÓhÜÔ i ÏÔhÏÓ {{ kÓ 2.4 Floating Point A ﬂoating-point representation encodes rational numbers of the form V = x × 2y. It is useful for performing computations involving very large numbers (|V |≫ 0), Section 2.4 Floating Point 145 Aside The IEEE The Institute of Electrical and Electronics Engineers (IEEE—pronounced “eye-triple-ee”) is a pro- fessional society that encompasses all of electronic and computer technology. It publishes journals, sponsors conferences, and sets up committees to deﬁne standards on topics ranging from power trans- mission to software engineering. Another example of an IEEE standard is the 802.11 standard for wireless networking. numbers very close to 0 (|V |≪ 1), and more generally as an approximation to real arithmetic. Up until the 1980s, every computer manufacturer devised its own conventions for how ﬂoating-point numbers were represented and the details of the operations performed on them. In addition, they often did not worry too much about the accuracy of the operations, viewing speed and ease of implementation as being more critical than numerical precision. All of this changed around 1985 with the advent of IEEE Standard 754, a carefully crafted standard for representing ﬂoating-point numbers and the oper- ations performed on them. This effort started in 1976 under Intel’s sponsorship with the design of the 8087, a chip that provided ﬂoating-point support for the 8086 processor. Intel hired William Kahan, a professor at the University of California, Berkeley, as a consultant to help design a ﬂoating-point standard for its future processors. They allowed Kahan to join forces with a committee generating an industry-wide standard under the auspices of the Institute of Electrical and Elec- tronics Engineers (IEEE). The committee ultimately adopted a standard close to the one Kahan had devised for Intel. Nowadays, virtually all computers support what has become known as IEEE ﬂoating point. This has greatly improved the portability of scientiﬁc application programs across different machines. In this section, we will see how numbers are represented in the IEEE ﬂoating- point format. We will also explore issues of rounding, when a number cannot be represented exactly in the format and hence must be adjusted upward or down- ward. We will then explore the mathematical properties of addition, multiplica- tion, and relational operators. Many programmers consider ﬂoating point to be at best uninteresting and at worst arcane and incomprehensible. We will see that since the IEEE format is based on a small and consistent set of principles, it is really quite elegant and understandable. 2.4.1 Fractional Binary Numbers A ﬁrst step in understanding ﬂoating-point numbers is to consider binary numbers having fractional values. Let us ﬁrst examine the more familiar decimal notation. Decimal notation uses a representation of the form dm dm−1 ... d1 d0 .d−1 d−2 ... d−n 146 Chapter 2 Representing and Manipulating Information Figure 2.31 Fractional binary representation. Digits to the left of the binary point have weights of the form 2i, while those to the right have weights of the form 1/2i. bm bm –1 · · · · · · b2 b1 b0 b–1 1 1/2 1/4 1/8 1/2n –1 1/2 n 2 4 2 m–1 2 m b–2 b–3 · · ·· · · · b–n +1 b–n where each decimal digit di ranges between 0 and 9. This notation represents a value d deﬁned as d = m∑ i=−n 10i × di The weighting of the digits is deﬁned relative to the decimal point symbol (‘.’), meaning that digits to the left are weighted by nonnegative powers of 10, giving integral values, while digits to the right are weighted by negative powers of 10, giving fractional values. For example, 12.3410 represents the number 1 × 101 + 2 × 100 + 3 × 10−1 + 4 × 10−2 = 12 34 100 . By analogy, consider a notation of the form bm bm−1 ... b1 b0 .b−1 b−2 ... b−n+1 b−n where each binary digit, or bit, bi ranges between 0 and 1, as is illustrated in Figure 2.31. This notation represents a number b deﬁned as b = m∑ i=−n 2i × bi (2.19) The symbol ‘.’ now becomes a binary point, with bits on the left being weighted by nonnegative powers of 2, and those on the right being weighted by negative powers of 2. For example, 101.112 represents the number 1 × 22 + 0 × 21 + 1 × 20 + 1 × 2−1 + 1 × 2−2 = 4 + 0 + 1 + 1 2 + 1 4 = 5 3 4 . One can readily see from Equation 2.19 that shifting the binary point one position to the left has the effect of dividing the number by 2. For example, while 101.112 represents the number 5 3 4 ,10.1112 represents the number 2 + 0 + 1 2 + Section 2.4 Floating Point 147 1 4 + 1 8 = 2 7 8 . Similarly, shifting the binary point one position to the right has the effect of multiplying the number by 2. For example, 1011.12 represents the number 8 + 0 + 2 + 1 + 1 2 = 11 1 2 . Note that numbers of the form 0.11 ... 12 represent numbers just below 1. For example, 0.1111112 represents 63 64 . We will use the shorthand notation 1.0 − ϵ to represent such values. Assuming we consider only ﬁnite-length encodings, decimal notation cannot represent numbers such as 1 3 and 5 7 exactly. Similarly, fractional binary notation can only represent numbers that can be written x × 2y. Other values can only be approximated. For example, the number 1 5 can be represented exactly as the frac- tional decimal number 0.20. As a fractional binary number, however, we cannot represent it exactly and instead must approximate it with increasing accuracy by lengthening the binary representation: Representation Value Decimal 0.02 0 2 0.010 0.012 1 4 0.2510 0.0102 2 8 0.2510 0.00112 3 16 0.187510 0.001102 6 32 0.187510 0.0011012 13 64 0.20312510 0.00110102 26 128 0.20312510 0.001100112 51 256 0.1992187510 Practice Problem 2.45 (solution page 193) Fill in the missing information in the following table: Fractional value Binary representation Decimal representation 1 8 0.001 0.125 3 4 5 16 10.1011 1.001 5.875 3.1875 Practice Problem 2.46 (solution page 194) The imprecision of ﬂoating-point arithmetic can have disastrous effects. On Febru- ary 25, 1991, during the ﬁrst Gulf War, an American Patriot Missile battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile. The Scud struck an American Army barracks and killed 28 soldiers. The US General 148 Chapter 2 Representing and Manipulating Information Accounting Ofﬁce (GAO) conducted a detailed analysis of the failure [76] and de- termined that the underlying cause was an imprecision in a numeric calculation. In this exercise, you will reproduce part of the GAO’s analysis. The Patriot system contains an internal clock, implemented as a counter that is incremented every 0.1 seconds. To determine the time in seconds, the program would multiply the value of this counter by a 24-bit quantity that was a fractional binary approximation to 1 10 . In particular, the binary representation of 1 10 is the nonterminating sequence 0.000110011[0011] ...2, where the portion in brackets is repeated indeﬁnitely. The program approximated 0.1, as a value x,by considering just the ﬁrst 23 bits of the sequence to the right of the binary point: x = 0.00011001100110011001100. (See Problem 2.51 for a discussion of how they could have approximated 0.1 more precisely.) A. What is the binary representation of 0.1 − x? B. What is the approximate decimal value of 0.1 − x? C. The clock starts at 0 when the system is ﬁrst powered up and keeps counting up from there. In this case, the system had been running for around 100 hours. What was the difference between the actual time and the time computed by the software? D. The system predicts where an incoming missile will appear based on its velocity and the time of the last radar detection. Given that a Scud travels at around 2,000 meters per second, how far off was its prediction? Normally, a slight error in the absolute time reported by a clock reading would not affect a tracking computation. Instead, it should depend on the relative time between two successive readings. The problem was that the Patriot software had been upgraded to use a more accurate function for reading time, but not all of the function calls had been replaced by the new code. As a result, the tracking software used the accurate time for one reading and the inaccurate time for the other [103]. 2.4.2 IEEE Floating-Point Representation Positional notation such as considered in the previous section would not be ef- ﬁcient for representing very large numbers. For example, the representation of 5 × 2100 would consist of the bit pattern 101 followed by 100 zeros. Instead, we would like to represent numbers in a form x × 2y by giving the values of x and y. The IEEE ﬂoating-point standard represents a number in a form V = (−1)s × M × 2E: . The sign s determines whether the number is negative (s = 1) or positive (s = 0), where the interpretation of the sign bit for numeric value 0 is handled as a special case. . The signiﬁcand M is a fractional binary number that ranges either between 1 and 2 − ϵ or between 0 and 1 − ϵ. . The exponent E weights the value by a (possibly negative) power of 2. Section 2.4 Floating Point 149 31 s exp frac 30 Single precision 23 022 63 s exp frac (51:32) 62 Double precision 52 3251 31 frac (31:0) 0 Figure 2.32 Standard ﬂoating-point formats. Floating-point numbers are represented by three ﬁelds. For the two most common formats, these are packed in 32-bit (single- precision) or 64-bit (double-precision) words. The bit representation of a ﬂoating-point number is divided into three ﬁelds to encode these values: . The single sign bit Í directly encodes the sign s. . The k-bit exponent ﬁeld −ÓÉ = ek−1 ... e1e0 encodes the exponent E. . The n-bit fraction ﬁeld ðËþ² = fn−1 ... f1f0 encodes the signiﬁcand M, but the value encoded also depends on whether or not the exponent ﬁeld equals 0. Figure 2.32 shows the packing of these three ﬁelds into words for the two most common formats. In the single-precision ﬂoating-point format (a ðÄÇþÎ in C), ﬁelds Í, −ÓÉ, and ðËþ² are 1, k = 8, and n = 23 bits each, yielding a 32- bit representation. In the double-precision ﬂoating-point format (a ®ÇÏ¾Ä− in C), ﬁelds Í, −ÓÉ, and ðËþ² are 1, k = 11, and n = 52 bits each, yielding a 64-bit representation. The value encoded by a given bit representation can be divided into three different cases (the latter having two variants), depending on the value of −ÓÉ. These are illustrated in Figure 2.33 for the single-precision format. Case 1: Normalized Values This is the most common case. It occurs when the bit pattern of −ÓÉ is neither all zeros (numeric value 0) nor all ones (numeric value 255 for single precision, 2047 for double). In this case, the exponent ﬁeld is interpreted as representing a signed integer in biased form. That is, the exponent value is E = e − Bias, where e is the unsigned number having bit representation ek−1 ... e1e0 and Bias is a bias value equal to 2k−1 − 1 (127 for single precision and 1023 for double). This yields exponent ranges from −126 to +127 for single precision and −1022 to +1023 for double precision. The fraction ﬁeld ðËþ² is interpreted as representing the fractional value f , where 0 ≤ f< 1, having binary representation 0.fn−1 ... f1f0, that is, with the 150 Chapter 2 Representing and Manipulating Information Aside Why set the bias this way for denormalized values? Having the exponent value be 1 − Bias rather than simply −Bias might seem counterintuitive. We will see shortly that it provides for smooth transition from denormalized to normalized values. s 0 0 0 0 0 0 0 0 f ≠ 0 2. Denormalized s 1111111100000000000000000000000 3a. Infinity s 11111111 3b. NaN s ≠ 0 and ≠ 255 f 1. Normalized Figure 2.33 Categories of single-precision ﬂoating-point values. The value of the exponent determines whether the number is (1) normalized, (2) denormalized, or (3) a special value. binary point to the left of the most signiﬁcant bit. The signiﬁcand is deﬁned to be M = 1 + f . This is sometimes called an implied leading 1 representation, because we can view M to be the number with binary representation 1.fn−1fn−2 ... f0. This representation is a trick for getting an additional bit of precision for free, since we can always adjust the exponent E so that signiﬁcand M is in the range 1 ≤ M< 2 (assuming there is no overﬂow). We therefore do not need to explicitly represent the leading bit, since it always equals 1. Case 2: Denormalized Values When the exponent ﬁeld is all zeros, the represented number is in denormalized form. In this case, the exponent value is E = 1 − Bias, and the signiﬁcand value is M = f , that is, the value of the fraction ﬁeld without an implied leading 1. Denormalized numbers serve two purposes. First, they provide a way to represent numeric value 0, since with a normalized number we must always have M ≥ 1, and hence we cannot represent 0. In fact, the ﬂoating-point representation of +0.0 has a bit pattern of all zeros: the sign bit is 0, the exponent ﬁeld is all zeros (indicating a denormalized value), and the fraction ﬁeld is all zeros, giving M = f = 0. Curiously, when the sign bit is 1, but the other ﬁelds are all zeros, we get the value −0.0. With IEEE ﬂoating-point format, the values −0.0 and +0.0 are considered different in some ways and the same in others. Section 2.4 Floating Point 151 A second function of denormalized numbers is to represent numbers that are very close to 0.0. They provide a property known as gradual underﬂow in which possible numeric values are spaced evenly near 0.0. Case 3: Special Values A ﬁnal category of values occurs when the exponent ﬁeld is all ones. When the fraction ﬁeld is all zeros, the resulting values represent inﬁnity, either +∞ when s = 0or −∞ when s = 1. Inﬁnity can represent results that overﬂow, as when we multiply two very large numbers, or when we divide by zero. When the fraction ﬁeld is nonzero, the resulting value is called a NaN, short for “not a number.” Such values are returned as the result of an operation where the result cannot be given as a real number or as inﬁnity, as when computing √−1or ∞−∞. They can also be useful in some applications for representing uninitialized data. 2.4.3 Example Numbers Figure 2.34 shows the set of values that can be represented in a hypothetical 6-bit format having k = 3 exponent bits and n = 2 fraction bits. The bias is 23−1 − 1 = 3. Part (a) of the ﬁgure shows all representable values (other than NaN). The two inﬁnities are at the extreme ends. The normalized numbers with maximum mag- nitude are ±14. The denormalized numbers are clustered around 0. These can be seen more clearly in part (b) of the ﬁgure, where we show just the numbers be- tween −1.0 and +1.0. The two zeros are special cases of denormalized numbers. Observe that the representable numbers are not uniformly distributed—they are denser nearer the origin. Figure 2.35 shows some examples for a hypothetical 8-bit ﬂoating-point for- mat having k = 4 exponent bits and n = 3 fraction bits. The bias is 24−1 − 1 = 7. The ﬁgure is divided into three regions representing the three classes of numbers. The different columns show how the exponent ﬁeld encodes the exponent E, while the fraction ﬁeld encodes the signiﬁcand M, and together they form the \u000210 \u00020.8 \u00020.6 \u00020.4 \u00020.2 \u00030.2 \u00030\u00020 \u00030.4 \u00030.6 \u00030.8 \u000310\u00021 \u000250 \u00035 \u000310 \u0003\u0004\u0002\u0004 Denormalized Normalized Infinity Denormalized Normalized Infinity (a) Complete range (b) Values between \u00021.0 and \u00031.0 Figure 2.34 Representable values for 6-bit ﬂoating-point format. There are k = 3 exponent bits and n = 2 fraction bits. The bias is 3. 152 Chapter 2 Representing and Manipulating Information Exponent Fraction Value Description Bit representation eE 2E fM 2E × MV Decimal Zero n nnnn nnn 0 −6 1 64 0 8 0 8 0 512 0 0.0 Smallest positive n nnnn nno 0 −6 1 64 1 8 1 8 1 512 1 512 0.001953 n nnnn non 0 −6 1 64 2 8 2 8 2 512 1 256 0.003906 n nnnn noo 0 −6 1 64 3 8 3 8 3 512 3 512 0.005859 ... Largest denormalized n nnnn ooo 0 −6 1 64 7 8 7 8 7 512 7 512 0.013672 Smallest normalized n nnno nnn 1 −6 1 64 0 8 8 8 8 512 1 64 0.015625 n nnno nno 1 −6 1 64 1 8 9 8 9 512 9 512 0.017578 ... n noon oon 6 −1 1 2 6 8 14 8 14 16 7 8 0.875 n noon ooo 6 −1 1 2 7 8 15 8 15 16 15 16 0.9375 One n nooo nnn 70 1 0 8 8 8 8 8 1 1.0 n nooo nno 70 1 1 8 9 8 9 8 9 8 1.125 n nooo non 70 1 2 8 10 8 10 8 5 4 1.25 ... n ooon oon 14 7 128 6 8 14 8 1792 8 224 224.0 Largest normalized n ooon ooo 14 7 128 7 8 15 8 1920 8 240 240.0 Inﬁnity n oooo nnn — ——— — — ∞ — Figure 2.35 Example nonnegative values for 8-bit ﬂoating-point format. There are k = 4 exponent bits and n = 3 fraction bits. The bias is 7. represented value V = 2E × M. Closest to 0 are the denormalized numbers, start- ing with 0 itself. Denormalized numbers in this format have E = 1 − 7 =−6, giv- ing a weight 2E = 1 64 . The fractions f and signiﬁcands M range over the values 0, 1 8 ,..., 7 8 , giving numbers V in the range 0 to 1 64 × 7 8 = 7 512 . The smallest normalized numbers in this format also have E = 1 − 7 =−6, and the fractions also range over the values 0, 1 8 ,... 7 8 . However, the signiﬁcands then range from 1 + 0 = 1to 1 + 7 8 = 15 8 , giving numbers V in the range 8 512 = 1 64 to 15 512 . Observe the smooth transition between the largest denormalized number 7 512 and the smallest normalized number 8 512 . This smoothness is due to our deﬁnition of E for denormalized values. By making it 1 − Bias rather than −Bias, we com- pensate for the fact that the signiﬁcand of a denormalized number does not have an implied leading 1. Section 2.4 Floating Point 153 As we increase the exponent, we get successively larger normalized values, passing through 1.0 and then to the largest normalized number. This number has exponent E = 7, giving a weight 2E = 128. The fraction equals 7 8 , giving a signiﬁ- cand M = 15 8 . Thus, the numeric value is V = 240. Going beyond this overﬂows to +∞. One interesting property of this representation is that if we interpret the bit representations of the values in Figure 2.35 as unsigned integers, they occur in ascending order, as do the values they represent as ﬂoating-point numbers. This is no accident—the IEEE format was designed so that ﬂoating-point numbers could be sorted using an integer sorting routine. A minor difﬁculty occurs when dealing with negative numbers, since they have a leading 1 and occur in descending order, but this can be overcome without requiring ﬂoating-point operations to perform comparisons (see Problem 2.84). Practice Problem 2.47 (solution page 194) Consider a 5-bit ﬂoating-point representation based on the IEEE ﬂoating-point format, with one sign bit, two exponent bits (k = 2), and two fraction bits (n = 2). The exponent bias is 22−1 − 1 = 1. The table that follows enumerates the entire nonnegative range for this 5-bit ﬂoating-point representation. Fill in the blank table entries using the following directions: e: The value represented by considering the exponent ﬁeld to be an unsigned integer E: The value of the exponent after biasing 2E: The numeric weight of the exponent f : The value of the fraction M: The value of the signiﬁcand 2E × M: The (unreduced) fractional value of the number V : The reduced fractional value of the number Decimal: The decimal representation of the number Express the values of 2E, f , M,2E × M, and V either as integers (when possible) or as fractions of the form x y , where y is a power of 2. You need not ﬁll in entries marked —. Bits eE 2E fM 2E × MV Decimal nnnnn nnnno nnnon nnnoo nnonn nnono 10 1 1 4 5 4 5 4 5 4 1.25 154 Chapter 2 Representing and Manipulating Information Bits eE 2E fM 2E × MV Decimal nnoon nnooo nonnn nonno nonon nonoo noonn ————— — — noono ————— — — nooon ————— — — noooo ————— — — Figure 2.36 shows the representations and numeric values of some important single- and double-precision ﬂoating-point numbers. As with the 8-bit format shown in Figure 2.35, we can see some general properties for a ﬂoating-point representation with a k-bit exponent and an n-bit fraction: . The value +0.0 always has a bit representation of all zeros. . The smallest positive denormalized value has a bit representation consisting of a 1 in the least signiﬁcant bit position and otherwise all zeros. It has a fraction (and signiﬁcand) value M = f = 2−n and an exponent value E =−2k−1 + 2. The numeric value is therefore V = 2−n−2k−1+2. . The largest denormalized value has a bit representation consisting of an exponent ﬁeld of all zeros and a fraction ﬁeld of all ones. It has a fraction (and signiﬁcand) value M = f = 1 − 2−n (which we have written 1 − ϵ) and an exponent value E =−2k−1 + 2. The numeric value is therefore V = (1 − 2−n) × 2−2k−1+2, which is just slightly smaller than the smallest normalized value. . The smallest positive normalized value has a bit representation witha1in the least signiﬁcant bit of the exponent ﬁeld and otherwise all zeros. It has a Single precision Double precision Description −ÓÉ ðËþ² Value Decimal Value Decimal Zero 00 ... 00 0 ... 00 0 0.00 0.0 Smallest denormalized 00 ... 00 0 ... 01 2−23 × 2−126 1.4 × 10−45 2−52 × 2−1022 4.9 × 10−324 Largest denormalized 00 ... 00 1 ... 11 (1 − ϵ) × 2−126 1.2 × 10−38 (1 − ϵ) × 2−1022 2.2 × 10−308 Smallest normalized 00 ... 01 0 ... 00 1 × 2−126 1.2 × 10−38 1 × 2−1022 2.2 × 10−308 One 01 ... 11 0 ... 00 1 × 20 1.01 × 20 1.0 Largest normalized 11 ... 10 1 ... 11 (2 − ϵ) × 2127 3.4 × 1038 (2 − ϵ) × 21023 1.8 × 10308 Figure 2.36 Examples of nonnegative ﬂoating-point numbers. Section 2.4 Floating Point 155 signiﬁcand value M = 1 and an exponent value E =−2k−1 + 2. The numeric value is therefore V = 2−2k−1+2. . The value 1.0 has a bit representation with all but the most signiﬁcant bit of the exponent ﬁeld equal to 1 and all other bits equal to 0. Its signiﬁcand value is M = 1 and its exponent value is E = 0. . The largest normalized value has a bit representation with a sign bit of 0, the least signiﬁcant bit of the exponent equal to 0, and all other bits equal to 1. It has a fraction value of f = 1 − 2−n, giving a signiﬁcand M = 2 − 2−n (which we have written 2 − ϵ.) It has an exponent value E = 2k−1 − 1, giving a numeric value V = (2 − 2−n) × 22k−1−1 = (1 − 2−n−1) × 22k−1. One useful exercise for understanding ﬂoating-point representations is to con- vert sample integer values into ﬂoating-point form. For example, we saw in Figure 2.15 that 12,345 has binary representation [11000000111001]. We create a normal- ized representation of this by shifting 13 positions to the right of a binary point, giving 12,345 = 1.10000001110012 × 213. To encode this in IEEE single-precision format, we construct the fraction ﬁeld by dropping the leading 1 and adding 10 zeros to the end, giving binary representation [10000001110010000000000]. To construct the exponent ﬁeld, we add bias 127 to 13, giving 140, which has bi- nary representation [10001100]. We combine this with a sign bit of 0 to get the ﬂoating-point representation in binary of [01000110010000001110010000000000]. Recall from Section 2.1.3 that we observed the following correlation in the bit- level representations of the integer value opqrs (nÓqnqw) and the single-precision ﬂoating-point value opqrsln (nÓrtrn¥rnn): nnnnqnqw nnnnnnnnnnnnnnnnnnoonnnnnnooonno hhhhhhhhhhhhh rtrn¥rnn nonnnoonnonnnnnnooonnonnnnnnnnnn We can now see that the region of correlation corresponds to the low-order bits of the integer, stopping just before the most signiﬁcant bit equal to 1 (this bit forms the implied leading 1), matching the high-order bits in the fraction part of the ﬂoating-point representation. Practice Problem 2.48 (solution page 195) As mentioned in Problem 2.6, the integer 3,510,593 has hexadecimal represen- tation nÓnnqsworo, while the single-precision ﬂoating-point number 3,510,593.0 has hexadecimal representation nÓr¡strsnr. Derive this ﬂoating-point represen- tation and explain the correlation between the bits of the integer and ﬂoating-point representations. 156 Chapter 2 Representing and Manipulating Information Practice Problem 2.49 (solution page 195) A. For a ﬂoating-point format with an n-bit fraction, give a formula for the smallest positive integer that cannot be represented exactly (because it would require an (n + 1)-bit fraction to be exact). Assume the exponent ﬁeld size k is large enough that the range of representable exponents does not provide a limitation for this problem. B. What is the numeric value of this integer for single-precision format (n = 23)? 2.4.4 Rounding Floating-point arithmetic can only approximate real arithmetic, since the repre- sentation has limited range and precision. Thus, for a value x, we generally want a systematic method of ﬁnding the “closest” matching value x′ that can be rep- resented in the desired ﬂoating-point format. This is the task of the rounding operation. One key problem is to deﬁne the direction to round a value that is halfway between two possibilities. For example, if I have $1.50 and want to round it to the nearest dollar, should the result be $1 or $2? An alternative approach is to maintain a lower and an upper bound on the actual number. For example, we could determine representable values x− and x+ such that the value x is guaran- teed to lie between them: x− ≤ x ≤ x+. The IEEE ﬂoating-point format deﬁnes four different rounding modes. The default method ﬁnds a closest match, while the other three can be used for computing upper and lower bounds. Figure 2.37 illustrates the four rounding modes applied to the problem of rounding a monetary amount to the nearest whole dollar. Round-to-even (also called round-to-nearest) is the default mode. It attempts to ﬁnd a closest match. Thus, it rounds $1.40 to $1 and $1.60 to $2, since these are the closest whole dollar values. The only design decision is to determine the effect of rounding values that are halfway between two possible results. Round-to-even mode adopts the convention that it rounds the number either upward or downward such that the least signiﬁcant digit of the result is even. Thus, it rounds both $1.50 and $2.50 to $2. The other three modes produce guaranteed bounds on the actual value. These can be useful in some numerical applications. Round-toward-zero mode rounds positive numbers downward and negative numbers upward, giving a value ˆx such Mode $1.40 $1.60 $1.50 $2.50 $–1.50 Round-to-even $1 $2 $2 $2 $–2 Round-toward-zero $1 $1 $1 $2 $–1 Round-down $1 $1 $1 $2 $–2 Round-up $2 $2 $2 $3 $–1 Figure 2.37 Illustration of rounding modes for dollar rounding. The ﬁrst rounds to a nearest value, while the other three bound the result above or below. Section 2.4 Floating Point 157 that |ˆx|≤|x|. Round-down mode rounds both positive and negative numbers downward, giving a value x− such that x− ≤ x. Round-up mode rounds both positive and negative numbers upward, giving a value x+ such that x ≤ x+. Round-to-even at ﬁrst seems like it has a rather arbitrary goal—why is there any reason to prefer even numbers? Why not consistently round values halfway between two representable values upward? The problem with such a convention is that one can easily imagine scenarios in which rounding a set of data values would then introduce a statistical bias into the computation of an average of the values. The average of a set of numbers that we rounded by this means would be slightly higher than the average of the numbers themselves. Conversely, if we always rounded numbers halfway between downward, the average of a set of rounded numbers would be slightly lower than the average of the numbers them- selves. Rounding toward even numbers avoids this statistical bias in most real-life situations. It will round upward about 50% of the time and round downward about 50% of the time. Round-to-even rounding can be applied even when we are not rounding to a whole number. We simply consider whether the least signiﬁcant digit is even or odd. For example, suppose we want to round decimal numbers to the nearest hundredth. We would round 1.2349999 to 1.23 and 1.2350001 to 1.24, regardless of rounding mode, since they are not halfway between 1.23 and 1.24. On the other hand, we would round both 1.2350000 and 1.2450000 to 1.24, since 4 is even. Similarly, round-to-even rounding can be applied to binary fractional num- bers. We consider least signiﬁcant bit value 0 to be even and 1 to be odd. In general, the rounding mode is only signiﬁcant when we have a bit pattern of the form XX ... X.Y Y ... Y 100 ..., where X and Y denote arbitrary bit values with the rightmost Y being the position to which we wish to round. Only bit patterns of this form denote values that are halfway between two possible results. As ex- amples, consider the problem of rounding values to the nearest quarter (i.e., 2 bits to the right of the binary point.) We would round 10.000112 (2 3 32 ) down to 10.002 (2), and 10.001102 (2 3 16 )upto10.012 (2 1 4 ), because these values are not halfway between two possible values. We would round 10.111002 (2 7 8 )upto11.002 (3) and 10.101002 (2 5 8 ) down to 10.102 (2 1 2 ), since these values are halfway between two possible results, and we prefer to have the least signiﬁcant bit equal to zero. Practice Problem 2.50 (solution page 195) Show how the following binary fractional values would be rounded to the nearest half (1 bit to the right of the binary point), according to the round-to-even rule. In each case, show the numeric values, both before and after rounding. A. 10.1112 B. 11.0102 C. 11.0002 D. 10.1102 158 Chapter 2 Representing and Manipulating Information Practice Problem 2.51 (solution page 195) We saw in Problem 2.46 that the Patriot missile software approximated 0.1as x = 0.000110011001100110011002. Suppose instead that they had used IEEE round- to-even mode to determine an approximation x′ to 0.1 with 23 bits to the right of the binary point. A. What is the binary representation of x′? B. What is the approximate decimal value of x′ − 0.1? C. How far off would the computed clock have been after 100 hours of opera- tion? D. How far off would the program’s prediction of the position of the Scud missile have been? Practice Problem 2.52 (solution page 196) Consider the following two 7-bit ﬂoating-point representations based on the IEEE ﬂoating-point format. Neither has a sign bit—they can only represent nonnegative numbers. 1. Format A There are k = 3 exponent bits. The exponent bias is 3. There are n = 4 fraction bits. 2. Format B There are k = 4 exponent bits. The exponent bias is 7. There are n = 3 fraction bits. Below, you are given some bit patterns in format A, and your task is to convert them to the closest value in format B. If necessary, you should apply the round-to- even rounding rule. In addition, give the values of numbers given by the format A and format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions (e.g., 17/64). Format A Format B Bits Value Bits Value noo nnnn 1 nooo nnn 1 ono ooon non onno oon oooo nnn nnno 2.4.5 Floating-Point Operations The IEEE standard speciﬁes a simple rule for determining the result of an arith- metic operation such as addition or multiplication. Viewing ﬂoating-point values x Section 2.4 Floating Point 159 and y as real numbers, and some operation ⊙ deﬁned over real numbers, the com- putation should yield Round(x ⊙ y), the result of applying rounding to the exact result of the real operation. In practice, there are clever tricks ﬂoating-point unit designers use to avoid performing this exact computation, since the computation need only be sufﬁciently precise to guarantee a correctly rounded result. When one of the arguments is a special value, such as −0, ∞,or NaN, the standard spec- iﬁes conventions that attempt to be reasonable. For example, 1/−0 is deﬁned to yield −∞, while 1/+0 is deﬁned to yield +∞. One strength of the IEEE standard’s method of specifying the behavior of ﬂoating-point operations is that it is independent of any particular hardware or software realization. Thus, we can examine its abstract mathematical properties without considering how it is actually implemented. We saw earlier that integer addition, both unsigned and two’s complement, forms an abelian group. Addition over real numbers also forms an abelian group, but we must consider what effect rounding has on these properties. Let us deﬁne x if y to be Round(x + y). This operation is deﬁned for all values of x and y, although it may yield inﬁnity even when both x and y are real numbers due to overﬂow. The operation is commutative, with x if y = y if x for all values of x and y. On the other hand, the operation is not associative. For example, with single- precision ﬂoating point the expression fqlorio−ongko−on evaluates to nln—the value 3.14 is lost due to rounding. On the other hand, the expression qlorifo−onk o−ong evaluates to qlor. As with an abelian group, most values have inverses under ﬂoating-point addition, that is, x if −x = 0. The exceptions are inﬁnities (since +∞−∞= NaN), and NaNs, since NaN if x = NaN for any x. The lack of associativity in ﬂoating-point addition is the most important group property that is lacking. It has important implications for scientiﬁc programmers and compiler writers. For example, suppose a compiler is given the following code fragment: Ó{þi¾i²y Ô{¾i²i®y The compiler might be tempted to save one ﬂoating-point addition by generating the following code: Î{¾i²y Ó{þiÎy Ô{Îi®y However, this computation might yield a different value for Ó than would the original, since it uses a different association of the addition operations. In most applications, the difference would be so small as to be inconsequential. Unfor- tunately, compilers have no way of knowing what trade-offs the user is willing to make between efﬁciency and faithfulness to the exact behavior of the original pro- gram. As a result, they tend to be very conservative, avoiding any optimizations that could have even the slightest effect on functionality. 160 Chapter 2 Representing and Manipulating Information On the other hand, ﬂoating-point addition satisﬁes the following monotonicity property: if a ≥ b, then x i f a ≥ x i f b for any values of a, b, and x other than NaN. This property of real (and integer) addition is not obeyed by unsigned or two’s- complement addition. Floating-point multiplication also obeys many of the properties one normally associates with multiplication. Let us deﬁne x h f y to be Round(x × y). This oper- ation is closed under multiplication (although possibly yielding inﬁnity or NaN), it is commutative, and it has 1.0 as a multiplicative identity. On the other hand, it is not associative, due to the possibility of overﬂow or the loss of precision due to rounding. For example, with single-precision ﬂoating point, the expression fo−pnho−pngho−kpn evaluates to +∞, while o−pnhfo−pnho−kpng evaluates to o−pn. In addition, ﬂoating-point multiplication does not distribute over addition. For example, with single-precision ﬂoating point, the expression o−pnhfo−pnk o−png evaluates to nln, while o−pnho−pnko−pnho−pn evaluates to ﬁþﬁ. On the other hand, ﬂoating-point multiplication satisﬁes the following mono- tonicity properties for any values of a, b, and c other than NaN: a ≥ b and c ≥ 0 ⇒ a hf c ≥ b hf c a ≥ b and c ≤ 0 ⇒ a hf c ≤ b hf c In addition, we are also guaranteed that a h f a ≥ 0, as long as a ̸= NaN.Aswe saw earlier, none of these monotonicity properties hold for unsigned or two’s- complement multiplication. This lack of associativity and distributivity is of serious concern to scientiﬁc programmers and to compiler writers. Even such a seemingly simple task as writing code to determine whether two lines intersect in three-dimensional space can be a major challenge. 2.4.6 Floating Point in C All versions of C provide two different ﬂoating-point data types: ðÄÇþÎ and ®ÇÏk ¾Ä−. On machines that support IEEE ﬂoating point, these data types correspond to single- and double-precision ﬂoating point. In addition, the machines use the round-to-even rounding mode. Unfortunately, since the C standards do not re- quire the machine to use IEEE ﬂoating point, there are no standard methods to change the rounding mode or to get special values such as −0, +∞, −∞,or NaN. Most systems provide a combination of include (l³) ﬁles and procedure libraries to provide access to these features, but the details vary from one system to an- other. For example, the GNU compiler gcc deﬁnes program constants 'ﬁƒ'ﬁ'¶» (for +∞) and ﬁ¡ﬁ (for NaN) when the following sequence occurs in the program ﬁle: a®−ð©Å− ˆ§ﬁ•ˆ·ﬂ•‡£¥ o a©Å²ÄÏ®− zÀþÎ³l³| Section 2.4 Floating Point 161 Practice Problem 2.53 (solution page 196) Fill in the following macro deﬁnitions to generate the double-precision values +∞, −∞, and −0: a®−ð©Å− –ﬂ·ˆ'ﬁƒ'ﬁ'¶» a®−ð©Å− ﬁ¥§ˆ'ﬁƒ'ﬁ'¶» a®−ð©Å− ﬁ¥§ˆ…¥‡ﬂ You cannot use any include ﬁles (such as ÀþÎ³l³), but you can make use of the fact that the largest ﬁnite number that can be represented with double precision is around 1.8 × 10308. When casting values between ©ÅÎ, ðÄÇþÎ, and ®ÇÏ¾Ä− formats, the program changes the numeric values and the bit representations as follows (assuming data type ©ÅÎ is 32 bits): . From ©ÅÎ to ðÄÇþÎ, the number cannot overﬂow, but it may be rounded. . From ©ÅÎ or ðÄÇþÎ to ®ÇÏ¾Ä−, the exact numeric value can be preserved be- cause ®ÇÏ¾Ä− has both greater range (i.e., the range of representable values), as well as greater precision (i.e., the number of signiﬁcant bits). . From ®ÇÏ¾Ä− to ðÄÇþÎ, the value can overﬂow to +∞ or −∞, since the range is smaller. Otherwise, it may be rounded, because the precision is smaller. . From ðÄÇþÎ or ®ÇÏ¾Ä− to ©ÅÎ, the value will be rounded toward zero. For example, 1.999 will be converted to 1, while −1.999 will be converted to −1. Furthermore, the value may overﬂow. The C standards do not specify a ﬁxed result for this case. Intel-compatible microprocessors designate the bit pattern [10 ... 00] (TMinw for word size w)asan integer indeﬁnite value. Any conversion from ﬂoating point to integer that cannot assign a reasonable integer approximation yields this value. Thus, the expression f©ÅÎg io−on yields kporvqtrv, generating a negative value from a positive one. Practice Problem 2.54 (solution page 196) Assume variables Ó, ð, and ® are of type ©ÅÎ, ðÄÇþÎ, and ®ÇÏ¾Ä−, respectively. Their values are arbitrary, except that neither ð nor ® equals +∞, −∞,or NaN. For each of the following C expressions, either argue that it will always be true (i.e., evaluate to 1) or give a value for the variables such that it is not true (i.e., evaluates to 0). A. Ó {{ f©ÅÎgf®ÇÏ¾Ä−g Ó B. Ó {{ f©ÅÎgfðÄÇþÎg Ó C. ® {{ f®ÇÏ¾Ä−gfðÄÇþÎg ® D. ð {{ fðÄÇþÎgf®ÇÏ¾Ä−g ð E. ð {{ kfkðg 162 Chapter 2 Representing and Manipulating Information F. olnmp {{ ompln G. ®h® |{ nln H. fði®gkð {{ ® 2.5 Summary Computers encode information as bits, generally organized as sequences of bytes. Different encodings are used for representing integers, real numbers, and charac- ter strings. Different models of computers use different conventions for encoding numbers and for ordering the bytes within multi-byte data. The C language is designed to accommodate a wide range of different imple- mentations in terms of word sizes and numeric encodings. Machines with 64-bit word sizes have become increasingly common, replacing the 32-bit machines that dominated the market for around 30 years. Because 64-bit machines can also run programs compiled for 32-bit machines, we have focused on the distinction be- tween 32- and 64-bit programs, rather than machines. The advantage of 64-bit pro- grams is that they can go beyond the 4 GB address limitation of 32-bit programs. Most machines encode signed numbers using a two’s-complement representa- tion and encode ﬂoating-point numbers using IEEE Standard 754. Understanding these encodings at the bit level, as well as understanding the mathematical char- acteristics of the arithmetic operations, is important for writing programs that operate correctly over the full range of numeric values. When casting between signed and unsigned integers of the same size, most C implementations follow the convention that the underlying bit pattern does not change. On a two’s-complement machine, this behavior is characterized by functions T2U w and U2T w, for a w-bit value. The implicit casting of C gives results that many programmers do not anticipate, often leading to program bugs. Due to the ﬁnite lengths of the encodings, computer arithmetic has properties quite different from conventional integer and real arithmetic. The ﬁnite length can cause numbers to overﬂow, when they exceed the range of the representation. Floating-point values can also underﬂow, when they are so close to 0.0 that they are changed to zero. The ﬁnite integer arithmetic implemented by C, as well as most other pro- gramming languages, has some peculiar properties compared to true integer arith- metic. For example, the expression ÓhÓ can evaluate to a negative number due to overﬂow. Nonetheless, both unsigned and two’s-complement arithmetic satisfy many of the other properties of integer arithmetic, including associativity, com- mutativity, and distributivity. This allows compilers to do many optimizations. For example, in replacing the expression uhÓ by fÓzzqgkÓ, we make use of the as- sociative, commutative, and distributive properties, along with the relationship between shifting and multiplying by powers of 2. We have seen several clever ways to exploit combinations of bit-level opera- tions and arithmetic operations. For example, we saw that with two’s-complement arithmetic, ÜÓio is equivalent to kÓ. As another example, suppose we want a bit Bibliographic Notes 163 Aside Ariane 5: The high cost of ﬂoating-point overﬂow Converting large ﬂoating-point numbers to integers is a common source of programming errors. Such an error had disastrous consequences for the maiden voyage of the Ariane 5 rocket, on June 4, 1996. Just 37 seconds after liftoff, the rocket veered off its ﬂight path, broke up, and exploded. Communication satellites valued at $500 million were on board the rocket. A later investigation [73, 33] showed that the computer controlling the inertial navigation system had sent invalid data to the computer controlling the engine nozzles. Instead of sending ﬂight control information, it had sent a diagnostic bit pattern indicating that an overﬂow had occurred during the conversion of a 64-bit ﬂoating-point number to a 16-bit signed integer. The value that overﬂowed measured the horizontal velocity of the rocket, which could be more than ﬁve times higher than that achieved by the earlier Ariane 4 rocket. In the design of the Ariane 4 software, they had carefully analyzed the numeric values and determined that the horizontal velocity would never overﬂow a 16-bit number. Unfortunately, they simply reused this part of the software in the Ariane 5 without checking the assumptions on which it had been based. pattern of the form [0, ... , 0, 1, ... , 1], consisting of w − k zeros followed by k ones. Such bit patterns are useful for masking operations. This pattern can be gen- erated by the C expression fozzÂgko, exploiting the property that the desired bit pattern has numeric value 2k − 1. For example, the expression fozzvgko will generate the bit pattern nÓƒƒ. Floating-point representations approximate real numbers by encoding num- bers of the form x × 2y. IEEE Standard 754 provides for several different preci- sions, with the most common being single (32 bits) and double (64 bits). IEEE ﬂoating point also has representations for special values representing plus and minus inﬁnity, as well as not-a-number. Floating-point arithmetic must be used very carefully, because it has only limited range and precision, and because it does not obey common mathematical properties such as associativity. Bibliographic Notes Reference books on C [45, 61] discuss properties of the different data types and operations. Of these two, only Steele and Harbison [45] cover the newer features found in ISO C99. There do not yet seem to be any books that cover the features found in ISO C11. The C standards do not specify details such as precise word sizes or numeric encodings. Such details are intentionally omitted to make it possible to implement C on a wide range of different machines. Several books have been written giving advice to C programmers [59, 74] that warn about problems with overﬂow, implicit casting to unsigned, and some of the other pitfalls we have covered in this chapter. These books also provide helpful advice on variable naming, coding styles, and code testing. Seacord’s book on security issues in C and C++ programs [97] combines information about C programs, how they are compiled and executed, and how vulnerabilities may arise. Books on Java (we 164 Chapter 2 Representing and Manipulating Information recommend the one coauthored by James Gosling, the creator of the language [5]) describe the data formats and arithmetic operations supported by Java. Most books on logic design [58, 116] have a section on encodings and arith- metic operations. Such books describe different ways of implementing arithmetic circuits. Overton’s book on IEEE ﬂoating point [82] provides a detailed descrip- tion of the format as well as the properties from the perspective of a numerical applications programmer. Homework Problems 2.55 ◆ Compile and run the sample code that uses Í³ÇÑˆ¾ÔÎ−Í (ﬁle Í³ÇÑk¾ÔÎ−Íl²)on different machines to which you have access. Determine the byte orderings used by these machines. 2.56 ◆ Try running the code for Í³ÇÑˆ¾ÔÎ−Í for different sample values. 2.57 ◆ Write procedures Í³ÇÑˆÍ³ÇËÎ, Í³ÇÑˆÄÇÅ×, and Í³ÇÑˆ®ÇÏ¾Ä− that print the byte representations of C objects of types Í³ÇËÎ, ÄÇÅ×, and ®ÇÏ¾Ä−, respectively. Try these out on several machines. 2.58 ◆◆ Write a procedure ©ÍˆÄ©ÎÎÄ−ˆ−Å®©þÅ that will return 1 when compiled and run on a little-endian machine, and will return 0 when compiled and run on a big- endian machine. This program should run on any machine, regardless of its word size. 2.59 ◆◆ Write a C expression that will yield a word consisting of the least signiﬁcant byte of Ó and the remaining bytes of Ô. For operands Ó = nÓvw¡¢£⁄¥ƒ and Ô = nÓutsrqpon, this would give nÓutsrqp¥ƒ. 2.60 ◆◆ Suppose we number the bytes in a w-bit word from 0 (least signiﬁcant) to w/8 − 1 (most signiﬁcant). Write code for the following C function, which will return an unsigned value in which byte © of argument Ó has been replaced by byte ¾: ÏÅÍ©×Å−® Ë−ÉÄþ²−ˆ¾ÔÎ− fÏÅÍ©×Å−® Ój ©ÅÎ ©j ÏÅÍ©×Å−® ²³þË ¾gy Here are some examples showing how the function should work: Ë−ÉÄþ²−ˆ¾ÔÎ−fnÓopqrstuvj pj nÓ¡¢g kk| nÓop¡¢stuv Ë−ÉÄþ²−ˆ¾ÔÎ−fnÓopqrstuvj nj nÓ¡¢g kk| nÓopqrst¡¢ Bit-Level Integer Coding Rules In several of the following problems, we will artiﬁcially restrict what programming constructs you can use to help you gain a better understanding of the bit-level, Homework Problems 165 logic, and arithmetic operations of C. In answering these problems, your code must follow these rules: . Assumptions Integers are represented in two’s-complement form. Right shifts of signed data are performed arithmetically. Data type ©ÅÎ is w bits long. For some of the problems, you will be given a speciﬁc value for w, but otherwise your code should work as long as w is a multiple of 8. You can use the expression Í©Ö−Çðf©ÅÎgzzq to compute w. . Forbidden Conditionals (©ð or }x), loops, switch statements, function calls, and macro invocations. Division, modulus, and multiplication. Relative comparison operators (z, |, z{, and |{). . Allowed operations All bit-level and logic operations. Left and right shifts, but only with shift amounts between 0 and w − 1. Addition and subtraction. Equality ({{) and inequality (_{) tests. (Some of the problems do not allow these.) Integer constants 'ﬁ¶ˆ›'ﬁ and 'ﬁ¶ˆ›¡”. Casting between data types ©ÅÎ and ÏÅÍ©×Å−®, either explicitly or im- plicitly. Even with these rules, you should try to make your code readable by choosing descriptive variable names and using comments to describe the logic behind your solutions. As an example, the following code extracts the most signiﬁcant byte from integer argument Ó: mh §−Î ÀÇÍÎ Í©×Å©ð©²þÅÎ ¾ÔÎ− ðËÇÀ Ó hm ©ÅÎ ×−ÎˆÀÍ¾f©ÅÎ Óg Õ mh ·³©ðÎ ¾Ô Ñkv hm ©ÅÎ Í³©ðÎˆÌþÄ { fÍ©Ö−Çðf©ÅÎgkogzzqy mh ¡Ë©Î³À−Î©² Í³©ðÎ hm ©ÅÎ ÓË©×³Î{Ó|| Í³©ðÎˆÌþÄy mh …−ËÇ þÄÄ ¾ÏÎ ‹·¢ hm Ë−ÎÏËÅ ÓË©×³Î d nÓƒƒy Û 2.61 ◆◆ Write C expressions that evaluate to 1 when the following conditions are true and to 0 when they are false. Assume Ó is of type ©ÅÎ. A. Any bit of Ó equals 1. B. Any bit of Ó equals 0. 166 Chapter 2 Representing and Manipulating Information C. Any bit in the least signiﬁcant byte of Ó equals 1. D. Any bit in the most signiﬁcant byte of Ó equals 0. Your code should follow the bit-level integer coding rules (page 164), with the additional restriction that you may not use equality ({{) or inequality (_{) tests. 2.62 ◆◆◆ Write a function ©ÅÎˆÍ³©ðÎÍˆþË−ˆþË©Î³À−Î©²fg that yields 1 when run on a machine that uses arithmetic right shifts for data type ©ÅÎ and yields 0 otherwise. Your code should work on a machine with any word size. Test your code on several machines. 2.63 ◆◆◆ Fill in code for the following C functions. Function ÍËÄ performs a logical right shift using an arithmetic right shift (given by value ÓÍËþ), followed by other oper- ations not including right shifts or division. Function ÍËþ performs an arithmetic right shift using a logical right shift (given by value ÓÍËÄ), followed by other operations not including right shifts or division. You may use the computation vhÍ©Ö−Çðf©ÅÎg to determine w, the number of bits in data type ©ÅÎ. The shift amount Â can range from 0 to w − 1. ÏÅÍ©×Å−® ÍËÄfÏÅÍ©×Å−® Ój ©ÅÎ Âg Õ mh –−ËðÇËÀ Í³©ðÎ þË©Î³À−Î©²þÄÄÔ hm ÏÅÍ©×Å−® ÓÍËþ { f©ÅÎg Ó || Ây l l l l l l Û ©ÅÎ ÍËþf©ÅÎ Ój ©ÅÎ Âg Õ mh –−ËðÇËÀ Í³©ðÎ ÄÇ×©²þÄÄÔ hm ©ÅÎ ÓÍËÄ { fÏÅÍ©×Å−®g Ó || Ây l l l l l l Û 2.64 ◆ Write code to implement the following function: mh ‡−ÎÏËÅ o Ñ³−Å þÅÔ Ç®® ¾©Î Çð Ó −ÊÏþÄÍ oy n ÇÎ³−ËÑ©Í−l ¡ÍÍÏÀ− Ñ{qp hm ©ÅÎ þÅÔˆÇ®®ˆÇÅ−fÏÅÍ©×Å−® Ógy Your function should follow the bit-level integer coding rules (page 164), except that you may assume that data type ©ÅÎ has w = 32 bits. Homework Problems 167 2.65 ◆◆◆◆ Write code to implement the following function: mh ‡−ÎÏËÅ o Ñ³−Å Ó ²ÇÅÎþ©ÅÍ þÅ Ç®® ÅÏÀ¾−Ë Çð oÍy n ÇÎ³−ËÑ©Í−l ¡ÍÍÏÀ− Ñ{qp hm ©ÅÎ Ç®®ˆÇÅ−ÍfÏÅÍ©×Å−® Ógy Your function should follow the bit-level integer coding rules (page 164), except that you may assume that data type ©ÅÎ has w = 32 bits. Your code should contain a total of at most 12 arithmetic, bitwise, and logical operations. 2.66 ◆◆◆ Write code to implement the following function: mh h §−Å−ËþÎ− ÀþÍÂ ©Å®©²þÎ©Å× Ä−ðÎÀÇÍÎ o ©Å Ól ¡ÍÍÏÀ− Ñ{qpl h ƒÇË −ÓþÀÉÄ−j nÓƒƒnn k| nÓvnnnj þÅ® nÓttnn kk| nÓrnnnl h'ðÓ{nj Î³−Å Ë−ÎÏËÅ nl hm ©ÅÎ Ä−ðÎÀÇÍÎˆÇÅ−fÏÅÍ©×Å−® Ógy Your function should follow the bit-level integer coding rules (page 164), except that you may assume that data type ©ÅÎ has w = 32 bits. Your code should contain a total of at most 15 arithmetic, bitwise, and logical operations. Hint: First transform Ó into a bit vector of the form [0 ... 011 ... 1]. 2.67 ◆◆ You are given the task of writing a procedure ©ÅÎˆÍ©Ö−ˆ©Íˆqpfg that yields 1 when run on a machine for which an ©ÅÎ is 32 bits, and yields 0 otherwise. You are not allowed to use the Í©Ö−Çð operator. Here is a ﬁrst attempt: 1 mh ¶³− ðÇÄÄÇÑ©Å× ²Ç®− ®Ç−Í ÅÇÎ ËÏÅ ÉËÇÉ−ËÄÔ ÇÅ ÍÇÀ− Àþ²³©Å−Í hm 2 ©ÅÎ ¾þ®ˆ©ÅÎˆÍ©Ö−ˆ©Íˆqpfg Õ 3 mh ·−Î ÀÇÍÎ Í©×Å©ð©²þÅÎ ¾©Î fÀÍ¾g Çð qpk¾©Î Àþ²³©Å− hm 4 ©ÅÎ Í−ÎˆÀÍ¾{ozzqoy 5 mh ·³©ðÎ ÉþÍÎ ÀÍ¾ Çð qpk¾©Î ÑÇË® hm 6 ©ÅÎ ¾−ÔÇÅ®ˆÀÍ¾{ozzqpy 7 8 mh Í−ÎˆÀÍ¾ ©Í ÅÇÅÖ−ËÇ Ñ³−Å ÑÇË® Í©Ö− |{ qp 9 ¾−ÔÇÅ®ˆÀÍ¾ ©Í Ö−ËÇ Ñ³−Å ÑÇË® Í©Ö− z{ qp hm 10 Ë−ÎÏËÅ Í−ÎˆÀÍ¾ dd _¾−ÔÇÅ®ˆÀÍ¾y 11 Û When compiled and run on a 32-bit SUN SPARC, however, this procedure returns 0. The following compiler message gives us an indication of the problem: ÑþËÅ©Å×x Ä−ðÎ Í³©ðÎ ²ÇÏÅÎ |{ Ñ©®Î³ Çð ÎÔÉ− 168 Chapter 2 Representing and Manipulating Information A. In what way does our code fail to comply with the C standard? B. Modify the code to run properly on any machine for which data type ©ÅÎ is at least 32 bits. C. Modify the code to run properly on any machine for which data type ©ÅÎ is at least 16 bits. 2.68 ◆◆ Write code for a function with the following prototype: mh h ›þÍÂ Ñ©Î³ Ä−þÍÎ Í©×Åð©²þÅÎ Å ¾©ÎÍ Í−Î ÎÇ o h ¥ÓþÀÉÄ−ÍxÅ{tkk| nÓqƒjÅ{oukk| nÓoƒƒƒƒ h ¡ÍÍÏÀ− o z{ Å z{ Ñ hm ©ÅÎ ÄÇÑ−ËˆÇÅ−ˆÀþÍÂf©ÅÎ Ågy Your function should follow the bit-level integer coding rules (page 164). Be careful of the case Å = w. 2.69 ◆◆◆ Write code for a function with the following prototype: mh h ⁄Ç ËÇÎþÎ©Å× Ä−ðÎ Í³©ðÎl ¡ÍÍÏÀ− n z{ÅzÑ h ¥ÓþÀÉÄ−Í Ñ³−Å Ó { nÓopqrstuv þÅ®Ñ{qpx h Å{r k| nÓpqrstuvoj Å{pn k| nÓtuvopqrs hm ÏÅÍ©×Å−® ËÇÎþÎ−ˆÄ−ðÎfÏÅÍ©×Å−® Ój ©ÅÎ Ågy Your function should follow the bit-level integer coding rules (page 164). Be careful of the case Å = 0. 2.70 ◆◆ Write code for the function with the following prototype: mh h ‡−ÎÏËÅ o Ñ³−Å Ó ²þÅ ¾− Ë−ÉË−Í−ÅÎ−® þÍ þÅ Åk¾©Îj p’Ík²ÇÀÉÄ−À−ÅÎ h ÅÏÀ¾−Ëy n ÇÎ³−ËÑ©Í− h ¡ÍÍÏÀ− o z{ Å z{ Ñ hm ©ÅÎ ð©ÎÍˆ¾©ÎÍf©ÅÎ Ój ©ÅÎ Ågy Your function should follow the bit-level integer coding rules (page 164). 2.71 ◆ You just started working for a company that is implementing a set of procedures to operate on a data structure where 4 signed bytes are packed into a 32-bit ÏÅÍ©×Å−®. Bytes within the word are numbered from 0 (least signiﬁcant) to 3 Homework Problems 169 (most signiﬁcant). You have been assigned the task of implementing a function for a machine using two’s-complement arithmetic and arithmetic right shifts with the following prototype: mh ⁄−²ÄþËþÎ©ÇÅ Çð ®þÎþ ÎÔÉ− Ñ³−Ë− r ¾ÔÎ−Í þË− Éþ²Â−® ©ÅÎÇ þÅ ÏÅÍ©×Å−® hm ÎÔÉ−®−ð ÏÅÍ©×Å−® Éþ²Â−®ˆÎy mh ¥ÓÎËþ²Î ¾ÔÎ− ðËÇÀ ÑÇË®l ‡−ÎÏËÅ þÍ Í©×Å−® ©ÅÎ−×−Ë hm ©ÅÎ Ó¾ÔÎ−fÉþ²Â−®ˆÎ ÑÇË®j ©ÅÎ ¾ÔÎ−ÅÏÀgy That is, the function will extract the designated byte and sign extend it to be a 32-bit ©ÅÎ. Your predecessor (who was ﬁred for incompetence) wrote the following code: mh ƒþ©Ä−® þÎÎ−ÀÉÎ þÎ Ó¾ÔÎ− hm ©ÅÎ Ó¾ÔÎ−fÉþ²Â−®ˆÎ ÑÇË®j ©ÅÎ ¾ÔÎ−ÅÏÀg Õ Ë−ÎÏËÅ fÑÇË® || f¾ÔÎ−ÅÏÀ zz qgg d nÓƒƒy Û A. What is wrong with this code? B. Give a correct implementation of the function that uses only left and right shifts, along with one subtraction. 2.72 ◆◆ You are given the task of writing a function that will copy an integer ÌþÄ into a buffer ¾Ïð, but it should do so only if enough space is available in the buffer. Here is the code you write: mh £ÇÉÔ ©ÅÎ−×−Ë ©ÅÎÇ ¾Ïðð−Ë ©ð ÍÉþ²− ©Í þÌþ©Äþ¾Ä− hm mh „¡‡ﬁ'ﬁ§x ¶³− ðÇÄÄÇÑ©Å× ²Ç®− ©Í ¾Ï××Ô hm ÌÇ©® ²ÇÉÔˆ©ÅÎf©ÅÎ ÌþÄj ÌÇ©® h¾Ïðj ©ÅÎ ÀþÓ¾ÔÎ−Íg Õ ©ð fÀþÓ¾ÔÎ−ÍkÍ©Ö−ÇðfÌþÄg |{ ng À−À²ÉÔf¾Ïðj fÌÇ©® hg dÌþÄj Í©Ö−ÇðfÌþÄggy Û This code makes use of the library function À−À²ÉÔ. Although its use is a bit artiﬁcial here, where we simply want to copy an ©ÅÎ, it illustrates an approach commonly used to copy larger data structures. You carefully test the code and discover that it always copies the value to the buffer, even when ÀþÓ¾ÔÎ−Í is too small. A. Explain why the conditional test in the code always succeeds. Hint: The Í©Ö−Çð operator returns a value of type Í©Ö−ˆÎ. B. Show how you can rewrite the conditional test to make it work properly. 170 Chapter 2 Representing and Manipulating Information 2.73 ◆◆ Write code for a function with the following prototype: mh ¡®®©Î©ÇÅ Î³þÎ ÍþÎÏËþÎ−Í ÎÇ ¶›©Å ÇË ¶›þÓ hm ©ÅÎ ÍþÎÏËþÎ©Å×ˆþ®®f©ÅÎ Ój ©ÅÎ Ôgy Instead of overﬂowing the way normal two’s-complement addition does, sat- urating addition returns TMax when there would be positive overﬂow, and TMin when there would be negative overﬂow. Saturating arithmetic is commonly used in programs that perform digital signal processing. Your function should follow the bit-level integer coding rules (page 164). 2.74 ◆◆ Write a function with the following prototype: mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− ÍÏ¾ÎËþ²Î−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm ©ÅÎ ÎÍÏ¾ˆÇÂf©ÅÎ Ój ©ÅÎ Ôgy This function should return 1 if the computation ÓkÔ does not overﬂow. 2.75 ◆◆◆ Suppose we want to compute the complete 2w-bit representation of x . y, where both x and y are unsigned, on a machine for which data type ÏÅÍ©×Å−® is w bits. The low-order w bits of the product can be computed with the expression ÓhÔ,so we only require a procedure with prototype ÏÅÍ©×Å−® ÏÅÍ©×Å−®ˆ³©×³ˆÉËÇ®fÏÅÍ©×Å−® Ój ÏÅÍ©×Å−® Ôgy that computes the high-order w bits of x . y for unsigned variables. We have access to a library function with prototype ©ÅÎ Í©×Å−®ˆ³©×³ˆÉËÇ®f©ÅÎ Ój ©ÅÎ Ôgy that computes the high-order w bits of x . y for the case where x and y are in two’s- complement form. Write code calling this procedure to implement the function for unsigned arguments. Justify the correctness of your solution. Hint: Look at the relationship between the signed product x . y and the un- signed product x′ . y′ in the derivation of Equation 2.18. 2.76 ◆ The library function ²þÄÄÇ² has the following declaration: ÌÇ©® h²þÄÄÇ²fÍ©Ö−ˆÎ ÅÀ−À¾j Í©Ö−ˆÎ Í©Ö−gy According to the library documentation, “The ²þÄÄÇ² function allocates memory for an array of ÅÀ−À¾ elements of Í©Ö− bytes each. The memory is set to zero. If ÅÀ−À¾ or Í©Ö− is zero, then ²þÄÄÇ² returns ﬁ•‹‹.” Write an implementation of ²þÄÄÇ² that performs the allocation by a call to ÀþÄÄÇ² and sets the memory to zero via À−ÀÍ−Î. Your code should not have any vulnerabilities due to arithmetic overﬂow, and it should work correctly regardless of the number of bits used to represent data of type Í©Ö−ˆÎ. As a reference, functions ÀþÄÄÇ² and À−ÀÍ−Î have the following declarations: Homework Problems 171 ÌÇ©® hÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−gy ÌÇ©® hÀ−ÀÍ−ÎfÌÇ©® hÍj ©ÅÎ ²j Í©Ö−ˆÎ Ågy 2.77 ◆◆ Suppose we are given the task of generating code to multiply integer variable Ó by various different constant factors K. To be efﬁcient, we want to use only the operations i, k, and zz. For the following values of K, write C expressions to perform the multiplication using at most three operations per expression. A. K = 17 B. K =−7 C. K = 60 D. K =−112 2.78 ◆◆ Write code for a function with the following prototype: mh ⁄©Ì©®− ¾Ô ÉÇÑ−Ë Çð pl ¡ÍÍÏÀ− n z{ Â z Ñko hm ©ÅÎ ®©Ì©®−ˆÉÇÑ−Ëpf©ÅÎ Ój ©ÅÎ Âgy The function should compute x/2k with correct rounding, and it should follow the bit-level integer coding rules (page 164). 2.79 ◆◆ Write code for a function ÀÏÄq®©Ìr that, for integer argument Ó, computes 3 ∗ Ó/4 but follows the bit-level integer coding rules (page 164). Your code should replicate the fact that the computation qhÓ can cause overﬂow. 2.80 ◆◆◆ Write code for a function Î³Ë−−ðÇÏËÎ³Í that, for integer argument Ó, computes the value of 3 4 Ó, rounded toward zero. It should not overﬂow. Your function should follow the bit-level integer coding rules (page 164). 2.81 ◆◆ Write C expressions to generate the bit patterns that follow, where ak represents k repetitions of symbol a. Assume a w-bit data type. Your code may contain references to parameters Á and Â, representing the values of j and k, but not a parameter representing w. A. 1w−k0k B. 0w−k−j 1k0j 2.82 ◆ We are running programs where values of type ©ÅÎ are 32 bits. They are repre- sented in two’s complement, and they are right shifted arithmetically. Values of type ÏÅÍ©×Å−® are also 32 bits. 172 Chapter 2 Representing and Manipulating Information We generate arbitrary values Ó and Ô, and convert them to unsigned values as follows: mh £Ë−þÎ− ÍÇÀ− þË¾©ÎËþËÔ ÌþÄÏ−Í hm ©ÅÎ Ó { ËþÅ®ÇÀfgy ©ÅÎ Ô { ËþÅ®ÇÀfgy mh £ÇÅÌ−ËÎ ÎÇ ÏÅÍ©×Å−® hm ÏÅÍ©×Å−® ÏÓ { fÏÅÍ©×Å−®g Óy ÏÅÍ©×Å−® ÏÔ { fÏÅÍ©×Å−®g Ôy For each of the following C expressions, you are to indicate whether or not the expression always yields 1. If it always yields 1, describe the underlying mathematical principles. Otherwise, give an example of arguments that make it yield 0. A. fÓzÔg {{ fkÓ|kÔg B. ffÓiÔgzzrg i ÔkÓ {{ ouhÔioshÓ C. ÜÓiÜÔio {{ ÜfÓiÔg D. fÏÓkÏÔg {{ kfÏÅÍ©×Å−®gfÔkÓg E. ffÓ || pg zz pg z{ Ó 2.83 ◆◆ Consider numbers having a binary representation consisting of an inﬁnite string of the form 0.yyyyyy ... , where y is a k-bit sequence. For example, the binary representation of 1 3 is 0.01010101 ... (y = 01), while the representation of 1 5 is 0.001100110011 ... (y = 0011). A. Let Y = B2U k(y), that is, the number having binary representation y. Give a formula in terms of Y and k for the value represented by the inﬁnite string. Hint: Consider the effect of shifting the binary point k positions to the right. B. What is the numeric value of the string for the following values of y? (a) 101 (b) 0110 (c) 010011 2.84 ◆ Fill in the return value for the following procedure, which tests whether its ﬁrst argument is less than or equal to its second. Assume the function ðpÏ returns an unsigned 32-bit number having the same bit representation as its ﬂoating-point argument. You can assume that neither argument is NaN. The two ﬂavors of zero, +0 and −0, are considered equal. ©ÅÎ ðÄÇþÎˆÄ−fðÄÇþÎ Ój ðÄÇþÎ Ôg Õ ÏÅÍ©×Å−® ÏÓ { ðpÏfÓgy ÏÅÍ©×Å−® ÏÔ { ðpÏfÔgy Homework Problems 173 mh §−Î Î³− Í©×Å ¾©ÎÍ hm ÏÅÍ©×Å−® ÍÓ { ÏÓ || qoy ÏÅÍ©×Å−® ÍÔ { ÏÔ || qoy mh §©Ì− þÅ −ÓÉË−ÍÍ©ÇÅ ÏÍ©Å× ÇÅÄÔ ÏÓj ÏÔj ÍÓj þÅ® ÍÔ hm Ë−ÎÏËÅ y Û 2.85 ◆ Given a ﬂoating-point format with a k-bit exponent and an n-bit fraction, write formulas for the exponent E, the signiﬁcand M, the fraction f , and the value V for the quantities that follow. In addition, describe the bit representation. A. The number 7.0 B. The largest odd integer that can be represented exactly C. The reciprocal of the smallest positive normalized value 2.86 ◆ Intel-compatible processors also support an “extended-precision” ﬂoating-point format with an 80-bit word divided into a sign bit, k = 15 exponent bits, a single integer bit, and n = 63 fraction bits. The integer bit is an explicit copy of the implied bit in the IEEE ﬂoating-point representation. That is, it equals 1 for normalized values and 0 for denormalized values. Fill in the following table giving the approximate values of some “interesting” numbers in this format: Extended precision Description Value Decimal Smallest positive denormalized Smallest positive normalized Largest normalized This format can be used in C programs compiled for Intel-compatible ma- chines by declaring the data to be of type ÄÇÅ× ®ÇÏ¾Ä−. However, it forces the compiler to generate code based on the legacy 8087 ﬂoating-point instructions. The resulting program will most likely run much slower than would be the case for data type ðÄÇþÎ or ®ÇÏ¾Ä−. 2.87 ◆ The 2008 version of the IEEE ﬂoating-point standard, named IEEE 754-2008, includes a 16-bit “half-precision” ﬂoating-point format. It was originally devised by computer graphics companies for storing data in which a higher dynamic range is required than can be achieved with 16-bit integers. This format has 1 sign bit, 5 exponent bits (k = 5), and 10 fraction bits (n = 10). The exponent bias is 25−1 − 1 = 15. Fill in the table that follows for each of the numbers given, with the following instructions for each column: 174 Chapter 2 Representing and Manipulating Information Hex: The four hexadecimal digits describing the encoded form. M: The value of the signiﬁcand. This should be a number of the form x or x y , where x is an integer and y is an integral power of 2. Examples include 0, 67 64 , and 1 256 . E: The integer value of the exponent. V : The numeric value represented. Use the notation x or x × 2z, where x and z are integers. D: The (possibly approximate) numerical value, as is printed using the cð formatting speciﬁcation of ÉË©ÅÎð. As an example, to represent the number 7 8 , we would have s = 0, M = 7 4 , and E =−1. Our number would therefore have an exponent ﬁeld of 011102 (decimal value 15 − 1 = 14) and a signiﬁcand ﬁeld of 11000000002, giving a hex representation q¢nn. The numerical value is 0.875. You need not ﬁll in entries marked —. Description Hex ME V D −0 −0 −0.0 Smallest value > 2 512 512 512.0 Largest denormalized −∞ —— −∞ −∞ Number with hex representation q¢¢n q¢¢n 2.88 ◆◆ Consider the following two 9-bit ﬂoating-point representations based on the IEEE ﬂoating-point format. 1. Format A There is 1 sign bit. There are k = 5 exponent bits. The exponent bias is 15. There are n = 3 fraction bits. 2. Format B There is 1 sign bit. There are k = 4 exponent bits. The exponent bias is 7. There are n = 4 fraction bits. In the following table, you are given some bit patterns in format A, and your task is to convert them to the closest value in format B. If rounding is necessary you should round toward +∞. In addition, give the values of numbers given by the format A and format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions (e.g., 17/64 or 17/26). Homework Problems 175 Format A Format B Bits Value Bits Value o noooo nno −9 8 o nooo nnon −9 8 n onoon noo o nnooo non n nnnnn ooo o ooonn nnn n onooo onn 2.89 ◆ We are running programs on a machine where values of type ©ÅÎ have a 32- bit two’s-complement representation. Values of type ðÄÇþÎ use the 32-bit IEEE format, and values of type ®ÇÏ¾Ä− use the 64-bit IEEE format. We generate arbitrary integer values Ó, Ô, and Ö, and convert them to values of type ®ÇÏ¾Ä− as follows: mh £Ë−þÎ− ÍÇÀ− þË¾©ÎËþËÔ ÌþÄÏ−Í hm ©ÅÎ Ó { ËþÅ®ÇÀfgy ©ÅÎ Ô { ËþÅ®ÇÀfgy ©ÅÎ Ö { ËþÅ®ÇÀfgy mh £ÇÅÌ−ËÎ ÎÇ ®ÇÏ¾Ä− hm ®ÇÏ¾Ä− ®Ó { f®ÇÏ¾Ä−g Óy ®ÇÏ¾Ä− ®Ô { f®ÇÏ¾Ä−g Ôy ®ÇÏ¾Ä− ®Ö { f®ÇÏ¾Ä−g Öy For each of the following C expressions, you are to indicate whether or not the expression always yields 1. If it always yields 1, describe the underlying mathematical principles. Otherwise, give an example of arguments that make it yield 0. Note that you cannot use an IA32 machine running gcc to test your answers, since it would use the 80-bit extended-precision representation for both ðÄÇþÎ and ®ÇÏ¾Ä−. A. fðÄÇþÎg Ó {{ fðÄÇþÎg ®Ó B. ®Ó k ®Ô {{ f®ÇÏ¾Ä−g fÓkÔg C. f®Ói®Ôgi®Ö{{®Óif®Ôi®Ög D. f®Óh®Ôgh®Ö{{®Óhf®Ôh®Ög E. ®Óm®Ó{{®Öm®Ö 2.90 ◆ You have been assigned the task of writing a C function to compute a ﬂoating- point representation of 2x. You decide that the best way to do this is to directly construct the IEEE single-precision representation of the result. When x is too small, your routine will return 0.0. When x is too large, it will return +∞. Fill in the blank portions of the code that follows to compute the correct result. Assume the 176 Chapter 2 Representing and Manipulating Information function Ïpð returns a ﬂoating-point value having an identical bit representation as its unsigned argument. ðÄÇþÎ ðÉÑËpf©ÅÎ Óg Õ mh ‡−ÍÏÄÎ −ÓÉÇÅ−ÅÎ þÅ® ðËþ²Î©ÇÅ hm ÏÅÍ©×Å−® −ÓÉj ðËþ²y ÏÅÍ©×Å−® Ïy ©ð fÓ z gÕ mh ¶ÇÇ ÍÀþÄÄl ‡−ÎÏËÅ nln hm −ÓÉ { y ðËþ² { y Û −ÄÍ− ©ð fÓ z gÕ mh ⁄−ÅÇËÀþÄ©Ö−® Ë−ÍÏÄÎ hm −ÓÉ { y ðËþ² { y Û −ÄÍ− ©ð fÓ z gÕ mh ﬁÇËÀþÄ©Ö−® Ë−ÍÏÄÎl hm −ÓÉ { y ðËþ² { y Û −ÄÍ− Õ mh ¶ÇÇ ¾©×l ‡−ÎÏËÅ iÇÇ hm −ÓÉ { y ðËþ² { y Û mh –þ²Â −ÓÉ þÅ® ðËþ² ©ÅÎÇ qp ¾©ÎÍ hm Ï { −ÓÉ zz pq Ú ðËþ²y mh ‡−ÎÏËÅ þÍ ðÄÇþÎ hm Ë−ÎÏËÅ ÏpðfÏgy Û 2.91 ◆ Around 250 B.C., the Greek mathematician Archimedes proved that 223 71 <π < 22 7 . Had he had access to a computer and the standard library zÀþÎ³l³|, he would have been able to determine that the single-precision ﬂoating-point approximation of π has the hexadecimal representation nÓrnrwnƒ⁄¢. Of course, all of these are just approximations, since π is not rational. A. What is the fractional binary number denoted by this ﬂoating-point value? B. What is the fractional binary representation of 22 7 ? Hint: See Problem 2.83. C. At what bit position (relative to the binary point) do these two approxima- tions to π diverge? Homework Problems 177 Bit-Level Floating-Point Coding Rules In the following problems, you will write code to implement ﬂoating-point func- tions, operating directly on bit-level representations of ﬂoating-point numbers. Your code should exactly replicate the conventions for IEEE ﬂoating-point oper- ations, including using round-to-even mode when rounding is required. To this end, we deﬁne data type ðÄÇþÎˆ¾©ÎÍ to be equivalent to ÏÅÍ©×Å−®: mh ¡²²−ÍÍ ¾©ÎkÄ−Ì−Ä Ë−ÉË−Í−ÅÎþÎ©ÇÅ ðÄÇþÎ©Å×kÉÇ©ÅÎ ÅÏÀ¾−Ë hm ÎÔÉ−®−ð ÏÅÍ©×Å−® ðÄÇþÎˆ¾©ÎÍy Rather than using data type ðÄÇþÎ in your code, you will use ðÄÇþÎˆ¾©ÎÍ. You may use both ©ÅÎ and ÏÅÍ©×Å−® data types, including unsigned and integer constants and operations. You may not use any unions, structs, or arrays. Most signiﬁcantly, you may not use any ﬂoating-point data types, operations, or con- stants. Instead, your code should perform the bit manipulations that implement the speciﬁed ﬂoating-point operations. The following function illustrates the use of these coding rules. For argument f , it returns ±0if f is denormalized (preserving the sign of f ), and returns f otherwise. mh 'ð ð ©Í ®−ÅÇËÀj Ë−ÎÏËÅ nl ﬂÎ³−ËÑ©Í−j Ë−ÎÏËÅ ð hm ðÄÇþÎˆ¾©ÎÍ ðÄÇþÎˆ®−ÅÇËÀˆÖ−ËÇfðÄÇþÎˆ¾©ÎÍ ðg Õ mh ⁄−²ÇÀÉÇÍ− ¾©Î Ë−ÉË−Í−ÅÎþÎ©ÇÅ ©ÅÎÇ ÉþËÎÍ hm ÏÅÍ©×Å−® Í©×Å { ð||qoy ÏÅÍ©×Å−® −ÓÉ { ð||pq d nÓƒƒy ÏÅÍ©×Å−® ðËþ² { ð d nÓuƒƒƒƒƒy ©ð f−ÓÉ {{ ng Õ mh ⁄−ÅÇËÀþÄ©Ö−®l ·−Î ðËþ²Î©ÇÅ ÎÇ n hm ðËþ² { ny Û mh ‡−þÍÍ−À¾Ä− ¾©ÎÍ hm Ë−ÎÏËÅ fÍ©×Å zz qog Ú f−ÓÉ zz pqg Ú ðËþ²y Û 2.92 ◆◆ Following the bit-level ﬂoating-point coding rules, implement the function with the following prototype: mh £ÇÀÉÏÎ− kðl 'ð ð ©Í ﬁþﬁj Î³−Å Ë−ÎÏËÅ ðl hm ðÄÇþÎˆ¾©ÎÍ ðÄÇþÎˆÅ−×þÎ−fðÄÇþÎˆ¾©ÎÍ ðgy For ﬂoating-point number f , this function computes −f .If f is NaN, your function should simply return f . Test your function by evaluating it for all 232 values of argument ð and com- paring the result to what would be obtained using your machine’s ﬂoating-point operations. 178 Chapter 2 Representing and Manipulating Information 2.93 ◆◆ Following the bit-level ﬂoating-point coding rules, implement the function with the following prototype: mh £ÇÀÉÏÎ− ÚðÚl 'ð ð ©Í ﬁþﬁj Î³−Å Ë−ÎÏËÅ ðl hm ðÄÇþÎˆ¾©ÎÍ ðÄÇþÎˆþ¾ÍÌþÄfðÄÇþÎˆ¾©ÎÍ ðgy For ﬂoating-point number f , this function computes |f |.If f is NaN, your function should simply return f . Test your function by evaluating it for all 232 values of argument ð and com- paring the result to what would be obtained using your machine’s ﬂoating-point operations. 2.94 ◆◆◆ Following the bit-level ﬂoating-point coding rules, implement the function with the following prototype: mh £ÇÀÉÏÎ− phðl 'ð ð ©Í ﬁþﬁj Î³−Å Ë−ÎÏËÅ ðl hm ðÄÇþÎˆ¾©ÎÍ ðÄÇþÎˆÎÑ©²−fðÄÇþÎˆ¾©ÎÍ ðgy For ﬂoating-point number f , this function computes 2.0 . f .If f is NaN, your function should simply return f . Test your function by evaluating it for all 232 values of argument ð and com- paring the result to what would be obtained using your machine’s ﬂoating-point operations. 2.95 ◆◆◆ Following the bit-level ﬂoating-point coding rules, implement the function with the following prototype: mh £ÇÀÉÏÎ− nlshðl 'ð ð ©Í ﬁþﬁj Î³−Å Ë−ÎÏËÅ ðl hm ðÄÇþÎˆ¾©ÎÍ ðÄÇþÎˆ³þÄðfðÄÇþÎˆ¾©ÎÍ ðgy For ﬂoating-point number f , this function computes 0.5 . f .If f is NaN, your function should simply return f . Test your function by evaluating it for all 232 values of argument ð and com- paring the result to what would be obtained using your machine’s ﬂoating-point operations. 2.96 ◆◆◆◆ Following the bit-level ﬂoating-point coding rules, implement the function with the following prototype: mh h £ÇÀÉÏÎ− f©ÅÎg ðl h 'ð ²ÇÅÌ−ËÍ©ÇÅ ²þÏÍ−Í ÇÌ−ËðÄÇÑ ÇË ð ©Í ﬁþﬁj Ë−ÎÏËÅ nÓvnnnnnnn hm ©ÅÎ ðÄÇþÎˆðp©fðÄÇþÎˆ¾©ÎÍ ðgy Solutions to Practice Problems 179 For ﬂoating-point number f , this function computes f©ÅÎg f . Your function should round toward zero. If f cannot be represented as an integer (e.g., it is out of range, or it is NaN), then the function should return nÓvnnnnnnn. Test your function by evaluating it for all 232 values of argument ð and com- paring the result to what would be obtained using your machine’s ﬂoating-point operations. 2.97 ◆◆◆◆ Following the bit-level ﬂoating-point coding rules, implement the function with the following prototype: mh £ÇÀÉÏÎ− fðÄÇþÎg © hm ðÄÇþÎˆ¾©ÎÍ ðÄÇþÎˆ©pðf©ÅÎ ©gy For argument ©, this function computes the bit-level representation of fðÄÇþÎg ©. Test your function by evaluating it for all 232 values of argument ð and com- paring the result to what would be obtained using your machine’s ﬂoating-point operations. Solutions to Practice Problems Solution to Problem 2.1 (page 73) Understanding the relation between hexadecimal and binary formats will be im- portant once we start looking at machine-level programs. The method for doing these conversions is in the text, but it takes a little practice to become familiar. A. nÓps¢w⁄p to binary: Hexadecimal ps¢w⁄ p Binary 0010 0101 1101 1001 1101 0010 B. Binary 1100 1001 0111 1011 to hexadecimal: Binary 1100 1001 0111 1011 Hexadecimal £wu¢ C. nÓ¡v¢q⁄ to binary: Hexadecimal ¡ v¢q⁄ Binary 1010 1000 1011 0011 1101 D. Binary 11 0010 0010 1101 1001 0110 to hexadecimal: Binary 11 0010 0010 1101 1001 0110 Hexadecimal q p p⁄w t Solution to Problem 2.2 (page 73) This problem gives you a chance to think about powers of 2 and their hexadecimal representations. 180 Chapter 2 Representing and Manipulating Information n 2n (decimal) 2n (hexadecimal) 532 nÓpn 23 8,388,608 nÓvnnnnn 15 32,768 nÓvnnn 13 8,192 nÓpnnn 12 4,096 nÓonnn 664 nÓrn 8 256 nÓonn Solution to Problem 2.3 (page 74) This problem gives you a chance to try out conversions between hexadecimal and decimal representations for some smaller numbers. For larger ones, it becomes much more convenient and reliable to use a calculator or conversion program. Decimal Binary Hexadecimal 0 0000 0000 nÓnn 158 = 16 . 9 + 14 1001 1110 nÓw¥ 76 = 16 . 4 + 12 0100 1100 nÓr£ 145 = 16 . 9 + 1 1001 0001 nÓwo 16 . 10 + 14 = 174 1010 1110 nÓ¡¥ 16 . 3 + 12 = 60 0011 1100 nÓq£ 16 . 15 + 1 = 241 1111 0001 nÓƒo 16 . 7 + 5 = 117 0111 0101 nÓus 16 . 11 + 13 = 189 1011 1101 nÓ¢⁄ 16 . 15 + 5 = 245 1111 0101 nÓƒs Solution to Problem 2.4 (page 75) When you begin debugging machine-level programs, you will ﬁnd many cases where some simple hexadecimal arithmetic would be useful. You can always convert numbers to decimal, perform the arithmetic, and convert them back, but being able to work directly in hexadecimal is more efﬁcient and informative. A. nÓtns£ + nÓs = nÓtnto. Adding s to hex £ gives o with a carry of o. B. nÓtns£ − nÓpn = nÓtnq£. Subtracting p from s in the second digit position requires no borrow from the third. This gives q. C. nÓtns£ + 32 = nÓtnu£. Decimal 32 (25) equals hexadecimal nÓpn. D. nÓtnƒ¡ − nÓtns£ = nÓw¥. To subtract hex £ (decimal 12) from hex ¡ (decimal 10), we borrow 16 from the second digit, giving hex ƒ (decimal 15). In the second digit, we now subtract 5 from hex ¥ (decimal 14), giving decimal 9. Solution to Problem 2.5 (page 84) This problem tests your understanding of the byte representation of data and the two different byte orderings. A. Little endian: uv Big endian: op B. Little endian: uv st Big endian: op qr Solutions to Practice Problems 181 C. Little endian: uv st qr Big endian: op qr st Recall that Í³ÇÑˆ¾ÔÎ−Í enumerates a series of bytes starting from the one with lowest address and working toward the one with highest address. On a little- endian machine, it will list the bytes from least signiﬁcant to most. On a big-endian machine, it will list bytes from the most signiﬁcant byte to the least. Solution to Problem 2.6 (page 85) This problem is another chance to practice hexadecimal to binary conversion. It also gets you thinking about integer and ﬂoating-point representations. We will explore these representations in more detail later in this chapter. A. Using the notation of the example in the text, we write the two strings as follows: nnpu£vƒv nnnnnnnnnnonnooooonnonnnooooonnn hhhhhhhhhhhhhhhhhhhhhh r¡oƒp q ¥ n nonnononnnnooooonnonnnooooonnnnn B. With the second word shifted two positions to the right relative to the ﬁrst, we ﬁnd a sequence with 21 matching bits. C. We ﬁnd all bits of the integer embedded in the ﬂoating-point number, except for the most signiﬁcant bit having value 0. Such is the case for the example in the text as well. In addition, the ﬂoating-point number has some nonzero high-order bits that do not match those of the integer. Solution to Problem 2.7 (page 85) It prints t⁄ t¥ tƒ un uo up. Recall also that the library routine ÍÎËÄ−Å does not count the terminating null character, and so Í³ÇÑˆ¾ÔÎ−Í printed only through the character ‘Ë’. Solution to Problem 2.8 (page 87) This problem is a drill to help you become more familiar with Boolean operations. Operation Result a [01001110] b [11100001] Üa [10110001] Üb [00011110] a d b [01000000] a Ú b [11101111] a ´ b [10101111] 182 Chapter 2 Representing and Manipulating Information Solution to Problem 2.9 (page 89) This problem illustrates how Boolean algebra can be used to describe and reason about real-world systems. We can see that this color algebra is identical to the Boolean algebra over bit vectors of length 3. A. Colors are complemented by complementing the values of R, G, and B. From this, we can see that white is the complement of black, yellow is the complement of blue, magenta is the complement of green, and cyan is the complement of red. B. We perform Boolean operations based on a bit-vector representation of the colors. From this we get the following: Blue (001) Ú Green (010) = Cyan (011) Yellow (110) d Cyan (011) = Green (010) Red (100) ´ Magenta (101) = Blue (001) Solution to Problem 2.10 (page 90) This procedure relies on the fact that exclusive-or is commutative and associative, and that a ´ a = 0 for any a. Step hÓ hÔ Initially ab Step 1 aa ´ b Step 2 a ´ (a ´ b) = (a ´ a) ´ b = ba ´ b Step 3 bb ´ (a ´ b) = (b ´ b) ´ a = a See Problem 2.11 for a case where this function will fail. Solution to Problem 2.11 (page 91) This problem illustrates a subtle and interesting feature of our inplace swap routine. A. Both ð©ËÍÎ and ÄþÍÎ have value k, so we are attempting to swap the middle element with itself. B. In this case, arguments Ó and Ô to ©ÅÉÄþ²−ˆÍÑþÉ both point to the same location. When we compute hÓ´hÔ, we get 0. We then store 0 as the middle element of the array, and the subsequent steps keep setting this element to 0. We can see that our reasoning in Problem 2.10 implicitly assumed that Ó and Ô denote different locations. C. Simply replace the test in line 4 of Ë−Ì−ËÍ−ˆþËËþÔ to be ð©ËÍÎ z ÄþÍÎ, since there is no need to swap the middle element with itself. Solution to Problem 2.12 (page 91) Here are the expressions: Solutions to Practice Problems 183 A. Ó d nÓƒƒ B. Ó ´ ÜnÓƒƒ C. Ó Ú nÓƒƒ These expressions are typical of the kind commonly found in performing low-level bit operations. The expression ÜnÓƒƒ creates a mask where the 8 least-signiﬁcant bits equal 0 and the rest equal 1. Observe that such a mask will be generated regardless of the word size. By contrast, the expression nÓƒƒƒƒƒƒnn would only work when data type ©ÅÎ is 32 bits. Solution to Problem 2.13 (page 92) These problems help you think about the relation between Boolean operations and typical ways that programmers apply masking operations. Here is the code: mh ⁄−²ÄþËþÎ©ÇÅÍ Çð ðÏÅ²Î©ÇÅÍ ©ÀÉÄ−À−ÅÎ©Å× ÇÉ−ËþÎ©ÇÅÍ ¾©Í þÅ® ¾©² hm ©ÅÎ ¾©Íf©ÅÎ Ój ©ÅÎ Àgy ©ÅÎ ¾©²f©ÅÎ Ój ©ÅÎ Àgy mh £ÇÀÉÏÎ− ÓÚÔ ÏÍ©Å× ÇÅÄÔ ²þÄÄÍ ÎÇ ðÏÅ²Î©ÇÅÍ ¾©Í þÅ® ¾©² hm ©ÅÎ ¾ÇÇÄˆÇËf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ Ë−ÍÏÄÎ { ¾©ÍfÓjÔgy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û mh £ÇÀÉÏÎ− Ó´Ô ÏÍ©Å× ÇÅÄÔ ²þÄÄÍ ÎÇ ðÏÅ²Î©ÇÅÍ ¾©Í þÅ® ¾©² hm ©ÅÎ ¾ÇÇÄˆÓÇËf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ Ë−ÍÏÄÎ { ¾©Íf¾©²fÓjÔgj ¾©²fÔjÓggy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û The ¾©Í operation is equivalent to Boolean or—a bit is set in Ö if either this bit is set in Ó or it is set in À. On the other hand, ¾©²fÓj Àg is equivalent to ÓdÜÀ; we want the result to equal 1 only when the corresponding bit of Ó is 1 and of À is 0. Given that, we can implement Ú with a single call to ¾©Í. To implement ´,we take advantage of the property x ´ y = (x dÜy) Ú (Üx d y) Solution to Problem 2.14 (page 93) This problem highlights the relation between bit-level Boolean operations and logical operations in C. A common programming error is to use a bit-level oper- ation when a logical one is intended, or vice versa. 184 Chapter 2 Representing and Manipulating Information Expression Value Expression Value þ d ¾ nÓrr þ dd ¾ nÓno þ Ú ¾ nÓsu þ ÚÚ ¾ nÓno Üþ Ú Ü¾ nÓ¢¢ _þ ÚÚ _¾ nÓnn þ d _¾ nÓnn þ dd Ü¾ nÓno Solution to Problem 2.15 (page 93) The expression is _fÓ´Ôg. That is, Ó´Ô will be zero if and only if every bit of Ó matches the corresponding bit of Ô. We then exploit the ability of _ to determine whether a word contains any nonzero bit. There is no real reason to use this expression rather than simply writing Ó{{ Ô, but it demonstrates some of the nuances of bit-level and logical operations. Solution to Problem 2.16 (page 94) This problem is a drill to help you understand the different shift operations. Logical Arithmetic Ó þ zz p þ || q þ || q Hex Binary Binary Hex Binary Hex Binary Hex nÓ⁄r [11010100] [01010000] nÓsn [00011010] nÓo¡ [11111010] nÓƒ¡ nÓtr [01100100] [10010000] nÓwn [00001100] nÓn£ [11101100] nÓ¥£ nÓup [01110010] [11001000] nÓ£v [00001110] nÓn¥ [00001110] nÓn¥ nÓrr [01000100] [00010000] nÓon [00001000] nÓnv [11101000] nÓ¥w Solution to Problem 2.17 (page 101) In general, working through examples for very small word sizes is a very good way to understand computer arithmetic. The unsigned values correspond to those in Figure 2.2. For the two’s- complement values, hex digits n through u have a most signiﬁcant bit of 0, yielding nonnegative values, while hex digits v through ƒ have a most signiﬁcant bit of 1, yielding a negative value. Hexadecimal Binary ⃗x B2U 4(⃗x) B2T 4(⃗x) nÓ¡ [1010] 23 + 21 = 10 −23 + 22 =−6 nÓo [0001] 20 =1 20 = 1 nÓ¢ [1011] 23 + 21 + 20 = 11 −23 + 21 + 20 =−5 nÓp [0010] 21 = 221 = 2 nÓu [0111] 22 + 21 + 20 = 722 + 21 + 20 = 7 nÓ£ [1100] 23 + 22 = 12 −23 + 22 =−4 Solutions to Practice Problems 185 Solution to Problem 2.18 (page 105) For a 32-bit word, any value consisting of 8 hexadecimal digits beginning with one of the digits v through ð represents a negative number. It is quite common to see numbers beginning with a string of ð’s, since the leading bits of a negative number are all ones. You must look carefully, though. For example, the number nÓvnrvqqu has only 7 digits. Filling this out with a leading zero gives nÓnvnrvqqu, a positive number. rnnr®nx rv vo −² −n np nn nn ÍÏ¾ bnÓp−njcËÍÉ A. 736 rnnr®ux rv v¾ rr pr þv ÀÇÌ knÓsvfcËÍÉgjcËþÓ B. -88 rnnr®²x rv nq ru pv þ®® nÓpvfcË®©gjcËþÓ C. 40 rnnr−nx rv vw rr pr ®n ÀÇÌ cËþÓjknÓqnfcËÍÉg D. -48 rnnr−sx rv v¾ rr pr uv ÀÇÌ nÓuvfcËÍÉgjcËþÓ E. 120 rnnr−þx rv vw vu vv nn nn nn ÀÇÌ cËþÓjnÓvvfcË®©g F. 136 rnnrðox rv v¾ vr pr ðv no nn ÀÇÌ nÓoðvfcËÍÉgjcËþÓ G. 504 rnnrðvx nn rnnrðwx rv nq rr pr nv þ®® nÓvfcËÍÉgjcËþÓ rnnrð−x rv vw vr pr ²n nn nn ÀÇÌ cËþÓjnÓ²nfcËÍÉg H. 192 rnnsnsx nn rnnsntx rv v¾ rr ®r ¾v ÀÇÌ knÓrvfcËÍÉjcË®ÓjvgjcËþÓ I. -72 Solution to Problem 2.19 (page 107) The functions T2U and U2T are very peculiar from a mathematical perspective. It is important to understand how they behave. We solve this problem by reordering the rows in the solution of Problem 2.17 according to the two’s-complement value and then listing the unsigned value as the result of the function application. We show the hexadecimal values to make this process more concrete. ⃗x (hex) x T2U 4(vecx) nÓƒ −115 nÓ¢ −511 nÓ¡ −610 nÓ£ −412 nÓo 11 nÓv 88 Solution to Problem 2.20 (page 109) This exercise tests your understanding of Equation 2.5. For the ﬁrst four entries, the values of x are negative and T2U 4(x) = x + 24. For the remaining two entries, the values of x are nonnegative and T2U 4(x) = x. Solution to Problem 2.21 (page 112) This problem reinforces your understanding of the relation between two’s- complement and unsigned representations, as well as the effects of the C promo- tion rules. Recall that TMin32 is −2,147,483,648, and that when cast to unsigned it 186 Chapter 2 Representing and Manipulating Information becomes 2,147,483,648. In addition, if either operand is unsigned, then the other operand will be cast to unsigned before comparing. Expression Type Evaluation kporurvqtruko {{ porurvqtrv• Unsigned o kporurvqtruko z porurvqtru Signed o kporurvqtruko• z porurvqtru Unsigned n kporurvqtruko z kporurvqtru Signed o kporurvqtruko• z kporurvqtru Unsigned o Solution to Problem 2.22 (page 115) This exercise provides a concrete demonstration of how sign extension preserves the numeric value of a two’s-complement representation. A. [1100] −23 + 22 =−8 + 4 =−4 B. [11100] −24 + 23 + 22 =−16 + 8 + 4 =−4 C. [111100] −25 + 24 + 23 + 22 =−32 + 16 + 8 + 4 =−4 Solution to Problem 2.23 (page 116) The expressions in these functions are common program “idioms” for extracting values from a word in which multiple bit ﬁelds have been packed. They exploit the zero-ﬁlling and sign-extending properties of the different shift operations. Note carefully the ordering of the cast and shift operations. In ðÏÅo, the shifts are performed on unsigned variable ÑÇË® and hence are logical. In ðÏÅp, shifts are performed after casting ÑÇË® to ©ÅÎ and hence are arithmetic. A. Ñ ðÏÅofÑg ðÏÅpfÑg nÓnnnnnnut nÓnnnnnnut nÓnnnnnnut nÓvutsrqpo nÓnnnnnnpo nÓnnnnnnpo nÓnnnnnn£w nÓnnnnnn£w nÓƒƒƒƒƒƒ£w nÓ¥⁄£¢¡wvu nÓnnnnnnvu nÓƒƒƒƒƒƒvu B. Function ðÏÅo extracts a value from the low-order 8 bits of the argument, giving an integer ranging between 0 and 255. Function ðÏÅp extracts a value from the low-order 8 bits of the argument, but it also performs sign extension. The result will be a number between −128 and 127. Solution to Problem 2.24 (page 118) The effect of truncation is fairly intuitive for unsigned numbers, but not for two’s- complement numbers. This exercise lets you explore its properties using very small word sizes. Solutions to Practice Problems 187 Hex Unsigned Two’s complement Original Truncated Original Truncated Original Truncated oo 11 1 1 qq 33 3 3 ss 55 5 5 £r 12 4 −44 ¥t 14 6 −26 As Equation 2.9 states, the effect of this truncation on unsigned values is to simply ﬁnd their residue, modulo 8. The effect of the truncation on signed values is a bit more complex. According to Equation 2.10, we ﬁrst compute the modulo 8 residue of the argument. This will give values 0 through 7 for arguments 0 through 7, and also for arguments −8 through −1. Then we apply function U2T 3 to these residues, giving two repetitions of the sequences 0 through 3 and −4 through −1. Solution to Problem 2.25 (page 119) This problem is designed to demonstrate how easily bugs can arise due to the implicit casting from signed to unsigned. It seems quite natural to pass parameter Ä−Å×Î³ as an unsigned, since one would never want to use a negative length. The stopping criterion © z{ Ä−Å×Î³ko also seems quite natural. But combining these two yields an unexpected outcome! Since parameter Ä−Å×Î³ is unsigned, the computation 0 − 1 is performed using unsigned arithmetic, which is equivalent to modular addition. The result is then UMax.The ≤ comparison is also performed using an unsigned comparison, and since any number is less than or equal to UMax, the comparison always holds! Thus, the code attempts to access invalid elements of array þ. The code can be ﬁxed either by declaring Ä−Å×Î³ to be an ©ÅÎ or by changing the test of the ðÇË loop to be © z Ä−Å×Î³. Solution to Problem 2.26 (page 119) This example demonstrates a subtle feature of unsigned arithmetic, and also the property that we sometimes perform unsigned arithmetic without realizing it. This can lead to very tricky bugs. A. For what cases will this function produce an incorrect result? The function will incorrectly return 1 when Í is shorter than Î. B. Explain how this incorrect result comes about. Since ÍÎËÄ−Å is deﬁned to yield an unsigned result, the difference and the comparison are both com- puted using unsigned arithmetic. When Í is shorter than Î, the difference ÍÎËÄ−ÅfÍg k ÍÎËÄ−ÅfÎg should be negative, but instead becomes a large, unsigned number, which is greater than 0. C. Show how to ﬁx the code so that it will work reliably. Replace the test with the following: Ë−ÎÏËÅ ÍÎËÄ−ÅfÍg | ÍÎËÄ−ÅfÎgy 188 Chapter 2 Representing and Manipulating Information Solution to Problem 2.27 (page 125) This function is a direct implementation of the rules given to determine whether or not an unsigned addition overﬂows. mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− þ®®−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm ©ÅÎ Ïþ®®ˆÇÂfÏÅÍ©×Å−® Ój ÏÅÍ©×Å−® Ôg Õ ÏÅÍ©×Å−® ÍÏÀ { ÓiÔy Ë−ÎÏËÅ ÍÏÀ |{ Óy Û Solution to Problem 2.28 (page 125) This problem is a simple demonstration of arithmetic modulo 16. The easiest way to solve it is to convert the hex pattern into its unsigned decimal value. For nonzero values of x, we must have (ku 4 x) + x = 16. Then we convert the complemented value back to hex. x ku 4 x Hex Decimal Decimal Hex o 115 ƒ r 412 £ u 79 w ¡ 10 6 t ¥ 14 2 p Solution to Problem 2.29 (page 129) This problem is an exercise to make sure you understand two’s-complement addition. xy x + yx i t 5 y Case −12 −15 −27 5 1 [10100] [10001] [100101] [00101] −8 −8 −16 −16 2 [11000] [11000] [110000] [10000] −98 −1 −12 [10111] [01000] [111111] [11111] 25 77 3 [00010] [00101] [000111] [00111] 12 4 16 −16 4 [01100] [00100] [010000] [10000] Solutions to Practice Problems 189 Solution to Problem 2.30 (page 130) This function is a direct implementation of the rules given to determine whether or not a two’s-complement addition overﬂows. mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë þË×ÏÀ−ÅÎÍ ²þÅ ¾− þ®®−® Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm ©ÅÎ Îþ®®ˆÇÂf©ÅÎ Ój ©ÅÎ Ôg Õ ©ÅÎ ÍÏÀ { ÓiÔy ©ÅÎ Å−×ˆÇÌ−Ë{Óz nddÔz nddÍÏÀ|{ny ©ÅÎ ÉÇÍˆÇÌ−Ë{Ó|{nddÔ|{nddÍÏÀz ny Ë−ÎÏËÅ _Å−×ˆÇÌ−Ë dd _ÉÇÍˆÇÌ−Ëy Û Solution to Problem 2.31 (page 130) Your coworker could have learned, by studying Section 2.3.2, that two’s- complement addition forms an abelian group, and so the expression fÓiÔgkÓ will evaluate to Ô regardless of whether or not the addition overﬂows, and that fÓiÔgkÔ will always evaluate to Ó. Solution to Problem 2.32 (page 130) This function will give correct values, except when Ô is TMin. In this case, we will have kÔ also equal to TMin, and so the call to function Îþ®®ˆÇÂ will indicate overﬂow when Ó is negative and no overﬂow when Ó is nonnegative. In fact, the opposite is true: ÎÍÏ¾ˆÇÂfÓj TMing should yield 0 when Ó is negative and 1 when it is nonnegative. One lesson to be learned from this exercise is that TMin should be included as one of the cases in any test procedure for a function. Solution to Problem 2.33 (page 131) This problem helps you understand two’s-complement negation using a very small word size. For w = 4, we have TMin4 =−8. So −8 is its own additive inverse, while other values are negated by integer negation. x k t 4 x Hex Decimal Decimal Hex p 2 −2 ¥ q 3 −3 ⁄ w −9 −9 u ¢ −55 s £ −44 r The bit patterns are the same as for unsigned negation. Solution to Problem 2.34 (page 134) This problem is an exercise to make sure you understand two’s-complement multiplication. 190 Chapter 2 Representing and Manipulating Information Mode xy x . y Truncated x . y Unsigned 4 [100] 5 [101] 20 [010100] 4 [100] Two’s complement −4 [100] −3 [101] 12 [001100] −4 [100] Unsigned 2 [010] 7 [111] 14 [001110] 6 [110] Two’s complement 2 [010] −1 [111] −2 [111110] −2 [110] Unsigned 6 [110] 6 [110] 36 [100100] 4 [100] Two’s complement −2 [110] −2 [110] 4 [000100] −4 [100] Solution to Problem 2.35 (page 135) It is not realistic to test this function for all possible values of Ó and Ô. Even if you could run 10 billion tests per second, it would require over 58 years to test all combinations when data type ©ÅÎ is 32 bits. On the other hand, it is feasible to test your code by writing the function with data type Í³ÇËÎ or ²³þË and then testing it exhaustively. Here’s a more principled approach, following the proposed set of arguments: 1. We know that x . y can be written as a 2w-bit two’s-complement number. Let u denote the unsigned number represented by the lower w bits, and v denote the two’s-complement number represented by the upper w bits. Then, based on Equation 2.3, we can see that x . y = v2w + u. We also know that u = T2U w(p), since they are unsigned and two’s- complement numbers arising from the same bit pattern, and so by Equation 2.6, we can write u = p + pw−12w, where pw−1 is the most signiﬁcant bit of p. Letting t = v + pw−1, we have x . y = p + t2w. When t = 0, we have x . y = p; the multiplication does not overﬂow. When t ̸= 0, we have x . y ̸= p; the multiplication does overﬂow. 2. By deﬁnition of integer division, dividing p by nonzero x gives a quotient q and a remainder r such that p = x . q + r, and |r| < |x|. (We use absolute values here, because the signs of x and r may differ. For example, dividing −7 by 2 gives quotient −3 and remainder −1.) 3. Suppose q = y. Then we have x . y = x . y + r + t2w. From this, we can see that r + t2w = 0. But |r| < |x|≤ 2w, and so this identity can hold only if t = 0, in which case r = 0. Suppose r = t = 0. Then we will have x . y = x . q, implying that y = q. When x equals 0, multiplication does not overﬂow, and so we see that our code provides a reliable way to test whether or not two’s-complement multiplication causes overﬂow. Solution to Problem 2.36 (page 135) With 64 bits, we can perform the multiplication without overﬂowing. We then test whether casting the product to 32 bits changes the value: Solutions to Practice Problems 191 1 mh ⁄−Î−ËÀ©Å− Ñ³−Î³−Ë Î³− þË×ÏÀ−ÅÎÍ ²þÅ ¾− ÀÏÄÎ©ÉÄ©−® 2 Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm 3 ©ÅÎ ÎÀÏÄÎˆÇÂf©ÅÎ Ój ©ÅÎ Ôg Õ 4 mh £ÇÀÉÏÎ− ÉËÇ®Ï²Î Ñ©Î³ÇÏÎ ÇÌ−ËðÄÇÑ hm 5 ©ÅÎtrˆÎ ÉÄÄ { f©ÅÎtrˆÎg ÓhÔy 6 mh ·−− ©ð ²þÍÎ©Å× ÎÇ ©ÅÎ ÉË−Í−ËÌ−Í ÌþÄÏ− hm 7 Ë−ÎÏËÅ ÉÄÄ {{ f©ÅÎg ÉÄÄy 8 Û Note that the casting on the right-hand side of line 5 is critical. If we instead wrote the line as ©ÅÎtrˆÎ ÉÄÄ { ÓhÔy the product would be computed as a 32-bit value (possibly overﬂowing) and then sign extended to 64 bits. Solution to Problem 2.37 (page 135) A. This change does not help at all. Even though the computation of þÍ©Ö− will be accurate, the call to ÀþÄÄÇ² will cause this value to be converted to a 32-bit unsigned number, and so the same overﬂow conditions will occur. B. With ÀþÄÄÇ² having a 32-bit unsigned number as its argument, it cannot possibly allocate a block of more than 232 bytes, and so there is no point attempting to allocate or copy this much memory. Instead, the function should abort and return ﬁ•‹‹, as illustrated by the following replacement to the original call to ÀþÄÄÇ² (line 9): Ï©ÅÎtrˆÎ Ë−ÊÏ©Ë−®ˆÍ©Ö− { −Ä−ˆ²ÅÎ h fÏ©ÅÎtrˆÎg −Ä−ˆÍ©Ö−y Í©Ö−ˆÎ Ë−ÊÏ−ÍÎˆÍ©Ö− { fÍ©Ö−ˆÎg Ë−ÊÏ©Ë−®ˆÍ©Ö−y ©ð fË−ÊÏ©Ë−®ˆÍ©Ö− _{ Ë−ÊÏ−ÍÎˆÍ©Ö−g mh ﬂÌ−ËðÄÇÑ ÀÏÍÎ ³þÌ− Ç²²ÏËË−®l ¡¾ÇËÎ ÇÉ−ËþÎ©ÇÅ hm Ë−ÎÏËÅ ﬁ•‹‹y ÌÇ©® hË−ÍÏÄÎ { ÀþÄÄÇ²fË−ÊÏ−ÍÎˆÍ©Ö−gy ©ð fË−ÍÏÄÎ {{ ﬁ•‹‹g mh ÀþÄÄÇ² ðþ©Ä−® hm Ë−ÎÏËÅ ﬁ•‹‹y Solution to Problem 2.38 (page 138) In Chapter 3, we will see many examples of the lea instruction in action. The instruction is provided to support pointer arithmetic, but the C compiler often uses it as a way to perform multiplication by small constants. For each value of k, we can compute two multiples: 2k (when ¾ is 0) and 2k + 1 (when ¾ is þ). Thus, we can compute multiples 1, 2, 3, 4, 5, 8, and 9. 192 Chapter 2 Representing and Manipulating Information Solution to Problem 2.39 (page 139) The expression simply becomes kfÓzzmg. To see this, let the word size be w so that n = w − 1. Form B states that we should compute fÓzzwg k fÓzzmg, but shifting Ó to the left by w will yield the value n. Solution to Problem 2.40 (page 139) This problem requires you to try out the optimizations already described and also to supply a bit of your own ingenuity. K Shifts Add/Subs Expression 71 1 fÓzzqg k Ó 30 4 3 fÓzzrg i fÓzzqg i fÓzzpg i fÓzzog 28 2 1 fÓzzsg k fÓzzpg 55 2 2 fÓzztg k fÓzzqg k Ó Observe that the fourth case uses a modiﬁed version of form B. We can view the bit pattern [11011] as having a run of 6 ones with a zero in the middle, and so we apply the rule for form B, but then we subtract the term corresponding to the middle zero bit. Solution to Problem 2.41 (page 139) Assuming that addition and subtraction have the same performance, the rule is to choose form A when n = m, either form when n = m + 1, and form B when n>m + 1. The justiﬁcation for this rule is as follows. Assume ﬁrst that m> 0. When n = m, form A requires only a single shift, while form B requires two shifts and a subtraction. When n = m + 1, both forms require two shifts and either an addition or a subtraction. When n>m + 1, form B requires only two shifts and one subtraction, while form A requires n − m + 1 > 2 shifts and n − m> 1 additions. For the case of m = 0, we get one fewer shift for both forms A and B, and so the same rules apply for choosing between the two. Solution to Problem 2.42 (page 143) The only challenge here is to compute the bias without any testing or conditional operations. We use the trick that the expression Ó||qo generates a word with all ones if Ó is negative, and all zeros otherwise. By masking off the appropriate bits, we get the desired bias value. ©ÅÎ ®©Ìotf©ÅÎ Óg Õ mh £ÇÀÉÏÎ− ¾©þÍ ÎÇ ¾− −©Î³−Ë n fÓ |{ ng ÇË os fÓ z ng hm ©ÅÎ ¾©þÍ { fÓ || qog d nÓƒy Ë−ÎÏËÅ fÓ i ¾©þÍg || ry Û Solutions to Practice Problems 193 Solution to Problem 2.43 (page 143) We have found that people have difﬁculty with this exercise when working di- rectly with assembly code. It becomes more clear when put in the form shown in ÇÉÎþË©Î³. We can see that › is 31; Óh› is computed as fÓzzsgkÓ. We can see that ﬁ is 8; a bias value of 7 is added when Ô is negative, and the right shift is by 3. Solution to Problem 2.44 (page 144) These “C puzzle” problems provide a clear demonstration that programmers must understand the properties of computer arithmetic: A. fÓ|ngÚÚfÓkozng False. Let Ó be −2,147,483,648 (TMin32). We will then have Óko equal to 2,147,483,647 (TMax32). B. fÓ d ug _{ u ÚÚ fÓzzpw z ng True.If fÓdug_{u evaluates to 0, then we must have bit x2 equal to 1. When shifted left by 29, this will become the sign bit. C. fÓhÓg|{n False. When Ó is 65,535 (nÓƒƒƒƒ), ÓhÓ is −131,071 (nÓƒƒƒ¥nnno). D. ÓznÚÚkÓz{n True.If Ó is nonnegative, then kÓ is nonpositive. E. Ó|nÚÚkÓ|{n False. Let Ó be −2,147,483,648 (TMin32). Then both Ó and kÓ are negative. F. ÓiÔ {{ ÏÔiÏÓ True. Two’s-complement and unsigned addition have the same bit-level be- havior, and they are commutative. G. ÓhÜÔ i ÏÔhÏÓ {{ kÓ True. ÜÔ equals kÔko. ÏÔhÏÓ equals ÓhÔ. Thus, the left-hand side is equivalent to ÓhkÔkÓiÓhÔ. Solution to Problem 2.45 (page 147) Understanding fractional binary representations is an important step to under- standing ﬂoating-point encodings. This exercise lets you try out some simple ex- amples. 1 8 0.001 0.125 3 4 0.11 0.75 25 16 1.1001 1.5625 43 16 10.1011 2.6875 9 8 1.001 1.125 47 8 101.111 5.875 51 16 11.0011 3.1875 194 Chapter 2 Representing and Manipulating Information One simple way to think about fractional binary representations is to repre- sent a number as a fraction of the form x 2k . We can write this in binary using the binary representation of x, with the binary point inserted k positions from the right. As an example, for 25 16 , we have 2510 = 110012. We then put the binary point four positions from the right to get 1.10012. Solution to Problem 2.46 (page 147) In most cases, the limited precision of ﬂoating-point numbers is not a major problem, because the relative error of the computation is still fairly low. In this example, however, the system was sensitive to the absolute error. A. We can see that 0.1 − x has the binary representation 0.000000000000000000000001100[1100] ... 2 B. Comparing this to the binary representation of 1 10 , we can see that it is simply 2−20 × 1 10 , which is around 9.54 × 10−8. C. 9.54 × 10−8 × 100 × 60 × 60 × 10 ≈ 0.343 seconds. D. 0.343 × 2,000 ≈ 687 meters. Solution to Problem 2.47 (page 153) Working through ﬂoating-point representations for very small word sizes helps clarify how IEEE ﬂoating point works. Note especially the transition between denormalized and normalized values. Bits eE 2E fM 2E × MV Decimal nnnnn 00 1 0 4 0 4 0 4 0 0.0 nnnno 00 1 1 4 1 4 1 4 1 4 0.25 nnnon 00 1 2 4 2 4 2 4 1 2 0.5 nnnoo 00 1 3 4 3 4 3 4 3 4 0.75 nnonn 10 1 0 4 4 4 4 4 1 1.0 nnono 10 1 1 4 5 4 5 4 5 4 1.25 nnoon 10 1 2 4 6 4 6 4 3 2 1.5 nnooo 10 1 3 4 7 4 7 4 7 4 1.75 nonnn 21 2 0 4 4 4 8 4 2 2.0 nonno 21 2 1 4 5 4 10 4 5 2 2.5 nonon 21 2 2 4 6 4 12 4 3 3.0 nonoo 21 2 3 4 7 4 14 4 7 2 3.5 noonn — ——— — — ∞ — noono — ——— — — NaN — nooon — ——— — — NaN — noooo — ——— — — NaN — Solutions to Practice Problems 195 Solution to Problem 2.48 (page 155) Hexadecimal nÓqsworo is equivalent to binary [1101011001000101000001]. Shift- ing this right 21 places gives 1.1010110010001010000012 × 221. We form the frac- tion ﬁeld by dropping the leading 1 and adding two zeros, giving [10101100100010100000100] The exponent is formed by adding bias 127 to 21, giving 148 (binary [10010100]). We combine this with a sign ﬁeld of 0 to give a binary representation [01001010010101100100010100000100] We see that the matching bits in the two representations correspond to the low- order bits of the integer, up to the most signiﬁcant bit equal to 1 matching the high-order 21 bits of the fraction: nnqsworo nnnnnnnnnnoononoonnonnnononnnnno hhhhhhhhhhhhhhhhhhhhh r¡strsnr nonnononnononoonnonnnononnnnnonn Solution to Problem 2.49 (page 156) This exercise helps you think about what numbers cannot be represented exactly in ﬂoating point. A. The number has binary representation 1, followed by n zeros, followed by 1, giving value 2n+1 + 1. B. When n = 23, the value is 224 + 1 = 16,777,217. Solution to Problem 2.50 (page 157) Performing rounding by hand helps reinforce the idea of round-to-even with binary numbers. Original Rounded 10.1112 2 7 8 11.03 11.0102 3 1 4 11.03 11.0002 311.03 10.1102 2 3 4 11.03 Solution to Problem 2.51 (page 158) A. Looking at the nonterminating sequence for 1 10 , we see that the 2 bits to the right of the rounding position are 1, so a better approximation to 1 10 would be obtained by incrementing x to get x′ = 0.000110011001100110011012, which is larger than 0.1. B. We can see that x′ − 0.1 has binary representation 0.0000000000000000000000000[1100] 196 Chapter 2 Representing and Manipulating Information Comparing this to the binary representation of 1 10 , we can see that it is 2−22 × 1 10 , which is around 2.38 × 10−8. C. 2.38 × 10−8 × 100 × 60 × 60 × 10 ≈ 0.086 seconds, a factor of 4 less than the error in the Patriot system. D. 0.086 × 2,000 ≈ 171 meters. Solution to Problem 2.52 (page 158) This problem tests a lot of concepts about ﬂoating-point representations, including the encoding of normalized and denormalized values, as well as rounding. Format A Format B Bits Value Bits Value Comments noo nnnn 1 nooo nnn 1 ono ooon 15 2 onno ooo 15 2 non onno 25 32 noon onn 3 4 Round down oon oooo 31 2 onoo nnn 16 Round up nnn nnno 1 64 nnno nnn 1 64 Denorm → norm Solution to Problem 2.53 (page 161) In general, it is better to use a library macro rather than inventing your own code. This code seems to work on a variety of machines, however. We assume that the value o−rnn overﬂows to inﬁnity. a®−ð©Å− –ﬂ·ˆ'ﬁƒ'ﬁ'¶» o−rnn a®−ð©Å− ﬁ¥§ˆ'ﬁƒ'ﬁ'¶» fk–ﬂ·ˆ'ﬁƒ'ﬁ'¶»g a®−ð©Å− ﬁ¥§ˆ…¥‡ﬂ fkolnm–ﬂ·ˆ'ﬁƒ'ﬁ'¶»g Solution to Problem 2.54 (page 161) Exercises such as this one help you develop your ability to reason about ﬂoating- point operations from a programmer’s perspective. Make sure you understand each of the answers. A. Ó {{ f©ÅÎgf®ÇÏ¾Ä−g Ó Yes, since ®ÇÏ¾Ä− has greater precision and range than ©ÅÎ. B. Ó {{ f©ÅÎgfðÄÇþÎg Ó No. For example, when Ó is TMax. C. ® {{ f®ÇÏ¾Ä−gfðÄÇþÎg ® No. For example, when ® is o−rn, we will get +∞ on the right. D. ð {{ fðÄÇþÎgf®ÇÏ¾Ä−g ð Yes, since ®ÇÏ¾Ä− has greater precision and range than ðÄÇþÎ. E. ð {{ kfkðg Yes, since a ﬂoating-point number is negated by simply inverting its sign bit. Solutions to Practice Problems 197 F. olnmp {{ ompln Yes, the numerators and denominators will both be converted to ﬂoating- point representations before the division is performed. G. ®h® |{ nln Yes, although it may overﬂow to +∞. H. fði®gkð {{ ® No. For example, when ð is oln−pn and ® is 1.0, the expression ði® will be rounded to oln−pn, and so the expression on the left-hand side will evaluate to 0.0, while the right-hand side will be 1.0. This page is intentionally left blank. CHAPTER 3 Machine-Level Representation of Programs 3.1 A Historical Perspective 202 3.2 Program Encodings 205 3.3 Data Formats 213 3.4 Accessing Information 215 3.5 Arithmetic and Logical Operations 227 3.6 Control 236 3.7 Procedures 274 3.8 Array Allocation and Access 291 3.9 Heterogeneous Data Structures 301 3.10 Combining Control and Data in Machine-Level Programs 312 3.11 Floating-Point Code 329 3.12 Summary 345 Bibliographic Notes 346 Homework Problems 347 Solutions to Practice Problems 361 199 200 Chapter 3 Machine-Level Representation of Programs Computers execute machine code, sequences of bytes encoding the low-level operations that manipulate data, manage memory, read and write data on storage devices, and communicate over networks. A compiler generates machine code through a series of stages, based on the rules of the programming language, the instruction set of the target machine, and the conventions followed by the op- erating system. The gcc C compiler generates its output in the form of assembly code, a textual representation of the machine code giving the individual instruc- tions in the program. Gcc then invokes both an assembler and a linker to generate the executable machine code from the assembly code. In this chapter, we will take a close look at machine code and its human-readable representation as assem- bly code. When programming in a high-level language such as C, and even more so in Java, we are shielded from the detailed machine-level implementation of our program. In contrast, when writing programs in assembly code (as was done in the early days of computing) a programmer must specify the low-level instructions the program uses to carry out a computation. Most of the time, it is much more productive and reliable to work at the higher level of abstraction provided by a high-level language. The type checking provided by a compiler helps detect many program errors and makes sure we reference and manipulate data in consistent ways. With modern optimizing compilers, the generated code is usually at least as efﬁcient as what a skilled assembly-language programmer would write by hand. Best of all, a program written in a high-level language can be compiled and executed on a number of different machines, whereas assembly code is highly machine speciﬁc. So why should we spend our time learning machine code? Even though com- pilers do most of the work in generating assembly code, being able to read and understand it is an important skill for serious programmers. By invoking the com- piler with appropriate command-line parameters, the compiler will generate a ﬁle showing its output in assembly-code form. By reading this code, we can under- stand the optimization capabilities of the compiler and analyze the underlying inefﬁciencies in the code. As we will experience in Chapter 5, programmers seek- ing to maximize the performance of a critical section of code often try different variations of the source code, each time compiling and examining the generated assembly code to get a sense of how efﬁciently the program will run. Furthermore, there are times when the layer of abstraction provided by a high-level language hides information about the run-time behavior of a program that we need to under- stand. For example, when writing concurrent programs using a thread package, as covered in Chapter 12, it is important to understand how program data are shared or kept private by the different threads and precisely how and where shared data are accessed. Such information is visible at the machine-code level. As another example, many of the ways programs can be attacked, allowing malware to in- fest a system, involve nuances of the way programs store their run-time control information. Many attacks involve exploiting weaknesses in system programs to overwrite information and thereby take control of the system. Understanding how these vulnerabilities arise and how to guard against them requires a knowledge of the machine-level representation of programs. The need for programmers to learn Chapter 3 Machine-Level Representation of Programs 201 machine code has shifted over the years from one of being able to write programs directly in assembly code to one of being able to read and understand the code generated by compilers. In this chapter, we will learn the details of one particular assembly language and see how C programs get compiled into this form of machine code. Reading the assembly code generated by a compiler involves a different set of skills than writing assembly code by hand. We must understand the transformations typical compilers make in converting the constructs of C into machine code. Relative to the computations expressed in the C code, optimizing compilers can rearrange execution order, eliminate unneeded computations, replace slow operations with faster ones, and even change recursive computations into iterative ones. Under- standing the relation between source code and the generated assembly can often be a challenge—it’s much like putting together a puzzle having a slightly differ- ent design than the picture on the box. It is a form of reverse engineering—trying to understand the process by which a system was created by studying the system and working backward. In this case, the system is a machine-generated assembly- language program, rather than something designed by a human. This simpliﬁes the task of reverse engineering because the generated code follows fairly regu- lar patterns and we can run experiments, having the compiler generate code for many different programs. In our presentation, we give many examples and pro- vide a number of exercises illustrating different aspects of assembly language and compilers. This is a subject where mastering the details is a prerequisite to under- standing the deeper and more fundamental concepts. Those who say “I understand the general principles, I don’t want to bother learning the details” are deluding themselves. It is critical for you to spend time studying the examples, working through the exercises, and checking your solutions with those provided. Our presentation is based on x86-64, the machine language for most of the processors found in today’s laptop and desktop machines, as well as those that power very large data centers and supercomputers. This language has evolved over a long history, starting with Intel Corporation’s ﬁrst 16-bit processor in 1978, through to the expansion to 32 bits, and most recently to 64 bits. Along the way, features have been added to make better use of the available semiconductor tech- nology, and to satisfy the demands of the marketplace. Much of the development has been driven by Intel, but its rival Advanced Micro Devices (AMD) has also made important contributions. The result is a rather peculiar design with features that make sense only when viewed from a historical perspective. It is also laden with features providing backward compatibility that are not used by modern com- pilers and operating systems. We will focus on the subset of the features used by gcc and Linux. This allows us to avoid much of the complexity and many of the arcane features of x86-64. Our technical presentation starts with a quick tour to show the relation be- tween C, assembly code, and machine code. We then proceed to the details of x86-64, starting with the representation and manipulation of data and the imple- mentation of control. We see how control constructs in C, such as ©ð, Ñ³©Ä−, and ÍÑ©Î²³ statements, are implemented. We then cover the implementation of pro- cedures, including how the program maintains a run-time stack to support the 202 Chapter 3 Machine-Level Representation of Programs Web Aside ASM:IA32 IA32 programming IA32, the 32-bit predecessor to x86-64, was introduced by Intel in 1985. It served as the machine language of choice for several decades. Most x86 microprocessors sold today, and most operating systems installed on these machines, are designed to run x86-64. However, they can also execute IA32 programs in a backward compatibility mode. As a result, many application programs are still based on IA32. In addition, many existing systems cannot execute x86-64, due to limitations of their hardware or system software. IA32 continues to be an important machine language. You will ﬁnd that having a background in x86-64 will enable you to learn the IA32 machine language quite readily. passing of data and control between procedures, as well as storage for local vari- ables. Next, we consider how data structures such as arrays, structures, and unions are implemented at the machine level. With this background in machine-level pro- gramming, we can examine the problems of out-of-bounds memory references and the vulnerability of systems to buffer overﬂow attacks. We ﬁnish this part of the presentation with some tips on using the gdb debugger for examining the run-time behavior of a machine-level program. The chapter concludes with a presentation on machine-program representations of code involving ﬂoating-point data and operations. The computer industry has recently made the transition from 32-bit to 64- bit machines. A 32-bit machine can only make use of around 4 gigabytes (232 bytes) of random access memory, With memory prices dropping at dramatic rates, and our computational demands and data sizes increasing, it has become both economically feasible and technically desirable to go beyond this limitation. Current 64-bit machines can use up to 256 terabytes (248 bytes) of memory, and could readily be extended to use up to 16 exabytes (264 bytes). Although it is hard to imagine having a machine with that much memory, keep in mind that 4 gigabytes seemed like an extreme amount of memory when 32-bit machines became commonplace in the 1970s and 1980s. Our presentation focuses on the types of machine-level programs generated when compiling C and similar programming languages targeting modern oper- ating systems. As a consequence, we make no attempt to describe many of the features of x86-64 that arise out of its legacy support for the styles of programs written in the early days of microprocessors, when much of the code was writ- ten manually and where programmers had to struggle with the limited range of addresses allowed by 16-bit machines. 3.1 A Historical Perspective The Intel processor line, colloquially referred to as x86, has followed a long evo- lutionary development. It started with one of the ﬁrst single-chip 16-bit micropro- cessors, where many compromises had to be made due to the limited capabilities of integrated circuit technology at the time. Since then, it has grown to take ad- Section 3.1 A Historical Perspective 203 vantage of technology improvements as well as to satisfy the demands for higher performance and for supporting more advanced operating systems. The list that follows shows some models of Intel processors and some of their key features, especially those affecting machine-level programming. We use the number of transistors required to implement the processors as an indication of how they have evolved in complexity. In this table, “K” denotes 1,000 (103), “M” denotes 1,000,000 (106), and “G” denotes 1,000,000,000 (109). 8086 (1978, 29 K transistors). One of the ﬁrst single-chip, 16-bit microproces- sors. The 8088, a variant of the 8086 with an 8-bit external bus, formed the heart of the original IBM personal computers. IBM contracted with then-tiny Microsoft to develop the MS-DOS operating system. The orig- inal models came with 32,768 bytes of memory and two ﬂoppy drives (no hard drive). Architecturally, the machines were limited to a 655,360-byte address space—addresses were only 20 bits long (1,048,576 bytes address- able), and the operating system reserved 393,216 bytes for its own use. In 1980, Intel introduced the 8087 ﬂoating-point coprocessor (45 K tran- sistors) to operate alongside an 8086 or 8088 processor, executing the ﬂoating-point instructions. The 8087 established the ﬂoating-point model for the x86 line, often referred to as “x87.” 80286 (1982, 134 K transistors). Added more (and now obsolete) addressing modes. Formed the basis of the IBM PC-AT personal computer, the original platform for MS Windows. i386 (1985, 275 K transistors). Expanded the architecture to 32 bits. Added the ﬂat addressing model used by Linux and recent versions of the Windows operating system. This was the ﬁrst machine in the series that could fully support a Unix operating system. i486 (1989, 1.2 M transistors). Improved performance and integrated the ﬂoat- ing-point unit onto the processor chip but did not signiﬁcantly change the instruction set. Pentium (1993, 3.1 M transistors). Improved performance but only added mi- nor extensions to the instruction set. PentiumPro (1995, 5.5 M transistors). Introduced a radically new processor design, internally known as the P6 microarchitecture. Added a class of “conditional move” instructions to the instruction set. Pentium/MMX (1997, 4.5 M transistors). Added new class of instructions to the Pentium processor for manipulating vectors of integers. Each datum can be 1, 2, or 4 bytes long. Each vector totals 64 bits. Pentium II (1997, 7 M transistors). Continuation of the P6 microarchitecture. Pentium III (1999, 8.2 M transistors). Introduced SSE, a class of instructions for manipulating vectors of integer or ﬂoating-point data. Each datum can be 1, 2, or 4 bytes, packed into vectors of 128 bits. Later versions of this chip 204 Chapter 3 Machine-Level Representation of Programs went up to 24 M transistors, due to the incorporation of the level-2 cache on chip. Pentium 4 (2000, 42 M transistors). Extended SSE to SSE2, adding new data types (including double-precision ﬂoating point), along with 144 new in- structions for these formats. With these extensions, compilers can use SSE instructions, rather than x87 instructions, to compile ﬂoating-point code. Pentium 4E (2004, 125 M transistors). Added hyperthreading, a method to run two programs simultaneously on a single processor, as well as EM64T, Intel’s implementation of a 64-bit extension to IA32 developed by Ad- vanced Micro Devices (AMD), which we refer to as x86-64. Core 2 (2006, 291 M transistors). Returned to a microarchitecture similar to P6. First multi-core Intel microprocessor, where multiple processors are implemented on a single chip. Did not support hyperthreading. Core i7, Nehalem (2008, 781 M transistors). Incorporated both hyperthreading and multi-core, with the initial version supporting two executing pro- grams on each core and up to four cores on each chip. Core i7, Sandy Bridge (2011, 1.17 G transistors). Introduced AVX, an exten- sion of the SSE to support data packed into 256-bit vectors. Core i7, Haswell (2013, 1.4 G transistors). Extended AVX to AVX2, adding more instructions and instruction formats. Each successive processor has been designed to be backward compatible— able to run code compiled for any earlier version. As we will see, there are many strange artifacts in the instruction set due to this evolutionary heritage. Intel has had several names for their processor line, including IA32, for “Intel Architecture 32-bit” and most recently Intel64, the 64-bit extension to IA32, which we will refer to as x86-64. We will refer to the overall line by the commonly used colloquial name “x86,” reﬂecting the processor naming conventions up through the i486. Over the years, several companies have produced processors that are com- patible with Intel processors, capable of running the exact same machine-level programs. Chief among these is Advanced Micro Devices (AMD). For years, AMD lagged just behind Intel in technology, forcing a marketing strategy where they produced processors that were less expensive although somewhat lower in performance. They became more competitive around 2002, being the ﬁrst to break the 1-gigahertz clock-speed barrier for a commercially available microprocessor, and introducing x86-64, the widely adopted 64-bit extension to Intel’s IA32. Al- though we will talk about Intel processors, our presentation holds just as well for the compatible processors produced by Intel’s rivals. Much of the complexity of x86 is not of concern to those interested in programs for the Linux operating system as generated by the gcc compiler. The memory model provided in the original 8086 and its extensions in the 80286 became ob- solete with the i386. The original x87 ﬂoating-point instructions became obsolete Section 3.2 Program Encodings 205 Aside Moore’s Law Intel microprocessor complexity 1.0E+10 1.0E+09 1.0E+08 1.0E+07 1.0E+06 1.0E+05 1.0E+04 1975 1980 8086 80286 i386 i486 Pentium Pentium 4 Pentium 4e Core 2 Duo Sandybridge Nehalem Haswell Pentium II Pentium III 1985 1990 1995 2000 2005 2010 2015 YearTransistors PentiumPro If we plot the number of transistors in the different Intel processors versus the year of introduction, and use a logarithmic scale for the y-axis, we can see that the growth has been phenomenal. Fitting a line through the data, we see that the number of transistors increases at an annual rate of approximately 37%, meaning that the number of transistors doubles about every 26 months. This growth has been sustained over the multiple-decade history of x86 microprocessors. In 1965, Gordon Moore, a founder of Intel Corporation, extrapolated from the chip technology of the day (by which they could fabricate circuits with around 64 transistors on a single chip) to predict that the number of transistors per chip would double every year for the next 10 years. This prediction became known as Moore’s Law. As it turns out, his prediction was just a little bit optimistic, but also too short-sighted. Over more than 50 years, the semiconductor industry has been able to double transistor counts on average every 18 months. Similar exponential growth rates have occurred for other aspects of computer technology, including the storage capacities of magnetic disks and semiconductor memories. These remarkable growth rates have been the major driving forces of the computer revolution. with the introduction of SSE2. Although we see vestiges of the historical evolu- tion of x86 in x86-64 programs, many of the most arcane features of x86 do not appear. 3.2 Program Encodings Suppose we write a C program as two ﬁles Éol² and Épl². We can then compile this code using a Unix command line: 206 Chapter 3 Machine-Level Representation of Programs Ä©ÅÏÓ| gcc -Og -o p p1.c p2.c The command ×²² indicates the gcc C compiler. Since this is the default compiler on Linux, we could also invoke it as simply ²². The command-line option kﬂ×1 instructs the compiler to apply a level of optimization that yields machine code that follows the overall structure of the original C code. Invoking higher levels of optimization can generate code that is so heavily transformed that the relationship between the generated machine code and the original source code is difﬁcult to understand. We will therefore use kﬂ× optimization as a learning tool and then see what happens as we increase the level of optimization. In practice, higher levels of optimization (e.g., speciﬁed with the option kﬂo or kﬂp) are considered a better choice in terms of the resulting program performance. The ×²² command invokes an entire sequence of programs to turn the source code into executable code. First, the C preprocessor expands the source code to include any ﬁles speciﬁed with a©Å²ÄÏ®− commands and to expand any macros, speciﬁed with a®−ð©Å− declarations. Second, the compiler generates assembly- code versions of the two source ﬁles having names ÉolÍ and ÉplÍ. Next, the assembler converts the assembly code into binary object-code ﬁles ÉolÇ and ÉplÇ. Object code is one form of machine code—it contains binary representations of all of the instructions, but the addresses of global values are not yet ﬁlled in. Finally, the linker merges these two object-code ﬁles along with code implementing library functions (e.g., ÉË©ÅÎð) and generates the ﬁnal executable code ﬁle É (as speciﬁed by the command-line directive kÇ É). Executable code is the second form of machine code we will consider—it is the exact form of code that is executed by the processor. The relation between these different forms of machine code and the linking process is described in more detail in Chapter 7. 3.2.1 Machine-Level Code As described in Section 1.9.3, computer systems employ several different forms of abstraction, hiding details of an implementation through the use of a simpler abstract model. Two of these are especially important for machine-level program- ming. First, the format and behavior of a machine-level program is deﬁned by the instruction set architecture, or ISA, deﬁning the processor state, the format of the instructions, and the effect each of these instructions will have on the state. Most ISAs, including x86-64, describe the behavior of a program as if each instruction is executed in sequence, with one instruction completing before the next one begins. The processor hardware is far more elaborate, executing many instructions con- currently, but it employs safeguards to ensure that the overall behavior matches the sequential operation dictated by the ISA. Second, the memory addresses used by a machine-level program are virtual addresses, providing a memory model that 1. This optimization level was introduced in gcc version 4.8. Earlier versions of gcc, as well as non- GNU compilers, will not recognize this option. For these, using optimization level one (speciﬁed with the command-line ﬂag kﬂo) is probably the best choice for generating code that follows the original program structure. Section 3.2 Program Encodings 207 appears to be a very large byte array. The actual implementation of the mem- ory system involves a combination of multiple hardware memories and operating system software, as described in Chapter 9. The compiler does most of the work in the overall compilation sequence, transforming programs expressed in the relatively abstract execution model pro- vided by C into the very elementary instructions that the processor executes. The assembly-code representation is very close to machine code. Its main feature is that it is in a more readable textual format, as compared to the binary format of machine code. Being able to understand assembly code and how it relates to the original C code is a key step in understanding how computers execute programs. The machine code for x86-64 differs greatly from the original C code. Parts of the processor state are visible that normally are hidden from the C programmer: . The program counter (commonly referred to as the PC, and called cË©É in x86- 64) indicates the address in memory of the next instruction to be executed. . The integer register ﬁle contains 16 named locations storing 64-bit values. These registers can hold addresses (corresponding to C pointers) or integer data. Some registers are used to keep track of critical parts of the program state, while others are used to hold temporary data, such as the arguments and local variables of a procedure, as well as the value to be returned by a function. . The condition code registers hold status information about the most recently executed arithmetic or logical instruction. These are used to implement con- ditional changes in the control or data ﬂow, such as is required to implement ©ð and Ñ³©Ä− statements. . A set of vector registers can each hold one or more integer or ﬂoating-point values. Whereas C provides a model in which objects of different data types can be declared and allocated in memory, machine code views the memory as simply a large byte-addressable array. Aggregate data types in C such as arrays and structures are represented in machine code as contiguous collections of bytes. Even for scalar data types, assembly code makes no distinctions between signed or unsigned integers, between different types of pointers, or even between pointers and integers. The program memory contains the executable machine code for the program, some information required by the operating system, a run-time stack for managing procedure calls and returns, and blocks of memory allocated by the user (e.g., by using the ÀþÄÄÇ² library function). As mentioned earlier, the program memory is addressed using virtual addresses. At any given time, only limited subranges of virtual addresses are considered valid. For example, x86-64 virtual addresses are represented by 64-bit words. In current implementations of these machines, the upper 16 bits must be set to zero, and so an address can potentially specify a byte over a range of 248, or 64 terabytes. More typical programs will only have access to a few megabytes, or perhaps several gigabytes. The operating system manages 208 Chapter 3 Machine-Level Representation of Programs Aside The ever-changing forms of generated code In our presentation, we will show the code generated by a particular version of gcc with particular settings of the command-line options. If you compile code on your own machine, chances are you will be using a different compiler or a different version of gcc and hence will generate different code. The open- source community supporting gcc keeps changing the code generator, attempting to generate more efﬁcient code according to changing code guidelines provided by the microprocessor manufacturers. Our goal in studying the examples shown in our presentation is to demonstrate how to examine assembly code and map it back to the constructs found in high-level programming languages. You will need to adapt these techniques to the style of code generated by your particular compiler. this virtual address space, translating virtual addresses into the physical addresses of values in the actual processor memory. A single machine instruction performs only a very elementary operation. For example, it might add two numbers stored in registers, transfer data between memory and a register, or conditionally branch to a new instruction address. The compiler must generate sequences of such instructions to implement program constructs such as arithmetic expression evaluation, loops, or procedure calls and returns. 3.2.2 Code Examples Suppose we write a C code ﬁle ÀÍÎÇË−l² containing the following function deﬁ- nition: ÄÇÅ× ÀÏÄÎpfÄÇÅ×j ÄÇÅ×gy ÌÇ©® ÀÏÄÎÍÎÇË−fÄÇÅ× Ój ÄÇÅ× Ôj ÄÇÅ× h®−ÍÎg Õ ÄÇÅ× Î { ÀÏÄÎpfÓj Ôgy h®−ÍÎ { Îy Û To see the assembly code generated by the C compiler, we can use the k· option on the command line: Ä©ÅÏÓ| gcc -Og -S mstore.c This will cause gcc to run the compiler, generating an assembly ﬁle ÀÍÎÇË−lÍ, and go no further. (Normally it would then invoke the assembler to generate an object-code ﬁle.) The assembly-code ﬁle contains various declarations, including the following set of lines: ÀÏÄÎÍÎÇË−x ÉÏÍ³Ê cË¾Ó Section 3.2 Program Encodings 209 Aside How do I display the byte representation of a program? To display the binary object code for a program (say, ÀÍÎÇË−), we use a disassembler (described below) to determine that the code for the procedure is 14 bytes long. Then we run the GNU debugging tool gdb on ﬁle ÀÍÎÇË−lÇ and give it the command f×®¾g x/14xb multstore telling it to display (abbreviated ‘Ó’) 14 hex-formatted (also ‘Ó’) bytes (‘¾’) starting at the address where function ÀÏÄÎÍÎÇË− is located. You will ﬁnd that gdb has many useful features for analyzing machine- level programs, as will be discussed in Section 3.10.2. ÀÇÌÊ cË®Ój cË¾Ó ²þÄÄ ÀÏÄÎp ÀÇÌÊ cËþÓj fcË¾Óg ÉÇÉÊ cË¾Ó Ë−Î Each indented line in the code corresponds to a single machine instruction. For example, the ÉÏÍ³Ê instruction indicates that the contents of register cË¾Ó should be pushed onto the program stack. All information about local variable names or data types has been stripped away. If we use the k² command-line option, gcc will both compile and assemble the code Ä©ÅÏÓ| gcc -Og -c mstore.c This will generate an object-code ﬁle ÀÍÎÇË−lÇ that is in binary format and hence cannot be viewed directly. Embedded within the 1,368 bytes of the ﬁle ÀÍÎÇË−lÇ is a 14-byte sequence with the hexadecimal representation sq rv vw ®q −v nn nn nn nn rv vw nq s¾ ²q This is the object code corresponding to the assembly instructions listed previously. A key lesson to learn from this is that the program executed by the machine is simply a sequence of bytes encoding a series of instructions. The machine has very little information about the source code from which these instructions were generated. To inspect the contents of machine-code ﬁles, a class of programs known as disassemblers can be invaluable. These programs generate a format similar to assembly code from the machine code. With Linux systems, the program objdump (for “object dump”) can serve this role given the k® command-line ﬂag: Ä©ÅÏÓ| objdump -d mstore.o The result (where we have added line numbers on the left and annotations in italicized text) is as follows: 210 Chapter 3 Machine-Level Representation of Programs Disassembly of function ÍÏÀ in binary file ÀÍÎÇË−lÇ 1 nnnnnnnnnnnnnnnn zÀÏÄÎÍÎÇË−|x Offset Bytes Equivalent assembly language 2 nx sq ÉÏÍ³ cË¾Ó 3 ox rv vw ®q ÀÇÌ cË®ÓjcË¾Ó 4 rx −v nn nn nn nn ²þÄÄÊ w zÀÏÄÎÍÎÇË−inÓw| 5 wx rv vw nq ÀÇÌ cËþÓjfcË¾Óg 6 ²x s¾ ÉÇÉ cË¾Ó 7 ®x ²q Ë−ÎÊ On the left we see the 14 hexadecimal byte values, listed in the byte sequence shown earlier, partitioned into groups of 1 to 5 bytes each. Each of these groups is a single instruction, with the assembly-language equivalent shown on the right. Several features about machine code and its disassembled representation are worth noting: . x86-64 instructions can range in length from 1 to 15 bytes. The instruction encoding is designed so that commonly used instructions and those with fewer operands require a smaller number of bytes than do less common ones or ones with more operands. . The instruction format is designed in such a way that from a given starting position, there is a unique decoding of the bytes into machine instructions. For example, only the instruction ÉÏÍ³Ê cË¾Ó can start with byte value sq. . The disassembler determines the assembly code based purely on the byte sequences in the machine-code ﬁle. It does not require access to the source or assembly-code versions of the program. . The disassembler uses a slightly different naming convention for the instruc- tions than does the assembly code generated by gcc. In our example, it has omitted the sufﬁx ‘Ê’ from many of the instructions. These sufﬁxes are size designators and can be omitted in most cases. Conversely, the disassembler adds the sufﬁx ‘Ê’tothe ²þÄÄ and Ë−Î instructions. Again, these sufﬁxes can safely be omitted. Generating the actual executable code requires running a linker on the set of object-code ﬁles, one of which must contain a function Àþ©Å. Suppose in ﬁle Àþ©Ål² we had the following function: a©Å²ÄÏ®− zÍÎ®©Çl³| ÌÇ©® ÀÏÄÎÍÎÇË−fÄÇÅ×j ÄÇÅ×j ÄÇÅ× hgy ©ÅÎ Àþ©Åfg Õ ÄÇÅ× ®y ÀÏÄÎÍÎÇË−fpj qj d®gy ÉË©ÅÎðf‘p h q kk| cÄ®¿Å‘j ®gy Ë−ÎÏËÅ ny Û Section 3.2 Program Encodings 211 ÄÇÅ× ÀÏÄÎpfÄÇÅ× þj ÄÇÅ× ¾g Õ ÄÇÅ×Í{þh¾y Ë−ÎÏËÅ Íy Û Then we could generate an executable program ÉËÇ× as follows: Ä©ÅÏÓ| gcc -Og -o prog main.c mstore.c The ﬁle ÉËÇ× has grown to 8,655 bytes, since it contains not just the machine code for the procedures we provided but also code used to start and terminate the program as well as to interact with the operating system. We can disassemble the ﬁle ÉËÇ×: Ä©ÅÏÓ| objdump -d prog The disassembler will extract various code sequences, including the following: Disassembly of function ÍÏÀ in binary file ÉËÇ× 1 nnnnnnnnnnrnnsrn zÀÏÄÎÍÎÇË−|x 2 rnnsrnx sq ÉÏÍ³ cË¾Ó 3 rnnsrox rv vw ®q ÀÇÌ cË®ÓjcË¾Ó 4 rnnsrrx −v rp nn nn nn ²þÄÄÊ rnnsv¾ zÀÏÄÎp| 5 rnnsrwx rv vw nq ÀÇÌ cËþÓjfcË¾Óg 6 rnnsr²x s¾ ÉÇÉ cË¾Ó 7 rnnsr®x ²q Ë−ÎÊ 8 rnnsr−x wn ÅÇÉ 9 rnnsrðx wn ÅÇÉ This code is almost identical to that generated by the disassembly of ÀÍÎÇË−l². One important difference is that the addresses listed along the left are different— the linker has shifted the location of this code to a different range of addresses. A second difference is that the linker has ﬁlled in the address that the ²þÄÄÊ instruc- tion should use in calling the function ÀÏÄÎp (line 4 of the disassembly). One task for the linker is to match function calls with the locations of the executable code for those functions. A ﬁnal difference is that we see two additional lines of code (lines 8–9). These instructions will have no effect on the program, since they occur after the return instruction (line 7). They have been inserted to grow the code for the function to 16 bytes, enabling a better placement of the next block of code in terms of memory system performance. 3.2.3 Notes on Formatting The assembly code generated by gcc is difﬁcult for a human to read. On one hand, it contains information with which we need not be concerned, while on the other hand, it does not provide any description of the program or how it works. For example, suppose we give the command Ä©ÅÏÓ| gcc -Og -S mstore.c 212 Chapter 3 Machine-Level Representation of Programs to generate the ﬁle ÀÍÎÇË−lÍ. The full content of the ﬁle is as follows: lð©Ä− ‘nonkÀÍÎÇË−l²‘ lÎ−ÓÎ l×ÄÇ¾Ä ÀÏÄÎÍÎÇË− lÎÔÉ− ÀÏÄÎÍÎÇË−j ~ðÏÅ²Î©ÇÅ ÀÏÄÎÍÎÇË−x ÉÏÍ³Ê cË¾Ó ÀÇÌÊ cË®Ój cË¾Ó ²þÄÄ ÀÏÄÎp ÀÇÌÊ cËþÓj fcË¾Óg ÉÇÉÊ cË¾Ó Ë−Î lÍ©Ö− ÀÏÄÎÍÎÇË−j lkÀÏÄÎÍÎÇË− l©®−ÅÎ ‘§££x f•¾ÏÅÎÏ rlvlokpÏ¾ÏÅÎÏoÜoplnrg rlvlo‘ lÍ−²Î©ÇÅ lÅÇÎ−l§ﬁ•kÍÎþ²Âj‘‘j~ÉËÇ×¾©ÎÍ All of the lines beginning with ‘l’ are directives to guide the assembler and linker. We can generally ignore these. On the other hand, there are no explanatory remarks about what the instructions do or how they relate to the source code. To provide a clearer presentation of assembly code, we will show it in a form that omits most of the directives, while including line numbers and explanatory annotations. For our example, an annotated version would appear as follows: void multstore(long x, long y, long *dest) x in %rdi, y in %rsi, dest in %rdx 1 ÀÏÄÎÍÎÇË−x 2 ÉÏÍ³Ê cË¾Ó Save %rbx 3 ÀÇÌÊ cË®Ój cË¾Ó Copy dest to %rbx 4 ²þÄÄ ÀÏÄÎp Call mult2(x, y) 5 ÀÇÌÊ cËþÓj fcË¾Óg Store result at *dest 6 ÉÇÉÊ cË¾Ó Restore %rbx 7 Ë−Î Return We typically show only the lines of code relevant to the point being discussed. Each line is numbered on the left for reference and annotated on the right by a brief description of the effect of the instruction and how it relates to the computa- tions of the original C code. This is a stylized version of the way assembly-language programmers format their code. We also provide Web asides to cover material intended for dedicated machine- language enthusiasts. One Web aside describes IA32 machine code. Having a background in x86-64 makes learning IA32 fairly simple. Another Web aside gives a brief presentation of ways to incorporate assembly code into C programs. For some applications, the programmer must drop down to assembly code to access low-level features of the machine. One approach is to write entire functions in assembly code and combine them with C functions during the linking stage. A Section 3.3 Data Formats 213 Aside ATT versus Intel assembly-code formats In our presentation, we show assembly code in ATT format (named after AT&T, the company that operated Bell Laboratories for many years), the default format for gcc, objdump, and the other tools we will consider. Other programming tools, including those from Microsoft as well as the documentation from Intel, show assembly code in Intel format. The two formats differ in a number of ways. As an example, gcc can generate code in Intel format for the ÍÏÀ function using the following command line: Ä©ÅÏÓ| gcc -Og -S -masm=intel mstore.c This gives the following assembly code: ÀÏÄÎÍÎÇË−x ÉÏÍ³ Ë¾Ó ÀÇÌ Ë¾Ój Ë®Ó ²þÄÄ ÀÏÄÎp ÀÇÌ †„ﬂ‡⁄ –¶‡ ‰Ë¾Ó`j ËþÓ ÉÇÉ Ë¾Ó Ë−Î We see that the Intel and ATT formats differ in the following ways: . The Intel code omits the size designation sufﬁxes. We see instruction ÉÏÍ³ and ÀÇÌ instead of ÉÏÍ³Ê and ÀÇÌÊ. . The Intel code omits the ‘c’ character in front of register names, using Ë¾Ó instead of cË¾Ó. . The Intel code has a different way of describing locations in memory—for example, †„ﬂ‡⁄ –¶‡ ‰Ë¾Ó` rather than fcË¾Óg. . Instructions with multiple operands list them in the reverse order. This can be very confusing when switching between the two formats. Although we will not be using Intel format in our presentation, you will encounter it in documentation from Intel and Microsoft. second is to use gcc’s support for embedding assembly code directly within C programs. 3.3 Data Formats Due to its origins as a 16-bit architecture that expanded into a 32-bit one, Intel uses the term “word” to refer to a 16-bit data type. Based on this, they refer to 32- bit quantities as “double words,” and 64-bit quantities as “quad words.” Figure 3.1 shows the x86-64 representations used for the primitive data types of C. Standard ©ÅÎ values are stored as double words (32 bits). Pointers (shown here as ²³þË h) are stored as 8-byte quad words, as would be expected in a 64-bit machine. With x86-64, data type ÄÇÅ× is implemented with 64 bits, allowing a very wide range of values. Most of our code examples in this chapter use pointers and ÄÇÅ× data 214 Chapter 3 Machine-Level Representation of Programs Web Aside ASM:EASM Combining assembly code with C programs Although a C compiler does a good job of converting the computations expressed in a program into machine code, there are some features of a machine that cannot be accessed by a C program. For example, every time an x86-64 processor executes an arithmetic or logical operation, it sets a 1-bit condition code ﬂag, named –ƒ (for “parity ﬂag”), to 1 when the lower 8 bits in the resulting computation have an even number of ones and to 0 otherwise. Computing this information in C requires at least seven shifting, masking, and exclusive-or operations (see Problem 2.65). Even though the hardware performs this computation as part of every arithmetic or logical operation, there is no way for a C program to determine the value of the –ƒ condition code ﬂag. This task can readily be performed by incorporating a small number of assembly-code instructions into the program. There are two ways to incorporate assembly code into C programs. First, we can write an entire function as a separate assembly-code ﬁle and let the assembler and linker combine this with code we have written in C. Second, we can use the inline assembly feature of gcc, where brief sections of assembly code can be incorporated into a C program using the þÍÀ directive. This approach has the advantage that it minimizes the amount of machine-speciﬁc code. Of course, including assembly code in a C program makes the code speciﬁc to a particular class of machines (such as x86-64), and so it should only be used when the desired feature can only be accessed in this way. C declaration Intel data type Assembly-code sufﬁx Size (bytes) ²³þË Byte ¾ 1 Í³ÇËÎ Word Ñ 2 ©ÅÎ Double word Ä 4 ÄÇÅ× Quad word Ê 8 ²³þË h Quad word Ê 8 ðÄÇþÎ Single precision Í 4 ®ÇÏ¾Ä− Double precision Ä 8 Figure 3.1 Sizes of C data types in x86-64. With a 64-bit machine, pointers are 8 bytes long. types, and so they will operate on quad words. The x86-64 instruction set includes a full complement of instructions for bytes, words, and double words as well. Floating-point numbers come in two principal formats: single-precision (4- byte) values, corresponding to C data type ðÄÇþÎ, and double-precision (8-byte) values, corresponding to C data type ®ÇÏ¾Ä−. Microprocessors in the x86 family historically implemented all ﬂoating-point operations with a special 80-bit (10- byte) ﬂoating-point format (see Problem 2.86). This format can be speciﬁed in C programs using the declaration ÄÇÅ× ®ÇÏ¾Ä−. We recommend against using this format, however. It is not portable to other classes of machines, and it is typically Section 3.4 Accessing Information 215 not implemented with the same high-performance hardware as is the case for single- and double-precision arithmetic. As the table of Figure 3.1 indicates, most assembly-code instructions gener- ated by gcc have a single-character sufﬁx denoting the size of the operand. For example, the data movement instruction has four variants: ÀÇÌ¾ (move byte), ÀÇÌÑ (move word), ÀÇÌÄ (move double word), and ÀÇÌÊ (move quad word). The sufﬁx ‘Ä’ is used for double words, since 32-bit quantities are considered to be “long words.” The assembly code uses the sufﬁx ‘Ä’ to denote a 4-byte integer as well as an 8-byte double-precision ﬂoating-point number. This causes no ambigu- ity, since ﬂoating-point code involves an entirely different set of instructions and registers. 3.4 Accessing Information An x86-64 central processing unit (CPU) contains a set of 16 general-purpose registers storing 64-bit values. These registers are used to store integer data as well as pointers. Figure 3.2 diagrams the 16 registers. Their names all begin with cË, but otherwise follow multiple different naming conventions, owing to the historical evolution of the instruction set. The original 8086 had eight 16-bit registers, shown in Figure 3.2 as registers cþÓ through c¾É. Each had a speciﬁc purpose, and hence they were given names that reﬂected how they were to be used. With the extension to IA32, these registers were expanded to 32-bit registers, labeled c−þÓ through c−¾É. In the extension to x86-64, the original eight registers were expanded to 64 bits, labeled cËþÓ through cË¾É. In addition, eight new registers were added, and these were given labels according to a new naming convention: cËv through cËos. As the nested boxes in Figure 3.2 indicate, instructions can operate on data of different sizes stored in the low-order bytes of the 16 registers. Byte-level operations can access the least signiﬁcant byte, 16-bit operations can access the least signiﬁcant 2 bytes, 32-bit operations can access the least signiﬁcant 4 bytes, and 64-bit operations can access entire registers. In later sections, we will present a number of instructions for copying and generating 1-, 2-, 4-, and 8-byte values. When these instructions have registers as destinations, two conventions arise for what happens to the remaining bytes in the register for instructions that generate less than 8 bytes: Those that generate 1- or 2-byte quantities leave the remaining bytes unchanged. Those that generate 4- byte quantities set the upper 4 bytes of the register to zero. The latter convention was adopted as part of the expansion from IA32 to x86-64. As the annotations along the right-hand side of Figure 3.2 indicate, different registers serve different roles in typical programs. Most unique among them is the stack pointer, cËÍÉ, used to indicate the end position in the run-time stack. Some instructions speciﬁcally read and write this register. The other 15 registers have more ﬂexibility in their uses. A small number of instructions make speciﬁc use of certain registers. More importantly, a set of standard programming conventions governs how the registers are to be used for managing the stack, passing function 216 Chapter 3 Machine-Level Representation of Programs 3163 15 7 0 %eax %ax %al %ebx %bx %bl %ecx %cx %cl %edx %dx %esi %si %edi %di %ebp %bp %esp %rax %rbx %rcx %rdx %rsi %rdi %rbp %rsp %sp %dl %sil %dil %bpl %spl %r8d%r8 %r8w %r8b %r9 %r10 %r11 %r12 %r13 %r14 %r15 Return value Callee saved 4th argument 3rd argument 2nd argument 1st argument Callee saved Stack pointer 5th argument 6th argument Caller saved Caller saved Callee saved Callee saved Callee saved Callee saved %r9d %r9w %r9b %r10d %r10w %r10b %r11d %r11w %r11b %r12d %r12w %r12b %r13d %r13w %r13b %r14d %r14w %r14b %r15d %r15w %r15b Figure 3.2 Integer registers. The low-order portions of all 16 registers can be accessed as byte, word (16-bit), double word (32-bit), and quad word (64-bit) quantities. arguments, returning values from functions, and storing local and temporary data. We will cover these conventions in our presentation, especially in Section 3.7, where we describe the implementation of procedures. 3.4.1 Operand Speciﬁers Most instructions have one or more operands specifying the source values to use in performing an operation and the destination location into which to place the Section 3.4 Accessing Information 217 Type Form Operand value Name Immediate bImm Imm Immediate Register Ëa R[Ëa] Register Memory Imm M[Imm] Absolute Memory fËag M[R[Ëa]] Indirect Memory ImmfËbg M[Imm + R[Ëb]] Base + displacement Memory fËbjËig M[R[Ëb] + R[Ëi]] Indexed Memory ImmfËbjËig M[Imm + R[Ëb] + R[Ëi]] Indexed Memory fjËijsg M[R[Ëi] . s] Scaled indexed Memory ImmfjËijsg M[Imm + R[Ëi] . s] Scaled indexed Memory fËbjËijsg M[R[Ëb] + R[Ëi] . s] Scaled indexed Memory ImmfËbjËijsg M[Imm + R[Ëb] + R[Ëi] . s] Scaled indexed Figure 3.3 Operand forms. Operands can denote immediate (constant) values, register values, or values from memory. The scaling factor s must be either 1, 2, 4, or 8. result. x86-64 supports a number of operand forms (see Figure 3.3). Source values can be given as constants or read from registers or memory. Results can be stored in either registers or memory. Thus, the different operand possibilities can be classiﬁed into three types. The ﬁrst type, immediate, is for constant values. In ATT- format assembly code, these are written with a ‘b’ followed by an integer using standard C notation—for example, bksuu or bnÓoƒ. Different instructions allow different ranges of immediate values; the assembler will automatically select the most compact way of encoding a value. The second type, register, denotes the contents of a register, one of the sixteen 8-, 4-, 2-, or 1-byte low-order portions of the registers for operands having 64, 32, 16, or 8 bits, respectively. In Figure 3.3, we use the notation Ëa to denote an arbitrary register a and indicate its value with the reference R[Ëa], viewing the set of registers as an array R indexed by register identiﬁers. The third type of operand is a memory reference, in which we access some memory location according to a computed address, often called the effective ad- dress. Since we view the memory as a large array of bytes, we use the notation Mb[Addr] to denote a reference to the b-byte value stored in memory starting at address Addr. To simplify things, we will generally drop the subscript b. As Figure 3.3 shows, there are many different addressing modes allowing dif- ferent forms of memory references. The most general form is shown at the bottom of the table with syntax ImmfËbjËijsg. Such a reference has four components: an immediate offset Imm, a base register Ëb, an index register Ëi, and a scale factor s, where s must be 1, 2, 4, or 8. Both the base and index must be 64-bit registers. The effective address is computed as Imm + R[Ëb] + R[Ëi] . s. This general form is often seen when referencing elements of arrays. The other forms are simply spe- cial cases of this general form where some of the components are omitted. As we 218 Chapter 3 Machine-Level Representation of Programs will see, the more complex addressing modes are useful when referencing array and structure elements. Practice Problem 3.1 (solution page 361) Assume the following values are stored at the indicated memory addresses and registers: Address Value Register Value nÓonn nÓƒƒ cËþÓ nÓonn nÓonr nÓ¡¢ cË²Ó nÓo nÓonv nÓoq cË®Ó nÓq nÓon£ nÓoo Fill in the following table showing the values for the indicated operands: Operand Value cËþÓ nÓonr bnÓonv fcËþÓg rfcËþÓg wfcËþÓjcË®Óg ptnfcË²ÓjcË®Óg nÓƒ£fjcË²Ójrg fcËþÓjcË®Ójrg 3.4.2 Data Movement Instructions Among the most heavily used instructions are those that copy data from one lo- cation to another. The generality of the operand notation allows a simple data movement instruction to express a range of possibilities that in many machines would require a number of different instructions. We present a number of differ- ent data movement instructions, differing in their source and destination types, what conversions they perform, and other side effects they may have. In our pre- sentation, we group the many different instructions into instruction classes, where the instructions in a class perform the same operation but with different operand sizes. Figure 3.4 lists the simplest form of data movement instructions—mov class. These instructions copy data from a source location to a destination location, without any transformation. The class consists of four instructions: ÀÇÌ¾, ÀÇÌÑ, ÀÇÌÄ, and ÀÇÌÊ. All four of these instructions have similar effects; they differ primarily in that they operate on data of different sizes: 1, 2, 4, and 8 bytes, respectively. Section 3.4 Accessing Information 219 Instruction Effect Description mov S, DD ← S Move ÀÇÌ¾ Move byte ÀÇÌÑ Move word ÀÇÌÄ Move double word ÀÇÌÊ Move quad word ÀÇÌþ¾ÍÊ I , RR ← I Move absolute quad word Figure 3.4 Simple data movement instructions. The source operand designates a value that is immediate, stored in a register, or stored in memory. The destination operand designates a location that is either a register or a memory address. x86-64 imposes the restriction that a move instruc- tion cannot have both operands refer to memory locations. Copying a value from one memory location to another requires two instructions—the ﬁrst to load the source value into a register, and the second to write this register value to the des- tination. Referring to Figure 3.2, register operands for these instructions can be the labeled portions of any of the 16 registers, where the size of the register must match the size designated by the last character of the instruction (‘¾’, ‘Ñ’, ‘Ä’, or ‘Ê’). For most cases, the mov instructions will only update the speciﬁc register bytes or memory locations indicated by the destination operand. The only exception is that when ÀÇÌÄ has a register as the destination, it will also set the high-order 4 bytes of the register to 0. This exception arises from the convention, adopted in x86-64, that any instruction that generates a 32-bit value for a register also sets the high-order portion of the register to 0. The following mov instruction examples show the ﬁve possible combinations of source and destination types. Recall that the source operand comes ﬁrst and the destination second. 1 ÀÇÌÄ bnÓrnsnjc−þÓ Immediate--Register, 4 bytes 2 ÀÇÌÑ c¾ÉjcÍÉ Register--Register, 2 bytes 3 ÀÇÌ¾ fcË®©jcË²ÓgjcþÄ Memory--Register, 1 byte 4 ÀÇÌ¾ bkoujfc−ÍÉg Immediate--Memory, 1 byte 5 ÀÇÌÊ cËþÓjkopfcË¾Ég Register--Memory, 8 bytes A ﬁnal instruction documented in Figure 3.4 is for dealing with 64-bit imme- diate data. The regular ÀÇÌÊ instruction can only have immediate source operands that can be represented as 32-bit two’s-complement numbers. This value is then sign extended to produce the 64-bit value for the destination. The ÀÇÌþ¾ÍÊ in- struction can have an arbitrary 64-bit immediate value as its source operand and can only have a register as a destination. Figures 3.5 and 3.6 document two classes of data movement instructions for use when copying a smaller source value to a larger destination. All of these instructions copy data from a source, which can be either a register or stored 220 Chapter 3 Machine-Level Representation of Programs Aside Understanding how data movement changes a destination register As described, there are two different conventions regarding whether and how data movement instruc- tions modify the upper bytes of a destination register. This distinction is illustrated by the following code sequence: 1 ÀÇÌþ¾ÍÊ bnÓnnooppqqrrssttuuj cËþÓ %rax = 0011223344556677 2 ÀÇÌ¾ bkoj cþÄ %rax = 00112233445566FF 3 ÀÇÌÑ bkoj cþÓ %rax = 001122334455FFFF 4 ÀÇÌÄ bkoj c−þÓ %rax = 00000000FFFFFFFF 5 ÀÇÌÊ bkoj cËþÓ %rax = FFFFFFFFFFFFFFFF In the following discussion, we use hexadecimal notation. In the example, the instruction on line 1 initializes register cËþÓ to the pattern nnooppqqrrssttuu. The remaining instructions have immediate value −1 as their source values. Recall that the hexadecimal representation of −1 is of the form ƒƒ...ƒ, where the number of ƒ’s is twice the number of bytes in the representation. The ÀÇÌ¾ instruction (line 2) therefore sets the low-order byte of cËþÓ to ƒƒ, while the ÀÇÌÑ instruction (line 3) sets the low-order 2 bytes to ƒƒƒƒ, with the remaining bytes unchanged. The ÀÇÌÄ instruction (line 4) sets the low-order 4 bytes to ƒƒƒƒƒƒƒƒ, but it also sets the high-order 4 bytes to nnnnnnnn. Finally, the ÀÇÌÊ instruction (line 5) sets the complete register to ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ. Instruction Effect Description movz S, RR ← ZeroExtend(S) Move with zero extension ÀÇÌÖ¾Ñ Move zero-extended byte to word ÀÇÌÖ¾Ä Move zero-extended byte to double word ÀÇÌÖÑÄ Move zero-extended word to double word ÀÇÌÖ¾Ê Move zero-extended byte to quad word ÀÇÌÖÑÊ Move zero-extended word to quad word Figure 3.5 Zero-extending data movement instructions. These instructions have a register or memory location as the source and a register as the destination. in memory, to a register destination. Instructions in the movz class ﬁll out the remaining bytes of the destination with zeros, while those in the movs class ﬁll them out by sign extension, replicating copies of the most signiﬁcant bit of the source operand. Observe that each instruction name has size designators as its ﬁnal two characters—the ﬁrst specifying the source size, and the second specifying the destination size. As can be seen, there are three instructions in each of these classes, covering all cases of 1- and 2-byte source sizes and 2- and 4-byte destination sizes, considering only cases where the destination is larger than the source, of course. Section 3.4 Accessing Information 221 Instruction Effect Description movs S, RR ← SignExtend(S) Move with sign extension ÀÇÌÍ¾Ñ Move sign-extended byte to word ÀÇÌÍ¾Ä Move sign-extended byte to double word ÀÇÌÍÑÄ Move sign-extended word to double word ÀÇÌÍ¾Ê Move sign-extended byte to quad word ÀÇÌÍÑÊ Move sign-extended word to quad word ÀÇÌÍÄÊ Move sign-extended double word to quad word ²ÄÎÊ cËþÓ ← SignExtend(c−þÓ) Sign-extend c−þÓ to cËþÓ Figure 3.6 Sign-extending data movement instructions. The movs instructions have a register or memory location as the source and a register as the destination. The ²ÄÎÊ instruction is speciﬁc to registers c−þÓ and cËþÓ. Note the absence of an explicit instruction to zero-extend a 4-byte source value to an 8-byte destination in Figure 3.5. Such an instruction would logically be named ÀÇÌÖÄÊ, but this instruction does not exist. Instead, this type of data movement can be implemented using a ÀÇÌÄ instruction having a register as the destination. This technique takes advantage of the property that an instruction generating a 4-byte value with a register as the destination will ﬁll the upper 4 bytes with zeros. Otherwise, for 64-bit destinations, moving with sign extension is supported for all three source types, and moving with zero extension is supported for the two smaller source types. Figure 3.6 also documents the ²ÄÎÊ instruction. This instruction has no operands—it always uses register c−þÓ as its source and cËþÓ as the destination for the sign-extended result. It therefore has the exact same effect as the instruction ÀÇÌÍÄÊ c−þÓj cËþÓ, but it has a more compact encoding. Practice Problem 3.2 (solution page 361) For each of the following lines of assembly language, determine the appropriate instruction sufﬁx based on the operands. (For example, ÀÇÌ can be rewritten as ÀÇÌ¾, ÀÇÌÑ, ÀÇÌÄ,or ÀÇÌÊ.) ÀÇÌ c−þÓj fcËÍÉg ÀÇÌ fcËþÓgj c®Ó ÀÇÌ bnÓƒƒj c¾Ä ÀÇÌ fcËÍÉjcË®Ójrgj c®Ä ÀÇÌ fcË®Ógj cËþÓ ÀÇÌ c®Ój fcËþÓg 222 Chapter 3 Machine-Level Representation of Programs Aside Comparing byte movement instructions The following example illustrates how different data movement instructions either do or do not change the high-order bytes of the destination. Observe that the three byte-movement instructions ÀÇÌ¾, ÀÇÌÍ¾Ê, and ÀÇÌÖ¾Ê differ from each other in subtle ways. Here is an example: 1 ÀÇÌþ¾ÍÊ bnÓnnooppqqrrssttuuj cËþÓ %rax = 0011223344556677 2 ÀÇÌ¾ bnÓ¡¡j c®Ä %dl = AA 3 ÀÇÌ¾ c®ÄjcþÄ %rax = 00112233445566AA 4 ÀÇÌÍ¾Ê c®ÄjcËþÓ %rax = FFFFFFFFFFFFFFAA 5 ÀÇÌÖ¾Ê c®ÄjcËþÓ %rax = 00000000000000AA In the following discussion, we use hexadecimal notation for all of the values. The ﬁrst two lines of the code initialize registers cËþÓ and c®Ä to nnooppqqrrssttuu and ¡¡, respectively. The remaining instructions all copy the low-order byte of cË®Ó to the low-order byte of cËþÓ.The ÀÇÌ¾ instruction (line 3) does not change the other bytes. The ÀÇÌÍ¾Ê instruction (line 4) sets the other 7 bytes to either all ones or all zeros depending on the high-order bit of the source byte. Since hexadecimal ¡ represents binary value onon, sign extension causes the higher-order bytes to each be set to ƒƒ.The ÀÇÌÖ¾Ê instruction (line 5) always sets the other 7 bytes to zero. Practice Problem 3.3 (solution page 362) Each of the following lines of code generates an error message when we invoke the assembler. Explain what is wrong with each line. ÀÇÌ¾ bnÓƒj fc−¾Óg ÀÇÌÄ cËþÓj fcËÍÉg ÀÇÌÑ fcËþÓgjrfcËÍÉg ÀÇÌ¾ cþÄjcÍÄ ÀÇÌÊ cËþÓjbnÓopq ÀÇÌÄ c−þÓjcË®Ó ÀÇÌ¾ cÍ©j vfcË¾Ég 3.4.3 Data Movement Example As an example of code that uses data movement instructions, consider the data exchange routine shown in Figure 3.7, both as C code and as assembly code generated by gcc. As Figure 3.7(b) shows, function −Ó²³þÅ×− is implemented with just three instructions: two data movements (ÀÇÌÊ) plus an instruction to return back to the point from which the function was called (Ë−Î). We will cover the details of function call and return in Section 3.7. Until then, it sufﬁces to say that arguments are passed to functions in registers. Our annotated assembly code documents these. A function returns a value by storing it in register cËþÓ, or in one of the low-order portions of this register. Section 3.4 Accessing Information 223 (a) C code ÄÇÅ× −Ó²³þÅ×−fÄÇÅ× hÓÉj ÄÇÅ× Ôg Õ ÄÇÅ× Ó { hÓÉy hÓÉ{Ôy Ë−ÎÏËÅ Óy Û (b) Assembly code long exchange(long *xp, long y) xp in %rdi, y in %rsi 1 −Ó²³þÅ×−x 2 ÀÇÌÊ fcË®©gj cËþÓ Get x at xp. Set as return value. 3 ÀÇÌÊ cËÍ©j fcË®©g Store y at xp. 4 Ë−Î Return. Figure 3.7 C and assembly code for exchange routine. Registers cË®© and cËÍ© hold parameters ÓÉ and Ô, respectively. When the procedure begins execution, procedure parameters ÓÉ and Ô are stored in registers cË®© and cËÍ©, respectively. Instruction 2 then reads Ó from memory and stores the value in register cËþÓ, a direct implementation of the operation Ó { hÓÉ in the C program. Later, register cËþÓ will be used to return a value from the function, and so the return value will be Ó. Instruction 3 writes Ô to the memory location designated by ÓÉ in register cË®©, a direct implementation of the operation hÓÉ{Ô. This example illustrates how the mov instructions can be used to read from memory to a register (line 2), and to write from a register to memory (line 3). Two features about this assembly code are worth noting. First, we see that what we call “pointers” in C are simply addresses. Dereferencing a pointer involves copying that pointer into a register, and then using this register in a memory reference. Second, local variables such as Ó are often kept in registers rather than stored in memory locations. Register access is much faster than memory access. Practice Problem 3.4 (solution page 362) Assume variables ÍÉ and ®É are declared with types ÍË²ˆÎ hÍÉy ®−ÍÎˆÎ h®Éy where ÍË²ˆÎ and ®−ÍÎˆÎ are data types declared with ÎÔÉ−®−ð. We wish to use the appropriate pair of data movement instructions to implement the operation h®É { f®−ÍÎˆÎg hÍÉy 224 Chapter 3 Machine-Level Representation of Programs New to C? Some examples of pointers Function −Ó²³þÅ×− (Figure 3.7(a)) provides a good illustration of the use of pointers in C. Argument ÓÉ is a pointer to a long integer, while Ô is a long integer itself. The statement ÄÇÅ× Ó { hÓÉy indicates that we should read the value stored in the location designated by ÓÉ and store it as a local variable named Ó. This read operation is known as pointer dereferencing. The C operator ‘h’ performs pointer dereferencing. The statement hÓÉ{Ôy does the reverse—it writes the value of parameter Ô at the location designated by ÓÉ. This is also a form of pointer dereferencing (and hence the operator h), but it indicates a write operation since it is on the left-hand side of the assignment. The following is an example of −Ó²³þÅ×− in action: ÄÇÅ×þ{ry ÄÇÅ× ¾ { −Ó²³þÅ×−fdþj qgy ÉË©ÅÎðf‘þ { cÄ®j ¾ { cÄ®¿Ì−Ë¾~¿~Å‘j þj ¾gy This code will print þ{qj¾{r The C operator ‘d’ (called the “address of” operator) creates a pointer, in this case to the location holding local variable þ. Function −Ó²³þÅ×− overwrites the value stored in þ with 3 but returns the previous value, 4, as the function value. Observe how by passing a pointer to −Ó²³þÅ×−, it could modify data held at some remote location. Assume that the values of ÍÉ and ®É are stored in registers cË®© and cËÍ©, respectively. For each entry in the table, show the two instructions that implement the speciﬁed data movement. The ﬁrst instruction in the sequence should read from memory, do the appropriate conversion, and set the appropriate portion of register cËþÓ. The second instruction should then write the appropriate portion of cËþÓ to memory. In both cases, the portions may be cËþÓ, c−þÓ, cþÓ,or cþÄ, and they may differ from one another. Recall that when performing a cast that involves both a size change and a change of “signedness” in C, the operation should change the size ﬁrst (Section 2.2.6). ÍË²ˆÎ ®−ÍÎˆÎ Instruction ÄÇÅ× ÄÇÅ× ÀÇÌÊ fcË®©gj cËþÓ ÀÇÌÊ cËþÓj fcËÍ©g ²³þË ©ÅÎ Section 3.4 Accessing Information 225 ²³þË ÏÅÍ©×Å−® ÏÅÍ©×Å−® ²³þË ÄÇÅ× ©ÅÎ ²³þË ÏÅÍ©×Å−® ÏÅÍ©×Å−® ²³þË ²³þË Í³ÇËÎ Practice Problem 3.5 (solution page 363) You are given the following information. A function with prototype ÌÇ©® ®−²Ç®−ofÄÇÅ× hÓÉj ÄÇÅ× hÔÉj ÄÇÅ× hÖÉgy is compiled into assembly code, yielding the following: void decode1(long *xp, long *yp, long *zp) xp in %rdi, yp in %rsi, zp in %rdx ®−²Ç®−ox ÀÇÌÊ fcË®©gj cËv ÀÇÌÊ fcËÍ©gj cË²Ó ÀÇÌÊ fcË®Ógj cËþÓ ÀÇÌÊ cËvj fcËÍ©g ÀÇÌÊ cË²Ój fcË®Óg ÀÇÌÊ cËþÓj fcË®©g Ë−Î Parameters ÓÉ, ÔÉ, and ÖÉ are stored in registers cË®©, cËÍ©, and cË®Ó, respec- tively. Write C code for ®−²Ç®−o that will have an effect equivalent to the assembly code shown. 3.4.4 Pushing and Popping Stack Data The ﬁnal two data movement operations are used to push data onto and pop data from the program stack, as documented in Figure 3.8. As we will see, the stack plays a vital role in the handling of procedure calls. By way of background, a stack is a data structure where values can be added or deleted, but only according to a “last-in, ﬁrst-out” discipline. We add data to a stack via a push operation and remove it via a pop operation, with the property that the value popped will always be the value that was most recently pushed and is still on the stack. A stack can be implemented as an array, where we always insert and remove elements from one 226 Chapter 3 Machine-Level Representation of Programs Instruction Effect Description ÉÏÍ³Ê S R[cËÍÉ] ← R[cËÍÉ] − 8; Push quad word M[R[cËÍÉ]] ← S ÉÇÉÊ DD ← M[R[cËÍÉ]]; Pop quad word R[cËÍÉ] ← R[cËÍÉ] + 8 Figure 3.8 Push and pop instructions. %rax %rdx %rsp 0x108 0 0x123 0x108 %rax %rdx %rsp 0x108 0x100 0 0x123 0x100 %rax %rdx %rsp 0x123 0x123 pushq %rax popq %rdx 0x108 Initially Stack “bottom” Increasing address Stack “top” Stack “bottom” 0x123 0x123 Stack “top” Stack “top” 0x108 Stack “bottom” Figure 3.9 Illustration of stack operation. By convention, we draw stacks upside down, so that the “top” of the stack is shown at the bottom. With x86-64, stacks grow toward lower addresses, so pushing involves decrementing the stack pointer (register cËÍÉ) and storing to memory, while popping involves reading from memory and incrementing the stack pointer. end of the array. This end is called the top of the stack. With x86-64, the program stack is stored in some region of memory. As illustrated in Figure 3.9, the stack grows downward such that the top element of the stack has the lowest address of all stack elements. (By convention, we draw stacks upside down, with the stack “top” shown at the bottom of the ﬁgure.) The stack pointer cËÍÉ holds the address of the top stack element. The ÉÏÍ³Ê instruction provides the ability to push data onto the stack, while the ÉÇÉÊ instruction pops it. Each of these instructions takes a single operand—the data source for pushing and the data destination for popping. Pushing a quad word value onto the stack involves ﬁrst decrementing the stack pointer by 8 and then writing the value at the new top-of-stack address. Section 3.5 Arithmetic and Logical Operations 227 Therefore, the behavior of the instruction ÉÏÍ³Ê cË¾É is equivalent to that of the pair of instructions ÍÏ¾Ê bvjcËÍÉ Decrement stack pointer ÀÇÌÊ cË¾ÉjfcËÍÉg Store %rbp on stack except that the ÉÏÍ³Ê instruction is encoded in the machine code as a single byte, whereas the pair of instructions shown above requires a total of 8 bytes. The ﬁrst two columns in Figure 3.9 illustrate the effect of executing the instruction ÉÏÍ³Ê cËþÓ when cËÍÉ is nÓonv and cËþÓ is nÓopq. First cËÍÉ is decremented by 8, giving nÓonn, and then nÓopq is stored at memory address nÓonn. Popping a quad word involves reading from the top-of-stack location and then incrementing the stack pointer by 8. Therefore, the instruction ÉÇÉÊ cËþÓ is equivalent to the following pair of instructions: ÀÇÌÊ fcËÍÉgjcËþÓ Read %rax from stack þ®®Ê bvjcËÍÉ Increment stack pointer The third column of Figure 3.9 illustrates the effect of executing the instruction ÉÇÉÊ c−®Ó immediately after executing the ÉÏÍ³Ê. Value nÓopq is read from memory and written to register cË®Ó. Register cËÍÉ is incremented back to nÓonv. As shown in the ﬁgure, the value nÓopq remains at memory location nÓonr until it is overwritten (e.g., by another push operation). However, the stack top is always considered to be the address indicated by cËÍÉ. Since the stack is contained in the same memory as the program code and other forms of program data, programs can access arbitrary positions within the stack using the standard memory addressing methods. For example, assuming the topmost element of the stack is a quad word, the instruction ÀÇÌÊ vfcËÍÉgjcË®Ó will copy the second quad word from the stack to register cË®Ó. 3.5 Arithmetic and Logical Operations Figure 3.10 lists some of the x86-64 integer and logic operations. Most of the operations are given as instruction classes, as they can have different variants with different operand sizes. (Only Ä−þÊ has no other size variants.) For example, the instruction class add consists of four addition instructions: þ®®¾, þ®®Ñ, þ®®Ä, and þ®®Ê, adding bytes, words, double words, and quad words, respectively. Indeed, each of the instruction classes shown has instructions for operating on these four different sizes of data. The operations are divided into four groups: load effective address, unary, binary, and shifts. Binary operations have two operands, while unary operations have one operand. These operands are speciﬁed using the same notation as described in Section 3.4. 3.5.1 Load Effective Address The load effective address instruction Ä−þÊ is actually a variant of the ÀÇÌÊ in- struction. It has the form of an instruction that reads from memory to a register, 228 Chapter 3 Machine-Level Representation of Programs Instruction Effect Description Ä−þÊ S, DD ← &S Load effective address inc DD ← D+1 Increment dec DD ← D−1 Decrement neg DD ← kD Negate not DD ← ÜD Complement add S, DD ← D + S Add sub S, DD ← D − S Subtract imul S, DD ← D ∗ S Multiply xor S, DD ← D ´ S Exclusive-or or S, DD ← D | S Or and S, DD ← D & S And sal k, DD ← D<< k Left shift shl k, DD ← D<< k Left shift (same as sal) sar k, DD ← D>>A k Arithmetic right shift shr k, DD ← D>>L k Logical right shift Figure 3.10 Integer arithmetic operations. The load effective address (Ä−þÊ) instruction is commonly used to perform simple arithmetic. The remaining ones are more standard unary or binary operations. We use the notation >>A and >>L to denote arithmetic and logical right shift, respectively. Note the nonintuitive ordering of the operands with ATT-format assembly code. but it does not reference memory at all. Its ﬁrst operand appears to be a mem- ory reference, but instead of reading from the designated location, the instruction copies the effective address to the destination. We indicate this computation in Figure 3.10 using the C address operator &S. This instruction can be used to gener- ate pointers for later memory references. In addition, it can be used to compactly describe common arithmetic operations. For example, if register cË®Ó contains value x, then the instruction Ä−þÊ ufcË®ÓjcË®Ójrgj cËþÓ will set register cËþÓ to 5x + 7. Compilers often ﬁnd clever uses of Ä−þÊ that have nothing to do with effective address computations. The destination operand must be a register. Practice Problem 3.6 (solution page 363) Suppose register cË¾Ó holds value p and cË®Ó holds value q. Fill in the table below with formulas indicating the value that will be stored in register cËþÓ for each of the given assembly-code instructions: Instruction Result Ä−þÊ wfcË®Ógj cËþÓ Ä−þÊ fcË®ÓjcË¾Ógj cËþÓ Ä−þÊ fcË®ÓjcË¾Ójqgj cËþÓ Ä−þÊ pfcË¾ÓjcË¾Ójugj cËþÓ Section 3.5 Arithmetic and Logical Operations 229 Ä−þÊ nÓ¥fjcË®Ójqgj cËþÓ Ä−þÊ tfcË¾ÓjcË®Ójugj cËþÓ As an illustration of the use of Ä−þÊ in compiled code, consider the following C program: ÄÇÅ× Í²þÄ−fÄÇÅ× Ój ÄÇÅ× Ôj ÄÇÅ× Ög Õ ÄÇÅ×Î{ÓirhÔiophÖy Ë−ÎÏËÅ Îy Û When compiled, the arithmetic operations of the function are implemented by a sequence of three Ä−þÊ functions, as is documented by the comments on the right-hand side: long scale(long x, long y, long z) x in %rdi, y in %rsi, z in %rdx Í²þÄ−x Ä−þÊ fcË®©jcËÍ©jrgj cËþÓ x + 4*y Ä−þÊ fcË®ÓjcË®Ójpgj cË®Ó z + 2*z = 3*z Ä−þÊ fcËþÓjcË®Ójrgj cËþÓ (x+4*y) + 4*(3*z)=x+4*y+ 12*z Ë−Î The ability of the Ä−þÊ instruction to perform addition and limited forms of multiplication proves useful when compiling simple arithmetic expressions such as this example. Practice Problem 3.7 (solution page 364) Consider the following code, in which we have omitted the expression being computed: Í³ÇËÎ Í²þÄ−qfÍ³ÇËÎ Ój Í³ÇËÎ Ôj Í³ÇËÎ Ög Õ Í³ÇËÎ Î { y Ë−ÎÏËÅ Îy Û Compiling the actual function with gcc yields the following assembly code: short scale3(short x, short y, short z) x in %rdi, y in %rsi, z in %rdx Í²þÄ−qx Ä−þÊ fcËÍ©jcËÍ©jwgj cË¾Ó Ä−þÊ fcË¾ÓjcË®Ógj cË¾Ó Ä−þÊ fcË¾ÓjcË®©jcËÍ©gj cË¾Ó Ë−Î Fill in the missing expression in the C code. 230 Chapter 3 Machine-Level Representation of Programs 3.5.2 Unary and Binary Operations Operations in the second group are unary operations, with the single operand serving as both source and destination. This operand can be either a register or a memory location. For example, the instruction ©Å²Ê fcËÍÉg causes the 8-byte element on the top of the stack to be incremented. This syntax is reminiscent of the C increment (ii) and decrement (kk) operators. The third group consists of binary operations, where the second operand is used as both a source and a destination. This syntax is reminiscent of the C assignment operators, such as Ók{Ô. Observe, however, that the source operand is given ﬁrst and the destination second. This looks peculiar for noncommutative operations. For example, the instruction ÍÏ¾Ê cËþÓjcË®Ó decrements register cË®Ó by the value in cËþÓ. (It helps to read the instruction as “Subtract cËþÓ from cË®Ó.”) The ﬁrst operand can be either an immediate value, a register, or a memory location. The second can be either a register or a memory location. As with the mov instructions, the two operands cannot both be memory locations. Note that when the second operand is a memory location, the processor must read the value from memory, perform the operation, and then write the result back to memory. Practice Problem 3.8 (solution page 364) Assume the following values are stored at the indicated memory addresses and registers: Address Value Register Value nÓonn nÓƒƒ cËþÓ nÓonn nÓonv nÓ¡¢ cË²Ó nÓo nÓoon nÓoq cË®Ó nÓq nÓoov nÓoo Fill in the following table showing the effects of the following instructions, in terms of both the register or memory location that will be updated and the resulting value: Instruction Destination Value þ®®Ê cË²ÓjfcËþÓg ÍÏ¾Ê cË®ÓjvfcËþÓg ©ÀÏÄÊ botjfcËþÓjcË®Ójvg ©Å²Ê otfcËþÓg ®−²Ê cË²Ó ÍÏ¾Ê cË®ÓjcËþÓ 3.5.3 Shift Operations The ﬁnal group consists of shift operations, where the shift amount is given ﬁrst and the value to shift is given second. Both arithmetic and logical right shifts are Section 3.5 Arithmetic and Logical Operations 231 possible. The different shift instructions can specify the shift amount either as an immediate value or with the single-byte register c²Ä. (These instructions are unusual in only allowing this speciﬁc register as the operand.) In principle, having a 1-byte shift amount would make it possible to encode shift amounts ranging up to 28 − 1 = 255. With x86-64, a shift instruction operating on data values that are w bits long determines the shift amount from the low-order m bits of register c²Ä, where 2m = w. The higher-order bits are ignored. So, for example, when register c²Ä has hexadecimal value nÓƒƒ, then instruction ÍþÄ¾ would shift by 7, while ÍþÄÑ would shift by 15, ÍþÄÄ would shift by 31, and ÍþÄÊ would shift by 63. As Figure 3.10 indicates, there are two names for the left shift instruction: sal and shl. Both have the same effect, ﬁlling from the right with zeros. The right shift instructions differ in that sar performs an arithmetic shift (ﬁll with copies of the sign bit), whereas shr performs a logical shift (ﬁll with zeros). The destination operand of a shift operation can be either a register or a memory location. We denote the two different right shift operations in Figure 3.10 as >>A (arithmetic) and >>L (logical). Practice Problem 3.9 (solution page 364) Suppose we want to generate assembly code for the following C function: ÄÇÅ× Í³©ðÎˆÄ−ðÎrˆË©×³ÎÅfÄÇÅ× Ój ÄÇÅ× Åg Õ Ó zz{ ry Ó ||{ Åy Ë−ÎÏËÅ Óy Û The code that follows is a portion of the assembly code that performs the actual shifts and leaves the ﬁnal value in register cËþÓ. Two key instructions have been omitted. Parameters Ó and Å are stored in registers cË®© and cËÍ©, respectively. long shift_left4_rightn(long x, long n) x in %rdi, n in %rsi Í³©ðÎˆÄ−ðÎrˆË©×³ÎÅx ÀÇÌÊ cË®©j cËþÓ Get x x <<= 4 ÀÇÌÄ c−Í©j c−²Ó Get n (4 bytes) x >>= n Fill in the missing instructions, following the annotations on the right. The right shift should be performed arithmetically. 232 Chapter 3 Machine-Level Representation of Programs (a) C code ÄÇÅ× þË©Î³fÄÇÅ× Ój ÄÇÅ× Ôj ÄÇÅ× Ög Õ ÄÇÅ× Îo{Ó´Ôy ÄÇÅ× Îp{Öhrvy ÄÇÅ× Îq { Îo d nÓnƒnƒnƒnƒy ÄÇÅ× Îr { Îp k Îqy Ë−ÎÏËÅ Îry Û (b) Assembly code long arith(long x, long y, long z) x in %rdi, y in %rsi, z in %rdx 1 þË©Î³x 2 ÓÇËÊ cËÍ©j cË®© t1=x^y 3 Ä−þÊ fcË®ÓjcË®Ójpgj cËþÓ 3*z 4 ÍþÄÊ brj cËþÓ t2 = 16 * (3*z) = 48*z 5 þÅ®Ä bpsptrsoqsj c−®© t3 = t1 & 0x0F0F0F0F 6 ÍÏ¾Ê cË®©j cËþÓ Return t2 - t3 7 Ë−Î Figure 3.11 C and assembly code for arithmetic function. 3.5.4 Discussion We see that most of the instructions shown in Figure 3.10 can be used for either unsigned or two’s-complement arithmetic. Only right shifting requires instructions that differentiate between signed versus unsigned data. This is one of the features that makes two’s-complement arithmetic the preferred way to implement signed integer arithmetic. Figure 3.11 shows an example of a function that performs arithmetic opera- tions and its translation into assembly code. Arguments Ó, Ô, and Ö are initially stored in registers cË®©, cËÍ©, and cË®Ó, respectively. The assembly-code instruc- tions correspond closely with the lines of C source code. Line 2 computes the value of Ó´Ô. Lines 3 and 4 compute the expression Öhrv by a combination of Ä−þÊ and shift instructions. Line 5 computes the and of Îo and nÓnƒnƒnƒnƒ. The ﬁnal sub- traction is computed by line 6. Since the destination of the subtraction is register cËþÓ, this will be the value returned by the function. In the assembly code of Figure 3.11, the sequence of values in register cËþÓ corresponds to program values qhÖ, Öhrv, and Îr (as the return value). In general, compilers generate code that uses individual registers for multiple program values and moves program values among the registers. Practice Problem 3.10 (solution page 365) Consider the following code, in which we have omitted the expression being computed: Section 3.5 Arithmetic and Logical Operations 233 Í³ÇËÎ þË©Î³qfÍ³ÇËÎ Ój Í³ÇËÎ Ôj Í³ÇËÎ Ög Õ Í³ÇËÎ Éo { y Í³ÇËÎ Ép { y Í³ÇËÎ Éq { y Í³ÇËÎ Ér { y Ë−ÎÏËÅ Éry Û The portion of the generated assembly code implementing these expressions is as follows: short arith3(short x, short y, short z) x in %rdi, y in %rsi, z in %rdx þË©Î³qx ÇËÊ cËÍ©j cË®Ó ÍþËÊ bwj cË®Ó ÅÇÎÊ cË®Ó ÀÇÌÊ cË®Ój c¾þÓ ÍÏ¾Ê cËÍ©j cË¾Ó Ë−Î Based on this assembly code, ﬁll in the missing portions of the C code. Practice Problem 3.11 (solution page 365) It is common to ﬁnd assembly-code lines of the form ÓÇËÊ cË²ÓjcË²Ó in code that was generated from C where no exclusive-or operations were present. A. Explain the effect of this particular exclusive-or instruction and what useful operation it implements. B. What would be the more straightforward way to express this operation in assembly code? C. Compare the number of bytes to encode any two of these three different implementations of the same operation. 3.5.5 Special Arithmetic Operations As we saw in Section 2.3, multiplying two 64-bit signed or unsigned integers can yield a product that requires 128 bits to represent. The x86-64 instruction set provides limited support for operations involving 128-bit (16-byte) numbers. Con- tinuing with the naming convention of word (2 bytes), double word (4 bytes), and quad word (8 bytes), Intel refers to a 16-byte quantity as an oct word. Figure 3.12 234 Chapter 3 Machine-Level Representation of Programs Instruction Effect Description ©ÀÏÄÊ S R[cË®Ó]:R[cËþÓ] ← S × R[cËþÓ] Signed full multiply ÀÏÄÊ S R[cË®Ó]:R[cËþÓ] ← S × R[cËþÓ] Unsigned full multiply ²ÊÎÇ R[cË®Ó]:R[cËþÓ] ← SignExtend(R[cËþÓ]) Convert to oct word ©®©ÌÊ S R[cË®Ó] ← R[cË®Ó]:R[cËþÓ] mod S; Signed divide R[cËþÓ] ← R[cË®Ó]:R[cËþÓ] ÷ S ®©ÌÊ S R[cË®Ó] ← R[cË®Ó]:R[cËþÓ] mod S; Unsigned divide R[cËþÓ] ← R[cË®Ó]:R[cËþÓ] ÷ S Figure 3.12 Special arithmetic operations. These operations provide full 128-bit multiplication and division, for both signed and unsigned numbers. The pair of registers cË®Ó and cËþÓ are viewed as forming a single 128-bit oct word. describes instructions that support generating the full 128-bit product of two 64-bit numbers, as well as integer division. The ©ÀÏÄÊ instruction has two different forms One form, shown in Figure 3.10, is as a member of the imul instruction class. In this form, it serves as a “two- operand” multiply instruction, generating a 64-bit product from two 64-bit oper- ands. It implements the operations h u 64 and ht 64 described in Sections 2.3.4 and 2.3.5. (Recall that when truncating the product to 64 bits, both unsigned multiply and two’s-complement multiply have the same bit-level behavior.) Additionally, the x86-64 instruction set includes two different “one-operand” multiply instructions to compute the full 128-bit product of two 64-bit values— one for unsigned (ÀÏÄÊ) and one for two’s-complement (©ÀÏÄÊ) multiplication. For both of these instructions, one argument must be in register cËþÓ, and the other is given as the instruction source operand. The product is then stored in registers cË®Ó (high-order 64 bits) and cËþÓ (low-order 64 bits). Although the name ©ÀÏÄÊ is used for two distinct multiplication operations, the assembler can tell which one is intended by counting the number of operands. As an example, the following C code demonstrates the generation of a 128-bit product of two unsigned 64-bit numbers Ó and Ô: a©Å²ÄÏ®− z©ÅÎÎÔÉ−Íl³| ÎÔÉ−®−ð ÏÅÍ©×Å−® ˆˆ©ÅÎopv Ï©ÅÎopvˆÎy ÌÇ©® ÍÎÇË−ˆÏÉËÇ®fÏ©ÅÎopvˆÎ h®−ÍÎj Ï©ÅÎtrˆÎ Ój Ï©ÅÎtrˆÎ Ôg Õ h®−ÍÎ{Óh fÏ©ÅÎopvˆÎg Ôy Û In this program, we explicitly declare Ó and Ô to be 64-bit numbers, using deﬁ- nitions declared in the ﬁle ©ÅÎÎÔÉ−Íl³ , as part of an extension of the C standard. Unfortunately, this standard does not make provisions for 128-bit values. Instead, Section 3.5 Arithmetic and Logical Operations 235 we rely on support provided by gcc for 128-bit integers, declared using the name ˆˆ©ÅÎopv. Our code uses a ÎÔÉ−®−ð declaration to deﬁne data type Ï©ÅÎopvˆÎ, following the naming pattern for other data types found in ©ÅÎÎÔÉ−Íl³. The code speciﬁes that the resulting product should be stored at the 16 bytes designated by pointer ®−ÍÎ. The assembly code generated by gcc for this function is as follows: void store_uprod(uint128_t *dest, uint64_t x, uint64_t y) dest in %rdi, x in %rsi, y in %rdx 1 ÍÎÇË−ˆÏÉËÇ®x 2 ÀÇÌÊ cËÍ©j cËþÓ Copy x to multiplicand 3 ÀÏÄÊ cË®Ó Multiply by y 4 ÀÇÌÊ cËþÓj fcË®©g Store lower 8 bytes at dest 5 ÀÇÌÊ cË®Ój vfcË®©g Store upper 8 bytes at dest+8 6 Ë−Î Observe that storing the product requires two ÀÇÌÊ instructions: one for the low-order 8 bytes (line 4), and one for the high-order 8 bytes (line 5). Since the code is generated for a little-endian machine, the high-order bytes are stored at higher addresses, as indicated by the address speciﬁcation vfcË®©g. Our earlier table of arithmetic operations (Figure 3.10) does not list any division or modulus operations. These operations are provided by the single- operand divide instructions similar to the single-operand multiply instructions. The signed division instruction ©®©ÌÄ takes as its dividend the 128-bit quantity in registers cË®Ó (high-order 64 bits) and cËþÓ (low-order 64 bits). The divisor is given as the instruction operand. The instruction stores the quotient in register cËþÓ and the remainder in register cË®Ó. For most applications of 64-bit addition, the dividend is given as a 64-bit value. This value should be stored in register cËþÓ. The bits of cË®Ó should then be set to either all zeros (unsigned arithmetic) or the sign bit of cËþÓ (signed arithmetic). The latter operation can be performed using the instruction ²ÊÎÇ.2 This instruction takes no operands—it implicitly reads the sign bit from cËþÓ and copies it across all of cË®Ó. As an illustration of the implementation of division with x86-64, the following C function computes the quotient and remainder of two 64-bit, signed numbers: ÌÇ©® Ë−À®©ÌfÄÇÅ× Ój ÄÇÅ× Ôj ÄÇÅ× hÊÉj ÄÇÅ× hËÉg Õ ÄÇÅ× Ê { ÓmÔy ÄÇÅ× Ë { ÓcÔy hÊÉ{Êy hËÉ{Ëy Û 2. This instruction is called ²ÊÇ in the Intel documentation, one of the few cases where the ATT-format name for an instruction does not match the Intel name. 236 Chapter 3 Machine-Level Representation of Programs This compiles to the following assembly code: void remdiv(long x, long y, long *qp, long *rp) x in %rdi, y in %rsi, qp in %rdx, rp in %rcx 1 Ë−À®©Ìx 2 ÀÇÌÊ cË®Ój cËv Copy qp 3 ÀÇÌÊ cË®©j cËþÓ Move x to lower 8 bytes of dividend 4 ²ÊÎÇ Sign-extend to upper 8 bytes of dividend 5 ©®©ÌÊ cËÍ© Divide by y 6 ÀÇÌÊ cËþÓj fcËvg Store quotient at qp 7 ÀÇÌÊ cË®Ój fcË²Óg Store remainder at rp 8 Ë−Î In this code, argument ËÉ must ﬁrst be saved in a different register (line 2), since argument register cË®Ó is required for the division operation. Lines 3–4 then prepare the dividend by copying and sign-extending Ó. Following the division, the quotient in register cËþÓ gets stored at ÊÉ (line 6), while the remainder in register cË®Ó gets stored at ËÉ (line 7). Unsigned division makes use of the ®©ÌÊ instruction. Typically, register cË®Ó is set to zero beforehand. Practice Problem 3.12 (solution page 365) Consider the following function for computing the quotient and remainder of two unsigned 64-bit numbers: ÌÇ©® ÏË−À®©ÌfÏÅÍ©×Å−® ÄÇÅ× Ój ÏÅÍ©×Å−® ÄÇÅ× Ôj ÏÅÍ©×Å−® ÄÇÅ× hÊÉj ÏÅÍ©×Å−® ÄÇÅ× hËÉg Õ ÏÅÍ©×Å−® ÄÇÅ× Ê { ÓmÔy ÏÅÍ©×Å−® ÄÇÅ× Ë { ÓcÔy hÊÉ{Êy hËÉ{Ëy Û Modify the assembly code shown for signed division to implement this function. 3.6 Control So far, we have only considered the behavior of straight-line code, where instruc- tions follow one another in sequence. Some constructs in C, such as conditionals, loops, and switches, require conditional execution, where the sequence of oper- ations that get performed depends on the outcomes of tests applied to the data. Machine code provides two basic low-level mechanisms for implementing condi- tional behavior: it tests data values and then alters either the control ﬂow or the data ﬂow based on the results of these tests. Data-dependent control ﬂow is the more general and more common approach for implementing conditional behavior, and so we will examine this ﬁrst. Normally, Section 3.6 Control 237 both statements in C and instructions in machine code are executed sequentially, in the order they appear in the program. The execution order of a set of machine- code instructions can be altered with a jump instruction, indicating that control should pass to some other part of the program, possibly contingent on the result of some test. The compiler must generate instruction sequences that build upon this low-level mechanism to implement the control constructs of C. In our presentation, we ﬁrst cover the two ways of implementing conditional operations. We then describe methods for presenting loops and ÍÑ©Î²³ state- ments. 3.6.1 Condition Codes In addition to the integer registers, the CPU maintains a set of single-bit condition code registers describing attributes of the most recent arithmetic or logical oper- ation. These registers can then be tested to perform conditional branches. These condition codes are the most useful: £ƒ: Carry ﬂag. The most recent operation generated a carry out of the most signiﬁcant bit. Used to detect overﬂow for unsigned operations. …ƒ: Zero ﬂag. The most recent operation yielded zero. ·ƒ: Sign ﬂag. The most recent operation yielded a negative value. ﬂƒ: Overﬂow ﬂag. The most recent operation caused a two’s-complement overﬂow—either negative or positive. For example, suppose we used one of the add instructions to perform the equivalent of the C assignment Î { þi¾, where variables þ, ¾, and Î are integers. Then the condition codes would be set according to the following C expressions: £ƒ fÏÅÍ©×Å−®g Î z fÏÅÍ©×Å−®g þ Unsigned overﬂow …ƒ fÎ {{ ng Zero ·ƒ fÎ z ng Negative ﬂƒ fþzn{{¾zngddfÎzn_{þzng Signed overﬂow The Ä−þÊ instruction does not alter any condition codes, since it is intended to be used in address computations. Otherwise, all of the instructions listed in Figure 3.10 cause the condition codes to be set. For the logical operations, such as xor, the carry and overﬂow ﬂags are set to zero. For the shift operations, the carry ﬂag is set to the last bit shifted out, while the overﬂow ﬂag is set to zero. For reasons that we will not delve into, the inc and dec instructions set the overﬂow and zero ﬂags, but they leave the carry ﬂag unchanged. In addition to the setting of condition codes by the instructions of Figure 3.10, there are two instruction classes (having 8-, 16-, 32-, and 64-bit forms) that set condition codes without altering any other registers; these are listed in Figure 3.13. The cmp instructions set the condition codes according to the differences of their two operands. They behave in the same way as the sub instructions, except that they set the condition codes without updating their destinations. With ATT format, 238 Chapter 3 Machine-Level Representation of Programs Instruction Based on Description cmp S1, S2 S2 − S1 Compare ²ÀÉ¾ Compare byte ²ÀÉÑ Compare word ²ÀÉÄ Compare double word ²ÀÉÊ Compare quad word test S1, S2 S1 & S2 Test Î−ÍÎ¾ Test byte Î−ÍÎÑ Test word Î−ÍÎÄ Test double word Î−ÍÎÊ Test quad word Figure 3.13 Comparison and test instructions. These instructions set the condition codes without updating any other registers. the operands are listed in reverse order, making the code difﬁcult to read. These instructions set the zero ﬂag if the two operands are equal. The other ﬂags can be used to determine ordering relations between the two operands. The test instructions behave in the same manner as the and instructions, except that they set the condition codes without altering their destinations. Typically, the same operand is repeated (e.g., Î−ÍÎÊ cËþÓjcËþÓ to see whether cËþÓ is negative, zero, or positive), or one of the operands is a mask indicating which bits should be tested. 3.6.2 Accessing the Condition Codes Rather than reading the condition codes directly, there are three common ways of using the condition codes: (1) we can set a single byte to 0 or 1 depending on some combination of the condition codes, (2) we can conditionally jump to some other part of the program, or (3) we can conditionally transfer data. For the ﬁrst case, the instructions described in Figure 3.14 set a single byte to 0 or to 1 depending on some combination of the condition codes. We refer to this entire class of instructions as the set instructions; they differ from one another based on which combinations of condition codes they consider, as indicated by the different sufﬁxes for the instruction names. It is important to recognize that the sufﬁxes for these instructions denote different conditions and not different operand sizes. For example, instructions Í−ÎÄ and Í−Î¾ denote “set less” and “set below,” not “set long word” or “set byte.” A set instruction has either one of the low-order single-byte register elements (Figure 3.2) or a single-byte memory location as its destination, setting this byte to either 0 or 1. To generate a 32-bit or 64-bit result, we must also clear the high-order bits. A typical instruction sequence to compute the C expression þz¾, where þ and ¾ are both of type ÄÇÅ×, proceeds as follows: Section 3.6 Control 239 Instruction Synonym Effect Set condition Í−Î− D Í−ÎÖ D ← …ƒ Equal / zero Í−ÎÅ− D Í−ÎÅÖ D ← Ü…ƒ Not equal / not zero Í−ÎÍ DD ← ·ƒ Negative Í−ÎÅÍ DD ← Ü·ƒ Nonnegative Í−Î× D Í−ÎÅÄ− D ← Ü (·ƒ ´ ﬂƒ) d Ü…ƒ Greater (signed |) Í−Î×− D Í−ÎÅÄ D ← Ü (·ƒ ´ ﬂƒ) Greater or equal (signed |{) Í−ÎÄ D Í−ÎÅ×− D ← ·ƒ ´ ﬂƒ Less (signed z) Í−ÎÄ− D Í−ÎÅ× D ← (·ƒ ´ ﬂƒ) Ú…ƒ Less or equal (signed z{) Í−Îþ D Í−ÎÅ¾− D ← Ü £ƒ d Ü…ƒ Above (unsigned |) Í−Îþ− D Í−ÎÅ¾ D ← Ü£ƒ Above or equal (unsigned |{) Í−Î¾ D Í−ÎÅþ− D ← £ƒ Below (unsigned z) Í−Î¾− D Í−ÎÅþ D ← £ƒ Ú …ƒ Below or equal (unsigned z{) Figure 3.14 The set instructions. Each instruction sets a single byte to 0 or 1 based on some combination of the condition codes. Some instructions have “synonyms,” that is, alternate names for the same machine instruction. int comp(data_t a, data_t b) a in %rdi, b in %rsi 1 ²ÇÀÉx 2 ²ÀÉÊ cËÍ©j cË®© Compare a:b 3 Í−ÎÄ cþÄ Set low-order byte of %eax to 0 or 1 4 ÀÇÌÖ¾Ä cþÄj c−þÓ Clear rest of %eax (and rest of %rax) 5 Ë−Î Note the comparison order of the ²ÀÉÊ instruction (line 2). Although the arguments are listed in the order cËÍ© (¾), then cË®© (þ), the comparison is really between þ and ¾. Recall also, as discussed in Section 3.4.2, that the ÀÇÌÖ¾Ä instruction (line 4) clears not just the high-order 3 bytes of c−þÓ, but the upper 4 bytes of the entire register, cËþÓ, as well. For some of the underlying machine instructions, there are multiple possible names, which we list as “synonyms.” For example, both Í−Î× (for “set greater”) and Í−ÎÅÄ− (for “set not less or equal”) refer to the same machine instruction. Compilers and disassemblers make arbitrary choices of which names to use. Although all arithmetic and logical operations set the condition codes, the de- scriptions of the different set instructions apply to the case where a comparison instruction has been executed, setting the condition codes according to the com- putation Î { þk¾. More speciﬁcally, let a, b, and t be the integers represented in two’s-complement form by variables þ, ¾, and Î, respectively, and so t = a kt w b, where w depends on the sizes associated with þ and ¾. 240 Chapter 3 Machine-Level Representation of Programs Consider the Í−Î−, or “set when equal,” instruction. When a = b, we will have t = 0, and hence the zero ﬂag indicates equality. Similarly, consider testing for signed comparison with the Í−ÎÄ, or “set when less,” instruction. When no overﬂow occurs (indicated by having ﬂƒ set to 0), we will have a< b when a kt w b< 0, indicated by having ·ƒ set to 1, and a ≥ b when a kt w b ≥ 0, indicated by having ·ƒ set to 0. On the other hand, when overﬂow occurs, we will have a< b when a kt w b> 0 (negative overﬂow) and a> b when a kt w b< 0 (positive overﬂow). We cannot have overﬂow when a = b. Thus, when ﬂƒ is set to 1, we will have a< b if and only if ·ƒ is set to 0. Combining these cases, the exclusive-or of the overﬂow and sign bits provides a test for whether a< b. The other signed comparison tests are based on other combinations of ·ƒ ´ ﬂƒ and …ƒ. For the testing of unsigned comparisons, we now let a and b be the integers represented in unsigned form by variables þ and ¾. In performing the computation Î { þk¾, the carry ﬂag will be set by the cmp instruction when a − b< 0, and so the unsigned comparisons use combinations of the carry and zero ﬂags. It is important to note how machine code does or does not distinguish be- tween signed and unsigned values. Unlike in C, it does not associate a data type with each program value. Instead, it mostly uses the same instructions for the two cases, because many arithmetic operations have the same bit-level behavior for unsigned and two’s-complement arithmetic. Some circumstances require different instructions to handle signed and unsigned operations, such as using differ- ent versions of right shifts, division and multiplication instructions, and different combinations of condition codes. Practice Problem 3.13 (solution page 366) The C code ©ÅÎ ²ÇÀÉf®þÎþˆÎ þj ®þÎþˆÎ ¾g Õ Ë−ÎÏËÅ þ £ﬂ›– ¾y Û shows a general comparison between arguments þ and ¾, where ®þÎþˆÎ, the data type of the arguments, is deﬁned (via ÎÔÉ−®−ð) to be one of the integer data types listed in Figure 3.1 and either signed or unsigned. The comparison £ﬂ›– is deﬁned via a®−ð©Å−. Suppose þ is in some portion of cË®Ó while ¾ is in some portion of cËÍ©.For each of the following instruction sequences, determine which data types ®þÎþˆÎ and which comparisons £ﬂ›– could cause the compiler to generate this code. (There can be multiple correct answers; you should list them all.) A. ²ÀÉÄ c−Í©j c−®© Í−ÎÄ cþÄ B. ²ÀÉÑ cÍ©j c®© Í−Î×− cþÄ Section 3.6 Control 241 C. ²ÀÉ¾ cÍ©Äj c®©Ä Í−Î¾− cþÄ D. ²ÀÉÊ cËÍ©j cË®© Í−ÎÅ− cþ Practice Problem 3.14 (solution page 366) The C code ©ÅÎ Î−ÍÎf®þÎþˆÎ þg Õ Ë−ÎÏËÅ þ ¶¥·¶ ny Û shows a general comparison between argument þ and 0, where we can set the data type of the argument by declaring ®þÎþˆÎ with a ÎÔÉ−®−ð, and the nature of the comparison by declaring ¶¥·¶ with a a®−ð©Å− declaration. The following instruction sequences implement the comparison, where þ is held in some portion of register cË®©. For each sequence, determine which data types ®þÎþˆÎ and which comparisons ¶¥·¶ could cause the compiler to generate this code. (There can be multiple correct answers; list all correct ones.) A. Î−ÍÎÊ cË®©j cË®© Í−Î×− cþÄ B. Î−ÍÎÑ c®©j c®© Í−Î− cþÄ C. Î−ÍÎ¾ c®©Äj c®©Ä Í−Îþ cþÄ D. Î−ÍÎÄ c−®©j c−®© Í−ÎÄ− cþÄ 3.6.3 Jump Instructions Under normal execution, instructions follow each other in the order they are listed. A jump instruction can cause the execution to switch to a completely new position in the program. These jump destinations are generally indicated in assembly code by a label. Consider the following (very contrived) assembly-code sequence: ÀÇÌÊ bnjcËþÓ Set %rax to 0 ÁÀÉ l‹o Goto .L1 ÀÇÌÊ fcËþÓgjcË®Ó Null pointer dereference (skipped) l‹ox ÉÇÉÊ cË®Ó Jump target 242 Chapter 3 Machine-Level Representation of Programs Instruction Synonym Jump condition Description ÁÀÉ Label 1 Direct jump ÁÀÉ hOperand 1 Indirect jump Á− Label ÁÖ …ƒ Equal / zero ÁÅ− Label ÁÅÖ Ü…ƒ Not equal / not zero ÁÍ Label ·ƒ Negative ÁÅÍ Label Ü·ƒ Nonnegative Á× Label ÁÅÄ− Ü(·ƒ ´ ﬂƒ) d Ü…ƒ Greater (signed |) Á×− Label ÁÅÄ Ü(·ƒ ´ ﬂƒ) Greater or equal (signed |{) ÁÄ Label ÁÅ×− ·ƒ ´ ﬂƒ Less (signed z) ÁÄ− Label ÁÅ× (·ƒ ´ ﬂƒ) Ú…ƒ Less or equal (signed z{) Áþ Label ÁÅ¾− Ü£ƒ d Ü…ƒ Above (unsigned |) Áþ− Label ÁÅ¾ Ü£ƒ Above or equal (unsigned |{) Á¾ Label ÁÅþ− £ƒ Below (unsigned z) Á¾− Label ÁÅþ £ƒ Ú …ƒ Below or equal (unsigned z{) Figure 3.15 The jump instructions. These instructions jump to a labeled destination when the jump condition holds. Some instructions have “synonyms,” alternate names for the same machine instruction. The instruction ÁÀÉ l‹o will cause the program to skip over the ÀÇÌÊ instruc- tion and instead resume execution with the ÉÇÉÊ instruction. In generating the object-code ﬁle, the assembler determines the addresses of all labeled instruc- tions and encodes the jump targets (the addresses of the destination instructions) as part of the jump instructions. Figure 3.15 shows the different jump instructions. The ÁÀÉ instruction jumps unconditionally. It can be either a direct jump, where the jump target is encoded as part of the instruction, or an indirect jump, where the jump target is read from a register or a memory location. Direct jumps are written in assembly code by giving a label as the jump target, for example, the label l‹o in the code shown. Indirect jumps are written using ‘h’ followed by an operand speciﬁer using one of the memory operand formats described in Figure 3.3. As examples, the instruction ÁÀÉ hcËþÓ uses the value in register cËþÓ as the jump target, and the instruction ÁÀÉ hfcËþÓg reads the jump target from memory, using the value in cËþÓ as the read address. The remaining jump instructions in the table are conditional—they either jump or continue executing at the next instruction in the code sequence, depending on some combination of the condition codes. The names of these instructions Section 3.6 Control 243 and the conditions under which they jump match those of the set instructions (see Figure 3.14). As with the set instructions, some of the underlying machine instructions have multiple names. Conditional jumps can only be direct. 3.6.4 Jump Instruction Encodings For the most part, we will not concern ourselves with the detailed format of ma- chine code. On the other hand, understanding how the targets of jump instructions are encoded will become important when we study linking in Chapter 7. In ad- dition, it helps when interpreting the output of a disassembler. In assembly code, jump targets are written using symbolic labels. The assembler, and later the linker, generate the proper encodings of the jump targets. There are several different en- codings for jumps, but some of the most commonly used ones are PC relative. That is, they encode the difference between the address of the target instruction and the address of the instruction immediately following the jump. These offsets can be encoded using 1, 2, or 4 bytes. A second encoding method is to give an “abso- lute” address, using 4 bytes to directly specify the target. The assembler and linker select the appropriate encodings of the jump destinations. As an example of PC-relative addressing, the following assembly code for a function was generated by compiling a ﬁle ¾ËþÅ²³l². It contains two jumps: the ÁÀÉ instruction on line 2 jumps forward to a higher address, while the Á× instruction on line 7 jumps back to a lower one. 1 ÀÇÌÊ cË®©j cËþÓ 2 ÁÀÉ l‹p 3 l‹qx 4 ÍþËÊ cËþÓ 5 l‹px 6 Î−ÍÎÊ cËþÓj cËþÓ 7 Á× l‹q 8 Ë−Éy Ë−Î The disassembled version of the lÇ format generated by the assembler is as follows: 1 nx rv vw ðv ÀÇÌ cË®©jcËþÓ 2 qx −¾ nq ÁÀÉ v zÄÇÇÉinÓv| 3 sx rv ®o ðv ÍþË cËþÓ 4 vx rv vs ²n Î−ÍÎ cËþÓjcËþÓ 5 ¾x uð ðv Á× s zÄÇÇÉinÓs| 6 ®x ðq ²q Ë−ÉÖ Ë−ÎÊ In the annotations on the right generated by the disassembler, the jump targets are indicated as nÓv for the jump instruction on line 2 and nÓs for the jump instruction on line 5 (the disassembler lists all numbers in hexadecimal). Looking at the byte encodings of the instructions, however, we see that the target of the ﬁrst jump instruction is encoded (in the second byte) as nÓnq. Adding this to nÓs, the 244 Chapter 3 Machine-Level Representation of Programs Aside What do the instructions Ë−É and Ë−ÉÖ do? Line 8 of the assembly code shown on page 243 contains the instruction combination Ë−Éy Ë−Î. These are rendered in the disassembled code (line 6) as Ë−ÉÖ Ë−ÎÊ. One can infer that Ë−ÉÖ is a synonym for Ë−É, just as Ë−ÎÊ is a synonym for Ë−Î. Looking at the Intel and AMD documentation for the Ë−É instruction, we ﬁnd that it is normally used to implement a repeating string operation [3, 51]. It seems completely inappropriate here. The answer to this puzzle can be seen in AMD’s guidelines to compiler writers [1]. They recommend using the combination of Ë−É followed by Ë−Î to avoid making the Ë−Î instruction the destination of a conditional jump instruction. Without the Ë−É instruction, the Á× instruction (line 7 of the assembly code) would proceed to the Ë−Î instruction when the branch is not taken. According to AMD, their processors cannot properly predict the destination of a Ë−Î instruction when it is reached from a jump instruction. The Ë−É instruction serves as a form of no-operation here, and so inserting it as the jump destination does not change behavior of the code, except to make it faster on AMD processors. We can safely ignore any Ë−É or Ë−ÉÖ instruction we see in the rest of the code presented in this book. address of the following instruction, we get jump target address nÓv, the address of the instruction on line 4. Similarly, the target of the second jump instruction is encoded as nÓðv (deci- mal −8) using a single-byte two’s-complement representation. Adding this to nÓ® (decimal 13), the address of the instruction on line 6, we get nÓs, the address of the instruction on line 3. As these examples illustrate, the value of the program counter when perform- ing PC-relative addressing is the address of the instruction following the jump, not that of the jump itself. This convention dates back to early implementations, when the processor would update the program counter as its ﬁrst step in executing an instruction. The following shows the disassembled version of the program after linking: 1 rnnr®nx rv vw ðv ÀÇÌ cË®©jcËþÓ 2 rnnr®qx −¾ nq ÁÀÉ rnnr®v zÄÇÇÉinÓv| 3 rnnr®sx rv ®o ðv ÍþË cËþÓ 4 rnnr®vx rv vs ²n Î−ÍÎ cËþÓjcËþÓ 5 rnnr®¾x uð ðv Á× rnnr®s zÄÇÇÉinÓs| 6 rnnr®®x ðq ²q Ë−ÉÖ Ë−ÎÊ The instructions have been relocated to different addresses, but the encodings of the jump targets in lines 2 and 5 remain unchanged. By using a PC-relative encoding of the jump targets, the instructions can be compactly encoded (requiring just 2 bytes), and the object code can be shifted to different positions in memory without alteration. Section 3.6 Control 245 Practice Problem 3.15 (solution page 366) In the following excerpts from a disassembled binary, some of the information has been replaced by ”’s. Answer the following questions about these instructions. A. What is the target of the Á− instruction below? (You do not need to know anything about the ²þÄÄÊ instruction here.) rnnqðþx ur np Á− ”””””” rnnqð²x ðð ®n ²þÄÄÊ hcËþÓ B. What is the target of the Á− instruction below? rnnrpðx ur ðr Á− ”””””” rnnrqox s® ÉÇÉ cË¾É C. What is the address of the Áþ and ÉÇÉ instructions? ””””””x uu np Áþ rnnsru ””””””x s® ÉÇÉ cË¾É D. In the code that follows, the jump target is encoded in PC-relative form as a 4- byte two’s-complement number. The bytes are listed from least signiﬁcant to most, reﬂecting the little-endian byte ordering of x86-64. What is the address of the jump target? rnns−vx −w uq ðð ðð ðð ÁÀÉÊ ””””””” rnns−®x wn ÅÇÉ The jump instructions provide a means to implement conditional execution (©ð), as well as several different loop constructs. 3.6.5 Implementing Conditional Branches with Conditional Control The most general way to translate conditional expressions and statements from C into machine code is to use combinations of conditional and unconditional jumps. (As an alternative, we will see in Section 3.6.6 that some conditionals can be implemented by conditional transfers of data rather than control.) For example, Figure 3.16(a) shows the C code for a function that computes the absolute value of the difference of two numbers.3 The function also has a side effect of incrementing one of two counters, encoded as global variables ÄÎˆ²ÅÎ and ×−ˆ ²ÅÎ. Gcc generates the assembly code shown as Figure 3.16(c). Our rendition of the machine code into C is shown as the function ×ÇÎÇ®©ððˆÍ− (Figure 3.16(b)). It uses the ×ÇÎÇ statement in C, which is similar to the unconditional jump of 3. Actually, it can return a negative value if one of the subtractions overﬂows. Our interest here is to demonstrate machine code, not to implement robust code. 246 Chapter 3 Machine-Level Representation of Programs (a) Original C code ÄÇÅ× ÄÎˆ²ÅÎ { ny ÄÇÅ× ×−ˆ²ÅÎ { ny ÄÇÅ× þ¾Í®©ððˆÍ−fÄÇÅ× Ój ÄÇÅ× Ôg Õ ÄÇÅ× Ë−ÍÏÄÎy ©ðfÓzÔgÕ ÄÎˆ²ÅÎiiy Ë−ÍÏÄÎ{ÔkÓy Û −ÄÍ− Õ ×−ˆ²ÅÎiiy Ë−ÍÏÄÎ{ÓkÔy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Equivalent goto version 1 ÄÇÅ× ×ÇÎÇ®©ððˆÍ−fÄÇÅ× Ój ÄÇÅ× Ôg 2 Õ 3 ÄÇÅ× Ë−ÍÏÄÎy 4 ©ð fÓ |{ Ôg 5 ×ÇÎÇ Óˆ×−ˆÔy 6 ÄÎˆ²ÅÎiiy 7 Ë−ÍÏÄÎ { Ô k Óy 8 Ë−ÎÏËÅ Ë−ÍÏÄÎy 9 Óˆ×−ˆÔx 10 ×−ˆ²ÅÎiiy 11 Ë−ÍÏÄÎ{ÓkÔy 12 Ë−ÎÏËÅ Ë−ÍÏÄÎy 13 Û (c) Generated assembly code long absdiff_se(long x, long y) x in %rdi, y in %rsi 1 þ¾Í®©ððˆÍ−x 2 ²ÀÉÊ cËÍ©j cË®© Compare x:y 3 Á×− l‹p If >= goto x_ge_y 4 þ®®Ê boj ÄÎˆ²ÅÎfcË©Ég lt_cnt++ 5 ÀÇÌÊ cËÍ©j cËþÓ 6 ÍÏ¾Ê cË®©j cËþÓ result=y-x 7 Ë−Î Return 8 l‹px x_ge_y: 9 þ®®Ê boj ×−ˆ²ÅÎfcË©Ég ge_cnt++ 10 ÀÇÌÊ cË®©j cËþÓ 11 ÍÏ¾Ê cËÍ©j cËþÓ result=x-y 12 Ë−Î Return Figure 3.16 Compilation of conditional statements. (a) C procedure þ¾Í®©ððˆÍ− contains an if-else statement. The generated assembly code is shown (c), along with (b) a C procedure ×ÇÎÇ®©ððˆÍ− that mimics the control ﬂow of the assembly code. assembly code. Using ×ÇÎÇ statements is generally considered a bad programming style, since their use can make code very difﬁcult to read and debug. We use them in our presentation as a way to construct C programs that describe the control ﬂow of machine code. We call this style of programming “goto code.” In the goto code (Figure 3.16(b)), the statement ×ÇÎÇ Óˆ×−ˆÔ on line 5 causes a jump to the label Óˆ×−ˆÔ (since it occurs when x ≥ y) on line 9. Continuing the Section 3.6 Control 247 Aside Describing machine code with C code Figure 3.16 shows an example of how we will demonstrate the translation of C language control constructs into machine code. The ﬁgure contains an example C function (a) and an annotated version of the assembly code generated by gcc (c). It also contains a version in C that closely matches the structure of the assembly code (b). Although these versions were generated in the sequence (a), (c), and (b), we recommend that you read them in the order (a), (b), and then (c). That is, the C rendition of the machine code will help you understand the key points, and this can guide you in understanding the actual assembly code. execution from this point, it completes the computations speciﬁed by the −ÄÍ− portion of function þ¾Í®©ððˆÍ− and returns. On the other hand, if the test Ó|{Ô fails, the program procedure will carry out the steps speciﬁed by the ©ð portion of þ¾Í®©ððˆÍ− and return. The assembly-code implementation (Figure 3.16(c)) ﬁrst compares the two operands (line 2), setting the condition codes. If the comparison result indicates that x is greater than or equal to y, it then jumps to a block of code starting at line 8 that increments global variable ×−ˆ²ÅÎ, computes ÓkÔ as the return value, and returns. Otherwise, it continues with the execution of code beginning at line 4 that increments global variable ÄÎˆ²ÅÎ, computes ÔkÓ as the return value, and returns. We can see, then, that the control ﬂow of the assembly code generated for þ¾Í®©ððˆÍ− closely follows the goto code of ×ÇÎÇ®©ððˆÍ−. The general form of an if-else statement in C is given by the template ©ð ftest-exprg then-statement −ÄÍ− else-statement where test-expr is an integer expression that evaluates either to zero (interpreted as meaning “false”) or to a nonzero value (interpreted as meaning “true”). Only one of the two branch statements (then-statement or else-statement) is executed. For this general form, the assembly implementation typically adheres to the following form, where we use C syntax to describe the control ﬂow: Î{ test-expry ©ð f_Îg ×ÇÎÇ ðþÄÍ−y then-statement ×ÇÎÇ ®ÇÅ−y ðþÄÍ−x else-statement ®ÇÅ−x 248 Chapter 3 Machine-Level Representation of Programs That is, the compiler generates separate blocks of code for then-statement and else-statement. It inserts conditional and unconditional branches to make sure the correct block is executed. Practice Problem 3.16 (solution page 367) When given the C code ÌÇ©® ²ÇÅ®fÍ³ÇËÎ þj Í³ÇËÎ hÉg Õ ©ð fþ dd hÉ z þg hÉ{þy Û gcc generates the following assembly code: void cond(short a, short *p) a in %rdi, p in %rsi ²ÇÅ®x Î−ÍÎÊ cË®©j cË®© Á− l‹o ²ÀÉÊ cËÍ©j fcË®©g ÁÄ− l‹o ÀÇÌÊ cË®©j fcËÍ©g l‹ox Ë−Éy Ë−Î A. Write a goto version in C that performs the same computation and mimics the control ﬂow of the assembly code, in the style shown in Figure 3.16(b). You might ﬁnd it helpful to ﬁrst annotate the assembly code as we have done in our examples. B. Explain why the assembly code contains two conditional branches, even though the C code has only one ©ð statement. Practice Problem 3.17 (solution page 367) An alternate rule for translating ©ð statements into goto code is as follows: Î{ test-expry ©ð fÎg ×ÇÎÇ ÎËÏ−y else-statement ×ÇÎÇ ®ÇÅ−y ÎËÏ−x then-statement ®ÇÅ−x Section 3.6 Control 249 A. Rewrite the goto version of þ¾Í®©ððˆÍ− based on this alternate rule. B. Can you think of any reasons for choosing one rule over the other? Practice Problem 3.18 (solution page 368) Starting with C code of the form Í³ÇËÎ Î−ÍÎfÍ³ÇËÎ Ój Í³ÇËÎ Ôj Í³ÇËÎ Ög Õ Í³ÇËÎ ÌþÄ { y ©ð f gÕ ©ð f g ÌþÄ { y −ÄÍ− ÌþÄ { y Û −ÄÍ− ©ð f g ÌþÄ { y Ë−ÎÏËÅ ÌþÄy Û gcc generates the following assembly code: short test(short x, short y, short z) x in %rdi, y in %rsi, z in %rdx Î−ÍÎx Ä−þÊ fcË®ÓjcËÍ©gj cËþÓ ÍÏ¾Ê cË®©j cËþÓ ²ÀÉÊ bsj cË®Ó ÁÄ− l‹p ²ÀÉÊ bpj cËÍ© ÁÄ− l‹q ÀÇÌÊ cË®©j cËþÓ ©®©ÌÊ cË®Ój cËþÓ Ë−Î l‹qx ÀÇÌÊ cË®©j cËþÓ ©®©ÌÊ cËÍ©j cËþÓ Ë−Î l‹px ²ÀÉÊ bqj cË®Ó Á×− l‹r ÀÇÌÊ cË®Ój cËþÓ ©®©ÌÊ cËÍ©j cËþÓ l‹rx Ë−Éy Ë−Î Fill in the missing expressions in the C code. 250 Chapter 3 Machine-Level Representation of Programs 3.6.6 Implementing Conditional Branches with Conditional Moves The conventional way to implement conditional operations is through a condi- tional transfer of control, where the program follows one execution path when a condition holds and another when it does not. This mechanism is simple and general, but it can be very inefﬁcient on modern processors. An alternate strategy is through a conditional transfer of data. This approach computes both outcomes of a conditional operation and then selects one based on whether or not the condition holds. This strategy makes sense only in restricted cases, but it can then be implemented by a simple conditional move instruction that is better matched to the performance characteristics of modern processors. Here, we examine this strategy and its implementation with x86-64. Figure 3.17(a) shows an example of code that can be compiled using a condi- tional move. The function computes the absolute value of its arguments Ó and Ô, as did our earlier example (Figure 3.16). Whereas the earlier example had side ef- fects in the branches, modifying the value of either ÄÎˆ²ÅÎ or ×−ˆ²ÅÎ, this version simply computes the value to be returned by the function. (a) Original C code ÄÇÅ× þ¾Í®©ððfÄÇÅ× Ój ÄÇÅ× Ôg Õ ÄÇÅ× Ë−ÍÏÄÎy ©ð fÓ z Ôg Ë−ÍÏÄÎ{ÔkÓy −ÄÍ− Ë−ÍÏÄÎ{ÓkÔy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Implementation using conditional assignment 1 ÄÇÅ× ²ÀÇÌ®©ððfÄÇÅ× Ój ÄÇÅ× Ôg 2 Õ 3 ÄÇÅ× ËÌþÄ { ÔkÓy 4 ÄÇÅ× −ÌþÄ { ÓkÔy 5 ÄÇÅ× ÅÎ−ÍÎ{Ó|{Ôy 6 mh ‹©Å− ¾−ÄÇÑ Ë−ÊÏ©Ë−Í 7 Í©Å×Ä− ©ÅÍÎËÏ²Î©ÇÅx hm 8 ©ð fÅÎ−ÍÎg ËÌþÄ { −ÌþÄy 9 Ë−ÎÏËÅ ËÌþÄy 10 Û (c) Generated assembly code long absdiff(long x, long y) x in %rdi, y in %rsi 1 þ¾Í®©ððx 2 ÀÇÌÊ cËÍ©j cËþÓ 3 ÍÏ¾Ê cË®©j cËþÓ rval = y-x 4 ÀÇÌÊ cË®©j cË®Ó 5 ÍÏ¾Ê cËÍ©j cË®Ó eval = x-y 6 ²ÀÉÊ cËÍ©j cË®© Compare x:y 7 ²ÀÇÌ×− cË®Ój cËþÓ If >=, rval = eval 8 Ë−Î Return tval Figure 3.17 Compilation of conditional statements using conditional assignment. (a) C function þ¾Í®©ðð contains a conditional expression. The generated assembly code is shown (c), along with (b) a C function ²ÀÇÌ®©ðð that mimics the operation of the assembly code. Section 3.6 Control 251 For this function, gcc generates the assembly code shown in Figure 3.17(c), having an approximate form shown by the C function ²ÀÇÌ®©ðð shown in Figure 3.17(b). Studying the C version, we can see that it computes both ÔkÓ and ÓkÔ, naming these ËÌþÄ and −ÌþÄ, respectively. It then tests whether x is greater than or equal to y, and if so, copies −ÌþÄ to ËÌþÄ before returning ËÌþÄ. The assembly code in Figure 3.17(c) follows the same logic. The key is that the single ²ÀÇÌ×− instruction (line 7) of the assembly code implements the conditional assignment (line 8) of ²ÀÇÌ®©ðð. It will transfer the data from the source register to the destination, only if the ²ÀÉÊ instruction of line 6 indicates that one value is greater than or equal to the other (as indicated by the sufﬁx ×−). To understand why code based on conditional data transfers can outperform code based on conditional control transfers (as in Figure 3.16), we must understand something about how modern processors operate. As we will see in Chapters 4 and 5, processors achieve high performance through pipelining, where an instruc- tion is processed via a sequence of stages, each performing one small portion of the required operations (e.g., fetching the instruction from memory, determining the instruction type, reading from memory, performing an arithmetic operation, writing to memory, and updating the program counter). This approach achieves high performance by overlapping the steps of the successive instructions, such as fetching one instruction while performing the arithmetic operations for a pre- vious instruction. To do this requires being able to determine the sequence of instructions to be executed well ahead of time in order to keep the pipeline full of instructions to be executed. When the machine encounters a conditional jump (re- ferred to as a “branch”), it cannot determine which way the branch will go until it has evaluated the branch condition. Processors employ sophisticated branch pre- diction logic to try to guess whether or not each jump instruction will be followed. As long as it can guess reliably (modern microprocessor designs try to achieve success rates on the order of 90%), the instruction pipeline will be kept full of instructions. Mispredicting a jump, on the other hand, requires that the processor discard much of the work it has already done on future instructions and then begin ﬁlling the pipeline with instructions starting at the correct location. As we will see, such a misprediction can incur a serious penalty, say, 15–30 clock cycles of wasted effort, causing a serious degradation of program performance. As an example, we ran timings of the þ¾Í®©ðð function on an Intel Haswell processor using both methods of implementing the conditional operation. In a typical application, the outcome of the test ÓzÔ is highly unpredictable, and so even the most sophisticated branch prediction hardware will guess correctly only around 50% of the time. In addition, the computations performed in each of the two code sequences require only a single clock cycle. As a consequence, the branch misprediction penalty dominates the performance of this function. For x86-64 code with conditional jumps, we found that the function requires around 8 clock cycles per call when the branching pattern is easily predictable, and around 17.50 clock cycles per call when the branching pattern is random. From this, we can infer that the branch misprediction penalty is around 19 clock cycles. That means time required by the function ranges between around 8 and 27 cycles, depending on whether or not the branch is predicted correctly. 252 Chapter 3 Machine-Level Representation of Programs Aside How did you determine this penalty? Assume the probability of misprediction is p, the time to execute the code without misprediction is TOK, and the misprediction penalty is TMP. Then the average time to execute the code as a function of p is Tavg(p) = (1 − p)TOK + p(TOK + TMP) = TOK + pTMP. We are given TOK and Tran, the average time when p = 0.5, and we want to determine TMP. Substituting into the equation, we get Tran = Tavg(0.5) = TOK + 0.5TMP, and therefore TMP = 2(Tran − TOK). So, for TOK = 8 and Tran = 17.5, we get TMP = 19. On the other hand, the code compiled using conditional moves requires around 8 clock cycles regardless of the data being tested. The ﬂow of control does not depend on data, and this makes it easier for the processor to keep its pipeline full. Practice Problem 3.19 (solution page 368) Running on a new processor model, our code required around 45 cycles when the branching pattern was random, and around 25 cycles when the pattern was highly predictable. A. What is the approximate miss penalty? B. How many cycles would the function require when the branch is mispre- dicted? Figure 3.18 illustrates some of the conditional move instructions available with x86-64. Each of these instructions has two operands: a source register or memory location S, and a destination register R. As with the different set (Section 3.6.2) and jump (Section 3.6.3) instructions, the outcome of these instructions depends on the values of the condition codes. The source value is read from either memory or the source register, but it is copied to the destination only if the speciﬁed condition holds. The source and destination values can be 16, 32, or 64 bits long. Single- byte conditional moves are not supported. Unlike the unconditional instructions, where the operand length is explicitly encoded in the instruction name (e.g., ÀÇÌÑ and ÀÇÌÄ), the assembler can infer the operand length of a conditional move instruction from the name of the destination register, and so the same instruction name can be used for all operand lengths. Unlike conditional jumps, the processor can execute conditional move in- structions without having to predict the outcome of the test. The processor simply reads the source value (possibly from memory), checks the condition code, and then either updates the destination register or keeps it the same. We will explore the implementation of conditional moves in Chapter 4. To understand how conditional operations can be implemented via condi- tional data transfers, consider the following general form of conditional expression and assignment: Section 3.6 Control 253 Instruction Synonym Move condition Description ²ÀÇÌ− S, R ²ÀÇÌÖ …ƒ Equal / zero ²ÀÇÌÅ− S, R ²ÀÇÌÅÖ Ü…ƒ Not equal / not zero ²ÀÇÌÍ S, R ·ƒ Negative ²ÀÇÌÅÍ S, R Ü·ƒ Nonnegative ²ÀÇÌ× S, R ²ÀÇÌÅÄ− Ü(·ƒ ´ ﬂƒ) d Ü…ƒ Greater (signed |) ²ÀÇÌ×− S, R ²ÀÇÌÅÄ Ü(·ƒ ´ ﬂƒ) Greater or equal (signed |{) ²ÀÇÌÄ S, R ²ÀÇÌÅ×− ·ƒ ´ ﬂƒ Less (signed z) ²ÀÇÌÄ− S, R ²ÀÇÌÅ× (·ƒ ´ ﬂƒ) Ú…ƒ Less or equal (signed z{) ²ÀÇÌþ S, R ²ÀÇÌÅ¾− Ü£ƒ d Ü…ƒ Above (unsigned |) ²ÀÇÌþ− S, R ²ÀÇÌÅ¾ Ü£ƒ Above or equal (Unsigned |{) ²ÀÇÌ¾ S, R ²ÀÇÌÅþ− £ƒ Below (unsigned z) ²ÀÇÌ¾− S, R ²ÀÇÌÅþ £ƒ Ú …ƒ Below or equal (unsigned z{) Figure 3.18 The conditional move instructions. These instructions copy the source value S to its destination R when the move condition holds. Some instructions have “synonyms,” alternate names for the same machine instruction. Ì{ test-expr } then-expr x else-expry The standard way to compile this expression using conditional control transfer would have the following form: ©ð f_test-exprg ×ÇÎÇ ðþÄÍ−y Ì{ then-expry ×ÇÎÇ ®ÇÅ−y ðþÄÍ−x Ì{ else-expry ®ÇÅ−x This code contains two code sequences—one evaluating then-expr and one evalu- ating else-expr. A combination of conditional and unconditional jumps is used to ensure that just one of the sequences is evaluated. For the code based on a conditional move, both the then-expr and the else- expr are evaluated, with the ﬁnal value chosen based on the evaluation test-expr. This can be described by the following abstract code: Ì{ then-expry Ì− { else-expry Î{ test-expry ©ð f_Îg Ì { Ì−y The ﬁnal statement in this sequence is implemented with a conditional move— value Ì− is copied to Ì only if test condition Î does not hold. 254 Chapter 3 Machine-Level Representation of Programs Not all conditional expressions can be compiled using conditional moves. Most signiﬁcantly, the abstract code we have shown evaluates both then-expr and else-expr regardless of the test outcome. If one of those two expressions could possibly generate an error condition or a side effect, this could lead to invalid behavior. Such is the case for our earlier example (Figure 3.16). Indeed, we put the side effects into this example speciﬁcally to force gcc to implement this function using conditional transfers. As a second illustration, consider the following C function: ÄÇÅ× ²Ë−þ®fÄÇÅ× hÓÉg Õ Ë−ÎÏËÅ fÓÉ } hÓÉ x ngy Û At ﬁrst, this seems like a good candidate to compile using a conditional move to set the result to zero when the pointer is null, as shown in the following assembly code: long cread(long *xp) Invalid implementation of function cread xp in register %rdi 1 ²Ë−þ®x 2 ÀÇÌÊ fcË®©gj cËþÓ v = *xp 3 Î−ÍÎÊ cË®©j cË®© Test x 4 ÀÇÌÄ bnj c−®Ó Set ve = 0 5 ²ÀÇÌ− cË®Ój cËþÓ If x==0,v=ve 6 Ë−Î Return v This implementation is invalid, however, since the dereferencing of ÓÉ by the ÀÇÌÊ instruction (line 2) occurs even when the test fails, causing a null pointer dereferencing error. Instead, this code must be compiled using branching code. Using conditional moves also does not always improve code efﬁciency. For example, if either the then-expr or the else-expr evaluation requires a signiﬁcant computation, then this effort is wasted when the corresponding condition does not hold. Compilers must take into account the relative performance of wasted computation versus the potential for performance penalty due to branch mispre- diction. In truth, they do not really have enough information to make this decision reliably; for example, they do not know how well the branches will follow pre- dictable patterns. Our experiments with gcc indicate that it only uses conditional moves when the two expressions can be computed very easily, for example, with single add instructions. In our experience, gcc uses conditional control transfers even in many cases where the cost of branch misprediction would exceed even more complex computations. Overall, then, we see that conditional data transfers offer an alternative strategy to conditional control transfers for implementing conditional operations. They can only be used in restricted cases, but these cases are fairly common and provide a much better match to the operation of modern processors. Section 3.6 Control 255 Practice Problem 3.20 (solution page 369) In the following C function, we have left the deﬁnition of operation ﬂ– incomplete: a®−ð©Å− ﬂ– mh •ÅÂÅÇÑÅ ÇÉ−ËþÎÇË hm Í³ÇËÎ þË©Î³fÍ³ÇËÎ Óg Õ Ë−ÎÏËÅ Ó ﬂ– oty Û When compiled, gcc generates the following assembly code: short arith(short x) x in %rdi þË©Î³x Ä−þÊ osfcË®©gj cË¾Ó Î−ÍÎÊ cË®©j cË®© ²ÀÇÌÅÍ cË®©j cË¾Ó ÍþËÊ brj cË¾Ó Ë−Î A. What operation is ﬂ–? B. Annotate the code to explain how it works. Practice Problem 3.21 (solution page 369) Starting with C code of the form Í³ÇËÎ Î−ÍÎfÍ³ÇËÎ Ój Í³ÇËÎ Ôg Õ Í³ÇËÎ ÌþÄ { y ©ð f gÕ ©ð f g ÌþÄ { y −ÄÍ− ÌþÄ { y Û −ÄÍ− ©ð f g ÌþÄ { y Ë−ÎÏËÅ ÌþÄy Û gcc generates the following assembly code: short test(short x, short y) x in %rdi, y in %rsi Î−ÍÎx Ä−þÊ opfcËÍ©gj cË¾Ó Î−ÍÎÊ cË®©j cË®© Á×− l‹p 256 Chapter 3 Machine-Level Representation of Programs ÀÇÌÊ cË®©j cË¾Ó ©ÀÏÄÊ cËÍ©j cË¾Ó ÀÇÌÊ cË®©j cË®Ó ÇËÊ cËÍ©j cË®Ó ²ÀÉÊ cËÍ©j cË®© ²ÀÇÌ×− cË®Ój cË¾Ó Ë−Î l‹px ©®©ÌÊ cËÍ©j cË®© ²ÀÉÊ bonj cËÍ© ²ÀÇÌ×− cË®©j cË¾Ó Ë−Î Fill in the missing expressions in the C code. 3.6.7 Loops C provides several looping constructs—namely, ®Ç-Ñ³©Ä−, Ñ³©Ä−, and ðÇË.No corresponding instructions exist in machine code. Instead, combinations of con- ditional tests and jumps are used to implement the effect of loops. Gcc and other compilers generate loop code based on the two basic loop patterns. We will study the translation of loops as a progression, starting with ®Ç-Ñ³©Ä− and then working toward ones with more complex implementations, covering both patterns. Do-While Loops The general form of a ®Ç-Ñ³©Ä− statement is as follows: ®Ç body-statement Ñ³©Ä− ftest-exprgy The effect of the loop is to repeatedly execute body-statement, evaluate test-expr, and continue the loop if the evaluation result is nonzero. Observe that body- statement is executed at least once. This general form can be translated into conditionals and ×ÇÎÇ statements as follows: ÄÇÇÉx body-statement Î{ test-expry ©ð fÎg ×ÇÎÇ ÄÇÇÉy That is, on each iteration the program evaluates the body statement and then the test expression. If the test succeeds, the program goes back for another iteration. Section 3.6 Control 257 (a) C code ÄÇÅ× ðþ²Îˆ®ÇfÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎ { oy ®Ç Õ Ë−ÍÏÄÎ h{ Åy Å { Åkoy Û Ñ³©Ä− fÅ | ogy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Equivalent goto version ÄÇÅ× ðþ²Îˆ®Çˆ×ÇÎÇfÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎ { oy ÄÇÇÉx Ë−ÍÏÄÎ h{ Åy Å { Åkoy ©ð fÅ | og ×ÇÎÇ ÄÇÇÉy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (c) Corresponding assembly-language code long fact_do(long n) n in %rdi 1 ðþ²Îˆ®Çx 2 ÀÇÌÄ boj c−þÓ Set result = 1 3 l‹px loop: 4 ©ÀÏÄÊ cË®©j cËþÓ Compute result *= n 5 ÍÏ¾Ê boj cË®© Decrement n 6 ²ÀÉÊ boj cË®© Compare n:1 7 Á× l‹p If >, goto loop 8 Ë−Éy Ë−Î Return Figure 3.19 Code for ®Ç–Ñ³©Ä− version of factorial program. A conditional jump causes the program to loop. As an example, Figure 3.19(a) shows an implementation of a routine to com- pute the factorial of its argument, written n!, with a ®Ç-Ñ³©Ä− loop. This function only computes the proper value for n> 0. Practice Problem 3.22 (solution page 369) A. Try to calculate 14! with a 32-bit int. Verify whether the computation of 14! overﬂows. B. What if the computation is done with a 64-bit long int? The goto code shown in Figure 3.19(b) shows how the loop gets turned into a lower-level combination of tests and conditional jumps. Following the initial- ization of Ë−ÍÏÄÎ, the program begins looping. First it executes the body of the loop, consisting here of updates to variables Ë−ÍÏÄÎ and n. It then tests whether n> 1, and, if so, it jumps back to the beginning of the loop. Figure 3.19(c) shows 258 Chapter 3 Machine-Level Representation of Programs Aside Reverse engineering loops A key to understanding how the generated assembly code relates to the original source code is to ﬁnd a mapping between program values and registers. This task was simple enough for the loop of Figure 3.19, but it can be much more challenging for more complex programs. The C compiler will often rearrange the computations, so that some variables in the C code have no counterpart in the machine code, and new values are introduced into the machine code that do not exist in the source code. Moreover, it will often try to minimize register usage by mapping multiple program values onto a single register. The process we described for ðþ²Îˆ®Ç works as a general strategy for reverse engineering loops. Look at how registers are initialized before the loop, updated and tested within the loop, and used after the loop. Each of these provides a clue that can be combined to solve a puzzle. Be prepared for surprising transformations, some of which are clearly cases where the compiler was able to optimize the code, and others where it is hard to explain why the compiler chose that particular strategy. the assembly code from which the goto code was generated. The conditional jump instruction Á× (line 7) is the key instruction in implementing a loop. It determines whether to continue iterating or to exit the loop. Reverse engineering assembly code, such as that of Figure 3.19(c), requires determining which registers are used for which program values. In this case, the mapping is fairly simple to determine: We know that n will be passed to the function in register cË®©. We can see register cËþÓ getting initialized to 1 (line 2). (Recall that, although the instruction has c−þÓ as its destination, it will also set the upper 4 bytes of cËþÓ to 0.) We can see that this register is also updated by multiplication on line 4. Furthermore, since cËþÓ is used to return the function value, it is often chosen to hold program values that are returned. We therefore conclude that cËþÓ corresponds to program value Ë−ÍÏÄÎ. Practice Problem 3.23 (solution page 370) For the C code Í³ÇËÎ ®ÑˆÄÇÇÉfÍ³ÇËÎ Óg Õ Í³ÇËÎ Ô { Ómwy Í³ÇËÎ hÉ { dÓy Í³ÇËÎ Å { rhÓy ®Ç Õ Ói{Ôy fhÉg i{ sy Åk{py Û Ñ³©Ä− fÅ | ngy Ë−ÎÏËÅ Óy Û gcc generates the following assembly code: Section 3.6 Control 259 short dw_loop(short x) x initially in %rdi 1 ®ÑˆÄÇÇÉx 2 ÀÇÌÊ cË®©j cË¾Ó 3 ÀÇÌÊ cË®©j cË²Ó 4 ©®©ÌÊ bwj cË²Ó 5 Ä−þÊ fjcË®©jrgj cË®Ó 6 l‹px 7 Ä−þÊ sfcË¾ÓjcË²Ógj cË²Ó 8 ÍÏ¾Ê boj cË®Ó 9 Î−ÍÎÊ cË®Ój cË®Ó 10 Á× l‹p 11 Ë−Éy Ë−Î A. Which registers are used to hold program values Ó, Ô, and Å? B. How has the compiler eliminated the need for pointer variable É and the pointer dereferencing implied by the expression fhÉgi{s? C. Add annotations to the assembly code describing the operation of the pro- gram, similar to those shown in Figure 3.19(c). While Loops The general form of a Ñ³©Ä− statement is as follows: Ñ³©Ä− ftest-exprg body-statement It differs from ®Ç-Ñ³©Ä− in that test-expr is evaluated and the loop is potentially terminated before the ﬁrst execution of body-statement. There are a number of ways to translate a Ñ³©Ä− loop into machine code, two of which are used in code generated by gcc. Both use the same loop structure as we saw for ®Ç-Ñ³©Ä− loops but differ in how to implement the initial test. The ﬁrst translation method, which we refer to as jump to middle, performs the initial test by performing an unconditional jump to the test at the end of the loop. It can be expressed by the following template for translating from the general Ñ³©Ä− loop form to goto code: ×ÇÎÇ Î−ÍÎy ÄÇÇÉx body-statement Î−ÍÎx Î{ test-expry ©ð fÎg ×ÇÎÇ ÄÇÇÉy As an example, Figure 3.20(a) shows an implementation of the factorial func- tion using a Ñ³©Ä− loop. This function correctly computes 0! = 1. The adjacent 260 Chapter 3 Machine-Level Representation of Programs (a) C code ÄÇÅ× ðþ²ÎˆÑ³©Ä−fÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎ { oy Ñ³©Ä− fÅ | og Õ Ë−ÍÏÄÎ h{ Åy Å { Åkoy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Equivalent goto version ÄÇÅ× ðþ²ÎˆÑ³©Ä−ˆÁÀˆ×ÇÎÇfÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎ { oy ×ÇÎÇ Î−ÍÎy ÄÇÇÉx Ë−ÍÏÄÎ h{ Åy Å { Åkoy Î−ÍÎx ©ð fÅ | og ×ÇÎÇ ÄÇÇÉy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (c) Corresponding assembly-language code long fact_while(long n) n in %rdi ðþ²ÎˆÑ³©Ä−x ÀÇÌÄ boj c−þÓ Set result = 1 ÁÀÉ l‹s Goto test l‹tx loop: ©ÀÏÄÊ cË®©j cËþÓ Compute result *= n ÍÏ¾Ê boj cË®© Decrement n l‹sx test: ²ÀÉÊ boj cË®© Compare n:1 Á× l‹t If >, goto loop Ë−Éy Ë−Î Return Figure 3.20 C and assembly code for Ñ³©Ä− version of factorial using jump-to- middle translation. The C function ðþ²ÎˆÑ³©Ä−ˆÁÀˆ×ÇÎÇ illustrates the operation of the assembly-code version. function ðþ²ÎˆÑ³©Ä−ˆÁÀˆ×ÇÎÇ (Figure 3.20(b)) is a C rendition of the assembly code generated by gcc when optimization is speciﬁed with the command-line op- tion kﬂ×. Comparing the goto code generated for ðþ²ÎˆÑ³©Ä− (Figure 3.20(b)) to that for ðþ²Îˆ®Ç (Figure 3.19(b)), we see that they are very similar, except that the statement ×ÇÎÇ Î−ÍÎ before the loop causes the program to ﬁrst perform the test of Å before modifying the values of Ë−ÍÏÄÎ or Å. The bottom portion of the ﬁgure (Figure 3.20(c)) shows the actual assembly code generated. Practice Problem 3.24 (solution page 371) For C code having the general form Í³ÇËÎ ÄÇÇÉˆÑ³©Ä−fÍ³ÇËÎ þj Í³ÇËÎ ¾g Õ Section 3.6 Control 261 Í³ÇËÎ Ë−ÍÏÄÎ { y Ñ³©Ä− f gÕ Ë−ÍÏÄÎ { y þ{ y Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û gcc, run with command-line option kﬂ×, produces the following code: short loop_while(short a, short b) a in %rdi, b in %rsi 1 ÄÇÇÉˆÑ³©Ä−x 2 ÀÇÌÄ bnj c−þÓ 3 ÁÀÉ l‹p 4 l‹qx 5 Ä−þÊ fjcËÍ©jcË®©gj cË®Ó 6 þ®®Ê cË®Ój cËþÓ 7 ÍÏ¾Ê boj cË®© 8 l‹px 9 ²ÀÉÊ cËÍ©j cË®© 10 Á× l‹q 11 Ë−Éy Ë−Î We can see that the compiler used a jump-to-middle translation, using the ÁÀÉ instruction on line 3 to jump to the test starting with label l‹p. Fill in the missing parts of the C code. The second translation method, which we refer to as guarded do, ﬁrst trans- forms the code into a ®Ç-Ñ³©Ä− loop by using a conditional branch to skip over the loop if the initial test fails. Gcc follows this strategy when compiling with higher levels of optimization, for example, with command-line option kﬂo. This method can be expressed by the following template for translating from the general Ñ³©Ä− loop form to a ®Ç-Ñ³©Ä− loop: Î{ test-expry ©ð f_Îg ×ÇÎÇ ®ÇÅ−y ®Ç body-statement Ñ³©Ä− ftest-exprgy ®ÇÅ−x This, in turn, can be transformed into goto code as Î{ test-expry ©ð f_Îg ×ÇÎÇ ®ÇÅ−y 262 Chapter 3 Machine-Level Representation of Programs ÄÇÇÉx body-statement Î{ test-expry ©ð fÎg ×ÇÎÇ ÄÇÇÉy ®ÇÅ−x Using this implementation strategy, the compiler can often optimize the initial test, for example, determining that the test condition will always hold. As an example, Figure 3.21 shows the same C code for a factorial function as in Figure 3.20, but demonstrates the compilation that occurs when gcc is given command-line option kﬂo. Figure 3.21(c) shows the actual assembly code generated, while Figure 3.21(b) renders this assembly code in a more readable C representation. Referring to this goto code, we see that the loop will be skipped if n ≤ 1, for the initial value of n. The loop itself has the same general structure as that generated for the ®Ç-Ñ³©Ä− version of the function (Figure 3.19). One interesting feature, however, is that the loop test (line 9 of the assembly code) has been changed from n> 1 in the original C code to n ̸= 1. The compiler has determined that the loop can only be entered when n> 1, and that decrementing n will result in either n> 1or n = 1. Therefore, the test n ̸= 1 will be equivalent to the test n ≤ 1. Practice Problem 3.25 (solution page 371) For C code having the general form ÄÇÅ× ÄÇÇÉˆÑ³©Ä−pfÄÇÅ× þj ÄÇÅ× ¾g Õ ÄÇÅ× Ë−ÍÏÄÎ { y Ñ³©Ä− f gÕ Ë−ÍÏÄÎ { y ¾{ y Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û gcc, run with command-line option kﬂo, produces the following code: a in %rdi, b in %rsi 1 ÄÇÇÉˆÑ³©Ä−px 2 Î−ÍÎÊ cËÍ©j cËÍ© 3 ÁÄ− l‹v 4 ÀÇÌÊ cËÍ©j cËþÓ 5 l‹ux 6 ©ÀÏÄÊ cË®©j cËþÓ 7 ÍÏ¾Ê cË®©j cËÍ© 8 Î−ÍÎÊ cËÍ©j cËÍ© Section 3.6 Control 263 (a) C code ÄÇÅ× ðþ²ÎˆÑ³©Ä−fÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎ { oy Ñ³©Ä− fÅ | og Õ Ë−ÍÏÄÎ h{ Åy Å { Åkoy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Equivalent goto version ÄÇÅ× ðþ²ÎˆÑ³©Ä−ˆ×®ˆ×ÇÎÇfÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎ { oy ©ð fÅ z{ og ×ÇÎÇ ®ÇÅ−y ÄÇÇÉx Ë−ÍÏÄÎ h{ Åy Å { Åkoy ©ð fÅ _{ og ×ÇÎÇ ÄÇÇÉy ®ÇÅ−x Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (c) Corresponding assembly-language code long fact_while(long n) n in %rdi 1 ðþ²ÎˆÑ³©Ä−x 2 ²ÀÉÊ boj cË®© Compare n:1 3 ÁÄ− l‹u If <=, goto done 4 ÀÇÌÄ boj c−þÓ Set result = 1 5 l‹tx loop: 6 ©ÀÏÄÊ cË®©j cËþÓ Compute result *= n 7 ÍÏ¾Ê boj cË®© Decrement n 8 ²ÀÉÊ boj cË®© Compare n:1 9 ÁÅ− l‹t If !=, goto loop 10 Ë−Éy Ë−Î Return 11 l‹ux done: 12 ÀÇÌÄ boj c−þÓ Compute result = 1 13 Ë−Î Return Figure 3.21 C and assembly code for Ñ³©Ä− version of factorial using guarded- do translation. The ðþ²ÎˆÑ³©Ä−ˆ×®ˆ×ÇÎÇ function illustrates the operation of the assembly-code version. 9 Á× l‹u 10 Ë−Éy Ë−Î 11 l‹vx 12 ÀÇÌÊ cËÍ©j cËþÓ 13 Ë−Î We can see that the compiler used a guarded-do translation, using the ÁÄ− instruction on line 3 to skip over the loop code when the initial test fails. Fill in the missing parts of the C code. Note that the control structure in the assembly 264 Chapter 3 Machine-Level Representation of Programs code does not exactly match what would be obtained by a direct translation of the C code according to our translation rules. In particular, it has two different Ë−Î instructions (lines 10 and 13). However, you can ﬁll out the missing portions of the C code in a way that it will have equivalent behavior to the assembly code. Practice Problem 3.26 (solution page 372) A function Î−ÍÎˆÇÅ− has the following overall structure: Í³ÇËÎ Î−ÍÎˆÇÅ−fÏÅÍ©×Å−® Í³ÇËÎ Óg Õ Í³ÇËÎ ÌþÄ { oy Ñ³©Ä− f lll g Õ l l l Û Ë−ÎÏËÅ llly Û The gcc C compiler generates the following assembly code: short test_one(unsigned short x) x in %rdi 1 Î−ÍÎˆÇÅ−x 2 ÀÇÌÄ boj c−þÓ 3 ÁÀÉ l‹s 4 l‹tx 5 ÓÇËÊ cË®©j cËþÓ 6 Í³ËÊ cË®© Shift right by 1 7 l‹sx 8 Î−ÍÎÊ cË®©j cË®© 9 ÁÅ− l‹t 10 þÅ®Ä bnj c−þÓ 11 Ë−Î Reverse engineer the operation of this code and then do the following: A. Determine what loop translation method was used. B. Use the assembly-code version to ﬁll in the missing parts of the C code. C. Describe in English what this function computes. For Loops The general form of a ðÇË loop is as follows: ðÇË finit-expry test-expry update-exprg body-statement Section 3.6 Control 265 The C language standard states (with one exception, highlighted in Problem 3.29) that the behavior of such a loop is identical to the following code using a Ñ³©Ä− loop: init-expry Ñ³©Ä− ftest-exprgÕ body-statement update-expry Û The program ﬁrst evaluates the initialization expression init-expr. It enters a loop where it ﬁrst evaluates the test condition test-expr, exiting if the test fails, then executes the body of the loop body-statement, and ﬁnally evaluates the update expression update-expr. The code generated by gcc for a ðÇË loop then follows one of our two trans- lation strategies for Ñ³©Ä− loops, depending on the optimization level. That is, the jump-to-middle strategy yields the goto code init-expry ×ÇÎÇ Î−ÍÎy ÄÇÇÉx body-statement update-expry Î−ÍÎx Î{ test-expry ©ð fÎg ×ÇÎÇ ÄÇÇÉy while the guarded-do strategy yields init-expry Î{ test-expry ©ð f_Îg ×ÇÎÇ ®ÇÅ−y ÄÇÇÉx body-statement update-expry Î{ test-expry ©ð fÎg ×ÇÎÇ ÄÇÇÉy ®ÇÅ−x As examples, consider a factorial function written with a ðÇË loop: ÄÇÅ× ðþ²ÎˆðÇËfÄÇÅ× Åg Õ ÄÇÅ× ©y ÄÇÅ× Ë−ÍÏÄÎ { oy 266 Chapter 3 Machine-Level Representation of Programs ðÇË f© { py © z{ Åy ©iig Ë−ÍÏÄÎ h{ ©y Ë−ÎÏËÅ Ë−ÍÏÄÎy Û As shown, the natural way of writing a factorial function with a ðÇË loop is to multiply factors from 2 up to n, and so this function is quite different from the code we showed using either a Ñ³©Ä− or a ®Ç-Ñ³©Ä− loop. We can identify the different components of the ðÇË loop in this code as follows: init-expr ©{p test-expr ©z{Å update-expr ©ii body-statement Ë−ÍÏÄÎ h{ ©y Substituting these components into the template we have shown to transform a ðÇË loop into a Ñ³©Ä− loop yields the following: ÄÇÅ× ðþ²ÎˆðÇËˆÑ³©Ä−fÄÇÅ× Åg Õ ÄÇÅ×©{py ÄÇÅ× Ë−ÍÏÄÎ { oy Ñ³©Ä− f© z{ Åg Õ Ë−ÍÏÄÎ h{ ©y ©iiy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û Applying the jump-to-middle transformation to the Ñ³©Ä− loop then yields the following version in goto code: ÄÇÅ× ðþ²ÎˆðÇËˆÁÀˆ×ÇÎÇfÄÇÅ× Åg Õ ÄÇÅ×©{py ÄÇÅ× Ë−ÍÏÄÎ { oy ×ÇÎÇ Î−ÍÎy ÄÇÇÉx Ë−ÍÏÄÎ h{ ©y ©iiy Î−ÍÎx ©ð f© z{ Åg ×ÇÎÇ ÄÇÇÉy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û Section 3.6 Control 267 Indeed, a close examination of the assembly code produced by gcc with command-line option kﬂ× closely follows this template: long fact_for(long n) n in %rdi ðþ²ÎˆðÇËx ÀÇÌÄ boj c−þÓ Set result = 1 ÀÇÌÄ bpj c−®Ó Seti=2 ÁÀÉ l‹v Goto test l‹wx loop: ©ÀÏÄÊ cË®Ój cËþÓ Compute result *= i þ®®Ê boj cË®Ó Increment i l‹vx test: ²ÀÉÊ cË®©j cË®Ó Compare i:n ÁÄ− l‹w If <=, goto loop Ë−Éy Ë−Î Return Practice Problem 3.27 (solution page 372) Write goto code for a function called ð©¾ÇÅþ²²© to print ﬁbonacci numbers using a Ñ³©Ä− loop. Apply the guarded-do transformation. We see from this presentation that all three forms of loops in C—®Ç-Ñ³©Ä−, Ñ³©Ä−, and ðÇË—can be translated by a simple strategy, generating code that con- tains one or more conditional branches. Conditional transfer of control provides the basic mechanism for translating loops into machine code. Practice Problem 3.28 (solution page 372) A function Î−ÍÎˆÎÑÇ has the following overall structure: Í³ÇËÎ Î−ÍÎˆÎÑÇfÏÅÍ©×Å−® Í³ÇËÎ Óg Õ Í³ÇËÎ ÌþÄ { ny Í³ÇËÎ ©y ðÇËflllylllylllgÕ l l l Û Ë−ÎÏËÅ ÌþÄy Û The gcc C compiler generates the following assembly code: test fun_b(unsigned test x) x in %rdi 1 Î−ÍÎˆÎÑÇx 2 ÀÇÌÄ boj c−®Ó 268 Chapter 3 Machine-Level Representation of Programs 3 ÀÇÌÄ btsj c−þÓ 4 l‹onx 5 ÀÇÌÊ cË®©j cË²Ó 6 þÅ®Ä boj c−²Ó 7 þ®®Ê cËþÓj cËþÓ 8 ÇËÊ cË²Ój cËþÓ 9 Í³ËÊ cË®© Shift right by 1 10 þ®®Ê boj cË®Ó 11 ÁÅ− l‹on 12 Ë−Éy Ë−Î Reverse engineer the operation of this code and then do the following: A. Use the assembly-code version to ﬁll in the missing parts of the C code. B. Explain why there is neither an initial test before the loop nor an initial jump to the test portion of the loop. C. Describe in English what this function computes. Practice Problem 3.29 (solution page 373) Executing a ²ÇÅÎ©ÅÏ− statement in C causes the program to jump to the end of the current loop iteration. The stated rule for translating a ðÇË loop into a Ñ³©Ä− loop needs some reﬁnement when dealing with ²ÇÅÎ©ÅÏ− statements. For example, consider the following code: mh ¥ÓþÀÉÄ− Çð ðÇË ÄÇÇÉ ²ÇÅÎþ©Å©Å× þ ²ÇÅÎ©ÅÏ− ÍÎþÎ−À−ÅÎ hm mh ·ÏÀ −Ì−Å ÅÏÀ¾−ËÍ ¾−ÎÑ−−Å n þÅ® w hm ÄÇÅ× ÍÏÀ { ny ÄÇÅ× ©y ðÇË f© { ny © z ony ©iig Õ ©ð f© d og ²ÇÅÎ©ÅÏ−y ÍÏÀ i{ ©y Û A. What would we get if we naively applied our rule for translating the ðÇË loop into a Ñ³©Ä− loop? What would be wrong with this code? B. How could you replace the ²ÇÅÎ©ÅÏ− statement with a ×ÇÎÇ statement to ensure that the Ñ³©Ä− loop correctly duplicates the behavior of the ðÇË loop? 3.6.8 Switch Statements A ÍÑ©Î²³ statement provides a multiway branching capability based on the value of an integer index. They are particularly useful when dealing with tests where Section 3.6 Control 269 there can be a large number of possible outcomes. Not only do they make the C code more readable, but they also allow an efﬁcient implementation using a data structure called a jump table. A jump table is an array where entry i is the address of a code segment implementing the action the program should take when the switch index equals i. The code performs an array reference into the jump table using the switch index to determine the target for a jump instruction. The advantage of using a jump table over a long sequence of if-else statements is that the time taken to perform the switch is independent of the number of switch cases. Gcc selects the method of translating a ÍÑ©Î²³ statement based on the number of cases and the sparsity of the case values. Jump tables are used when there are a number of cases (e.g., four or more) and they span a small range of values. Figure 3.22(a) shows an example of a C ÍÑ©Î²³ statement. This example has a number of interesting features, including case labels that do not span a contiguous range (there are no labels for cases 101 and 105), cases with multiple labels (cases 104 and 106), and cases that fall through to other cases (case 102) because the code for the case does not end with a ¾Ë−þÂ statement. Figure 3.23 shows the assembly code generated when compiling ÍÑ©Î²³ˆ−×. The behavior of this code is shown in C as the procedure ÍÑ©Î²³ˆ−×ˆ©ÀÉÄ in Figure 3.22(b). This code makes use of support provided by gcc for jump tables, as an extension to the C language. The array ÁÎ contains seven entries, each of which is the address of a block of code. These locations are deﬁned by labels in the code and indicated in the entries in ÁÎ by code pointers, consisting of the labels preﬁxed by dd. (Recall that the operator ‘d’ creates a pointer for a data value. In making this extension, the authors of gcc created a new operator dd to create a pointer for a code location.) We recommend that you study the C procedure ÍÑ©Î²³ˆ−×ˆ©ÀÉÄ and how it relates to the assembly-code version. Our original C code has cases for values 100, 102–104, and 106, but the switch variable Å can be an arbitrary integer. The compiler ﬁrst shifts the range to between 0 and 6 by subtracting 100 from Å, creating a new program variable that we call ©Å®−Ó in our C version. It further simpliﬁes the branching possibilities by treating ©Å®−Ó as an unsigned value, making use of the fact that negative numbers in a two’s-complement representation map to large positive numbers in an unsigned representation. It can therefore test whether ©Å®−Ó is outside of the range 0–6 by testing whether it is greater than 6. In the C and assembly code, there are ﬁve distinct locations to jump to, based on the value of ©Å®−Ó. These are ÄÇ²ˆ¡ (identiﬁed in the assembly code as l‹q), ÄÇ²ˆ¢ (l‹s), ÄÇ²ˆ£ (l‹t), ÄÇ²ˆ⁄ (l‹u), and ÄÇ²ˆ®−ð (l‹v), where the latter is the destination for the default case. Each of these labels identiﬁes a block of code implementing one of the case branches. In both the C and the assembly code, the program compares ©Å®−Ó to 6 and jumps to the code for the default case if it is greater. The key step in executing a ÍÑ©Î²³ statement is to access a code location through the jump table. This occurs in line 16 in the C code, with a ×ÇÎÇ statement that references the jump table ÁÎ. This computed goto is supported by gcc as an extension to the C language. In our assembly-code version, a similar operation occurs on line 5, where the ÁÀÉ instruction’s operand is preﬁxed with ‘h’, indicating 270 Chapter 3 Machine-Level Representation of Programs (a) Switch statement ÌÇ©® ÍÑ©Î²³ˆ−×fÄÇÅ× Ój ÄÇÅ× Åj ÄÇÅ× h®−ÍÎg Õ ÄÇÅ× ÌþÄ { Óy ÍÑ©Î²³ fÅg Õ ²þÍ− onnx ÌþÄ h{ oqy ¾Ë−þÂy ²þÍ− onpx ÌþÄ i{ ony mh ƒþÄÄ Î³ËÇÏ×³ hm ²þÍ− onqx ÌþÄ i{ ooy ¾Ë−þÂy ²þÍ− onrx ²þÍ− ontx ÌþÄ h{ ÌþÄy ¾Ë−þÂy ®−ðþÏÄÎx ÌþÄ{ny Û h®−ÍÎ { ÌþÄy Û (b) Translation into extended C 1 ÌÇ©® ÍÑ©Î²³ˆ−×ˆ©ÀÉÄfÄÇÅ× Ój ÄÇÅ× Åj 2 ÄÇÅ× h®−ÍÎg 3 Õ 4 mh ¶þ¾Ä− Çð ²Ç®− ÉÇ©ÅÎ−ËÍ hm 5 ÍÎþÎ©² ÌÇ©® hÁÎ‰u` { Õ 6 ddÄÇ²ˆ¡j ddÄÇ²ˆ®−ðj ddÄÇ²ˆ¢j 7 ddÄÇ²ˆ£j ddÄÇ²ˆ⁄j ddÄÇ²ˆ®−ðj 8 ddÄÇ²ˆ⁄ 9 Ûy 10 ÏÅÍ©×Å−® ÄÇÅ× ©Å®−Ó{Åk onny 11 ÄÇÅ× ÌþÄy 12 13 ©ð f©Å®−Ó | tg 14 ×ÇÎÇ ÄÇ²ˆ®−ðy 15 mh ›ÏÄÎ©ÑþÔ ¾ËþÅ²³ hm 16 ×ÇÎÇ hÁÎ‰©Å®−Ó`y 17 18 ÄÇ²ˆ¡x mh £þÍ− onn hm 19 ÌþÄ{Óhoqy 20 ×ÇÎÇ ®ÇÅ−y 21 ÄÇ²ˆ¢x mh £þÍ− onp hm 22 Ó{Óiony 23 mh ƒþÄÄ Î³ËÇÏ×³ hm 24 ÄÇ²ˆ£x mh £þÍ− onq hm 25 ÌþÄ{Óiooy 26 ×ÇÎÇ ®ÇÅ−y 27 ÄÇ²ˆ⁄x mh £þÍ−Í onrj ont hm 28 ÌþÄ{ÓhÓy 29 ×ÇÎÇ ®ÇÅ−y 30 ÄÇ²ˆ®−ðx mh ⁄−ðþÏÄÎ ²þÍ− hm 31 ÌþÄ{ny 32 ®ÇÅ−x 33 h®−ÍÎ { ÌþÄy 34 Û Figure 3.22 Example ÍÑ©Î²³ statement and its translation into extended C. The translation shows the structure of jump table ÁÎ and how it is accessed. Such tables are supported by gcc as an extension to the C language. an indirect jump, and the operand speciﬁes a memory location indexed by register c−þÓ, which holds the value of ©Å®−Ó. (We will see in Section 3.8 how array references are translated into machine code.) Our C code declares the jump table as an array of seven elements, each of which is a pointer to a code location. These elements span values 0–6 of Section 3.6 Control 271 void switch_eg(long x, long n, long *dest) x in %rdi, n in %rsi, dest in %rdx 1 ÍÑ©Î²³ˆ−×x 2 ÍÏ¾Ê bonnj cËÍ© Compute index = n-100 3 ²ÀÉÊ btj cËÍ© Compare index:6 4 Áþ l‹v If >, goto ÄÇ²ˆ®−ð 5 ÁÀÉ hl‹rfjcËÍ©jvg Goto *jg[index] 6 l‹qx ÄÇ²ˆ¡x 7 Ä−þÊ fcË®©jcË®©jpgj cËþÓ 3*x 8 Ä−þÊ fcË®©jcËþÓjrgj cË®© val = 13*x 9 ÁÀÉ l‹p Goto ®ÇÅ− 10 l‹sx ÄÇ²ˆ¢x 11 þ®®Ê bonj cË®© x=x+10 12 l‹tx ÄÇ²ˆ£x 13 þ®®Ê booj cË®© val=x+11 14 ÁÀÉ l‹p Goto ®ÇÅ− 15 l‹ux ÄÇ²ˆ⁄x 16 ©ÀÏÄÊ cË®©j cË®© val=x*x 17 ÁÀÉ l‹p Goto ®ÇÅ− 18 l‹vx ÄÇ²ˆ®−ðx 19 ÀÇÌÄ bnj c−®© val=0 20 l‹px ®ÇÅ−x 21 ÀÇÌÊ cË®©j fcË®Óg *dest = val 22 Ë−Î Return Figure 3.23 Assembly code for ÍÑ©Î²³ statement example in Figure 3.22. ©Å®−Ó, corresponding to values 100–106 of Å. Observe that the jump table handles duplicate cases by simply having the same code label (ÄÇ²ˆ⁄) for entries 4 and 6, and it handles missing cases by using the label for the default case (ÄÇ²ˆ®−ð)as entries 1 and 5. In the assembly code, the jump table is indicated by the following declarations, to which we have added comments: 1 lÍ−²Î©ÇÅ lËÇ®þÎþ 2 lþÄ©×Å v Align address to multiple of 8 3 l‹rx 4 lÊÏþ® l‹q Case 100: loc_A 5 lÊÏþ® l‹v Case 101: loc_def 6 lÊÏþ® l‹s Case 102: loc_B 7 lÊÏþ® l‹t Case 103: loc_C 8 lÊÏþ® l‹u Case 104: loc_D 9 lÊÏþ® l‹v Case 105: loc_def 10 lÊÏþ® l‹u Case 106: loc_D 272 Chapter 3 Machine-Level Representation of Programs These declarations state that within the segment of the object-code ﬁle called lËÇ®þÎþ (for “read-only data”), there should be a sequence of seven “quad” (8- byte) words, where the value of each word is given by the instruction address associated with the indicated assembly-code labels (e.g., l‹q). Label l‹r marks the start of this allocation. The address associated with this label serves as the base for the indirect jump (line 5). The different code blocks (C labels ÄÇ²ˆ¡ through ÄÇ²ˆ⁄ and ÄÇ²ˆ®−ð) im- plement the different branches of the ÍÑ©Î²³ statement. Most of them simply compute a value for ÌþÄ and then go to the end of the function. Similarly, the assembly-code blocks compute a value for register cË®© and jump to the position indicated by label l‹p at the end of the function. Only the code for case label 102 does not follow this pattern, to account for the way the code for this case falls through to the block with label 103 in the original C code. This is handled in the assembly-code block starting with label l‹s, by omitting the ÁÀÉ instruction at the end of the block, so that the code continues execution of the next block. Simi- larly, the C version ÍÑ©Î²³ˆ−×ˆ©ÀÉÄ has no ×ÇÎÇ statement at the end of the block starting with label ÄÇ²ˆ¢. Examining all of this code requires careful study, but the key point is to see that the use of a jump table allows a very efﬁcient way to implement a multiway branch. In our case, the program could branch to ﬁve distinct locations with a single jump table reference. Even if we had a ÍÑ©Î²³ statement with hundreds of cases, they could be handled by a single jump table access. Practice Problem 3.30 (solution page 374) In the C function that follows, we have omitted the body of the ÍÑ©Î²³ statement. In the C code, the case labels did not span a contiguous range, and some cases had multiple labels. ÌÇ©® ÍÑ©Î²³pfÍ³ÇËÎ Ój Í³ÇËÎ h®−ÍÎg Õ Í³ÇËÎ ÌþÄ { ny ÍÑ©Î²³ fÓg Õ l l l Body of switch statement omitted Û h®−ÍÎ { ÌþÄy Û In compiling the function, gcc generates the assembly code that follows for the initial part of the procedure, with variable Ó in cË®©: void switch2(short x, short *dest) x in %rdi 1 ÍÑ©Î²³px 2 þ®®Ê bpj cË®© 3 ²ÀÉÊ bvj cË®© 4 Áþ l‹p 5 ÁÀÉ hl‹rfjcË®©jvg Section 3.6 Control 273 It generates the following code for the jump table: 1 l‹rx 2 lÊÏþ® l‹w 3 lÊÏþ® l‹s 4 lÊÏþ® l‹t 5 lÊÏþ® l‹u 6 lÊÏþ® l‹p 7 lÊÏþ® l‹u 8 lÊÏþ® l‹v 9 lÊÏþ® l‹p 10 lÊÏþ® l‹s Based on this information, answer the following questions: A. What were the values of the case labels in the ÍÑ©Î²³ statement? B. What cases had multiple labels in the C code? Practice Problem 3.31 (solution page 374) For a C function ÍÑ©Î²³−Ë with the general structure ÌÇ©® ÍÑ©Î²³−ËfÄÇÅ× þj ÄÇÅ× ¾j ÄÇÅ× ²j ÄÇÅ× h®−ÍÎg Õ ÄÇÅ× ÌþÄy ÍÑ©Î²³fþg Õ ²þÍ− x mh£þÍ−¡hm ²{ y mh ƒþÄÄ Î³ËÇÏ×³ hm ²þÍ− x mh£þÍ−¢hm ÌþÄ { y ¾Ë−þÂy ²þÍ− x mh£þÍ−£hm ²þÍ− x mh£þÍ−⁄hm ÌþÄ { y ¾Ë−þÂy ²þÍ− x mh£þÍ−¥hm ÌþÄ { y ¾Ë−þÂy ®−ðþÏÄÎx ÌþÄ { y Û h®−ÍÎ { ÌþÄy Û gcc generates the assembly code and jump table shown in Figure 3.24. Fill in the missing parts of the C code. Except for the ordering of case labels £ and ⁄, there is only one way to ﬁt the different cases into the template. 274 Chapter 3 Machine-Level Representation of Programs (a) Code void switcher(long a, long b, long c, long *dest) a in %rsi, b in %rdi, c in %rdx, d in %rcx 1 ÍÑ©Î²³−Ëx 2 ²ÀÉÊ buj cË®© 3 Áþ l‹p 4 ÁÀÉ hl‹rfjcË®©jvg 5 lÍ−²Î©ÇÅ lËÇ®þÎþ 6 l‹ux 7 ÓÇËÊ bosj cËÍ© 8 ÀÇÌÊ cËÍ©j cË®Ó 9 l‹qx 10 Ä−þÊ oopfcË®Ógj cË®© 11 ÁÀÉ l‹t 12 l‹sx 13 Ä−þÊ fcË®ÓjcËÍ©gj cË®© 14 ÍþÄÊ bpj cË®© 15 ÁÀÉ l‹t 16 l‹px 17 ÀÇÌÊ cËÍ©j cË®© 18 l‹tx 19 ÀÇÌÊ cË®©j fcË²Óg 20 Ë−Î (b) Jump table 1 l‹rx 2 lÊÏþ® l‹q 3 lÊÏþ® l‹p 4 lÊÏþ® l‹s 5 lÊÏþ® l‹p 6 lÊÏþ® l‹t 7 lÊÏþ® l‹u 8 lÊÏþ® l‹p 9 lÊÏþ® l‹s Figure 3.24 Assembly code and jump table for Problem 3.31. 3.7 Procedures Procedures are a key abstraction in software. They provide a way to package code that implements some functionality with a designated set of arguments and an optional return value. This function can then be invoked from different points in a program. Well-designed software uses procedures as an abstraction mechanism, hiding the detailed implementation of some action while providing a clear and concise interface deﬁnition of what values will be computed and what effects the procedure will have on the program state. Procedures come in many guises Section 3.7 Procedures 275 in different programming languages—functions, methods, subroutines, handlers, and so on—but they all share a general set of features. There are many different attributes that must be handled when providing machine-level support for procedures. For discussion purposes, suppose proce- dure – calls procedure †, and † then executes and returns back to –. These actions involve one or more of the following mechanisms: Passing control. The program counter must be set to the starting address of the code for † upon entry and then set to the instruction in – following the call to † upon return. Passing data. – must be able to provide one or more parameters to †, and † must be able to return a value back to –. Allocating and deallocating memory. † may need to allocate space for local variables when it begins and then free that storage before it returns. The x86-64 implementation of procedures involves a combination of special instructions and a set of conventions on how to use the machine resources, such as the registers and the program memory. Great effort has been made to minimize the overhead involved in invoking a procedure. As a consequence, it follows what can be seen as a minimalist strategy, implementing only as much of the above set of mechanisms as is required for each particular procedure. In our presentation, we build up the different mechanisms step by step, ﬁrst describing control, then data passing, and, ﬁnally, memory management. 3.7.1 The Run-Time Stack A key feature of the procedure-calling mechanism of C, and of most other lan- guages, is that it can make use of the last-in, ﬁrst-out memory management disci- pline provided by a stack data structure. Using our example of procedure – calling procedure †, we can see that while † is executing, –, along with any of the proce- dures in the chain of calls up to –, is temporarily suspended. While † is running, only it will need the ability to allocate new storage for its local variables or to set up a call to another procedure. On the other hand, when † returns, any local storage it has allocated can be freed. Therefore, a program can manage the storage required by its procedures using a stack, where the stack and the program registers store the information required for passing control and data, and for allocating memory. As – calls †, control and data information are added to the end of the stack. This information gets deallocated when – returns. As described in Section 3.4.4, the x86-64 stack grows toward lower addresses and the stack pointer cËÍÉ points to the top element of the stack. Data can be stored on and retrieved from the stack using the ÉÏÍ³Ê and ÉÇÉÊ instructions. Space for data with no speciﬁed initial value can be allocated on the stack by simply decrementing the stack pointer by an appropriate amount. Similarly, space can be deallocated by incrementing the stack pointer. When an x86-64 procedure requires storage beyond what it can hold in reg- isters, it allocates space on the stack. This region is referred to as the procedure’s 276 Chapter 3 Machine-Level Representation of Programs Figure 3.25 General stack frame structure. The stack can be used for passing arguments, for storing return information, for saving registers, and for local storage. Portions may be omitted when not needed.. . .. . . Stack “bottom” Stack “top” Argument n Argument 7 Argument build area Return address Local variables Saved registers Stack pointer %rsp Earlier frames Increasing address Frame for calling function P Frame for executing function Q stack frame. Figure 3.25 shows the overall structure of the run-time stack, includ- ing its partitioning into stack frames, in its most general form. The frame for the currently executing procedure is always at the top of the stack. When procedure – calls procedure †, it will push the return address onto the stack, indicating where within – the program should resume execution once † returns. We consider the return address to be part of –’s stack frame, since it holds state relevant to –.The code for † allocates the space required for its stack frame by extending the cur- rent stack boundary. Within that space, it can save the values of registers, allocate Section 3.7 Procedures 277 space for local variables, and set up arguments for the procedures it calls. The stack frames for most procedures are of ﬁxed size, allocated at the beginning of the procedure. Some procedures, however, require variable-size frames. This issue is discussed in Section 3.10.5. Procedure – can pass up to six integral values (i.e., pointers and integers) on the stack, but if † requires more arguments, these can be stored by – within its stack frame prior to the call. In the interest of space and time efﬁciency, x86-64 procedures allocate only the portions of stack frames they require. For example, many procedures have six or fewer arguments, and so all of their parameters can be passed in registers. Thus, parts of the stack frame diagrammed in Figure 3.25 may be omitted. Indeed, many functions do not even require a stack frame. This occurs when all of the local variables can be held in registers and the function does not call any other functions (sometimes referred to as a leaf procedure, in reference to the tree structure of procedure calls). For example, none of the functions we have examined thus far required stack frames. 3.7.2 Control Transfer Passing control from function – to function † involves simply setting the program counter (PC) to the starting address of the code for †. However, when it later comes time for † to return, the processor must have some record of the code location where it should resume the execution of –. This information is recorded in x86-64 machines by invoking procedure † with the instruction ²þÄÄ †. This instruction pushes an address A onto the stack and sets the PC to the beginning of †. The pushed address A is referred to as the return address and is computed as the address of the instruction immediately following the ²þÄÄ instruction. The counterpart instruction Ë−Î pops an address A off the stack and sets the PC to A. The general forms of the ²þÄÄ and Ë−Î instructions are described as follows: Instruction Description ²þÄÄ Label Procedure call ²þÄÄ hOperand Procedure call Ë−Î Return from call (These instructions are referred to as ²þÄÄÊ and Ë−ÎÊ in the disassembly outputs generated by the program objdump. The added sufﬁx ‘Ê’ simply emphasizes that these are x86-64 versions of call and return instructions, not IA32. In x86-64 assembly code, both versions can be used interchangeably.) The ²þÄÄ instruction has a target indicating the address of the instruction where the called procedure starts. Like jumps, a call can be either direct or indirect. In assembly code, the target of a direct call is given as a label, while the target of an indirect call is given by ‘h’ followed by an operand speciﬁer using one of the formats described in Figure 3.3. 278 Chapter 3 Machine-Level Representation of Programs %rip %rsp 0x400563 0x7fffffffe840 (a) Executing call %rip %rsp 0x400540 0x7fffffffe838 0x400568 (b) After call %rip %rsp 0x400568 0x7fffffffe840 (c) After ret Figure 3.26 Illustration of ²þÄÄ and Ë−Î functions. The ²þÄÄ instruction transfers control to the start of a function, while the Ë−Î instruction returns back to the instruction following the call. Figure 3.26 illustrates the execution of the ²þÄÄ and Ë−Î instructions for the ÀÏÄÎÍÎÇË− and Àþ©Å functions introduced in Section 3.2.2. The following are excerpts of the disassembled code for the two functions: Beginning of function multstore 1 nnnnnnnnnnrnnsrn zÀÏÄÎÍÎÇË−|x 2 rnnsrnx sq ÉÏÍ³ cË¾Ó 3 rnnsrox rv vw ®q ÀÇÌ cË®ÓjcË¾Ó ... Return from function multstore 4 rnnsr®x ²q Ë−ÎÊ ... Call to multstore from main 5 rnnstqx −v ®v ðð ðð ðð ²þÄÄÊ rnnsrn zÀÏÄÎÍÎÇË−| 6 rnnstvx rv v¾ sr pr nv ÀÇÌ nÓvfcËÍÉgjcË®Ó In this code, we can see that the ²þÄÄ instruction with address nÓrnnstq in Àþ©Å calls function ÀÏÄÎÍÎÇË−. This status is shown in Figure 3.26(a), with the indicated values for the stack pointer cËÍÉ and the program counter cË©É.The effect of the ²þÄÄ is to push the return address nÓrnnstv onto the stack and to jump to the ﬁrst instruction in function ÀÏÄÎÍÎÇË−, at address nÓnrnnsrn (3.26(b)). The execution of function ÀÏÄÎÍÎÇË− continues until it hits the Ë−Î instruction at address nÓrnnsr®. This instruction pops the value nÓrnnstv from the stack and jumps to this address, resuming the execution of Àþ©Å just after the ²þÄÄ instruction (3.26(c)). As a more detailed example of passing control to and from procedures, Figure 3.27(a) shows the disassembled code for two functions, ÎÇÉ and Ä−þð, as well as the portion of code in function Àþ©Å where ÎÇÉ gets called. Each instruction is identiﬁed by labels ‹o–‹p (in Ä−þð), ¶o–¶r (in ÎÇÉ), and ›o–›p in Àþ©Å. Part (b) of the ﬁgure shows a detailed trace of the code execution, in which Àþ©Å calls ÎÇÉfonng, causing ÎÇÉ to call Ä−þðfwsg. Function Ä−þð returns 97 to ÎÇÉ, which Section 3.7 Procedures 279 (a) Disassembled code for demonstrating procedure calls and returns Disassembly of leaf(long y) y in %rdi 1 nnnnnnnnnnrnnsrn zÄ−þð|x 2 rnnsrnx rv v® ru np Ä−þ nÓpfcË®©gjcËþÓ L1: z+2 3 rnnsrrx ²q Ë−ÎÊ L2: Return 4 nnnnnnnnnnrnnsrs zÎÇÉ|x Disassembly of top(long x) x in %rdi 5 rnnsrsx rv vq −ð ns ÍÏ¾ bnÓsjcË®© T1: x-5 6 rnnsrwx −v ðp ðð ðð ðð ²þÄÄÊ rnnsrn zÄ−þð| T2: Call leaf(x-5) 7 rnnsr−x rv no ²n þ®® cËþÓjcËþÓ T3: Double result 8 rnnssox ²q Ë−ÎÊ T4: Return ... Call to top from function main 9 rnnss¾x −v −s ðð ðð ðð ²þÄÄÊ rnnsrs zÎÇÉ| M1: Call top(100) 10 rnnstnx rv vw ²p ÀÇÌ cËþÓjcË®Ó M2: Resume (b) Execution trace of example code Instruction State values (at beginning) Label PC Instruction cË®© cËþÓ cËÍÉ hcËÍÉ Description M1 nÓrnnss¾ ²þÄÄÊ 100 — nÓuððððððð−vpn — Call ÎÇÉfonng T1 nÓrnnsrs ÍÏ¾ 100 — nÓuððððððð−vov nÓrnnstn Entry of ÎÇÉ T2 nÓrnnsrw ²þÄÄÊ 95 — nÓuððððððð−vov nÓrnnstn Call Ä−þðfwsg L1 nÓrnnsrn Ä−þ 95 — nÓuððððððð−von nÓrnnsr− Entry of Ä−þð L2 nÓrnnsrr Ë−ÎÊ —97 nÓuððððððð−von nÓrnnsr− Return 97 from Ä−þð T3 nÓrnnsr− þ®® —97 nÓuððððððð−vov nÓrnnstn Resume ÎÇÉ T4 nÓrnnsso Ë−ÎÊ — 194 nÓuððððððð−vov nÓrnnstn Return 194 from ÎÇÉ M2 nÓrnnstn ÀÇÌ — 194 nÓuððððððð−vpn — Resume Àþ©Å Figure 3.27 Detailed execution of program involving procedure calls and returns. Using the stack to store return addresses makes it possible to return to the right point in the procedures. then returns 194 to Àþ©Å. The ﬁrst three columns describe the instruction being executed, including the instruction label, the address, and the instruction type. The next four columns show the state of the program before the instruction is executed, including the contents of registers cË®©, cËþÓ, and cËÍÉ, as well as the value at the top of the stack. The contents of this table should be studied carefully, as they 280 Chapter 3 Machine-Level Representation of Programs demonstrate the important role of the run-time stack in managing the storage needed to support procedure calls and returns. Instruction ‹o of Ä−þð sets cËþÓ to 97, the value to be returned. Instruction ‹p then returns. It pops nÓrnnnsr− from the stack. In setting the PC to this popped value, control transfers back to instruction ¶q of ÎÇÉ. The program has successfully completed the call to Ä−þð and returned to ÎÇÉ. Instruction ¶q sets cËþÓ to 194, the value to be returned from ÎÇÉ. Instruction ¶r then returns. It pops nÓrnnnstn from the stack, thereby setting the PC to instruction ›p of Àþ©Å. The program has successfully completed the call to ÎÇÉ and returned to Àþ©Å. We see that the stack pointer has also been restored to nÓuððððððð−vpn, the value it had before the call to ÎÇÉ. We can see that this simple mechanism of pushing the return address onto the stack makes it possible for the function to later return to the proper point in the program. The standard call/return mechanism of C (and of most program- ming languages) conveniently matches the last-in, ﬁrst-out memory management discipline provided by a stack. Practice Problem 3.32 (solution page 375) The disassembled code for two functions ð©ËÍÎ and ÄþÍÎ is shown below, along with the code for a call of ð©ËÍÎ by function Àþ©Å: Disassembly of last(long u, long v) u in %rdi, v in %rsi 1 nnnnnnnnnnrnnsrn zÄþÍÎ|x 2 rnnsrnx rv vw ðv ÀÇÌ cË®©jcËþÓ L1: u 3 rnnsrqx rv nð þð ²t ©ÀÏÄ cËÍ©jcËþÓ L2: u*v 4 rnnsrux ²q Ë−ÎÊ L3: Return Disassembly of last(long x) x in %rdi 5 nnnnnnnnnnrnnsrv zð©ËÍÎ|x 6 rnnsrvx rv v® uu no Ä−þ nÓofcË®©gjcËÍ© F1: x+1 7 rnnsr²x rv vq −ð no ÍÏ¾ bnÓojcË®© F2: x-1 8 rnnssnx −v −¾ ðð ðð ðð ²þÄÄÊ rnnsrn zÄþÍÎ| F3: Call last(x-1,x+1) 9 rnnsssx ðq ²q Ë−ÉÖ Ë−ÎÊ F4: Return l l l 10 rnnstnx −v −q ðð ðð ðð ²þÄÄÊ rnnsrv zð©ËÍÎ| M1: Call first(10) 11 rnnstsx rv vw ²p ÀÇÌ cËþÓjcË®Ó M2: Resume Each of these instructions is given a label, similar to those in Figure 3.27(a). Starting with the calling of ð©ËÍÎfong by Àþ©Å, ﬁll in the following table to trace instruction execution through to the point where the program returns back to Àþ©Å. Section 3.7 Procedures 281 Instruction State values (at beginning) Label PC Instruction cË®© cËÍ© cËþÓ cËÍÉ hcËÍÉ Description M1 nÓrnnstn ²þÄÄÊ 10—— nÓuððððððð−vpn — Call ð©ËÍÎfong F1 F2 F3 L1 L2 L3 F4 M2 3.7.3 Data Transfer In addition to passing control to a procedure when called, and then back again when the procedure returns, procedure calls may involve passing data as argu- ments, and returning from a procedure may also involve returning a value. With x86-64, most of these data passing to and from procedures take place via regis- ters. For example, we have already seen numerous examples of functions where arguments are passed in registers cË®©, cËÍ©, and others, and where values are re- turned in register cËþÓ. When procedure – calls procedure †, the code for – must ﬁrst copy the arguments into the proper registers. Similarly, when † returns back to –, the code for – can access the returned value in register cËþÓ. In this section, we explore these conventions in greater detail. With x86-64, up to six integral (i.e., integer and pointer) arguments can be passed via registers. The registers are used in a speciﬁed order, with the name used for a register depending on the size of the data type being passed. These are shown in Figure 3.28. Arguments are allocated to these registers according to their Argument numberOperand size (bits) 123456 64 cË®© cËÍ© cË®Ó cË²Ó cËv cËw 32 c−®© c−Í© c−®Ó c−²Ó cËv® cËw® 16 c®© cÍ© c®Ó c²Ó cËvÑ cËwÑ 8 c®©Ä cÍ©Ä c®Ä c²Ä cËv¾ cËw¾ Figure 3.28 Registers for passing function arguments. The registers are used in a speciﬁed order and named according to the argument sizes. 282 Chapter 3 Machine-Level Representation of Programs ordering in the argument list. Arguments smaller than 64 bits can be accessed using the appropriate subsection of the 64-bit register. For example, if the ﬁrst argument is 32 bits, it can be accessed as c−®©. When a function has more than six integral arguments, the other ones are passed on the stack. Assume that procedure – calls procedure † with n integral arguments, such that n> 6. Then the code for – must allocate a stack frame with enough storage for arguments 7 through n, as illustrated in Figure 3.25. It copies arguments 1–6 into the appropriate registers, and it puts arguments 7 through n onto the stack, with argument 7 at the top of the stack. When passing parameters on the stack, all data sizes are rounded up to be multiples of eight. With the arguments in place, the program can then execute a ²þÄÄ instruction to transfer control to procedure †. Procedure † can access its arguments via registers and possibly from the stack. If †, in turn, calls some function that has more than six arguments, it can allocate space within its stack frame for these, as is illustrated by the area labeled “Argument build area” in Figure 3.25. As an example of argument passing, consider the C function ÉËÇ² shown in Figure 3.29(a). This function has eight arguments, including integers with different numbers of bytes (8, 4, 2, and 1), as well as different types of pointers, each of which is 8 bytes. The assembly code generated for ÉËÇ² is shown in Figure 3.29(b). The ﬁrst six arguments are passed in registers. The last two are passed on the stack, as documented by the diagram of Figure 3.30. This diagram shows the state of the stack during the execution of ÉËÇ². We can see that the return address was pushed onto the stack as part of the procedure call. The two arguments, therefore, are at positions 8 and 16 relative to the stack pointer. Within the code, we can see that different versions of the add instruction are used according to the sizes of the operands: þ®®Ê for þo (ÄÇÅ×), þ®®Ä for þp (©ÅÎ), þ®®Ñ for þq (Í³ÇËÎ), and þ®®¾ for þr (²³þË). Observe that the ÀÇÌÄ instruction of line 6 reads 4 bytes from memory; the following þ®®¾ instruction only makes use of the low-order byte. Practice Problem 3.33 (solution page 375) A C function ÉËÇ²ÉËÇ¾ has four arguments Ï, þ, Ì, and ¾. Each is either a signed number or a pointer to a signed number, where the numbers have different sizes. The function has the following body: hÏ i{ þy hÌ i{ ¾y Ë−ÎÏËÅ Í©Ö−Çðfþg i Í©Ö−Çðf¾gy It compiles to the following x86-64 code: 1 ÉËÇ²ÉËÇ¾x 2 ÀÇÌÍÄÊ c−®©j cË®© 3 þ®®Ê cË®©j fcË®Óg 4 þ®®¾ cÍ©Äj fcË²Óg Section 3.7 Procedures 283 (a) C code ÌÇ©® ÉËÇ²fÄÇÅ× þoj ÄÇÅ× hþoÉj ©ÅÎ þpj ©ÅÎ hþpÉj Í³ÇËÎ þqj Í³ÇËÎ hþqÉj ²³þË þrj ²³þË hþrÉg Õ hþoÉ i{ þoy hþpÉ i{ þpy hþqÉ i{ þqy hþrÉ i{ þry Û (b) Generated assembly code void proc(a1, a1p, a2, a2p, a3, a3p, a4, a4p) Arguments passed as follows: a1 in %rdi (64 bits) a1p in %rsi (64 bits) a2 in %edx (32 bits) a2p in %rcx (64 bits) a3 in %r8w (16 bits) a3p in %r9 (64 bits) a4 at %rsp+8 ( 8 bits) a4p at %rsp+16 (64 bits) 1 ÉËÇ²x 2 ÀÇÌÊ otfcËÍÉgj cËþÓ Fetch a4p (64 bits) 3 þ®®Ê cË®©j fcËÍ©g *a1p += a1 (64 bits) 4 þ®®Ä c−®Ój fcË²Óg *a2p += a2 (32 bits) 5 þ®®Ñ cËvÑj fcËwg *a3p += a3 (16 bits) 6 ÀÇÌÄ vfcËÍÉgj c−®Ó Fetch a4 ( 8 bits) 7 þ®®¾ c®Äj fcËþÓg *a4p += a4 ( 8 bits) 8 Ë−Î Return Figure 3.29 Example of function with multiple arguments of different types. Arguments 1–6 are passed in registers, while arguments 7–8 are passed on the stack. Figure 3.30 Stack frame structure for function ÉËÇ². Arguments þr and þrÉ are passed on the stack. Return address 16 8 0 a4 a4p Stack pointer %rsp 284 Chapter 3 Machine-Level Representation of Programs 5 ÀÇÌÄ btj c−þÓ 6 Ë−Î Determine a valid ordering and types of the four parameters. There are two correct answers. 3.7.4 Local Storage on the Stack Most of the procedure examples we have seen so far did not require any local storage beyond what could be held in registers. At times, however, local data must be stored in memory. Common cases of this include these: . There are not enough registers to hold all of the local data. . The address operator ‘d’ is applied to a local variable, and hence we must be able to generate an address for it. . Some of the local variables are arrays or structures and hence must be accessed by array or structure references. We will discuss this possibility when we describe how arrays and structures are allocated. Typically, a procedure allocates space on the stack frame by decrementing the stack pointer. This results in the portion of the stack frame labeled “Local vari- ables” in Figure 3.25. As an example of the handling of the address operator, consider the two functions shown in Figure 3.31(a). The function ÍÑþÉˆþ®® swaps the two values designated by pointers ÓÉ and ÔÉ and also returns the sum of the two values. The function ²þÄÄ−Ë creates pointers to local variables þË×o and þË×p and passes these to ÍÑþÉˆþ®®. Figure 3.31(b) shows how ²þÄÄ−Ë uses a stack frame to implement these local variables. The code for ²þÄÄ−Ë starts by decrementing the stack pointer by 16; this effectively allocates 16 bytes on the stack. Letting S denote the value of the stack pointer, we can see that the code computes dþË×p as S + 8 (line 5), dþË×o as S (line 6). We can therefore infer that local variables þË×o and þË×p are stored within the stack frame at offsets 0 and 8 relative to the stack pointer. When the call to ÍÑþÉˆþ®® completes, the code for ²þÄÄ−Ë then retrieves the two values from the stack (lines 8–9), computes their difference, and multiplies this by the value returned by ÍÑþÉˆþ®® in register cËþÓ (line 10). Finally, the function deallocates its stack frame by incrementing the stack pointer by 16 (line 11.) We can see with this example that the run-time stack provides a simple mechanism for allocating local storage when it is required and deallocating it when the function completes. As a more complex example, the function ²þÄÄˆÉËÇ², shown in Figure 3.32, illustrates many aspects of the x86-64 stack discipline. Despite the length of this example, it is worth studying carefully. It shows a function that must allocate storage on the stack for local variables, as well as to pass values to the 8-argument function ÉËÇ² (Figure 3.29). The function creates a stack frame, diagrammed in Figure 3.33. Looking at the assembly code for ²þÄÄˆÉËÇ² (Figure 3.32(b)), we can see that a large portion of the code (lines 2–15) involves preparing to call function Section 3.7 Procedures 285 (a) Code for ÍÑþÉˆþ®® and calling function ÄÇÅ× ÍÑþÉˆþ®®fÄÇÅ× hÓÉj ÄÇÅ× hÔÉg Õ ÄÇÅ× Ó { hÓÉy ÄÇÅ× Ô { hÔÉy hÓÉ{Ôy hÔÉ{Óy Ë−ÎÏËÅÓiÔy Û ÄÇÅ× ²þÄÄ−Ëfg Õ ÄÇÅ× þË×o { sqry ÄÇÅ× þË×p { onsuy ÄÇÅ× ÍÏÀ { ÍÑþÉˆþ®®fdþË×oj dþË×pgy ÄÇÅ× ®©ðð { þË×o k þË×py Ë−ÎÏËÅ ÍÏÀ h ®©ððy Û (b) Generated assembly code for calling function long caller() 1 ²þÄÄ−Ëx 2 ÍÏ¾Ê botj cËÍÉ Allocate 16 bytes for stack frame 3 ÀÇÌÊ bsqrj fcËÍÉg Store 534 in arg1 4 ÀÇÌÊ bonsuj vfcËÍÉg Store 1057 in arg2 5 Ä−þÊ vfcËÍÉgj cËÍ© Compute &arg2 as second argument 6 ÀÇÌÊ cËÍÉj cË®© Compute &arg1 as first argument 7 ²þÄÄ ÍÑþÉˆþ®® Call swap_add(&arg1, &arg2) 8 ÀÇÌÊ fcËÍÉgj cË®Ó Get arg1 9 ÍÏ¾Ê vfcËÍÉgj cË®Ó Compute diff = arg1 - arg2 10 ©ÀÏÄÊ cË®Ój cËþÓ Compute sum * diff 11 þ®®Ê botj cËÍÉ Deallocate stack frame 12 Ë−Î Return Figure 3.31 Example of procedure deﬁnition and call. The calling code must allocate a stack frame due to the presence of address operators. ÉËÇ². This includes setting up the stack frame for the local variables and function parameters, and for loading function arguments into registers. As Figure 3.33 shows, local variables Óo–Ór are allocated on the stack and have different sizes. Expressing their locations as offsets relative to the stack pointer, they occupy bytes 24–31 (Óo), 20–23 (Óp), 18–19 (Óq), and 17 (Íq). Pointers to these locations are generated by Ä−þÊ instructions (lines 7, 10, 12, and 14). Arguments 7 (with value 4) and 8 (a pointer to the location of Ór) are stored on the stack at offsets 0 and 8 relative to the stack pointer. 286 Chapter 3 Machine-Level Representation of Programs (a) C code for calling function ÄÇÅ× ²þÄÄˆÉËÇ²fg Õ ÄÇÅ× Óo { oy ©ÅÎ Óp { py Í³ÇËÎ Óq { qy ²³þË Ór { ry ÉËÇ²fÓoj dÓoj Ópj dÓpj Óqj dÓqj Órj dÓrgy Ë−ÎÏËÅ fÓoiÓpghfÓqkÓrgy Û (b) Generated assembly code long call_proc() 1 ²þÄÄˆÉËÇ²x Set up arguments to proc 2 ÍÏ¾Ê bqpj cËÍÉ Allocate 32-byte stack frame 3 ÀÇÌÊ boj prfcËÍÉg Store 1 in &x1 4 ÀÇÌÄ bpj pnfcËÍÉg Store 2 in &x2 5 ÀÇÌÑ bqj ovfcËÍÉg Store 3 in &x3 6 ÀÇÌ¾ brj oufcËÍÉg Store 4 in &x4 7 Ä−þÊ oufcËÍÉgj cËþÓ Create &x4 8 ÀÇÌÊ cËþÓj vfcËÍÉg Store &x4 as argument 8 9 ÀÇÌÄ brj fcËÍÉg Store 4 as argument 7 10 Ä−þÊ ovfcËÍÉgj cËw Pass &x3 as argument 6 11 ÀÇÌÄ bqj cËv® Pass 3 as argument 5 12 Ä−þÊ pnfcËÍÉgj cË²Ó Pass &x2 as argument 4 13 ÀÇÌÄ bpj c−®Ó Pass 2 as argument 3 14 Ä−þÊ prfcËÍÉgj cËÍ© Pass &x1 as argument 2 15 ÀÇÌÄ boj c−®© Pass 1 as argument 1 Call proc 16 ²þÄÄ ÉËÇ² Retrieve changes to memory 17 ÀÇÌÍÄÊ pnfcËÍÉgj cË®Ó Get x2 and convert to long 18 þ®®Ê prfcËÍÉgj cË®Ó Compute x1+x2 19 ÀÇÌÍÑÄ ovfcËÍÉgj c−þÓ Get x3 and convert to int 20 ÀÇÌÍ¾Ä oufcËÍÉgj c−²Ó Get x4 and convert to int 21 ÍÏ¾Ä c−²Ój c−þÓ Compute x3-x4 22 ²ÄÎÊ Convert to long 23 ©ÀÏÄÊ cË®Ój cËþÓ Compute (x1+x2) * (x3-x4) 24 þ®®Ê bqpj cËÍÉ Deallocate stack frame 25 Ë−Î Return Figure 3.32 Example of code to call function ÉËÇ², deﬁned in Figure 3.29. This code creates a stack frame. Section 3.7 Procedures 287 Figure 3.33 Stack frame for function ²þÄÄˆÉËÇ². The stack frame contains local variables, as well as two of the arguments to pass to function ÉËÇ². Stack pointer %rsp Argument 8 = &x4 Argument 7 Return address x1 32 24 16 8 0 171820 x4x3x2 x4 4 When procedure ÉËÇ² is called, the program will begin executing the code shown in Figure 3.29(b). As shown in Figure 3.30, arguments 7 and 8 are now at offsets 8 and 16 relative to the stack pointer, because the return address was pushed onto the stack. When the program returns to ²þÄÄˆÉËÇ², the code retrieves the values of the four local variables (lines 17–20) and performs the ﬁnal computations. It ﬁnishes by incrementing the stack pointer by 32 to deallocate the stack frame. 3.7.5 Local Storage in Registers The set of program registers acts as a single resource shared by all of the proce- dures. Although only one procedure can be active at a given time, we must make sure that when one procedure (the caller) calls another (the callee), the callee does not overwrite some register value that the caller planned to use later. For this rea- son, x86-64 adopts a uniform set of conventions for register usage that must be respected by all procedures, including those in program libraries. By convention, registers cË¾Ó, cË¾É, and cËop–cËos are classiﬁed as callee- saved registers. When procedure – calls procedure †, † must preserve the values of these registers, ensuring that they have the same values when † returns to – as they did when † was called. Procedure † can preserve a register value by either not changing it at all or by pushing the original value on the stack, altering it, and then popping the old value from the stack before returning. The pushing of register values has the effect of creating the portion of the stack frame labeled “Saved registers” in Figure 3.25. With this convention, the code for – can safely store a value in a callee-saved register (after saving the previous value on the stack, of course), call †, and then use the value in the register without risk of it having been corrupted. All other registers, except for the stack pointer cËÍÉ, are classiﬁed as caller- saved registers. This means that they can be modiﬁed by any function. The name “caller saved” can be understood in the context of a procedure – having some local data in such a register and calling procedure †. Since † is free to alter this register, it is incumbent upon – (the caller) to ﬁrst save the data before it makes the call. As an example, consider the function – shown in Figure 3.34(a). It calls † twice. During the ﬁrst call, it must retain the value of Ó for use later. Similarly, during the second call, it must retain the value computed for †fÔg. In Figure 3.34(b), 288 Chapter 3 Machine-Level Representation of Programs (a) Calling function ÄÇÅ× –fÄÇÅ× Ój ÄÇÅ× Ôg Õ ÄÇÅ× Ï { †fÔgy ÄÇÅ× Ì { †fÓgy Ë−ÎÏËÅÏiÌy Û (b) Generated assembly code for the calling function long P(long x, long y) x in %rdi, y in %rsi 1 –x 2 ÉÏÍ³Ê cË¾É Save %rbp 3 ÉÏÍ³Ê cË¾Ó Save %rbx 4 ÍÏ¾Ê bvj cËÍÉ Align stack frame 5 ÀÇÌÊ cË®©j cË¾É Save x 6 ÀÇÌÊ cËÍ©j cË®© Move y to first argument 7 ²þÄÄ † Call Q(y) 8 ÀÇÌÊ cËþÓj cË¾Ó Save result 9 ÀÇÌÊ cË¾Éj cË®© Move x to first argument 10 ²þÄÄ † Call Q(x) 11 þ®®Ê cË¾Ój cËþÓ Add saved Q(y) to Q(x) 12 þ®®Ê bvj cËÍÉ Deallocate last part of stack 13 ÉÇÉÊ cË¾Ó Restore %rbx 14 ÉÇÉÊ cË¾É Restore %rbp 15 Ë−Î Figure 3.34 Code demonstrating use of callee-saved registers. Value Ó must be preserved during the ﬁrst call, and value †fÔg must be preserved during the second. we can see that the code generated by gcc uses two callee-saved registers: cË¾É to hold Ó, and cË¾Ó to hold the computed value of †fÔg. At the beginning of the function, it saves the values of these two registers on the stack (lines 2–3). It copies argument Ó to cË¾É before the ﬁrst call to † (line 5). It copies the result of this call to cË¾Ó before the second call to † (line 8). At the end of the function (lines 13– 14), it restores the values of the two callee-saved registers by popping them off the stack. Note how they are popped in the reverse order from how they were pushed, to account for the last-in, ﬁrst-out discipline of a stack. Practice Problem 3.34 (solution page 376) Consider a function –, which generates local values, named þn–þv. It then calls function † using these generated values as arguments. Gcc produces the following code for the ﬁrst part of –: Section 3.7 Procedures 289 long P(long x) x in %rdi 1 –x 2 ÉÏÍ³Ê cËos 3 ÉÏÍ³Ê cËor 4 ÉÏÍ³Ê cËoq 5 ÉÏÍ³Ê cËop 6 ÉÏÍ³Ê cË¾É 7 ÉÏÍ³Ê cË¾Ó 8 ÍÏ¾Ê bprj cËÍÉ 9 ÀÇÌÊ cË®©j cË¾Ó 10 Ä−þÊ ofcË®©gj cËos 11 Ä−þÊ pfcË®©gj cËor 12 Ä−þÊ qfcË®©gj cËoq 13 Ä−þÊ rfcË®©gj cËop 14 Ä−þÊ sfcË®©gj cË¾É 15 Ä−þÊ tfcË®©gj cËþÓ 16 ÀÇÌÊ cËþÓj fcËÍÉg 17 Ä−þÊ ufcË®©gj cË®Ó 18 ÀÇÌÊ cË®Ój vfcËÍÉg 19 ÀÇÌÄ bnj c−þÓ 20 ²þÄÄ † ... A. Identify which local values get stored in callee-saved registers. B. Identify which local values get stored on the stack. C. Explain why the program could not store all of the local values in callee- saved registers. 3.7.6 Recursive Procedures The conventions we have described for using the registers and the stack allow x86-64 procedures to call themselves recursively. Each procedure call has its own private space on the stack, and so the local variables of the multiple outstanding calls do not interfere with one another. Furthermore, the stack discipline naturally provides the proper policy for allocating local storage when the procedure is called and deallocating it before returning. Figure 3.35 shows both the C code and the generated assembly code for a recursive factorial function. We can see that the assembly code uses register cË¾Ó to hold the parameter Å, after ﬁrst saving the existing value on the stack (line 2) and later restoring the value before returning (line 11). Due to the stack discipline, and the register-saving conventions, we can be assured that when the recursive call to Ëðþ²ÎfÅkog returns (line 9) that (1) the result of the call will be held in register 290 Chapter 3 Machine-Level Representation of Programs (a) C code ÄÇÅ× Ëðþ²ÎfÄÇÅ× Åg Õ ÄÇÅ× Ë−ÍÏÄÎy ©ð fÅ z{ og Ë−ÍÏÄÎ { oy −ÄÍ− Ë−ÍÏÄÎ{Åh Ëðþ²ÎfÅkogy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Generated assembly code long rfact(long n) n in %rdi 1 Ëðþ²Îx 2 ÉÏÍ³Ê cË¾Ó Save %rbx 3 ÀÇÌÊ cË®©j cË¾Ó Store n in callee-saved register 4 ÀÇÌÄ boj c−þÓ Set return value = 1 5 ²ÀÉÊ boj cË®© Compare n:1 6 ÁÄ− l‹qs If <=, goto done 7 Ä−þÊ kofcË®©gj cË®© Compute n-1 8 ²þÄÄ Ëðþ²Î Call rfact(n-1) 9 ©ÀÏÄÊ cË¾Ój cËþÓ Multiply result by n 10 l‹qsx done: 11 ÉÇÉÊ cË¾Ó Restore %rbx 12 Ë−Î Return Figure 3.35 Code for recursive factorial program. The standard procedure handling mechanisms sufﬁce for implementing recursive functions. cËþÓ, and (2) the value of argument Å will held in register cË¾Ó. Multiplying these two values then computes the desired result. We can see from this example that calling a function recursively proceeds just like any other function call. Our stack discipline provides a mechanism where each invocation of a function has its own private storage for state information (saved values of the return location and callee-saved registers). If need be, it can also provide storage for local variables. The stack discipline of allocation and deallocation naturally matches the call-return ordering of functions. This method of implementing function calls and returns even works for more complex patterns, including mutual recursion (e.g., when procedure – calls †, which in turn calls –). Practice Problem 3.35 (solution page 376) For a C function having the general structure Section 3.8 Array Allocation and Access 291 ÄÇÅ× ËðÏÅfÏÅÍ©×Å−® ÄÇÅ× Óg Õ ©ð f g Ë−ÎÏËÅ y ÏÅÍ©×Å−® ÄÇÅ× ÅÓ { y ÄÇÅ× ËÌ { ËðÏÅfÅÓgy Ë−ÎÏËÅ y Û gcc generates the following assembly code: long rfun(unsigned long x) x in %rdi 1 ËðÏÅx 2 ÉÏÍ³Ê cË¾Ó 3 ÀÇÌÊ cË®©j cË¾Ó 4 ÀÇÌÄ bnj c−þÓ 5 Î−ÍÎÊ cË®©j cË®© 6 Á− l‹p 7 Í³ËÊ bpj cË®© 8 ²þÄÄ ËðÏÅ 9 þ®®Ê cË¾Ój cËþÓ 10 l‹px 11 ÉÇÉÊ cË¾Ó 12 Ë−Î A. What value does ËðÏÅ store in the callee-saved register cË¾Ó? B. Fill in the missing expressions in the C code shown above. 3.8 Array Allocation and Access Arrays in C are one means of aggregating scalar data into larger data types. C uses a particularly simple implementation of arrays, and hence the translation into machine code is fairly straightforward. One unusual feature of C is that we can generate pointers to elements within arrays and perform arithmetic with these pointers. These are translated into address computations in machine code. Optimizing compilers are particularly good at simplifying the address compu- tations used by array indexing. This can make the correspondence between the C code and its translation into machine code somewhat difﬁcult to decipher. 3.8.1 Basic Principles For data type T and integer constant N , consider a declaration of the form T ¡‰N`y 292 Chapter 3 Machine-Level Representation of Programs Let us denote the starting location as x¡. The declaration has two effects. First, it allocates a contiguous region of L . N bytes in memory, where L is the size (in bytes) of data type T . Second, it introduces an identiﬁer ¡ that can be used as a pointer to the beginning of the array. The value of this pointer will be x¡.The array elements can be accessed using an integer index ranging between 0 and N −1. Array element i will be stored at address x¡ + L . i. As examples, consider the following declarations: ²³þË ¡‰op`y ²³þË h¢‰v`y ©ÅÎ £‰t`y ®ÇÏ¾Ä− h⁄‰s`y These declarations will generate arrays with the following parameters: Array Element size Total size Start address Element i ¡ 112 x¡ x¡ + i ¢ 864 x¢ x¢ + 8i £ 424 x£ x£ + 4i ⁄ 840 x⁄ x⁄ + 8i Array ¡ consists of 12 single-byte (²³þË) elements. Array £ consists of 6 integers, each requiring 4 bytes. ¢ and ⁄ are both arrays of pointers, and hence the array elements are 8 bytes each. The memory referencing instructions of x86-64 are designed to simplify array access. For example, suppose ¥ is an array of values of type ©ÅÎ and we wish to evaluate ¥‰©`, where the address of ¥ is stored in register cË®Ó and i is stored in register cË²Ó. Then the instruction ÀÇÌÄ fcË®ÓjcË²Ójrgjc−þÓ will perform the address computation x¥ + 4i, read that memory location, and copy the result to register c−þÓ. The allowed scaling factors of 1, 2, 4, and 8 cover the sizes of the common primitive data types. Practice Problem 3.36 (solution page 377) Consider the following declarations: ©ÅÎ –‰s`y Í³ÇËÎ †‰p`y ©ÅÎ hh‡‰w`y ®ÇÏ¾Ä− h·‰on`y Í³ÇËÎ h¶‰p`y Fill in the following table describing the element size, the total size, and the address of element i for each of these arrays. Section 3.8 Array Allocation and Access 293 Array Element size Total size Start address Element i – x– † x† ‡ x‡ · x· ¶ x¶ 3.8.2 Pointer Arithmetic C allows arithmetic on pointers, where the computed value is scaled according to the size of the data type referenced by the pointer. That is, if É is a pointer to data of type T , and the value of É is xÉ, then the expression Éi© has value xÉ + L . i, where L is the size of data type T . The unary operators ‘d’ and ‘h’ allow the generation and dereferencing of pointers. That is, for an expression Expr denoting some object, dExpr is a pointer giving the address of the object. For an expression AExpr denoting an address, hAExpr gives the value at that address. The expressions Expr and hdExpr are therefore equivalent. The array subscripting operation can be applied to both arrays and pointers. The array reference ¡‰©` is identical to the expression hf¡i©g. It computes the address of the ith array element and then accesses this memory location. Expanding on our earlier example, suppose the starting address of integer array ¥ and integer index i are stored in registers cË®Ó and cË²Ó, respectively. The following are some expressions involving ¥. We also show an assembly-code implementation of each expression, with the result being stored in either register c−þÓ (for data) or register cËþÓ (for pointers). Expression Type Value Assembly code ¥ ©ÅÎ h x¥ ÀÇÌÄ cË®ÓjcËþÓ ¥‰n` ©ÅÎ M[x¥] ÀÇÌÄ fcË®Ógjc−þÓ ¥‰©` ©ÅÎ M[x¥ + 4i] ÀÇÌÄ fcË®ÓjcË²Ójrgjc−þÓ d¥‰p` ©ÅÎ h x¥ + 8 Ä−þÊ vfcË®ÓgjcËþÓ ¥i©ko ©ÅÎ h x¥ + 4i − 4 Ä−þÊ krfcË®ÓjcË²ÓjrgjcËþÓ hf¥i©kqg ©ÅÎ M[x¥ + 4i − 12] ÀÇÌÄ kopfcË®ÓjcË²Ójrgjc−þÓ d¥‰©`k¥ ÄÇÅ× i ÀÇÌÊ cË²ÓjcËþÓ In these examples, we see that operations that return array values have type ©ÅÎ, and hence involve 4-byte operations (e.g., ÀÇÌÄ) and registers (e.g., c−þÓ). Those that return pointers have type ©ÅÎ h, and hence involve 8-byte operations (e.g., Ä−þÊ) and registers (e.g., cËþÓ). The ﬁnal example shows that one can compute the difference of two pointers within the same data structure, with the result being data having type ÄÇÅ× and value equal to the difference of the two addresses divided by the size of the data type. 294 Chapter 3 Machine-Level Representation of Programs Practice Problem 3.37 (solution page 377) Suppose x–, the address of short integer array –, and long integer index i are stored in registers cË®Ó and cË²Ó, respectively. For each of the following expressions, give its type, a formula for its value, and an assembly-code implementation. The result should be stored in register cËþÓ if it is a pointer and register element cþÓ if it has data type Í³ÇËÎ. Expression Type Value Assembly code –‰o` –iqi© –‰©htks` –‰p` d–‰© i p` 3.8.3 Nested Arrays The general principles of array allocation and referencing hold even when we create arrays of arrays. For example, the declaration ©ÅÎ ¡‰s`‰q`y is equivalent to the declaration ÎÔÉ−®−ð ©ÅÎ ËÇÑqˆÎ‰q`y ËÇÑqˆÎ ¡‰s`y Data type ËÇÑqˆÎ is deﬁned to be an array of three integers. Array ¡ contains ﬁve such elements, each requiring 12 bytes to store the three integers. The total array size is then 4 . 5 . 3 = 60 bytes. Array ¡ can also be viewed as a two-dimensional array with ﬁve rows and three columns, referenced as ¡‰n`‰n` through ¡‰r`‰p`. The array elements are ordered in memory in row-major order, meaning all elements of row 0, which can be written ¡‰n`, followed by all elements of row 1 (¡‰o`), and so on. This is illustrated in Figure 3.36. This ordering is a consequence of our nested declaration. Viewing ¡ as an array of ﬁve elements, each of which is an array of three ©ÅÎ’s, we ﬁrst have ¡‰n`, followed by ¡‰o`, and so on. To access elements of multidimensional arrays, the compiler generates code to compute the offset of the desired element and then uses one of the mov instructions with the start of the array as the base address and the (possibly scaled) offset as an index. In general, for an array declared as T ⁄‰R`‰C`y array element ⁄‰©`‰Á` is at memory address &⁄[©][Á] = x⁄ + L(C . i + j) (3.1) Section 3.8 Array Allocation and Access 295 Figure 3.36 Elements of array in row-major order. A[0][0]A[0] xA ElementRow Address A[0][1] xA + 4 A[0][2] xA + 8 A[1][0]A[1] xA + 12 A[1][1] xA + 16 A[1][2] xA + 20 A[2][0]A[2] xA + 24 A[2][1] xA + 28 A[2][2] xA + 32 A[3][0]A[3] xA + 36 A[3][1] xA + 40 A[4][2] xA + 44 A[4][0]A[4] xA + 48 A[4][1] xA + 52 A[4][2] xA + 56 where L is the size of data type T in bytes. As an example, consider the 5 × 3 integer array ¡ deﬁned earlier. Suppose x¡, i, and j are in registers cË®©, cËÍ©, and cË®Ó, respectively. Then array element ¡‰©`‰Á` can be copied to register c−þÓ by the following code: A in %rdi, i in %rsi, and j in %rdx 1 Ä−þÊ fcËÍ©jcËÍ©jpgj cËþÓ Compute 3i 2 Ä−þÊ fcË®©jcËþÓjrgj cËþÓ Compute x¡ + 12i 3 ÀÇÌÄ fcËþÓjcË®Ójrgj c−þÓ Read from M[x¡ + 12i + 4] As can be seen, this code computes the element’s address as x¡ + 12i + 4j = x¡ + 4(3i + j) using the scaling and addition capabilities of x86-64 address arithmetic. Practice Problem 3.38 (solution page 377) Consider the following source code, where M and N are constants declared with a®−ð©Å−: ÄÇÅ× –‰›`‰ﬁ`y ÄÇÅ× †‰ﬁ`‰›`y ÄÇÅ× ÍÏÀˆ−Ä−À−ÅÎfÄÇÅ× ©j ÄÇÅ× Ág Õ Ë−ÎÏËÅ –‰©`‰Á` i †‰Á`‰©`y Û In compiling this program, gcc generates the following assembly code: 296 Chapter 3 Machine-Level Representation of Programs long sum_element(long i, long j) i in %rdi, j in %rsi 1 ÍÏÀˆ−Ä−À−ÅÎx 2 Ä−þÊ nfjcË®©jvgj cË®Ó 3 ÍÏ¾Ê cË®©j cË®Ó 4 þ®®Ê cËÍ©j cË®Ó 5 Ä−þÊ fcËÍ©jcËÍ©jrgj cËþÓ 6 þ®®Ê cËþÓj cË®© 7 ÀÇÌÊ †fjcË®©jvgj cËþÓ 8 þ®®Ê –fjcË®Ójvgj cËþÓ 9 Ë−Î Use your reverse engineering skills to determine the values of M and N based on this assembly code. 3.8.4 Fixed-Size Arrays The C compiler is able to make many optimizations for code operating on multi- dimensional arrays of ﬁxed size. Here we demonstrate some of the optimizations made by gcc when the optimization level is set with the ﬂag kﬂo. Suppose we declare data type ð©ÓˆÀþÎË©Ó to be 16 × 16 arrays of integers as follows: a®−ð©Å− ﬁ ot ÎÔÉ−®−ð ©ÅÎ ð©ÓˆÀþÎË©Ó‰ﬁ`‰ﬁ`y (This example illustrates a good coding practice. Whenever a program uses some constant as an array dimension or buffer size, it is best to associate a name with it via a a®−ð©Å− declaration, and then use this name consistently, rather than the numeric value. That way, if an occasion ever arises to change the value, it can be done by simply modifying the a®−ð©Å− declaration.) The code in Figure 3.37(a) computes element i, k of the product of arrays ¡ and ¢—that is, the inner product of row i from ¡ and column k from ¢. This product is given by the formula ∑0≤j<N ai,j . bj,k. Gcc generates code that we then recoded into C, shown as function ð©ÓˆÉËÇ®ˆ−Ä−ˆÇÉÎ in Figure 3.37(b). This code contains a number of clever optimizations. It removes the integer index Á and converts all array references to pointer dereferences. This involves (1) generating a pointer, which we have named ¡ÉÎË, that points to successive elements in row i of ¡, (2) generating a pointer, which we have named ¢ÉÎË, that points to successive elements in column k of ¢, and (3) generating a pointer, which we have named ¢−Å®, that equals the value ¢ÉÎË will have when it is time to terminate the loop. The initial value for ¡ÉÎË is the address of the ﬁrst element of row i of ¡, given by the C expression d¡‰©`‰n`. The initial value for ¢ÉÎË is the address of the ﬁrst element of column k of ¢, given by the C expression d¢‰n`‰Â`. The value for ¢−Å® is the index of what would be the (n + 1)st element in column j of ¢, given by the C expression d¢‰ﬁ`‰Â`. Section 3.8 Array Allocation and Access 297 (a) Original C code mh £ÇÀÉÏÎ− ©jÂ Çð ð©Ó−® ÀþÎË©Ó ÉËÇ®Ï²Î hm ©ÅÎ ð©ÓˆÉËÇ®ˆ−Ä− fð©ÓˆÀþÎË©Ó ¡j ð©ÓˆÀþÎË©Ó ¢j ÄÇÅ× ©j ÄÇÅ× Âg Õ ÄÇÅ× Áy ©ÅÎ Ë−ÍÏÄÎ { ny ðÇËfÁ{nyÁzﬁy Áiig Ë−ÍÏÄÎ i{ ¡‰©`‰Á` h ¢‰Á`‰Â`y Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Optimized C code 1 mh £ÇÀÉÏÎ− ©jÂ Çð ð©Ó−® ÀþÎË©Ó ÉËÇ®Ï²Î hm 2 ©ÅÎ ð©ÓˆÉËÇ®ˆ−Ä−ˆÇÉÎfð©ÓˆÀþÎË©Ó ¡j ð©ÓˆÀþÎË©Ó ¢j ÄÇÅ× ©j ÄÇÅ× Âg Õ 3 ©ÅÎ h¡ÉÎË { d¡‰©`‰n`y mh –Ç©ÅÎÍ ÎÇ −Ä−À−ÅÎÍ ©Å ËÇÑ © Çð ¡ hm 4 ©ÅÎ h¢ÉÎË { d¢‰n`‰Â`y mh –Ç©ÅÎÍ ÎÇ −Ä−À−ÅÎÍ ©Å ²ÇÄÏÀÅ Â Çð ¢ hm 5 ©ÅÎ h¢−Å® { d¢‰ﬁ`‰Â`y mh ›þËÂÍ ÍÎÇÉÉ©Å× ÉÇ©ÅÎ ðÇË ¢ÉÎË hm 6 ©ÅÎ Ë−ÍÏÄÎ { ny 7 ®Ç Õ mh ﬁÇ Å−−® ðÇË ©Å©Î©þÄ Î−ÍÎ hm 8 Ë−ÍÏÄÎ i{ h¡ÉÎË h h¢ÉÎËy mh ¡®® Å−ÓÎ ÉËÇ®Ï²Î ÎÇ ÍÏÀ hm 9 ¡ÉÎË iiy mh ›ÇÌ− ¡ÉÎË ÎÇ Å−ÓÎ ²ÇÄÏÀÅ hm 10 ¢ÉÎË i{ ﬁy mh ›ÇÌ− ¢ÉÎË ÎÇ Å−ÓÎ ËÇÑ hm 11 Û Ñ³©Ä− f¢ÉÎË _{ ¢−Å®gy mh ¶−ÍÎ ðÇË ÍÎÇÉÉ©Å× ÉÇ©ÅÎ hm 12 Ë−ÎÏËÅ Ë−ÍÏÄÎy 13 Û Figure 3.37 Original and optimized code to compute element i, k of matrix product for ﬁxed-length arrays. The compiler performs these optimizations automatically. The following is the actual assembly code generated by gcc for function ð©Óˆ ÉËÇ®ˆ−Ä−. We see that four registers are used as follows: c−þÓ holds Ë−ÍÏÄÎ, cË®© holds ¡ÉÎË, cË²Ó holds ¢ÉÎË, and cËÍ© holds ¢−Å®. int fix_prod_ele_opt(fix_matrix A, fix_matrix B, long i, long k) A in %rdi, B in %rsi, i in %rdx, k in %rcx 1 ð©ÓˆÉËÇ®ˆ−Ä−x 2 ÍþÄÊ btj cË®Ó Compute 64 * i 3 þ®®Ê cË®Ój cË®© Compute Aptr = x¡ + 64i = &A[i][0] 4 Ä−þÊ fcËÍ©jcË²Ójrgj cË²Ó Compute Bptr = x¢ + 4k = &B[0][k] 5 Ä−þÊ onprfcË²Ógj cËÍ© Compute Bend = x¢ + 4k + 1024 = &B[N][k] 6 ÀÇÌÄ bnj c−þÓ Set result = 0 7 l‹ux loop: 8 ÀÇÌÄ fcË®©gj c−®Ó Read *Aptr 9 ©ÀÏÄÄ fcË²Ógj c−®Ó Multiply by *Bptr 10 þ®®Ä c−®Ój c−þÓ Add to result 298 Chapter 3 Machine-Level Representation of Programs 11 þ®®Ê brj cË®© Increment Aptr ++ 12 þ®®Ê btrj cË²Ó Increment Bptr += N 13 ²ÀÉÊ cËÍ©j cË²Ó Compare Bptr:Bend 14 ÁÅ− l‹u If !=, goto loop 15 Ë−Éy Ë−Î Return Practice Problem 3.39 (solution page 378) Use Equation 3.1 to explain how the computations of the initial values for ¡ÉÎË, ¢ÉÎË, and ¢−Å® in the C code of Figure 3.37(b) (lines 3–5) correctly describe their computations in the assembly code generated for ð©ÓˆÉËÇ®ˆ−Ä− (lines 3–5). Practice Problem 3.40 (solution page 378) The following C code sets the diagonal elements of one of our ﬁxed-size arrays to ÌþÄ: mh ·−Î þÄÄ ®©þ×ÇÅþÄ −Ä−À−ÅÎÍ ÎÇ ÌþÄ hm ÌÇ©® ð©ÓˆÍ−Îˆ®©þ×fð©ÓˆÀþÎË©Ó ¡j ©ÅÎ ÌþÄg Õ ÄÇÅ× ©y ðÇËf©{ny©zﬁy ©iig ¡‰©`‰©` { ÌþÄy Û When compiled with optimization level kﬂo, gcc generates the following assembly code: 1 ð©ÓˆÍ−Îˆ®©þ×x void fix_set_diag(fix_matrix A, int val) A in %rdi, val in %rsi 2 ÀÇÌÄ bnj c−þÓ 3 l‹oqx 4 ÀÇÌÄ c−Í©j fcË®©jcËþÓg 5 þ®®Ê btvj cËþÓ 6 ²ÀÉÊ bonvvj cËþÓ 7 ÁÅ− l‹oq 8 Ë−Éy Ë−Î Create a C code program ð©ÓˆÍ−Îˆ®©þ×ˆÇÉÎ that uses optimizations similar to those in the assembly code, in the same style as the code in Figure 3.37(b). Use expressions involving the parameter N rather than integer constants, so that your code will work correctly if N is redeﬁned. 3.8.5 Variable-Size Arrays Historically, C only supported multidimensional arrays where the sizes (with the possible exception of the ﬁrst dimension) could be determined at compile time. Section 3.8 Array Allocation and Access 299 Programmers requiring variable-size arrays had to allocate storage for these arrays using functions such as ÀþÄÄÇ² or ²þÄÄÇ², and they had to explicitly encode the mapping of multidimensional arrays into single-dimension ones via row-major in- dexing, as expressed in Equation 3.1. ISO C99 introduced the capability of having array dimension expressions that are computed as the array is being allocated. In the C version of variable-size arrays, we can declare an array ©ÅÎ ¡‰expr1`‰expr2` either as a local variable or as an argument to a function, and then the dimensions of the array are determined by evaluating the expressions expr1 and expr2 at the time the declaration is encountered. So, for example, we can write a function to access element i, j of an n × n array as follows: ©ÅÎ ÌþËˆ−Ä−fÄÇÅ× Åj ©ÅÎ ¡‰Å`‰Å`j ÄÇÅ× ©j ÄÇÅ× Ág Õ Ë−ÎÏËÅ ¡‰©`‰Á`y Û The parameter Å must precede the parameter ¡‰Å`‰Å`, so that the function can compute the array dimensions as the parameter is encountered. Gcc generates code for this referencing function as int var_ele(long n, int A[n][n], long i, long j) n in %rdi, A in %rsi, i in %rdx, j in %rcx 1 ÌþËˆ−Ä−x 2 ©ÀÏÄÊ cË®Ój cË®© Compute n . i 3 Ä−þÊ fcËÍ©jcË®©jrgj cËþÓ Compute x¡ + 4(n . i 4 ÀÇÌÄ fcËþÓjcË²Ójrgj c−þÓ Read from M[x¡ + 4(n . i) + 4j ] 5 Ë−Î As the annotations show, this code computes the address of element i, j as x¡ + 4(n . i) + 4j = x¡ + 4(n . i + j). The address computation is similar to that of the ﬁxed-size array (Section 3.8.3), except that (1) the register usage changes due to added parameter Å, and (2) a multiply instruction is used (line 2) to compute n . i, rather than an Ä−þÊ instruction to compute 3i. We see therefore that referencing variable-size arrays requires only a slight generalization over ﬁxed-size ones. The dynamic version must use a multiplication instruction to scale i by n, rather than a series of shifts and adds. In some processors, this multiplication can incur a signiﬁcant performance penalty, but it is unavoidable in this case. When variable-size arrays are referenced within a loop, the compiler can often optimize the index computations by exploiting the regularity of the access patterns. For example, Figure 3.38(a) shows C code to compute element i, k of the product of two n × n arrays ¡ and ¢. Gcc generates assembly code, which we have recast into C (Figure 3.38(b)). This code follows a different style from the optimized code for the ﬁxed-size array (Figure 3.37), but that is more an artifact of the choices made by the compiler, rather than a fundamental requirement for the two different functions. The code of Figure 3.38(b) retains loop variable Á, both to detect when 300 Chapter 3 Machine-Level Representation of Programs (a) Original C code 1 mh £ÇÀÉÏÎ− ©jÂ Çð ÌþË©þ¾Ä− ÀþÎË©Ó ÉËÇ®Ï²Î hm 2 ©ÅÎ ÌþËˆÉËÇ®ˆ−Ä−fÄÇÅ× Åj ©ÅÎ ¡‰Å`‰Å`j ©ÅÎ ¢‰Å`‰Å`j ÄÇÅ× ©j ÄÇÅ× Âg Õ 3 ÄÇÅ× Áy 4 ©ÅÎ Ë−ÍÏÄÎ { ny 5 6 ðÇËfÁ{nyÁzÅy Áiig 7 Ë−ÍÏÄÎ i{ ¡‰©`‰Á` h ¢‰Á`‰Â`y 8 9 Ë−ÎÏËÅ Ë−ÍÏÄÎy 10 Û (b) Optimized C code mh £ÇÀÉÏÎ− ©jÂ Çð ÌþË©þ¾Ä− ÀþÎË©Ó ÉËÇ®Ï²Î hm ©ÅÎ ÌþËˆÉËÇ®ˆ−Ä−ˆÇÉÎfÄÇÅ× Åj ©ÅÎ ¡‰Å`‰Å`j ©ÅÎ ¢‰Å`‰Å`j ÄÇÅ× ©j ÄÇÅ× Âg Õ ©ÅÎ h¡ËÇÑ { ¡‰©`y ©ÅÎ h¢ÉÎË { d¢‰n`‰Â`y ©ÅÎ Ë−ÍÏÄÎ { ny ÄÇÅ× Áy ðÇËfÁ{nyÁzÅy Áiig Õ Ë−ÍÏÄÎ i{ ¡ËÇÑ‰Á` h h¢ÉÎËy ¢ÉÎË i{ Åy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û Figure 3.38 Original and optimized code to compute element i, k of matrix product for variable-size arrays. The compiler performs these optimizations automatically. the loop has terminated and to index into an array consisting of the elements of row i of ¡. The following is the assembly code for the loop of ÌþËˆÉËÇ®ˆ−Ä−: Registers: n in %rdi, Arow in %rsi, Bptr in %rcx 4n in %r9, result in %eax, j in %edx 1 l‹prx loop: 2 ÀÇÌÄ fcËÍ©jcË®Ójrgj cËv® Read Arow[j] 3 ©ÀÏÄÄ fcË²Ógj cËv® Multiply by *Bptr 4 þ®®Ä cËv®j c−þÓ Add to result 5 þ®®Ê boj cË®Ó j++ 6 þ®®Ê cËwj cË²Ó Bptr += n 7 ²ÀÉÊ cË®©j cË®Ó Compare j:n 8 ÁÅ− l‹pr If !=, goto loop We see that the program makes use of both a scaled value 4n (register cËw) for incrementing ¢ÉÎË as well as the value of n (register cË®©) to check the loop Section 3.9 Heterogeneous Data Structures 301 bounds. The need for two values does not show up in the C code, due to the scaling of pointer arithmetic. We have seen that, with optimizations enabled, gcc is able to recognize pat- terns that arise when a program steps through the elements of a multidimensional array. It can then generate code that avoids the multiplication that would result from a direct application of Equation 3.1. Whether it generates the pointer-based code of Figure 3.37(b) or the array-based code of Figure 3.38(b), these optimiza- tions will signiﬁcantly improve program performance. 3.9 Heterogeneous Data Structures C provides two mechanisms for creating data types by combining objects of dif- ferent types: structures, declared using the keyword ÍÎËÏ²Î, aggregate multiple objects into a single unit; unions, declared using the keyword ÏÅ©ÇÅ, allow an object to be referenced using several different types. 3.9.1 Structures The C ÍÎËÏ²Î declaration creates a data type that groups objects of possibly different types into a single object. The different components of a structure are referenced by names. The implementation of structures is similar to that of arrays in that all of the components of a structure are stored in a contiguous region of memory and a pointer to a structure is the address of its ﬁrst byte. The compiler maintains information about each structure type indicating the byte offset of each ﬁeld. It generates references to structure elements using these offsets as displacements in memory referencing instructions. As an example, consider the following structure declaration: ÍÎËÏ²Î Ë−² Õ ©ÅÎ ©y ©ÅÎ Áy ©ÅÎ þ‰p`y ©ÅÎ hÉy Ûy This structure contains four ﬁelds: two 4-byte values of type ©ÅÎ, a two-element array of type ©ÅÎ, and an 8-byte integer pointer, giving a total of 24 bytes: Offset Contents i 048 16 24 j a[0] a[1] p Observe that array þ is embedded within the structure. The numbers along the top of the diagram give the byte offsets of the ﬁelds from the beginning of the structure. To access the ﬁelds of a structure, the compiler generates code that adds the appropriate offset to the address of the structure. For example, suppose variable Ë 302 Chapter 3 Machine-Level Representation of Programs New to C? Representing an object as a ÍÎËÏ²Î The ÍÎËÏ²Î data type constructor is the closest thing C provides to the objects of C++ and Java. It allows the programmer to keep information about some entity in a single data structure and to reference that information with names. For example, a graphics program might represent a rectangle as a structure: ÍÎËÏ²Î Ë−²Î Õ ÄÇÅ× ÄÄÓy mh ” ²ÇÇË®©ÅþÎ− Çð ÄÇÑ−ËkÄ−ðÎ ²ÇËÅ−Ë hm ÄÇÅ× ÄÄÔy mh » ²ÇÇË®©ÅþÎ− Çð ÄÇÑ−ËkÄ−ðÎ ²ÇËÅ−Ë hm ÏÅÍ©×Å−® ÄÇÅ× Ñ©®Î³y mh „©®Î³ f©Å É©Ó−ÄÍg hm ÏÅÍ©×Å−® ÄÇÅ× ³−©×³Îy mh ¤−©×³Î f©Å É©Ó−ÄÍg hm ÏÅÍ©×Å−® ²ÇÄÇËy mh £Ç®©Å× Çð ²ÇÄÇË hm Ûy We can declare a variable Ë of type ÍÎËÏ²Î Ë−²Î and set its ﬁeld values as follows: ÍÎËÏ²Î Ë−²Î Ëy ËlÄÄÓ { ËlÄÄÔ { ny Ël²ÇÄÇË { nÓƒƒnnƒƒy ËlÑ©®Î³ { ony Ël³−©×³Î { pny where the expression ËlÄÄÓ selects ﬁeld ÄÄÓ of structure Ë. Alternatively, we can both declare the variable and initialize its ﬁelds with a single statement: ÍÎËÏ²Î Ë−²ÎË{Õnjnj nÓƒƒnnƒƒj onj pn Ûy It is common to pass pointers to structures from one place to another rather than copying them. For example, the following function computes the area of a rectangle, where a pointer to the rectangle ÍÎËÏ²Î is passed to the function: ÄÇÅ× þË−þfÍÎËÏ²Î Ë−²Î hËÉg Õ Ë−ÎÏËÅ fhËÉglÑ©®Î³ h fhËÉgl³−©×³Îy Û The expression fhËÉglÑ©®Î³ dereferences the pointer and selects the Ñ©®Î³ ﬁeld of the resulting structure. Parentheses are required, because the compiler would interpret the expression hËÉlÑ©®Î³ as hfËÉlÑ©®Î³g, which is not valid. This combination of dereferencing and ﬁeld selection is so common that C provides an alternative notation using k|. That is, ËÉk|Ñ©®Î³ is equivalent to the expression fhËÉglÑ©®Î³. For example, we can write a function that rotates a rectangle counterclockwise by 90 degrees as ÌÇ©® ËÇÎþÎ−ˆÄ−ðÎfÍÎËÏ²Î Ë−²Î hËÉg Õ mh ¥Ó²³þÅ×− Ñ©®Î³ þÅ® ³−©×³Î hm ÄÇÅ× Î { ËÉk|³−©×³Îy ËÉk|³−©×³Î { ËÉk|Ñ©®Î³y ËÉk|Ñ©®Î³ { Îy mh ·³©ðÎ ÎÇ Å−Ñ ÄÇÑ−ËkÄ−ðÎ ²ÇËÅ−Ë hm ËÉk|ÄÄÓ k{ Îy Û Section 3.9 Heterogeneous Data Structures 303 New to C? Representing an object as a ÍÎËÏ²Î (continued) The objects of C++ and Java are more elaborate than structures in C, in that they also associate a set of methods with an object that can be invoked to perform computation. In C, we would simply write these as ordinary functions, such as the functions þË−þ and ËÇÎþÎ−ˆÄ−ðÎ shown previously. of type ÍÎËÏ²Î Ë−² h is in register cË®©. Then the following code copies element Ëk|© to element Ëk|Á: Registers: r in %rdi 1 ÀÇÌÄ fcË®©gj c−þÓ Get r->i 2 ÀÇÌÄ c−þÓj rfcË®©g Store in r->j Since the offset of ﬁeld © is 0, the address of this ﬁeld is simply the value of Ë.To store into ﬁeld Á, the code adds offset 4 to the address of Ë. To generate a pointer to an object within a structure, we can simply add the ﬁeld’s offset to the structure address. For example, we can generate the pointer dfËk|þ‰o`g by adding offset 8 + 4 . 1 = 12. For pointer Ë in register cË®© and long integer variable © in register cËÍ©, we can generate the pointer value dfËk|þ‰©`g with the single instruction Registers: r in %rdi, i %rsi 1 Ä−þÊ vfcË®©jcËÍ©jrgj cËþÓ Set %rax to &r->a[i] As a ﬁnal example, the following code implements the statement Ëk|É { dËk|þ‰Ëk|© i Ëk|Á`y starting with Ë in register cË®©: Registers: r in %rdi 1 ÀÇÌÄ rfcË®©gj c−þÓ Get r->j 2 þ®®Ä fcË®©gj c−þÓ Add r->i 3 ²ÄÎÊ Extend to 8 bytes 4 Ä−þÊ vfcË®©jcËþÓjrgj cËþÓ Compute &r->a[r->i + r->j] 5 ÀÇÌÊ cËþÓj otfcË®©g Store in r->p As these examples show, the selection of the different ﬁelds of a structure is handled completely at compile time. The machine code contains no information about the ﬁeld declarations or the names of the ﬁelds. 304 Chapter 3 Machine-Level Representation of Programs Practice Problem 3.41 (solution page 379) Consider the following structure declaration: ÍÎËÏ²Î Î−ÍÎ Õ Í³ÇËÎ hÉy ÍÎËÏ²Î Õ Í³ÇËÎ Óy Í³ÇËÎ Ôy ÛÍy ÍÎËÏ²Î Î−ÍÎ hÅ−ÓÎy Ûy This declaration illustrates that one structure can be embedded within another, just as arrays can be embedded within structures and arrays can be embedded within arrays. The following procedure (with some expressions omitted) operates on this structure: ÌÇ©® ÍÎˆ©Å©ÎfÍÎËÏ²Î Î−ÍÎ hÍÎg Õ ÍÎk|ÍlÔ { y ÍÎk|É { y ÍÎk|Å−ÓÎ { y Û A. What are the offsets (in bytes) of the following ﬁelds? É: ÍlÓ: ÍlÔ: Å−ÓÎ: B. How many total bytes does the structure require? C. The compiler generates the following assembly code for ÍÎˆ©Å©Î: void st_init(struct test *st) st in %rdi 1 ÍÎˆ©Å©Îx 2 ÀÇÌÄ vfcË®©gj c−þÓ 3 ÀÇÌÄ c−þÓj onfcË®©g 4 Ä−þÊ onfcË®©gj cËþÓ 5 ÀÇÌÊ cËþÓj fcË®©g 6 ÀÇÌÊ cË®©j opfcË®©g 7 Ë−Î On the basis of this information, ﬁll in the missing expressions in the code for ÍÎˆ©Å©Î. Section 3.9 Heterogeneous Data Structures 305 Practice Problem 3.42 (solution page 379) The following code shows the declaration of a structure of type ¡£¥ and the prototype for a function Î−ÍÎ: ÍÎËÏ²Î ¡£¥ Õ Í³ÇËÎ Ìy ÍÎËÏ²Î ¡£¥ hÉy Ûy Í³ÇËÎ Î−ÍÎfÍÎËÏ²Î ¡£¥ hÉÎËgy When the code for ðÏÅ is compiled, gcc generates the following assembly code: short test(struct ACE *ptr) ptr in %rdi 1 Î−ÍÎx 2 ÀÇÌÄ boj c−þÓ 3 ÁÀÉ l‹p 4 l‹qx 5 ©ÀÏÄÊ fcË®©gj cËþÓ 6 ÀÇÌÊ pfcË®©gj cË®© 7 l‹px 8 Î−ÍÎÊ cË®©j cË®© 9 ÁÅ− l‹q 10 Ë−Éy Ë−Î A. Use your reverse engineering skills to write C code for Î−ÍÎ. B. Describe the data structure that this structure implements and the operation performed by Î−ÍÎ. 3.9.2 Unions Unions provide a way to circumvent the type system of C, allowing a single object to be referenced according to multiple types. The syntax of a union declaration is identical to that for structures, but its semantics are very different. Rather than having the different ﬁelds reference different blocks of memory, they all reference the same block. Consider the following declarations: ÍÎËÏ²Î ·q Õ ²³þË ²y ©ÅÎ ©‰p`y ®ÇÏ¾Ä− Ìy Ûy 306 Chapter 3 Machine-Level Representation of Programs ÏÅ©ÇÅ •q Õ ²³þË ²y ©ÅÎ ©‰p`y ®ÇÏ¾Ä− Ìy Ûy When compiled on an x86-64 Linux machine, the offsets of the ﬁelds, as well as the total size of data types ·q and •q, are as shown in the following table: Type ²© Ì Size ·q 0 4 16 24 •q 00 0 8 (We will see shortly why © has offset 4 in ·q rather than 1, and why Ì has offset 16, rather than 9 or 12.) For pointer É of type ÏÅ©ÇÅ •q h, references Ék|², Ék|©‰n`, and Ék|Ì would all reference the beginning of the data structure. Observe also that the overall size of a union equals the maximum size of any of its ﬁelds. Unions can be useful in several contexts. However, they can also lead to nasty bugs, since they bypass the safety provided by the C type system. One application is when we know in advance that the use of two different ﬁelds in a data structure will be mutually exclusive. Then, declaring these two ﬁelds as part of a union rather than a structure will reduce the total space allocated. For example, suppose we want to implement a binary tree data structure where each leaf node has two ®ÇÏ¾Ä− data values and each internal node has pointers to two children but no data. If we declare this as ÍÎËÏ²Î ÅÇ®−ˆÍ Õ ÍÎËÏ²Î ÅÇ®−ˆÍ hÄ−ðÎy ÍÎËÏ²Î ÅÇ®−ˆÍ hË©×³Îy ®ÇÏ¾Ä− ®þÎþ‰p`y Ûy then every node requires 32 bytes, with half the bytes wasted for each type of node. On the other hand, if we declare a node as ÏÅ©ÇÅ ÅÇ®−ˆÏ Õ ÍÎËÏ²Î Õ ÏÅ©ÇÅ ÅÇ®−ˆÏ hÄ−ðÎy ÏÅ©ÇÅ ÅÇ®−ˆÏ hË©×³Îy Û ©ÅÎ−ËÅþÄy ®ÇÏ¾Ä− ®þÎþ‰p`y Ûy then every node will require just 16 bytes. If Å is a pointer to a node of type ÏÅ©ÇÅ ÅÇ®−ˆÏ h, we would reference the data of a leaf node as Åk|®þÎþ‰n` and Åk|®þÎþ‰o`, and the children of an internal node as Åk|©ÅÎ−ËÅþÄlÄ−ðÎ and Åk|©ÅÎ−ËÅþÄlË©×³Î. Section 3.9 Heterogeneous Data Structures 307 With this encoding, however, there is no way to determine whether a given node is a leaf or an internal node. A common method is to introduce an enumer- ated type deﬁning the different possible choices for the union, and then create a structure containing a tag ﬁeld and the union: ÎÔÉ−®−ð −ÅÏÀ Õ ﬁˆ‹¥¡ƒj ﬁˆ'ﬁ¶¥‡ﬁ¡‹ Û ÅÇ®−ÎÔÉ−ˆÎy ÍÎËÏ²Î ÅÇ®−ˆÎ Õ ÅÇ®−ÎÔÉ−ˆÎ ÎÔÉ−y ÏÅ©ÇÅ Õ ÍÎËÏ²Î Õ ÍÎËÏ²Î ÅÇ®−ˆÎ hÄ−ðÎy ÍÎËÏ²Î ÅÇ®−ˆÎ hË©×³Îy Û ©ÅÎ−ËÅþÄy ®ÇÏ¾Ä− ®þÎþ‰p`y Û ©ÅðÇy Ûy This structure requires a total of 24 bytes: 4 for ÎÔÉ−, and either 8 each for ©ÅðÇl©ÅÎ−ËÅþÄlÄ−ðÎ and ©ÅðÇl©ÅÎ−ËÅþÄlË©×³Î or 16 for ©ÅðÇl®þÎþ. As we will discuss shortly, an additional 4 bytes of padding is required between the ﬁeld for ÎÔÉ− and the union elements, bringing the total structure size to 4 + 4 + 16 = 24. In this case, the savings gain of using a union is small relative to the awkwardness of the resulting code. For data structures with more ﬁelds, the savings can be more compelling. Unions can also be used to access the bit patterns of different data types. For example, suppose we use a simple cast to convert a value ® of type ®ÇÏ¾Ä− to a value Ï of type ÏÅÍ©×Å−® ÄÇÅ×: ÏÅÍ©×Å−® ÄÇÅ× Ï { fÏÅÍ©×Å−® ÄÇÅ×g ®y Value Ï will be an integer representation of ®. Except for the case where ® is 0.0, the bit representation of Ï will be very different from that of ®. Now consider the following code to generate a value of type ÏÅÍ©×Å−® ÄÇÅ× from a ®ÇÏ¾Ä−: ÏÅÍ©×Å−® ÄÇÅ× ®ÇÏ¾Ä−p¾©ÎÍf®ÇÏ¾Ä− ®g Õ ÏÅ©ÇÅ Õ ®ÇÏ¾Ä− ®y ÏÅÍ©×Å−® ÄÇÅ× Ïy Û Î−ÀÉy Î−ÀÉl® { ®y Ë−ÎÏËÅ Î−ÀÉlÏy Ûy In this code, we store the argument in the union using one data type and access it using another. The result will be that Ï will have the same bit representation as ®, including ﬁelds for the sign bit, the exponent, and the signiﬁcand, as described in 308 Chapter 3 Machine-Level Representation of Programs Section 3.11. The numeric value of Ï will bear no relation to that of ®, except for the case when ® is 0.0. When using unions to combine data types of different sizes, byte-ordering issues can become important. For example, suppose we write a procedure that will create an 8-byte ®ÇÏ¾Ä− using the bit patterns given by two 4-byte ÏÅÍ©×Å−® values: ®ÇÏ¾Ä− ÏÏp®ÇÏ¾Ä−fÏÅÍ©×Å−® ÑÇË®nj ÏÅÍ©×Å−® ÑÇË®og Õ ÏÅ©ÇÅ Õ ®ÇÏ¾Ä− ®y ÏÅÍ©×Å−® Ï‰p`y Û Î−ÀÉy Î−ÀÉlÏ‰n` { ÑÇË®ny Î−ÀÉlÏ‰o` { ÑÇË®oy Ë−ÎÏËÅ Î−ÀÉl®y Û On a little-endian machine, such as an x86-64 processor, argument ÑÇË®n will become the low-order 4 bytes of ®, while ÑÇË®o will become the high-order 4 bytes. On a big-endian machine, the role of the two arguments will be reversed. Practice Problem 3.43 (solution page 380) Suppose you are given the job of checking that a C compiler generates the proper code for structure and union access. You write the following structure declaration: ÎÔÉ−®−ð ÏÅ©ÇÅ Õ ÍÎËÏ²Î Õ ÄÇÅ× Ïy Í³ÇËÎ Ìy ²³þË Ñy Û Îoy ÍÎËÏ²Î Õ ©ÅÎ þ‰p`y ²³þË hÉy Û Îpy Û ÏˆÎÔÉ−y You write a series of functions of the form ÌÇ©® ×−ÎfÏˆÎÔÉ− hÏÉj type h®−ÍÎg Õ h®−ÍÎ { expry Û with different access expressions expr and with destination data type type set according to type associated with expr. You then examine the code generated when compiling the functions to see if they match your expectations. Section 3.9 Heterogeneous Data Structures 309 Suppose in these functions that ÏÉ and ®−ÍÎ are loaded into registers cË®© and cËÍ©, respectively. Fill in the following table with data type type and sequences of one to three instructions to compute the expression and store the result at ®−ÍÎ. expr type Code ÏÉk|ÎolÏ ÄÇÅ× ÀÇÌÊ fcË®©gj cËþÓ ÀÇÌÊ cËþÓj fcËÍ©g ÏÉk|ÎolÌ dÏÉk|ÎolÑ ÏÉk|Îplþ ÏÉk|Îplþ‰ÏÉk|ÎolÏ` hÏÉk|ÎplÉ 3.9.3 Data Alignment Many computer systems place restrictions on the allowable addresses for the primitive data types, requiring that the address for some objects must be a multiple of some value K (typically 2, 4, or 8). Such alignment restrictions simplify the design of the hardware forming the interface between the processor and the memory system. For example, suppose a processor always fetches 8 bytes from memory with an address that must be a multiple of 8. If we can guarantee that any ®ÇÏ¾Ä− will be aligned to have its address be a multiple of 8, then the value can be read or written with a single memory operation. Otherwise, we may need to perform two memory accesses, since the object might be split across two 8-byte memory blocks. The x86-64 hardware will work correctly regardless of the alignment of data. However, Intel recommends that data be aligned to improve memory system performance. Their alignment rule is based on the principle that any primitive object of K bytes must have an address that is a multiple of K. We can see that this rule leads to the following alignments: K Types 1 ²³þË 2 Í³ÇËÎ 4 ©ÅÎ, ðÄÇþÎ 8 ÄÇÅ×, ®ÇÏ¾Ä−, ²³þË h 310 Chapter 3 Machine-Level Representation of Programs Alignment is enforced by making sure that every data type is organized and allocated in such a way that every object within the type satisﬁes its alignment restrictions. The compiler places directives in the assembly code indicating the desired alignment for global data. For example, the assembly-code declaration of the jump table on page 271 contains the following directive on line 2: lþÄ©×Å v This ensures that the data following it (in this case the start of the jump table) will start with an address that is a multiple of 8. Since each table entry is 8 bytes long, the successive elements will obey the 8-byte alignment restriction. For code involving structures, the compiler may need to insert gaps in the ﬁeld allocation to ensure that each structure element satisﬁes its alignment re- quirement. The structure will then have some required alignment for its starting address. For example, consider the structure declaration ÍÎËÏ²Î ·o Õ ©ÅÎ ©y ²³þË ²y ©ÅÎ Áy Ûy Suppose the compiler used the minimal 9-byte allocation, diagrammed as follows: Offset Contents i 04 59 cj Then it would be impossible to satisfy the 4-byte alignment requirement for both ﬁelds © (offset 0) and Á (offset 5). Instead, the compiler inserts a 3-byte gap (shown here as shaded in blue) between ﬁelds ² and Á: Offset Contents i 04 5 812 cj As a result, Á has offset 8, and the overall structure size is 12 bytes. Furthermore, the compiler must ensure that any pointer É of type ÍÎËÏ²Î ·oh satisﬁes a 4-byte alignment. Using our earlier notation, let pointer É have value xÉ. Then xÉ must be a multiple of 4. This guarantees that both Ék|© (address xÉ) and Ék|Á (address xÉ + 8) will satisfy their 4-byte alignment requirements. In addition, the compiler may need to add padding to the end of the structure so that each element in an array of structures will satisfy its alignment requirement. For example, consider the following structure declaration: Section 3.9 Heterogeneous Data Structures 311 ÍÎËÏ²Î ·p Õ ©ÅÎ ©y ©ÅÎ Áy ²³þË ²y Ûy If we pack this structure into 9 bytes, we can still satisfy the alignment requirements for ﬁelds © and Á by making sure that the starting address of the structure satisﬁes a 4-byte alignment requirement. Consider, however, the following declaration: ÍÎËÏ²Î ·p ®‰r`y With the 9-byte allocation, it is not possible to satisfy the alignment requirement for each element of ®, because these elements will have addresses x®, x® + 9, x® + 18, and x® + 27. Instead, the compiler allocates 12 bytes for structure ·p, with the ﬁnal 3 bytes being wasted space: Offset Contents i 04 9812 cj That way, the elements of ® will have addresses x®, x® + 12, x® + 24, and x® + 36. As long as x® is a multiple of 4, all of the alignment restrictions will be satisﬁed. Practice Problem 3.44 (solution page 381) For each of the following structure declarations, determine the offset of each ﬁeld, the total size of the structure, and its alignment requirement for x86-64: A. ÍÎËÏ²Î –o Õ Í³ÇËÎ ©y ©ÅÎ ²y ©ÅÎ hÁy Í³ÇËÎ h®y Ûy B. ÍÎËÏ²Î –p Õ ©ÅÎ ©‰p`y ²³þË ²‰v`y Í³ÇËÎ Í‰r`y ÄÇÅ× hÁy Ûy C. ÍÎËÏ²Î –q Õ ÄÇÅ× Ñ‰p`y ©ÅÎ h²‰p` Ûy D. ÍÎËÏ²Î –r Õ ²³þË Ñ‰ot`y ²³þË h²‰p` Ûy E. ÍÎËÏ²Î –s Õ ÍÎËÏ²Î –r þ‰p`y ÍÎËÏ²Î –o Î Ûy Practice Problem 3.45 (solution page 381) Answer the following for the structure declaration ÍÎËÏ²Î Õ ©ÅÎ hþy ðÄÇþÎ ¾y ²³þË ²y Í³ÇËÎ ®y ÄÇÅ× −y ®ÇÏ¾Ä− ðy 312 Chapter 3 Machine-Level Representation of Programs Aside A case of mandatory alignment For most x86-64 instructions, keeping data aligned improves efﬁciency, but it does not affect program behavior. On the other hand, some models of Intel and AMD processors will not work correctly with unaligned data for some of the SSE instructions implementing multimedia operations. These instructions operate on 16-byte blocks of data, and the instructions that transfer data between the SSE unit and memory require the memory addresses to be multiples of 16. Any attempt to access memory with an address that does not satisfy this alignment will lead to an exception (see Section 8.1), with the default behavior for the program to terminate. As a result, any compiler and run-time system for an x86-64 processor must ensure that any memory allocated to hold a data structure that may be read from or stored into an SSE register must satisfy a 16-byte alignment. This requirement has the following two consequences: . The starting address for any block generated by a memory allocation function (þÄÄÇ²þ, ÀþÄÄÇ², ²þÄÄÇ²,or Ë−þÄÄÇ²) must be a multiple of 16. . The stack frame for most functions must be aligned on a 16-byte boundary. (This requirement has a number of exceptions.) More recent versions of x86-64 processors implement the AVX multimedia instructions. In addi- tion to providing a superset of the SSE instructions, processors supporting AVX also do not have a mandatory alignment requirement. ©ÅÎ ×y ²³þË h³y Û Ë−²y A. What are the byte offsets of all the ﬁelds in the structure? B. What is the total size of the structure? C. Rearrange the ﬁelds of the structure to minimize wasted space, and then show the byte offsets and total size for the rearranged structure. 3.10 Combining Control and Data in Machine-Level Programs So far, we have looked separately at how machine-level code implements the control aspects of a program and how it implements different data structures. In this section, we look at ways in which data and control interact with each other. We start by taking a deep look into pointers, one of the most important concepts in the C programming language, but one for which many programmers only have a shallow understanding. We review the use of the symbolic debugger gdb for examining the detailed operation of machine-level programs. Next, we see how understanding machine-level programs enables us to study buffer overﬂow, an important security vulnerability in many real-world systems. Finally, we examine Section 3.10 Combining Control and Data in Machine-Level Programs 313 how machine-level programs implement cases where the amount of stack storage required by a function can vary from one execution to another. 3.10.1 Understanding Pointers Pointers are a central feature of the C programming language. They serve as a uniform way to generate references to elements within different data structures. Pointers are a source of confusion for novice programmers, but the underlying concepts are fairly simple. Here we highlight some key principles of pointers and their mapping into machine code. . Every pointer has an associated type. This type indicates what kind of object the pointer points to. Using the following pointer declarations as illustrations ©ÅÎ h©Éy ²³þË hh²ÉÉy variable ©É is a pointer to an object of type ©ÅÎ, while ²ÉÉ is a pointer to an object that itself is a pointer to an object of type ²³þË. In general, if the object has type T , then the pointer has type hT . The special ÌÇ©® h type represents a generic pointer. For example, the ÀþÄÄÇ² function returns a generic pointer, which is converted to a typed pointer via either an explicit cast or by the implicit casting of the assignment operation. Pointer types are not part of machine code; they are an abstraction provided by C to help programmers avoid addressing errors. . Every pointer has a value. This value is an address of some object of the designated type. The special ﬁ•‹‹ (0) value indicates that the pointer does not point anywhere. . Pointers are created with the ‘d’ operator. This operator can be applied to any C expression that is categorized as an lvalue, meaning an expression that can appear on the left side of an assignment. Examples include variables and the elements of structures, unions, and arrays. We have seen that the machine- code realization of the ‘d’ operator often uses the Ä−þÊ instruction to compute the expression value, since this instruction is designed to compute the address of a memory reference. . Pointers are dereferenced with the ‘h’ operator. The result is a value having the type associated with the pointer. Dereferencing is implemented by a memory reference, either storing to or retrieving from the speciﬁed address. . Arrays and pointers are closely related. The name of an array can be referenced (but not updated) as if it were a pointer variable. Array referencing (e.g., þ‰q`) has the exact same effect as pointer arithmetic and dereferencing (e.g., hfþiqg). Both array referencing and pointer arithmetic require scaling the offsets by the object size. When we write an expression Éi© for pointer É with value p, the resulting address is computed as p + L . i, where L is the size of the data type associated with É. 314 Chapter 3 Machine-Level Representation of Programs . Casting from one type of pointer to another changes its type but not its value. One effect of casting is to change any scaling of pointer arithmetic. So, for example, if É is a pointer of type ²³þË h having value p, then the expression f©ÅÎ hg Éiu computes p + 28, while f©ÅÎ hg fÉiug computes p + 7. (Recall that casting has higher precedence than addition.) . Pointers can also point to functions. This provides a powerful capability for storing and passing references to code, which can be invoked in some other part of the program. For example, if we have a function deﬁned by the proto- type ©ÅÎ ðÏÅf©ÅÎ Ój ©ÅÎ hÉgy then we can declare and assign a pointer ðÉ to this function by the following code sequence: ©ÅÎ fhðÉgf©ÅÎj ©ÅÎ hgy ðÉ { ðÏÅy We can then invoke the function using this pointer: ©ÅÎÔ{oy ©ÅÎ Ë−ÍÏÄÎ { ðÉfqj dÔgy The value of a function pointer is the address of the ﬁrst instruction in the machine-code representation of the function. New to C? Function pointers The syntax for declaring function pointers is especially difﬁcult for novice programmers to understand. For a declaration such as ©ÅÎ fhðgf©ÅÎhgy it helps to read it starting from the inside (starting with ‘ð’) and working outward. Thus, we see that ð is a pointer, as indicated by fhðg. It is a pointer to a function that has a single ©ÅÎ h as an argument, as indicated by fhðgf©ÅÎhg. Finally, we see that it is a pointer to a function that takes an ©ÅÎ h as an argument and returns ©ÅÎ. The parentheses around hð are required, because otherwise the declaration ©ÅÎ hðf©ÅÎhgy would be read as f©ÅÎ hg ðf©ÅÎhgy That is, it would be interpreted as a function prototype, declaring a function ð that has an ©ÅÎ h as its argument and returns an ©ÅÎ h. Kernighan and Ritchie [61, Sect. 5.12] present a helpful tutorial on reading C declarations. Section 3.10 Combining Control and Data in Machine-Level Programs 315 3.10.2 Life in the Real World: Using the gdb Debugger The GNU debugger gdb provides a number of useful features to support the run-time evaluation and analysis of machine-level programs. With the examples and exercises in this book, we attempt to infer the behavior of a program by just looking at the code. Using gdb, it becomes possible to study the behavior by watching the program in action while having considerable control over its execution. Figure 3.39 shows examples of some gdb commands that help when working with machine-level x86-64 programs. It is very helpful to ﬁrst run objdump to get a disassembled version of the program. Our examples are based on running gdb on the ﬁle ÉËÇ×, described and disassembled on page 211. We start gdb with the following command line: Ä©ÅÏÓ| gdb prog The general scheme is to set breakpoints near points of interest in the pro- gram. These can be set to just after the entry of a function or at a program address. When one of the breakpoints is hit during program execution, the program will halt and return control to the user. From a breakpoint, we can examine different registers and memory locations in various formats. We can also single-step the program, running just a few instructions at a time, or we can proceed to the next breakpoint. As our examples suggest, gdb has an obscure command syntax, but the online help information (invoked within gdb with the ³−ÄÉ command) overcomes this shortcoming. Rather than using the command-line interface to gdb, many pro- grammers prefer using ddd, an extension to gdb that provides a graphical user interface. 3.10.3 Out-of-Bounds Memory References and Buffer Overﬂow We have seen that C does not perform any bounds checking for array references, and that local variables are stored on the stack along with state information such as saved register values and return addresses. This combination can lead to serious program errors, where the state stored on the stack gets corrupted by a write to an out-of-bounds array element. When the program then tries to reload the register or execute a Ë−Î instruction with this corrupted state, things can go seriously wrong. A particularly common source of state corruption is known as buffer overﬂow. Typically, some character array is allocated on the stack to hold a string, but the size of the string exceeds the space allocated for the array. This is demonstrated by the following program example: mh 'ÀÉÄ−À−ÅÎþÎ©ÇÅ Çð Ä©¾ËþËÔ ðÏÅ²Î©ÇÅ ×−ÎÍfg hm ²³þË h×−ÎÍf²³þË hÍg Õ ©ÅÎ ²y ²³þË h®−ÍÎ { Íy 316 Chapter 3 Machine-Level Representation of Programs Command Effect Starting and stopping ÊÏ©Î Exit gdb ËÏÅ Run your program (give command-line arguments here) Â©ÄÄ Stop your program Breakpoints ¾Ë−þÂ ÀÏÄÎÍÎÇË− Set breakpoint at entry to function ÀÏÄÎÍÎÇË− ¾Ë−þÂ hnÓrnnsrn Set breakpoint at address nÓrnnsrn ®−Ä−Î− o Delete breakpoint 1 ®−Ä−Î− Delete all breakpoints Execution ÍÎ−É© Execute one instruction ÍÎ−É© r Execute four instructions Å−ÓÎ© Like ÍÎ−É©, but proceed through function calls ²ÇÅÎ©ÅÏ− Resume execution ð©Å©Í³ Run until current function returns Examining code ®©ÍþÍ Disassemble current function ®©ÍþÍ ÀÏÄÎÍÎÇË− Disassemble function ÀÏÄÎÍÎÇË− ®©ÍþÍ nÓrnnsrr Disassemble function around address nÓrnnsrr ®©ÍþÍ nÓrnnsrnj nÓrnnsr® Disassemble code within speciﬁed address range ÉË©ÅÎ mÓ bË©É Print program counter in hex Examining data ÉË©ÅÎ bËþÓ Print contents of cËþÓ in decimal ÉË©ÅÎ mÓ bËþÓ Print contents of cËþÓ in hex ÉË©ÅÎ mÎ bËþÓ Print contents of cËþÓ in binary ÉË©ÅÎ nÓonn Print decimal representation of nÓonn ÉË©ÅÎ mÓ sss Print hex representation of 555 ÉË©ÅÎ mÓ fbËÍÉivg Print contents of cËÍÉ plus 8 in hex ÉË©ÅÎ hfÄÇÅ× hg nÓuððððððð−vov Print long integer at address nÓuððððððð−vov ÉË©ÅÎ hfÄÇÅ× hg fbËÍÉivg Print long integer at address cËÍÉ +8 Ómp× nÓuððððððð−vov Examine two (8-byte) words starting at address nÓuððððððð−vov Ómpn¾ ÀÏÄÎÍÎÇË− Examine ﬁrst 20 bytes of function ÀÏÄÎÍÎÇË− Useful information ©ÅðÇ ðËþÀ− Information about current stack frame ©ÅðÇ Ë−×©ÍÎ−ËÍ Values of all the registers ³−ÄÉ Get information about gdb Figure 3.39 Example gdb commands. These examples illustrate some of the ways gdb supports debugging of machine-level programs. Section 3.10 Combining Control and Data in Machine-Level Programs 317 Figure 3.40 Stack organization for −²³Ç function. Character array ¾Ïð is just part of the saved state. An out-of- bounds write to ¾Ïð can corrupt the program state. Stack frame for caller Stack frame for echo Return address %rsp+24 [7] buf = %rsp[6][5][4][3][2][1][0] Ñ³©Ä− ff² { ×−Î²³þËfgg _{ ’¿Å’ dd ² _{ ¥ﬂƒg h®−ÍÎii { ²y ©ð f² {{ ¥ﬂƒ dd ®−ÍÎ {{ Íg mh ﬁÇ ²³þËþ²Î−ËÍ Ë−þ® hm Ë−ÎÏËÅ ﬁ•‹‹y h®−ÍÎii { ’¿n’y mh ¶−ËÀ©ÅþÎ− ÍÎË©Å× hm Ë−ÎÏËÅ Íy Û mh ‡−þ® ©ÅÉÏÎ Ä©Å− þÅ® ÑË©Î− ©Î ¾þ²Â hm ÌÇ©® −²³Çfg Õ ²³þË ¾Ïð‰v`y mh „þÔ ÎÇÇ ÍÀþÄÄ_ hm ×−ÎÍf¾Ïðgy ÉÏÎÍf¾Ïðgy Û The preceding code shows an implementation of the library function ×−ÎÍ to demonstrate a serious problem with this function. It reads a line from the standard input, stopping when either a terminating newline character or some error condition is encountered. It copies this string to the location designated by argument Í and terminates the string with a null character. We show the use of ×−ÎÍ in the function −²³Ç, which simply reads a line from standard input and echos it back to standard output. The problem with ×−ÎÍ is that it has no way to determine whether sufﬁcient space has been allocated to hold the entire string. In our −²³Ç example, we have purposely made the buffer very small—just eight characters long. Any string longer than seven characters will cause an out-of-bounds write. By examining the assembly code generated by gcc for −²³Ç, we can infer how the stack is organized: void echo() 1 −²³Çx 2 ÍÏ¾Ê bprj cËÍÉ Allocate 24 bytes on stack 3 ÀÇÌÊ cËÍÉj cË®© Compute buf as %rsp 4 ²þÄÄ ×−ÎÍ Call gets 5 ÀÇÌÊ cËÍÉj cË®© Compute buf as %rsp 318 Chapter 3 Machine-Level Representation of Programs 6 ²þÄÄ ÉÏÎÍ Call puts 7 þ®®Ê bprj cËÍÉ Deallocate stack space 8 Ë−Î Return Figure 3.40 illustrates the stack organization during the execution of −²³Ç.The program allocates 24 bytes on the stack by subtracting 24 from the stack pointer (line 2). Character ¾Ïð is positioned at the top of the stack, as can be seen by the fact that cËÍÉ is copied to cË®© to be used as the argument to the calls to both ×−ÎÍ and ÉÏÎÍ. The 16 bytes between ¾Ïð and the stored return pointer are not used. As long as the user types at most seven characters, the string returned by ×−ÎÍ (including the terminating null) will ﬁt within the space allocated for ¾Ïð. A longer string, however, will cause ×−ÎÍ to overwrite some of the information stored on the stack. As the string gets longer, the following information will get corrupted: Characters typed Additional corrupted state 0–7 None 9–23 Unused stack space 24–31 Return address 32+ Saved state in caller No serious consequence occurs for strings of up to 23 characters, but beyond that, the value of the return pointer, and possibly additional saved state, will be corrupted. If the stored value of the return address is corrupted, then the Ë−Î instruction (line 8) will cause the program to jump to a totally unexpected location. None of these behaviors would seem possible based on the C code. The impact of out-of-bounds writing to memory by functions such as ×−ÎÍ can only be understood by studying the program at the machine-code level. Our code for −²³Ç is simple but sloppy. A better version involves using the function ð×−ÎÍ, which includes as an argument a count on the maximum number of bytes to read. Problem 3.71 asks you to write an echo function that can handle an input string of arbitrary length. In general, using ×−ÎÍ or any function that can overﬂow storage is considered a bad programming practice. Unfortunately, a number of commonly used library functions, including ÍÎË²ÉÔ, ÍÎË²þÎ, and ÍÉË©ÅÎð, have the property that they can generate a byte sequence without being given any indication of the size of the destination buffer [97]. Such conditions can lead to vulnerabilities to buffer overﬂow. Practice Problem 3.46 (solution page 382) Figure 3.41 shows a (low-quality) implementation of a function that reads a line from standard input, copies the string to newly allocated storage, and returns a pointer to the result. Consider the following scenario. Procedure ×−ÎˆÄ©Å− is called with the return address equal to nÓrnnuut and register cË¾Ó equal to nÓnopqrstuvw¡¢£⁄¥ƒ.You type in the string nopqrstuvwnopqrstuvwnopqr Section 3.10 Combining Control and Data in Machine-Level Programs 319 (a) C code mh ¶³©Í ©Í Ì−ËÔ ÄÇÑkÊÏþÄ©ÎÔ ²Ç®−l 'Î ©Í ©ÅÎ−Å®−® ÎÇ ©ÄÄÏÍÎËþÎ− ¾þ® ÉËÇ×ËþÀÀ©Å× ÉËþ²Î©²−Íl ·−− –Ëþ²Î©²− –ËÇ¾Ä−À qlrtl hm ²³þË h×−ÎˆÄ©Å−fg Õ ²³þË ¾Ïð‰r`y ²³þË hË−ÍÏÄÎy ×−ÎÍf¾Ïðgy Ë−ÍÏÄÎ { ÀþÄÄÇ²fÍÎËÄ−Åf¾Ïðggy ÍÎË²ÉÔfË−ÍÏÄÎj ¾Ïðgy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Disassembly up through call to ×−ÎÍ char *get_line() 1 nnnnnnnnnnrnnupn z×−ÎˆÄ©Å−|x 2 rnnupnx sq ÉÏÍ³ cË¾Ó 3 rnnupox rv vq −² on ÍÏ¾ bnÓonjcËÍÉ Diagram stack at this point 4 rnnupsx rv vw −u ÀÇÌ cËÍÉjcË®© 5 rnnupvx −v uq ðð ðð ðð ²þÄÄÊ rnntþn z×−ÎÍ| Modify diagram to show stack contents at this point Figure 3.41 C and disassembled code for Practice Problem 3.46. The program terminates with a segmentation fault. You run gdb and determine that the error occurs during the execution of the Ë−Î instruction of ×−ÎˆÄ©Å−. A. Fill in the diagram that follows, indicating as much as you can about the stack just after executing the instruction at line 3 in the disassembly. Label the quantities stored on the stack (e.g., “Return address”) on the right, and their hexadecimal values (if known) within the box. Each box represents 8 bytes. Indicate the position of cËÍÉ. Recall that the ASCII codes for characters n–w are nÓqn–nÓqw. 00 00 00 00 00 40 00 76 Return address B. Modify your diagram to show the effect of the call to ×−ÎÍ (line 5). C. To what address does the program attempt to return? 320 Chapter 3 Machine-Level Representation of Programs D. What register(s) have corrupted value(s) when ×−ÎˆÄ©Å− returns? E. Besides the potential for buffer overﬂow, what two other things are wrong with the code for ×−ÎˆÄ©Å−? A more pernicious use of buffer overﬂow is to get a program to perform a function that it would otherwise be unwilling to do. This is one of the most common methods to attack the security of a system over a computer network. Typically, the program is fed with a string that contains the byte encoding of some executable code, called the exploit code, plus some extra bytes that overwrite the return address with a pointer to the exploit code. The effect of executing the Ë−Î instruction is then to jump to the exploit code. In one form of attack, the exploit code then uses a system call to start up a shell program, providing the attacker with a range of operating system functions. In another form, the exploit code performs some otherwise unauthorized task, repairs the damage to the stack, and then executes Ë−Î a second time, causing an (apparently) normal return to the caller. As an example, the famous Internet worm of November 1988 used four dif- ferent ways to gain access to many of the computers across the Internet. One was a buffer overﬂow attack on the ﬁnger daemon ð©Å×−Ë®, which serves requests by the ﬁnger command. By invoking ﬁnger with an appropriate string, the worm could make the daemon at a remote site have a buffer overﬂow and execute code that gave the worm access to the remote system. Once the worm gained access to a system, it would replicate itself and consume virtually all of the machine’s comput- ing resources. As a consequence, hundreds of machines were effectively paralyzed until security experts could determine how to eliminate the worm. The author of the worm was caught and prosecuted. He was sentenced to 3 years probation, 400 hours of community service, and a $10,500 ﬁne. Even to this day, however, people continue to ﬁnd security leaks in systems that leave them vulnerable to buffer overﬂow attacks. This highlights the need for careful programming. Any interface to the external environment should be made “bulletproof” so that no behavior by an external agent can cause the system to misbehave. 3.10.4 Thwarting Buffer Overﬂow Attacks Buffer overﬂow attacks have become so pervasive and have caused so many problems with computer systems that modern compilers and operating systems have implemented mechanisms to make it more difﬁcult to mount these attacks and to limit the ways by which an intruder can seize control of a system via a buffer overﬂow attack. In this section, we will present mechanisms that are provided by recent versions of gcc for Linux. Stack Randomization In order to insert exploit code into a system, the attacker needs to inject both the code as well as a pointer to this code as part of the attack string. Generating Section 3.10 Combining Control and Data in Machine-Level Programs 321 Aside Worms and viruses Both worms and viruses are pieces of code that attempt to spread themselves among computers. As described by Spafford [105], a worm is a program that can run by itself and can propagate a fully working version of itself to other machines. A virus is a piece of code that adds itself to other programs, including operating systems. It cannot run independently. In the popular press, the term “virus” is used to refer to a variety of different strategies for spreading attacking code among systems, and so you will hear people saying “virus” for what more properly should be called a “worm.” this pointer requires knowing the stack address where the string will be located. Historically, the stack addresses for a program were highly predictable. For all systems running the same combination of program and operating system version, the stack locations were fairly stable across many machines. So, for example, if an attacker could determine the stack addresses used by a common Web server, it could devise an attack that would work on many machines. Using infectious disease as an analogy, many systems were vulnerable to the exact same strain of a virus, a phenomenon often referred to as a security monoculture [96]. The idea of stack randomization is to make the position of the stack vary from one run of a program to another. Thus, even if many machines are running identical code, they would all be using different stack addresses. This is implemented by allocating a random amount of space between 0 and n bytes on the stack at the start of a program, for example, by using the allocation function þÄÄÇ²þ, which allocates space for a speciﬁed number of bytes on the stack. This allocated space is not used by the program, but it causes all subsequent stack locations to vary from one execution of a program to another. The allocation range n needs to be large enough to get sufﬁcient variations in the stack addresses, yet small enough that it does not waste too much space in the program. The following code shows a simple way to determine a “typical” stack address: ©ÅÎ Àþ©Åfg Õ ÄÇÅ× ÄÇ²þÄy ÉË©ÅÎðf‘ÄÇ²þÄ þÎ cÉ¿Å‘j dÄÇ²þÄgy Ë−ÎÏËÅ ny Û This code simply prints the address of a local variable in the Àþ©Å function. Running the code 10,000 times on a Linux machine in 32-bit mode, the addresses ranged from nÓððuð²sw² to nÓðððð®nw², a range of around 223. Running in 64- bit mode on the newer machine, the addresses ranged from nÓuðððnnno¾twv to nÓuððððððþþrþv, a range of nearly 232. Stack randomization has become standard practice in Linux systems. It is one of a larger class of techniques known as address-space layout randomization, or ASLR [99]. With ASLR, different parts of the program, including program code, library code, stack, global variables, and heap data, are loaded into different 322 Chapter 3 Machine-Level Representation of Programs regions of memory each time a program is run. That means that a program running on one machine will have very different address mappings than the same program running on other machines. This can thwart some forms of attack. Overall, however, a persistent attacker can overcome randomization by brute force, repeatedly attempting attacks with different addresses. A common trick is to include a long sequence of ÅÇÉ (pronounced “no op,” short for “no operation”) instructions before the actual exploit code. Executing this instruction has no ef- fect, other than incrementing the program counter to the next instruction. As long as the attacker can guess an address somewhere within this sequence, the program will run through the sequence and then hit the exploit code. The common term for this sequence is a “nop sled” [97], expressing the idea that the program “slides” through the sequence. If we set up a 256-byte nop sled, then the randomization over n = 223 can be cracked by enumerating 215 = 32,768 starting addresses, which is entirely feasible for a determined attacker. For the 64-bit case, trying to enumer- ate 224 = 16,777,216 is a bit more daunting. We can see that stack randomization and other aspects of ASLR can increase the effort required to successfully attack a system, and therefore greatly reduce the rate at which a virus or worm can spread, but it cannot provide a complete safeguard. Practice Problem 3.47 (solution page 383) Running our stack-checking code 10,000 times on a system running Linux ver- sion 2.6.16, we obtained addresses ranging from a minimum of nÓðððð¾usr to a maximum of nÓðððð®usr. A. What is the approximate range of addresses? B. If we attempted a buffer overrun with a 128-byte nop sled, about how many attempts would it take to test all starting addresses? Stack Corruption Detection A second line of defense is to be able to detect when a stack has been corrupted. We saw in the example of the −²³Ç function (Figure 3.40) that the corruption typically occurs when the program overruns the bounds of a local buffer. In C, there is no reliable way to prevent writing beyond the bounds of an array. Instead, the program can attempt to detect when such a write has occurred before it can have any harmful effects. Recent versions of gcc incorporate a mechanism known as a stack protector into the generated code to detect buffer overruns. The idea is to store a special canary value4 in the stack frame between any local buffer and the rest of the stack state, as illustrated in Figure 3.42 [26, 97]. This canary value, also referred to as a guard value, is generated randomly each time the program runs, and so there is no 4. The term “canary” refers to the historic use of these birds to detect the presence of dangerous gases in coal mines. Section 3.10 Combining Control and Data in Machine-Level Programs 323 Stack frame for caller Stack frame for echo Return address Canary %rsp+24 [7] buf = %rsp[6][5][4][3][2][1][0] Figure 3.42 Stack organization for −²³Ç function with stack protector enabled. A special “canary” value is positioned between array ¾Ïð and the saved state. The code checks the canary value to determine whether or not the stack state has been corrupted. easy way for an attacker to determine what it is. Before restoring the register state and returning from the function, the program checks if the canary has been altered by some operation of this function or one that it has called. If so, the program aborts with an error. Recent versions of gcc try to determine whether a function is vulnerable to a stack overﬂow and insert this type of overﬂow detection automatically. In fact, for our earlier demonstration of stack overﬂow, we had to give the command-line option kðÅÇkÍÎþ²ÂkÉËÇÎ−²ÎÇË to prevent gcc from inserting this code. Compiling the function −²³Ç without this option, and hence with the stack protector enabled, gives the following assembly code: void echo() 1 −²³Çx 2 ÍÏ¾Ê bprj cËÍÉ Allocate 24 bytes on stack 3 ÀÇÌÊ cðÍxrnj cËþÓ Retrieve canary 4 ÀÇÌÊ cËþÓj vfcËÍÉg Store on stack 5 ÓÇËÄ c−þÓj c−þÓ Zero out register 6 ÀÇÌÊ cËÍÉj cË®© Compute buf as %rsp 7 ²þÄÄ ×−ÎÍ Call gets 8 ÀÇÌÊ cËÍÉj cË®© Compute buf as %rsp 9 ²þÄÄ ÉÏÎÍ Call puts 10 ÀÇÌÊ vfcËÍÉgj cËþÓ Retrieve canary 11 ÓÇËÊ cðÍxrnj cËþÓ Compare to stored value 12 Á− l‹w If =, goto ok 13 ²þÄÄ ˆˆÍÎþ²Âˆ²³Âˆðþ©Ä Stack corrupted! 14 l‹wx ok: 15 þ®®Ê bprj cËÍÉ Deallocate stack space 16 Ë−Î We see that this version of the function retrieves a value from memory (line 3) and stores it on the stack at offset 8 from cËÍÉ, just beyond the region allocated for ¾Ïð. The instruction argument cðÍxrn is an indication that the canary value is read from memory using segmented addressing, an addressing mechanism that dates 324 Chapter 3 Machine-Level Representation of Programs back to the 80286 and is seldom found in programs running on modern systems. By storing the canary in a special segment, it can be marked as “read only,” so that an attacker cannot overwrite the stored canary value. Before restoring the register state and returning, the function compares the value stored at the stack location with the canary value (via the ÓÇËÊ instruction on line 11). If the two are identical, the ÓÇËÊ instruction will yield zero, and the function will complete in the normal fashion. A nonzero value indicates that the canary on the stack has been modiﬁed, and so the code will call an error routine. Stack protection does a good job of preventing a buffer overﬂow attack from corrupting state stored on the program stack. It incurs only a small performance penalty, especially because gcc only inserts it when there is a local buffer of type ²³þË in the function. Of course, there are other ways to corrupt the state of an executing program, but reducing the vulnerability of the stack thwarts many common attack strategies. Practice Problem 3.48 (solution page 383) The functions ©ÅÎÄ−Å, Ä−Å, and ©ÉÎÇþ provide a very convoluted way to compute the number of decimal digits required to represent an integer. We will use this as a way to study some aspects of the gcc stack protector facility. ©ÅÎ Ä−Åf²³þË hÍg Õ Ë−ÎÏËÅ ÍÎËÄ−ÅfÍgy Û ÌÇ©® ©ÉÎÇþf²³þË hÍj ÄÇÅ× hÉg Õ ÄÇÅ× ÌþÄ { hÉy ÍÉË©ÅÎðfÍj ‘cÄ®‘j ÌþÄgy Û ©ÅÎ ©ÅÎÄ−ÅfÄÇÅ× Óg Õ ÄÇÅ× Ìy ²³þË ¾Ïð‰op`y Ì{Óy ©ÉÎÇþf¾Ïðj dÌgy Ë−ÎÏËÅ Ä−Åf¾Ïðgy Û The following show portions of the code for ©ÅÎÄ−Å, compiled both with and without stack protector: (a) Without protector int intlen(long x) x in %rdi 1 ©ÅÎÄ−Åx 2 ÍÏ¾Ê brnj cËÍÉ 3 ÀÇÌÊ cË®©j prfcËÍÉg Section 3.10 Combining Control and Data in Machine-Level Programs 325 4 Ä−þÊ prfcËÍÉgj cËÍ© 5 ÀÇÌÊ cËÍÉj cË®© 6 ²þÄÄ ©ÉÎÇþ (b) With protector int intlen(long x) x in %rdi 1 ©ÅÎÄ−Åx 2 ÍÏ¾Ê bstj cËÍÉ 3 ÀÇÌÊ cðÍxrnj cËþÓ 4 ÀÇÌÊ cËþÓj rnfcËÍÉg 5 ÓÇËÄ c−þÓj c−þÓ 6 ÀÇÌÊ cË®©j vfcËÍÉg 7 Ä−þÊ vfcËÍÉgj cËÍ© 8 Ä−þÊ otfcËÍÉgj cË®© 9 ²þÄÄ ©ÉÎÇþ A. For both versions: What are the positions in the stack frame for ¾Ïð, Ì, and (when present) the canary value? B. How does the rearranged ordering of the local variables in the protected code provide greater security against a buffer overrun attack? Limiting Executable Code Regions A ﬁnal step is to eliminate the ability of an attacker to insert executable code into a system. One method is to limit which memory regions hold executable code. In typical programs, only the portion of memory holding the code generated by the compiler need be executable. The other portions can be restricted to allow just reading and writing. As we will see in Chapter 9, the virtual memory space is logically divided into pages, typically with 2,048 or 4,096 bytes per page. The hardware supports different forms of memory protection, indicating the forms of access allowed by both user programs and the operating system kernel. Many sys- tems allow control over three forms of access: read (reading data from memory), write (storing data into memory), and execute (treating the memory contents as machine-level code). Historically, the x86 architecture merged the read and exe- cute access controls into a single 1-bit ﬂag, so that any page marked as readable was also executable. The stack had to be kept both readable and writable, and therefore the bytes on the stack were also executable. Various schemes were im- plemented to be able to limit some pages to being readable but not executable, but these generally introduced signiﬁcant inefﬁciencies. More recently, AMD introduced an NX (for “no-execute”) bit into the mem- ory protection for its 64-bit processors, separating the read and execute access modes, and Intel followed suit. With this feature, the stack can be marked as be- ing readable and writable, but not executable, and the checking of whether a page is executable is performed in hardware, with no penalty in efﬁciency. 326 Chapter 3 Machine-Level Representation of Programs Some types of programs require the ability to dynamically generate and ex- ecute code. For example, “just-in-time” compilation techniques dynamically gen- erate code for programs written in interpreted languages, such as Java, to improve execution performance. Whether or not the run-time system can restrict the ex- ecutable code to just that part generated by the compiler in creating the original program depends on the language and the operating system. The techniques we have outlined—randomization, stack protection, and lim- iting which portions of memory can hold executable code—are three of the most common mechanisms used to minimize the vulnerability of programs to buffer overﬂow attacks. They all have the properties that they require no special effort on the part of the programmer and incur very little or no performance penalty. Each separately reduces the level of vulnerability, and in combination they be- come even more effective. Unfortunately, there are still ways to attack computers [85, 97], and so worms and viruses continue to compromise the integrity of many machines. 3.10.5 Supporting Variable-Size Stack Frames We have examined the machine-level code for a variety of functions so far, but they all have the property that the compiler can determine in advance the amount of space that must be allocated for their stack frames. Some functions, however, require a variable amount of local storage. This can occur, for example, when the function calls þÄÄÇ²þ, a standard library function that can allocate an arbitrary number of bytes of storage on the stack. It can also occur when the code declares a local array of variable size. Although the information presented in this section should rightfully be con- sidered an aspect of how procedures are implemented, we have deferred the presentation to this point, since it requires an understanding of arrays and align- ment. The code of Figure 3.43(a) gives an example of a function containing a variable-size array. The function declares local array É of n pointers, where n is given by the ﬁrst argument. This requires allocating 8n bytes on the stack, where the value of n may vary from one call of the function to another. The compiler therefore cannot determine how much space it must allocate for the function’s stack frame. In addition, the program generates a reference to the address of local variable ©, and so this variable must also be stored on the stack. During execution, the program must be able to access both local variable © and the elements of array É. On returning, the function must deallocate the stack frame and set the stack pointer to the position of the stored return address. To manage a variable-size stack frame, x86-64 code uses register cË¾É to serve as a frame pointer (sometimes referred to as a base pointer, and hence the letters ¾É in cË¾É). When using a frame pointer, the stack frame is organized as shown for the case of function ÌðËþÀ− in Figure 3.44. We see that the code must save the previous version of cË¾É on the stack, since it is a callee-saved register. It then keeps cË¾É pointing to this position throughout the execution of the function, and it references ﬁxed-length local variables, such as ©, at offsets relative to cË¾É. Section 3.10 Combining Control and Data in Machine-Level Programs 327 (a) C code ÄÇÅ× ÌðËþÀ−fÄÇÅ× Åj ÄÇÅ× ©®Ój ÄÇÅ× hÊg Õ ÄÇÅ× ©y ÄÇÅ× hÉ‰Å`y É‰n` { d©y ðÇËf©{oy©zÅy ©iig É‰©` { Êy Ë−ÎÏËÅ hÉ‰©®Ó`y Û (b) Portions of generated assembly code long vframe(long n, long idx, long *q) n in %rdi, idx in %rsi, q in %rdx Only portions of code shown 1 ÌðËþÀ−x 2 ÉÏÍ³Ê cË¾É Save old %rbp 3 ÀÇÌÊ cËÍÉj cË¾É Set frame pointer 4 ÍÏ¾Ê botj cËÍÉ Allocate space for i (%rsp = s1) 5 Ä−þÊ ppfjcË®©jvgj cËþÓ 6 þÅ®Ê bkotj cËþÓ 7 ÍÏ¾Ê cËþÓj cËÍÉ Allocate space for array p (%rsp = s2) 8 Ä−þÊ ufcËÍÉgj cËþÓ 9 Í³ËÊ bqj cËþÓ 10 Ä−þÊ nfjcËþÓjvgj cËv Set %r8 to &p[0] 11 ÀÇÌÊ cËvj cË²Ó Set %rcx to &p[0] (%rcx = p) ... Code for initialization loop i in %rax and on stack, n in %rdi, p in %rcx, q in %rdx 12 l‹qx loop: 13 ÀÇÌÊ cË®Ój fcË²ÓjcËþÓjvg Set p[i] to q 14 þ®®Ê boj cËþÓ Increment i 15 ÀÇÌÊ cËþÓj kvfcË¾Ég Store on stack 16 l‹px 17 ÀÇÌÊ kvfcË¾Égj cËþÓ Retrieve i from stack 18 ²ÀÉÊ cË®©j cËþÓ Compare i:n 19 ÁÄ l‹q If <, goto loop ... Code for function exit 20 Ä−þÌ− Restore %rbp and %rsp 21 Ë−Î Return Figure 3.43 Function requiring the use of a frame pointer. The variable-size array implies that the size of the stack frame cannot be determined at compile time. 328 Chapter 3 Machine-Level Representation of Programs Figure 3.44 Stack frame structure for function ÌðËþÀ−. The function uses register cË¾É as a frame pointer. The annotations along the right-hand side are in reference to Practice Problem 3.49. 8n bytes 8 0 –8 –16 (Unused) Return address Saved %rbp i p Stack pointer %rsp Frame pointer %rbp e2 e1 p s2 s1 Figure 3.43(b) shows portions of the code gcc generates for function ÌðËþÀ−. At the beginning of the function, we see code that sets up the stack frame and allocates space for array É. The code starts by pushing the current value of cË¾É onto the stack and setting cË¾É to point to this stack position (lines 2–3). Next, it allocates 16 bytes on the stack, the ﬁrst 8 of which are used to store local variable ©, and the second 8 of which are unused. Then it allocates space for array É (lines 5–11). The details of how much space it allocates and where it positions É within this space are explored in Practice Problem 3.49. Sufﬁce it to say that by the time the program reaches line 11, it has (1) allocated at least 8n bytes on the stack and (2) positioned array É within the allocated region such that at least 8n bytes are available for its use. The code for the initialization loop shows examples of how local variables © and É are referenced. Line 13 shows array element É‰©` being set to Ê. This instruction uses the value in register cË²Ó as the address for the start of É.Wecan see instances where local variable © is updated (line 15) and read (line 17). The address of © is given by reference kvfcË¾Ég—that is, at offset −8 relative to the frame pointer. At the end of the function, the frame pointer is restored to its previous value using the Ä−þÌ− instruction (line 20). This instruction takes no arguments. It is equivalent to executing the following two instructions: ÀÇÌÊ cË¾Éj cËÍÉ Set stack pointer to beginning of frame ÉÇÉÊ cË¾É Restore saved %rbp and set stack ptr to end of caller’s frame That is, the stack pointer is ﬁrst set to the position of the saved value of cË¾É, and then this value is popped from the stack into cË¾É. This instruction combination has the effect of deallocating the entire stack frame. Section 3.11 Floating-Point Code 329 In earlier versions of x86 code, the frame pointer was used with every function call. With x86-64 code, it is used only in cases where the stack frame may be of variable size, as is the case for function ÌðËþÀ−. Historically, most compilers used frame pointers when generating IA32 code. Recent versions of gcc have dropped this convention. Observe that it is acceptable to mix code that uses frame pointers with code that does not, as long as all functions treat cË¾É as a callee-saved register. Practice Problem 3.49 (solution page 383) In this problem, we will explore the logic behind the code in lines 5–11 of Fig- ure 3.43(b), where space is allocated for variable-size array É. As the annotations of the code indicate, let us let s1 denote the address of the stack pointer after exe- cuting the ÍÏ¾Ê instruction of line 4. This instruction allocates the space for local variable ©. Let s2 denote the value of the stack pointer after executing the ÍÏ¾Ê instruction of line 7. This instruction allocates the storage for local array É. Finally, let p denote the value assigned to registers cËv and cË²Ó in the instructions of lines 10–11. Both of these registers are used to reference array É. The right-hand side of Figure 3.44 diagrams the positions of the locations indicated by s1, s2, and p. It also shows that there may be an offset of e2 bytes between the values of s1 and p. This space will not be used. There may also be an offset of e1 bytes between the end of array É and the position indicated by s1. A. Explain, in mathematical terms, the logic in the computation of s2 on lines 5–7. Hint: Think about the bit-level representation of −16 and its effect in the þÅ®Ê instruction of line 6. B. Explain, in mathematical terms, the logic in the computation of p on lines 8–10. Hint: You may want to refer to the discussion on division by powers of 2 in Section 2.3.7. C. For the following values of n and s1, trace the execution of the code to determine what the resulting values would be for s2, p, e1, and e2. ns1 s2 pe1 e2 5 2,065 6 2,064 D. What alignment properties does this code guarantee for the values of s2 and p? 3.11 Floating-Point Code The ﬂoating-point architecture for a processor consists of the different aspects that affect how programs operating on ﬂoating-point data are mapped onto the machine, including . How ﬂoating-point values are stored and accessed. This is typically via some form of registers. 330 Chapter 3 Machine-Level Representation of Programs . The instructions that operate on ﬂoating-point data. . The conventions used for passing ﬂoating-point values as arguments to func- tions and for returning them as results. . The conventions for how registers are preserved during function calls—for example, with some registers designated as caller saved, and others as callee saved. To understand the x86-64 ﬂoating-point architecture, it is helpful to have a brief historical perspective. Since the introduction of the Pentium/MMX in 1997, both Intel and AMD have incorporated successive generations of media instruc- tions to support graphics and image processing. These instructions originally fo- cused on allowing multiple operations to be performed in a parallel mode known as single instruction, multiple data, or SIMD (pronounced sim-dee). In this mode the same operation is performed on a number of different data values in parallel. Over the years, there has been a progression of these extensions. The names have changed through a series of major revisions from MMX to SSE (for “streaming SIMD extensions”) and most recently AVX (for “advanced vector extensions”). Within each generation, there have also been different versions. Each of these ex- tensions manages data in sets of registers, referred to as “MM” registers for MMX, “XMM” for SSE, and “YMM” for AVX, ranging from 64 bits for MM registers, to 128 for XMM, to 256 for YMM. So, for example, each YMM register can hold eight 32-bit values, or four 64-bit values, where these values can be either integer or ﬂoating point. Starting with SSE2, introduced with the Pentium 4 in 2000, the media in- structions have included ones to operate on scalar ﬂoating-point data, using single values in the low-order 32 or 64 bits of XMM or YMM registers. This scalar mode provides a set of registers and instructions that are more typical of the way other processors support ﬂoating point. All processors capable of executing x86-64 code support SSE2 or higher, and hence x86-64 ﬂoating point is based on SSE or AVX, including conventions for passing procedure arguments and return values [77]. Our presentation is based on AVX2, the second version of AVX, introduced with the Core i7 Haswell processor in 2013. Gcc will generate AVX2 code when given the command-line parameter kÀþÌÓp. Code based on the different versions of SSE, as well as the ﬁrst version of AVX, is conceptually similar, although they differ in the instruction names and formats. We present only instructions that arise in compiling ﬂoating-point programs with gcc. These are, for the most part, the scalar AVX instructions, although we document occasions where instructions intended for operating on entire data vectors arise. A more complete coverage of how to exploit the SIMD capabilities of SSE and AVX is presented in Web Aside opt:simd on page 582. Readers may wish to refer to the AMD and Intel documentation for the individual instructions [4, 51]. As with integer operations, note that the ATT format we use in our presentation differs from the Intel format used in these documents. In particular, the instruction operands are listed in a different order in these two versions. Section 3.11 Floating-Point Code 331 127255 0 %ymm0 %ymm1 %ymm2 %ymm3 %ymm4 %ymm5 %ymm6 %ymm7 %ymm8 %ymm9 %ymm10 %ymm11 %ymm12 %ymm13 %ymm14 %ymm15 %xmm0 %xmm1 %xmm2 %xmm3 %xmm4 %xmm5 %xmm6 %xmm7 %xmm8 %xmm9 %xmm10 %xmm11 %xmm12 %ymm13 %xmm14 %xmm15 1st FP arg./Return value 2nd FP argument 3rd FP argument 4th FP argument 5th FP argument 6th FP argument 7th FP argument 8th FP argument Caller saved Caller saved Caller saved Caller saved Caller saved Caller saved Caller saved Caller saved Figure 3.45 Media registers. These registers are used to hold ﬂoating-point data. Each YMM register holds 32 bytes. The low-order 16 bytes can be accessed as an XMM register. As is illustrated in Figure 3.45, the AVX ﬂoating-point architecture allows data to be stored in 16 YMM registers, named cÔÀÀn–cÔÀÀos. Each YMM register is 256 bits (32 bytes) long. When operating on scalar data, these registers only hold ﬂoating-point data, and only the low-order 32 bits (for ðÄÇþÎ) or 64 bits (for ®ÇÏ¾Ä−) are used. The assembly code refers to the registers by their SSE XMM register names cÓÀÀn–cÓÀÀos, where each XMM register is the low-order 128 bits (16 bytes) of the corresponding YMM register. 332 Chapter 3 Machine-Level Representation of Programs Instruction Source Destination Description ÌÀÇÌÍÍ M32 X Move single precision ÌÀÇÌÍÍ XM32 Move single precision ÌÀÇÌÍ® M64 X Move double precision ÌÀÇÌÍ® XM64 Move double precision ÌÀÇÌþÉÍ XX Move aligned, packed single precision ÌÀÇÌþÉ® XX Move aligned, packed double precision Figure 3.46 Floating-point movement instructions. These operations transfer values between memory and registers, as well as between pairs of registers. (X: XMM register (e.g., cÓÀÀq); M32: 32-bit memory range; M64: 64-bit memory range) 3.11.1 Floating-Point Movement and Conversion Operations Figure 3.46 shows a set of instructions for transferring ﬂoating-point data between memory and XMM registers, as well as from one XMM register to another without any conversions. Those that reference memory are scalar instructions, meaning that they operate on individual, rather than packed, data values. The data are held either in memory (indicated in the table as M32 and M64) or in XMM registers (shown in the table as X). These instructions will work correctly regardless of the alignment of data, although the code optimization guidelines recommend that 32- bit memory data satisfy a 4-byte alignment and that 64-bit data satisfy an 8-byte alignment. Memory references are speciﬁed in the same way as for the integer mov instructions, with all of the different possible combinations of displacement, base register, index register, and scaling factor. Gcc uses the scalar movement operations only to transfer data from memory to an XMM register or from an XMM register to memory. For transferring data between two XMM registers, it uses one of two different instructions for copying the entire contents of one XMM register to another—namely, ÌÀÇÌþÉÍ for single- precision and ÌÀÇÌþÉ® for double-precision values. For these cases, whether the program copies the entire register or just the low-order value affects neither the program functionality nor the execution speed, and so using these instructions rather than ones speciﬁc to scalar data makes no real difference. The letter ‘þ’ in these instruction names stands for “aligned.” When used to read and write memory, they will cause an exception if the address does not satisfy a 16-byte alignment. For transferring between two registers, there is no possibility of an incorrect alignment. As an example of the different ﬂoating-point move operations, consider the C function ðÄÇþÎ ðÄÇþÎˆÀÇÌfðÄÇþÎ Ìoj ðÄÇþÎ hÍË²j ðÄÇþÎ h®ÍÎg Õ ðÄÇþÎ Ìp { hÍË²y h®ÍÎ { Ìoy Ë−ÎÏËÅ Ìpy Û Section 3.11 Floating-Point Code 333 Instruction Source Destination Description Ì²ÌÎÎÍÍpÍ© X/M32 R32 Convert with truncation single precision to integer Ì²ÌÎÎÍ®pÍ© X/M64 R32 Convert with truncation double precision to integer Ì²ÌÎÎÍÍpÍ©Ê X/M32 R64 Convert with truncation single precision to quad word integer Ì²ÌÎÎÍ®pÍ©Ê X/M64 R64 Convert with truncation double precision to quad word integer Figure 3.47 Two-operand ﬂoating-point conversion operations. These convert ﬂoating-point data to integers. (X: XMM register (e.g., cÓÀÀq); R32: 32-bit general-purpose register (e.g., c−þÓ); R64: 64-bit general-purpose register (e.g., cËþÓ); M32: 32-bit memory range; M64: 64-bit memory range) Instruction Source 1 Source 2 Destination Description Ì²ÌÎÍ©pÍÍ M32/R32 XX Convert integer to single precision Ì²ÌÎÍ©pÍ® M32/R32 XX Convert integer to double precision Ì²ÌÎÍ©pÍÍÊ M64/R64 XX Convert quad word integer to single precision Ì²ÌÎÍ©pÍ®Ê M64/R64 XX Convert quad word integer to double precision Figure 3.48 Three-operand ﬂoating-point conversion operations. These instructions convert from the data type of the ﬁrst source to the data type of the destination. The second source value has no effect on the low-order bytes of the result. (X: XMM register (e.g., cÓÀÀq); M32: 32-bit memory range; M64: 64-bit memory range) and its associated x86-64 assembly code float float_mov(float v1, float *src, float *dst) v1 in %xmm0, src in %rdi, dst in %rsi 1 ðÄÇþÎˆÀÇÌx 2 ÌÀÇÌþÉÍ cÓÀÀnj cÓÀÀo Copy v1 3 ÌÀÇÌÍÍ fcË®©gj cÓÀÀn Read v2 from src 4 ÌÀÇÌÍÍ cÓÀÀoj fcËÍ©g Write v1 to dst 5 Ë−Î Return v2 in %xmm0 We can see in this example the use of the ÌÀÇÌþÉÍ instruction to copy data from one register to another and the use of the ÌÀÇÌÍÍ instruction to copy data from memory to an XMM register and from an XMM register to memory. Figures 3.47 and 3.48 show sets of instructions for converting between ﬂoating- point and integer data types, as well as between different ﬂoating-point formats. These are all scalar instructions operating on individual data values. Those in Figure 3.47 convert from a ﬂoating-point value read from either an XMM register or memory and write the result to a general-purpose register (e.g., cËþÓ, c−¾Ó, etc.). When converting ﬂoating-point values to integers, they perform truncation, rounding values toward zero, as is required by C and most other programming languages. The instructions in Figure 3.48 convert from integer to ﬂoating point. They use an unusual three-operand format, with two sources and a destination. The 334 Chapter 3 Machine-Level Representation of Programs ﬁrst operand is read from memory or from a general-purpose register. For our purposes, we can ignore the second operand, since its value only affects the upper bytes of the result. The destination must be an XMM register. In common usage, both the second source and the destination operands are identical, as in the instruction Ì²ÌÎÍ©pÍ®Ê cËþÓj cÓÀÀoj cÓÀÀo This instruction reads a long integer from register cËþÓ, converts it to data type ®ÇÏ¾Ä−, and stores the result in the lower bytes of XMM register cÓÀÀo. Finally, for converting between two different ﬂoating-point formats, current versions of gcc generate code that requires separate documentation. Suppose the low-order 4 bytes of cÓÀÀn hold a single-precision value; then it would seem straightforward to use the instruction Ì²ÌÎÍÍpÍ® cÓÀÀnj cÓÀÀnj cÓÀÀn to convert this to a double-precision value and store the result in the lower 8 bytes of register cÓÀÀn. Instead, we ﬁnd the following code generated by gcc: Conversion from single to double precision 1 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn Replicate first vector element 2 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn Convert two vector elements to double The ÌÏÅÉ²ÂÄÉÍ instruction is normally used to interleave the values in two XMM registers and store them in a third. That is, if one source register contains words [s3,s2,s1,s0] and the other contains words [d3,d2,d1,d0], then the value of the destination register will be [s1,d1,s0,d0]. In the code above, we see the same register being used for all three operands, and so if the original register held values [x3,x2,x1,x0], then the instruction will update the register to hold values [x1,x1,x0,x0]. The Ì²ÌÎÉÍpÉ® instruction expands the two low-order single- precision values in the source XMM register to be the two double-precision values in the destination XMM register. Applying this to the result of the preceding ÌÏÅÉ²ÂÄÉÍ instruction would give values [dx0,dx0], where dx0 is the result of converting x to double precision. That is, the net effect of the two instructions is to convert the original single-precision value in the low-order 4 bytes of cÓÀÀn to double precision and store two copies of it in cÓÀÀn. It is unclear why gcc generates this code. There is neither beneﬁt nor need to have the value duplicated within the XMM register. Gcc generates similar code for converting from double precision to single precision: Conversion from double to single precision 1 ÌÀÇÌ®®ÏÉ cÓÀÀnj cÓÀÀn Replicate first vector element 2 Ì²ÌÎÉ®pÉÍÓ cÓÀÀnj cÓÀÀn Convert two vector elements to single Section 3.11 Floating-Point Code 335 Suppose these instructions start with register cÓÀÀn holding two double-precision values [x1,x0]. Then the ÌÀÇÌ®®ÏÉ instruction will set it to [x0,x0]. The Ì²ÌÎÉ®pÉÍÓ instruction will convert these values to single precision, pack them into the low-order half of the register, and set the upper half to 0, yielding a result [0.0, 0.0,x0,x0] (recall that ﬂoating-point value 0.0 is represented by a bit pat- tern of all zeros). Again, there is no clear value in computing the conversion from one precision to another this way, rather than by using the single instruction Ì²ÌÎÍ®pÍÍ cÓÀÀnj cÓÀÀnj cÓÀÀn As an example of the different ﬂoating-point conversion operations, consider the C function ®ÇÏ¾Ä− ð²ÌÎf©ÅÎ ©j ðÄÇþÎ hðÉj ®ÇÏ¾Ä− h®Éj ÄÇÅ× hÄÉg Õ ðÄÇþÎ ð { hðÉy ®ÇÏ¾Ä− ® { h®Éy ÄÇÅ× Ä { hÄÉy hÄÉ { fÄÇÅ×g ®y hðÉ { fðÄÇþÎg ©y h®É { f®ÇÏ¾Ä−g Äy Ë−ÎÏËÅ f®ÇÏ¾Ä−g ðy Û and its associated x86-64 assembly code double fcvt(int i, float *fp, double *dp, long *lp) i in %edi, fp in %rsi, dp in %rdx, lp in %rcx 1 ð²ÌÎx 2 ÌÀÇÌÍÍ fcËÍ©gj cÓÀÀn Get f = *fp 3 ÀÇÌÊ fcË²Ógj cËþÓ Get l = *lp 4 Ì²ÌÎÎÍ®pÍ©Ê fcË®Ógj cËv Get d = *dp and convert to long 5 ÀÇÌÊ cËvj fcË²Óg Store at lp 6 Ì²ÌÎÍ©pÍÍ c−®©j cÓÀÀoj cÓÀÀo Convert i to float 7 ÌÀÇÌÍÍ cÓÀÀoj fcËÍ©g Store at fp 8 Ì²ÌÎÍ©pÍ®Ê cËþÓj cÓÀÀoj cÓÀÀo Convert l to double 9 ÌÀÇÌÍ® cÓÀÀoj fcË®Óg Store at dp The following two instructions convert f to double 10 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn 11 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn 12 Ë−Î Return f All of the arguments to ð²ÌÎ are passed through the general-purpose registers, since they are either integers or pointers. The result is returned in register cÓÀÀn. As is documented in Figure 3.45, this is the designated return register for ðÄÇþÎ or ®ÇÏ¾Ä− values. In this code, we see a number of the movement and conversion instructions of Figures 3.46–3.48, as well as gcc’s preferred method of converting from single to double precision. 336 Chapter 3 Machine-Level Representation of Programs Practice Problem 3.50 (solution page 383) For the following C code, the expressions ÌþÄo–ÌþÄr all map to the program values ©, ð, ®, and Ä: ®ÇÏ¾Ä− ð²ÌÎpf©ÅÎ h©Éj ðÄÇþÎ hðÉj ®ÇÏ¾Ä− h®Éj ÄÇÅ× Äg Õ ©ÅÎ © { h©Éy ðÄÇþÎ ð { hðÉy ®ÇÏ¾Ä− ® { h®Éy h©É { f©ÅÎg ÌþÄoy hðÉ { fðÄÇþÎg ÌþÄpy h®É { f®ÇÏ¾Ä−g ÌþÄqy Ë−ÎÏËÅ f®ÇÏ¾Ä−g ÌþÄry Û Determine the mapping, based on the following x86-64 code for the function: double fcvt2(int *ip, float *fp, double *dp, long l) ip in %rdi, fp in %rsi, dp in %rdx, l in %rcx Result returned in %xmm0 1 ð²ÌÎpx 2 ÀÇÌÄ fcË®©gj c−þÓ 3 ÌÀÇÌÍÍ fcËÍ©gj cÓÀÀn 4 Ì²ÌÎÎÍ®pÍ© fcË®Ógj cËv® 5 ÀÇÌÄ cËv®j fcË®©g 6 Ì²ÌÎÍ©pÍÍ c−þÓj cÓÀÀoj cÓÀÀo 7 ÌÀÇÌÍÍ cÓÀÀoj fcËÍ©g 8 Ì²ÌÎÍ©pÍ®Ê cË²Ój cÓÀÀoj cÓÀÀo 9 ÌÀÇÌÍ® cÓÀÀoj fcË®Óg 10 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn 11 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn 12 Ë−Î Practice Problem 3.51 (solution page 384) The following C function converts an argument of type ÍË²ˆÎ to a return value of type ®ÍÎˆÎ, where these two types are deﬁned using ÎÔÉ−®−ð: ®−ÍÎˆÎ ²ÌÎfÍË²ˆÎ Óg Õ ®−ÍÎˆÎ Ô { f®−ÍÎˆÎg Óy Ë−ÎÏËÅ Ôy Û For execution on x86-64, assume that argument Ó is either in cÓÀÀn or in the appropriately named portion of register cË®© (i.e., cË®© or c−®©). One or two instructions are to be used to perform the type conversion and to copy the value to the appropriately named portion of register cËþÓ (integer result) or Section 3.11 Floating-Point Code 337 cÓÀÀn (ﬂoating-point result). Show the instruction(s), including the source and destination registers. TÓ TÔ Instruction(s) ÄÇÅ× ®ÇÏ¾Ä− Ì²ÌÎÍ©pÍ®Ê cË®©j cÓÀÀn ®ÇÏ¾Ä− ©ÅÎ ®ÇÏ¾Ä− ðÄÇþÎ ÄÇÅ× ðÄÇþÎ ðÄÇþÎ ÄÇÅ× 3.11.2 Floating-Point Code in Procedures With x86-64, the XMM registers are used for passing ﬂoating-point arguments to functions and for returning ﬂoating-point values from them. As is illustrated in Figure 3.45, the following conventions are observed: . Up to eight ﬂoating-point arguments can be passed in XMM registers cÓÀÀn– cÓÀÀu. These registers are used in the order the arguments are listed. Addi- tional ﬂoating-point arguments can be passed on the stack. . A function that returns a ﬂoating-point value does so in register cÓÀÀn. . All XMM registers are caller saved. The callee may overwrite any of these registers without ﬁrst saving it. When a function contains a combination of pointer, integer, and ﬂoating- point arguments, the pointers and integers are passed in general-purpose registers, while the ﬂoating-point values are passed in XMM registers. This means that the mapping of arguments to registers depends on both their types and their ordering. Here are several examples: ®ÇÏ¾Ä− ðof©ÅÎ Ój ®ÇÏ¾Ä− Ôj ÄÇÅ× Ögy This function would have Ó in c−®©, Ô in cÓÀÀn, and Ö in cËÍ©. ®ÇÏ¾Ä− ðpf®ÇÏ¾Ä− Ôj ©ÅÎ Ój ÄÇÅ× Ögy This function would have the same register assignment as function ðo. ®ÇÏ¾Ä− ðofðÄÇþÎ Ój ®ÇÏ¾Ä− hÔj ÄÇÅ× hÖgy This function would have Ó in cÓÀÀn, Ô in cË®©, and Ö in cËÍ©. Practice Problem 3.52 (solution page 384) For each of the following function declarations, determine the register assignments for the arguments: A. ®ÇÏ¾Ä− ×of®ÇÏ¾Ä− þj ÄÇÅ× ¾j ðÄÇþÎ ²j ©ÅÎ ®gy 338 Chapter 3 Machine-Level Representation of Programs B. ®ÇÏ¾Ä− ×pf©ÅÎ þj ®ÇÏ¾Ä− h¾j ðÄÇþÎ h²j ÄÇÅ× ®gy C. ®ÇÏ¾Ä− ×qf®ÇÏ¾Ä− hþj ®ÇÏ¾Ä− ¾j ©ÅÎ ²j ðÄÇþÎ ®gy D. ®ÇÏ¾Ä− ×rfðÄÇþÎ þj ©ÅÎ h¾j ðÄÇþÎ ²j ®ÇÏ¾Ä− ®gy 3.11.3 Floating-Point Arithmetic Operations Figure 3.49 documents a set of scalar AVX2 ﬂoating-point instructions that per- form arithmetic operations. Each has either one (S1) or two (S1, S2) source oper- ands and a destination operand D. The ﬁrst source operand S1 can be either an XMM register or a memory location. The second source operand and the desti- nation operands must be XMM registers. Each operation has an instruction for single precision and an instruction for double precision. The result is stored in the destination register. As an example, consider the following ﬂoating-point function: ®ÇÏ¾Ä− ðÏÅ²Îf®ÇÏ¾Ä− þj ðÄÇþÎ Ój ®ÇÏ¾Ä− ¾j ©ÅÎ ©g Õ Ë−ÎÏËÅ þhÓ k ¾m©y Û The x86-64 code is as follows: double funct(double a, float x, double b, int i) a in %xmm0, x in %xmm1, b in %xmm2, i in %edi 1 ðÏÅ²Îx The following two instructions convert x to double 2 ÌÏÅÉ²ÂÄÉÍ cÓÀÀoj cÓÀÀoj cÓÀÀo 3 Ì²ÌÎÉÍpÉ® cÓÀÀoj cÓÀÀo 4 ÌÀÏÄÍ® cÓÀÀnj cÓÀÀoj cÓÀÀn Multiply a by x 5 Ì²ÌÎÍ©pÍ® c−®©j cÓÀÀoj cÓÀÀo Convert i to double 6 Ì®©ÌÍ® cÓÀÀoj cÓÀÀpj cÓÀÀp Compute b/i Single Double Effect Description Ìþ®®ÍÍ Ìþ®®Í® D ← S2 + S1 Floating-point add ÌÍÏ¾ÍÍ ÌÍÏ¾Í® D ← S2 − S1 Floating-point subtract ÌÀÏÄÍÍ ÌÀÏÄÍ® D ← S2 × S1 Floating-point multiply Ì®©ÌÍÍ Ì®©ÌÍ® D ← S2/S1 Floating-point divide ÌÀþÓÍÍ ÌÀþÓÍ® D ← max(S2,S1) Floating-point maximum ÌÀ©ÅÍÍ ÌÀ©ÅÍ® D ← min(S2,S1) Floating-point minimum ÍÊËÎÍÍ ÍÊËÎÍ® D ← √ S1 Floating-point square root Figure 3.49 Scalar ﬂoating-point arithmetic operations. These have either one or two source operands and a destination operand. Section 3.11 Floating-Point Code 339 7 ÌÍÏ¾Í® cÓÀÀpj cÓÀÀnj cÓÀÀn Subtract from a*x 8 Ë−Î Return The three ﬂoating-point arguments þ, Ó, and ¾ are passed in XMM registers cÓÀÀn–cÓÀÀp, while integer argument © is passed in register c−®©. The standard two-instruction sequence is used to convert argument Ó to double (lines 2–3). Another conversion instruction is required to convert argument © to double (line 5). The function value is returned in register cÓÀÀn. Practice Problem 3.53 (solution page 384) For the following C function, the types of the four arguments are deﬁned by ÎÔÉ−®−ð: ®ÇÏ¾Ä− ðÏÅ²ÎofþË×oˆÎ Éj þË×pˆÎ Êj þË×qˆÎ Ëj þË×rˆÎ Íg Õ Ë−ÎÏËÅ ÉmfÊiËg k Íy Û When compiled, gcc generates the following code: double funct1(arg1_t p, arg2_t q, arg3_t r, arg4_t s) 1 ðÏÅ²Îox 2 Ì²ÌÎÍ©pÍÍÊ cËÍ©j cÓÀÀpj cÓÀÀp 3 Ìþ®®ÍÍ cÓÀÀnj cÓÀÀpj cÓÀÀn 4 Ì²ÌÎÍ©pÍÍ c−®©j cÓÀÀpj cÓÀÀp 5 Ì®©ÌÍÍ cÓÀÀnj cÓÀÀpj cÓÀÀn 6 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn 7 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn 8 ÌÍÏ¾Í® cÓÀÀoj cÓÀÀnj cÓÀÀn 9 Ë−Î Determine the possible combinations of types of the four arguments (there may be more than one). Practice Problem 3.54 (solution page 385) Function ðÏÅ²Îp has the following prototype: ®ÇÏ¾Ä− ðÏÅ²Îpf®ÇÏ¾Ä− Ñj ©ÅÎ Ój ðÄÇþÎ Ôj ÄÇÅ× Ögy Gcc generates the following code for the function: double funct2(double w, int x, float y, long z) w in %xmm0, x in %edi, y in %xmm1, z in %rsi 1 ðÏÅ²Îpx 2 Ì²ÌÎÍ©pÍÍ c−®©j cÓÀÀpj cÓÀÀp 3 ÌÀÏÄÍÍ cÓÀÀoj cÓÀÀpj cÓÀÀo 340 Chapter 3 Machine-Level Representation of Programs 4 ÌÏÅÉ²ÂÄÉÍ cÓÀÀoj cÓÀÀoj cÓÀÀo 5 Ì²ÌÎÉÍpÉ® cÓÀÀoj cÓÀÀp 6 Ì²ÌÎÍ©pÍ®Ê cËÍ©j cÓÀÀoj cÓÀÀo 7 Ì®©ÌÍ® cÓÀÀoj cÓÀÀnj cÓÀÀn 8 ÌÍÏ¾Í® cÓÀÀnj cÓÀÀpj cÓÀÀn 9 Ë−Î Write a C version of ðÏÅ²Îp. 3.11.4 Deﬁning and Using Floating-Point Constants Unlike integer arithmetic operations, AVX ﬂoating-point operations cannot have immediate values as operands. Instead, the compiler must allocate and initialize storage for any constant values. The code then reads the values from memory. This is illustrated by the following Celsius to Fahrenheit conversion function: ®ÇÏ¾Ä− ²−Äpðþ³Ëf®ÇÏ¾Ä− Î−ÀÉg Õ Ë−ÎÏËÅ olv h Î−ÀÉ i qplny Û The relevant parts of the x86-64 assembly code are as follows: double cel2fahr(double temp) temp in %xmm0 1 ²−Äpðþ³Ëx 2 ÌÀÏÄÍ® l‹£pfcË©Égj cÓÀÀnj cÓÀÀn Multiply by 1.8 3 Ìþ®®Í® l‹£qfcË©Égj cÓÀÀnj cÓÀÀn Add 32.0 4 Ë−Î 5 l‹£px 6 lÄÇÅ× qrqswuqvqu Low-order 4 bytes of 1.8 7 lÄÇÅ× onuqsqponv High-order 4 bytes of 1.8 8 l‹£qx 9 lÄÇÅ× n Low-order 4 bytes of 32.0 10 lÄÇÅ× onuuwqtopv High-order 4 bytes of 32.0 We see that the function reads the value 1.8 from the memory location labeled l‹£p and the value 32.0 from the memory location labeled l‹£q. Looking at the values associated with these labels, we see that each is speciﬁed by a pair of lÄÇÅ× declarations with the values given in decimal. How should these be interpreted as ﬂoating-point values? Looking at the declaration labeled l‹£p, we see that the two values are 3435973837 (nÓ²²²²²²²®) and 1073532108 (nÓqðð²²²²².) Since the machine uses little-endian byte ordering, the ﬁrst value gives the low-order 4 bytes, while the second gives the high-order 4 bytes. From the high-order bytes, we can extract an exponent ﬁeld of nÓqðð (1023), from which we subtract a bias of 1023 to get an exponent of 0. Concatenating the fraction bits of the two values, we get a fraction ﬁeld of nÓ²²²²²²²²²²²²®, which can be shown to be the fractional binary representation of 0.8, to which we add the implied leading one to get 1.8. Section 3.11 Floating-Point Code 341 Single Double Effect Description ÌÓÇËÉÍ ÓÇËÉ® D ← S2 ´ S1 Bitwise exclusive-or ÌþÅ®ÉÍ þÅ®É® D ← S2 d S1 Bitwise and Figure 3.50 Bitwise operations on packed data. These instructions perform Boolean operations on all 128 bits in an XMM register. Practice Problem 3.55 (solution page 385) Show how the numbers declared at label l‹£q encode the number 32.0. 3.11.5 Using Bitwise Operations in Floating-Point Code At times, we ﬁnd gcc generating code that performs bitwise operations on XMM registers to implement useful ﬂoating-point results. Figure 3.50 shows some rele- vant instructions, similar to their counterparts for operating on general-purpose registers. These operations all act on packed data, meaning that they update the entire destination XMM register, applying the bitwise operation to all the data in the two source registers. Once again, our only interest for scalar data is the effect these instructions have on the low-order 4 or 8 bytes of the destination. These op- erations are often simple and convenient ways to manipulate ﬂoating-point values, as is explored in the following problem. Practice Problem 3.56 (solution page 386) Consider the following C function, where ¥”–‡ is a macro deﬁned with a®−ð©Å−: ®ÇÏ¾Ä− Í©ÀÉÄ−ðÏÅf®ÇÏ¾Ä− Óg Õ Ë−ÎÏËÅ ¥”–‡fÓgy Û Below, we show the AVX2 code generated for different deﬁnitions of ¥”–‡, where value Ó is held in cÓÀÀn. All of them correspond to some useful operation on ﬂoating-point values. Identify what the operations are. Your answers will require you to understand the bit patterns of the constant words being retrieved from memory. A. 1 ÌÀÇÌÍ® l‹£ofcË©Égj cÓÀÀo 2 ÌþÅ®É® cÓÀÀoj cÓÀÀnj cÓÀÀn 3 l‹£ox 4 lÄÇÅ× rpwrwtupws 5 lÄÇÅ× porurvqtru 6 lÄÇÅ× n 7 lÄÇÅ× n B. 1 ÌÓÇËÉ® cÓÀÀnj cÓÀÀnj cÓÀÀn 342 Chapter 3 Machine-Level Representation of Programs C. 1 ÌÀÇÌÍ® l‹£pfcË©Égj cÓÀÀo 2 ÌÓÇËÉ® cÓÀÀoj cÓÀÀnj cÓÀÀn 3 l‹£px 4 lÄÇÅ× n 5 lÄÇÅ× kporurvqtrv 6 lÄÇÅ× n 7 lÄÇÅ× n 3.11.6 Floating-Point Comparison Operations AVX2 provides two instructions for comparing ﬂoating-point values: Instruction Based on Description Ï²ÇÀ©ÍÍ S1, S2 S2−S1 Compare single precision Ï²ÇÀ©Í® S1, S2 S2−S1 Compare double precision These instructions are similar to the cmp instructions (see Section 3.6), in that they compare operands S1 and S2 (but in the opposite order one might expect) and set the condition codes to indicate their relative values. As with ²ÀÉÊ, they follow the ATT-format convention of listing the operands in reverse order. Argument S2 must be in an XMM register, while S1 can be either in an XMM register or in memory. The ﬂoating-point comparison instructions set three condition codes: the zero ﬂag …ƒ, the carry ﬂag £ƒ, and the parity ﬂag –ƒ. We did not document the parity ﬂag in Section 3.6.1, because it is not commonly found in gcc-generated x86 code. For integer operations, this ﬂag is set when the most recent arithmetic or logical operation yielded a value where the least signiﬁcant byte has even parity (i.e., an even number of ones in the byte). For ﬂoating-point comparisons, however, the ﬂag is set when either operand is NaN. By convention, any comparison in C is considered to fail when one of the arguments is NaN, and this ﬂag is used to detect such a condition. For example, even the comparison Ó{{Ó yields 0 when Ó is NaN. The condition codes are set as follows: Ordering S2:S1 £ƒ …ƒ –ƒ Unordered 1 1 1 S2 <S1 100 S2 = S1 010 S2 >S1 000 The unordered case occurs when either operand is NaN. This can be detected with the parity ﬂag. Commonly, the ÁÉ (for “jump on parity”) instruction is used to conditionally jump when a ﬂoating-point comparison yields an unordered result. Except for this case, the values of the carry and zero ﬂags are the same as those for an unsigned comparison: …ƒ is set when the two operands are equal, and £ƒ is Section 3.11 Floating-Point Code 343 (a) C code ÎÔÉ−®−ð −ÅÏÀ Õﬁ¥§j …¥‡ﬂj –ﬂ·j ﬂ¶¤¥‡Û ËþÅ×−ˆÎy ËþÅ×−ˆÎ ð©Å®ˆËþÅ×−fðÄÇþÎ Óg Õ ©ÅÎ Ë−ÍÏÄÎy ©ð fÓ z ng Ë−ÍÏÄÎ { ﬁ¥§y −ÄÍ− ©ð fÓ {{ ng Ë−ÍÏÄÎ { …¥‡ﬂy −ÄÍ− ©ð fÓ | ng Ë−ÍÏÄÎ { –ﬂ·y −ÄÍ− Ë−ÍÏÄÎ { ﬂ¶¤¥‡y Ë−ÎÏËÅ Ë−ÍÏÄÎy Û (b) Generated assembly code range_t find_range(float x) x in %xmm0 1 ð©Å®ˆËþÅ×−x 2 ÌÓÇËÉÍ cÓÀÀoj cÓÀÀoj cÓÀÀo Set %xmm1 = 0 3 ÌÏ²ÇÀ©ÍÍ cÓÀÀnj cÓÀÀo Compare 0:x 4 Áþ l‹s If >, goto neg 5 ÌÏ²ÇÀ©ÍÍ cÓÀÀoj cÓÀÀn Compare x:0 6 ÁÉ l‹v If NaN, goto posornan 7 ÀÇÌÄ boj c−þÓ result = ZERO 8 Á− l‹q If =, goto done 9 l‹vx posornan: 10 ÌÏ²ÇÀ©ÍÍ l‹£nfcË©Égj cÓÀÀn Compare x:0 11 Í−Î¾− cþÄ Set result = NaN?1:0 12 ÀÇÌÖ¾Ä cþÄj c−þÓ Zero-extend 13 þ®®Ä bpj c−þÓ result += 2 (POS for > 0, OTHER for NaN) 14 Ë−Î Return 15 l‹sx neg: 16 ÀÇÌÄ bnj c−þÓ result = NEG 17 l‹qx done: 18 Ë−Éy Ë−Î Return Figure 3.51 Illustration of conditional branching in ﬂoating-point code. 344 Chapter 3 Machine-Level Representation of Programs set when S2 <S1. Instructions such as Áþ and Á¾ are used to conditionally jump on various combinations of these ﬂags. As an example of ﬂoating-point comparisons, the C function of Figure 3.51(a) classiﬁes argument Ó according to its relation to 0.0, returning an enumerated type as the result. Enumerated types in C are encoded as integers, and so the possible function values are: 0 (ﬁ¥§), 1 (…¥‡ﬂ), 2 (–ﬂ·), and 3 (ﬂ¶¤¥‡). This ﬁnal outcome occurs when the value of Ó is NaN. Gcc generates the code shown in Figure 3.51(b) for ð©Å®ˆËþÅ×−. The code is not very efﬁcient—it compares Ó to 0.0 three times, even though the required information could be obtained with a single comparison. It also generates ﬂoating- point constant 0.0 twice—once using ÌÓÇËÉÍ, and once by reading the value from memory. Let us trace the ﬂow of the function for the four possible comparison results: Ó < 0.0The Áþ branch on line 4 will be taken, jumping to the end with a return value of 0. Ó = 0.0The Áþ (line 4) and ÁÉ (line 6) branches will not be taken, but the Á− branch (line 8) will, returning with c−þÓ equal to 1. Ó > 0.0 None of the three branches will be taken. The Í−Î¾− (line 11) will yield 0, and this will be incremented by the þ®®Ä instruction (line 13) to give a return value of 2. Ó = NaN The ÁÉ branch (line 6) will be taken. The third ÌÏ²ÇÀ©ÍÍ instruction (line 10) will set both the carry and the zero ﬂag, and so the Í−Î¾− instruction (line 11) and the following instruction will set c−þÓ to 1. This gets incremented by the þ®®Ä instruction (line 13) to give a return value of 3. In Homework Problems 3.73 and 3.74, you are challenged to hand-generate more efﬁcient implementations of ð©Å®ˆËþÅ×−. Practice Problem 3.57 (solution page 386) Function ðÏÅ²Îq has the following prototype: ®ÇÏ¾Ä− ðÏÅ²Îqf©ÅÎ hþÉj ®ÇÏ¾Ä− ¾j ÄÇÅ× ²j ðÄÇþÎ h®Égy For this function, gcc generates the following code: double funct3(int *ap, double b, long c, float *dp) ap in %rdi, b in %xmm0, c in %rsi, dp in %rdx 1 ðÏÅ²Îqx 2 ÌÀÇÌÍÍ fcË®Ógj cÓÀÀo 3 Ì²ÌÎÍ©pÍ® fcË®©gj cÓÀÀpj cÓÀÀp 4 ÌÏ²ÇÀ©Í® cÓÀÀpj cÓÀÀn 5 Á¾− l‹v 6 Ì²ÌÎÍ©pÍÍÊ cËÍ©j cÓÀÀnj cÓÀÀn 7 ÌÀÏÄÍÍ cÓÀÀoj cÓÀÀnj cÓÀÀo Section 3.12 Summary 345 8 ÌÏÅÉ²ÂÄÉÍ cÓÀÀoj cÓÀÀoj cÓÀÀo 9 Ì²ÌÎÉÍpÉ® cÓÀÀoj cÓÀÀn 10 Ë−Î 11 l‹vx 12 Ìþ®®ÍÍ cÓÀÀoj cÓÀÀoj cÓÀÀo 13 Ì²ÌÎÍ©pÍÍÊ cËÍ©j cÓÀÀnj cÓÀÀn 14 Ìþ®®ÍÍ cÓÀÀoj cÓÀÀnj cÓÀÀn 15 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn 16 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn 17 Ë−Î Write a C version of ðÏÅ²Îq. 3.11.7 Observations about Floating-Point Code We see that the general style of machine code generated for operating on ﬂoating- point data with AVX2 is similar to what we have seen for operating on integer data. Both use a collection of registers to hold and operate on values, and they use these registers for passing function arguments. Of course, there are many complexities in dealing with the different data types and the rules for evaluating expressions containing a mixture of data types, and AVX2 code involves many more different instructions and formats than is usually seen with functions that perform only integer arithmetic. AVX2 also has the potential to make computations run faster by performing parallel operations on packed data. Compiler developers are working on automat- ing the conversion of scalar code to parallel code, but currently the most reliable way to achieve higher performance through parallelism is to use the extensions to the C language supported by gcc for manipulating vectors of data. See Web Aside opt:simd on page 582 to see how this can be done. 3.12 Summary In this chapter, we have peered beneath the layer of abstraction provided by the C language to get a view of machine-level programming. By having the compiler generate an assembly-code representation of the machine-level program, we gain insights into both the compiler and its optimization capabilities, along with the ma- chine, its data types, and its instruction set. In Chapter 5, we will see that knowing the characteristics of a compiler can help when trying to write programs that have efﬁcient mappings onto the machine. We have also gotten a more complete picture of how the program stores data in different memory regions. In Chapter 12, we will see many examples where application programmers need to know whether a program variable is on the run-time stack, in some dynamically allocated data structure, or part of the global program data. Understanding how programs map onto machines makes it easier to understand the differences between these kinds of storage. 346 Chapter 3 Machine-Level Representation of Programs Machine-level programs, and their representation by assembly code, differ in many ways from C programs. There is minimal distinction between different data types. The program is expressed as a sequence of instructions, each of which performs a single operation. Parts of the program state, such as registers and the run-time stack, are directly visible to the programmer. Only low-level operations are provided to support data manipulation and program control. The compiler must use multiple instructions to generate and operate on different data structures and to implement control constructs such as conditionals, loops, and procedures. We have covered many different aspects of C and how it gets compiled. We have seen that the lack of bounds checking in C makes many programs prone to buffer overﬂows. This has made many systems vulnerable to attacks by malicious intruders, although recent safeguards provided by the run-time system and the compiler help make programs more secure. We have only examined the mapping of C onto x86-64, but much of what we have covered is handled in a similar way for other combinations of language and machine. For example, compiling C++ is very similar to compiling C. In fact, early implementations of C++ ﬁrst performed a source-to-source conversion from C++ to C and generated object code by running a C compiler on the result. C++ objects are represented by structures, similar to a C ÍÎËÏ²Î. Methods are represented by pointers to the code implementing the methods. By contrast, Java is implemented in an entirely different fashion. The object code of Java is a special binary repre- sentation known as Java byte code. This code can be viewed as a machine-level program for a virtual machine. As its name suggests, this machine is not imple- mented directly in hardware. Instead, software interpreters process the byte code, simulating the behavior of the virtual machine. Alternatively, an approach known as just-in-time compilation dynamically translates byte code sequences into ma- chine instructions. This approach provides faster execution when code is executed multiple times, such as in loops. The advantage of using byte code as the low-level representation of a program is that the same code can be “executed” on many different machines, whereas the machine code we have considered runs only on x86-64 machines. Bibliographic Notes Both Intel and AMD provide extensive documentation on their processors. This includes general descriptions of an assembly-language programmer’s view of the hardware [2, 50], as well as detailed references about the individual instruc- tions [3, 51]. Reading the instruction descriptions is complicated by the facts that (1) all documentation is based on the Intel assembly-code format, (2) there are many variations for each instruction due to the different addressing and execution modes, and (3) there are no illustrative examples. Still, these remain the authori- tative references about the behavior of each instruction. The organization x86-64.org has been responsible for deﬁning the application binary interface (ABI) for x86-64 code running on Linux systems [77]. This inter- face describes details for procedure linkages, binary code ﬁles, and a number of other features that are required for machine-code programs to execute properly. Homework Problems 347 As we have discussed, the ATT format used by gcc is very different from the Intel format used in Intel documentation and by other compilers (including the Microsoft compilers). Muchnick’s book on compiler design [80] is considered the most comprehen- sive reference on code-optimization techniques. It covers many of the techniques we discuss here, such as register usage conventions. Much has been written about the use of buffer overﬂow to attack systems over the Internet. Detailed analyses of the 1988 Internet worm have been published by Spafford [105] as well as by members of the team at MIT who helped stop its spread [35]. Since then a number of papers and projects have generated ways both to create and to prevent buffer overﬂow attacks. Seacord’s book [97] provides a wealth of information about buffer overﬂow and other attacks on code generated by C compilers. Homework Problems 3.58 ◆ For a function with prototype ÄÇÅ× ®−²Ç®−pfÄÇÅ× Ój ÄÇÅ× Ôj ÄÇÅ× Ögy gcc generates the following assembly code: 1 ®−²Ç®−px 2 ÍÏ¾Ê cË®Ój cËÍ© 3 ©ÀÏÄÊ cËÍ©j cË®© 4 ÀÇÌÊ cËÍ©j cËþÓ 5 ÍþÄÊ btqj cËþÓ 6 ÍþËÊ btqj cËþÓ 7 ÓÇËÊ cË®©j cËþÓ 8 Ë−Î Parameters Ó, Ô, and Ö are passed in registers cË®©, cËÍ©, and cË®Ó. The code stores the return value in register cËþÓ. Write C code for ®−²Ç®−p that will have an effect equivalent to the assembly code shown. 3.59 ◆◆ The following code computes the 128-bit product of two 64-bit signed values x and y and stores the result in memory: 1 ÎÔÉ−®−ð ˆˆ©ÅÎopv ©ÅÎopvˆÎy 2 3 ÌÇ©® ÍÎÇË−ˆÉËÇ®f©ÅÎopvˆÎ h®−ÍÎj ©ÅÎtrˆÎ Ój ©ÅÎtrˆÎ Ôg Õ 4 h®−ÍÎ{Óh f©ÅÎopvˆÎg Ôy 5 Û Gcc generates the following assembly code implementing the computation: 348 Chapter 3 Machine-Level Representation of Programs 1 ÍÎÇË−ˆÉËÇ®x 2 ÀÇÌÊ cË®Ój cËþÓ 3 ²ÊÎÇ 4 ÀÇÌÊ cËÍ©j cË²Ó 5 ÍþËÊ btqj cË²Ó 6 ©ÀÏÄÊ cËþÓj cË²Ó 7 ©ÀÏÄÊ cËÍ©j cË®Ó 8 þ®®Ê cË®Ój cË²Ó 9 ÀÏÄÊ cËÍ© 10 þ®®Ê cË²Ój cË®Ó 11 ÀÇÌÊ cËþÓj fcË®©g 12 ÀÇÌÊ cË®Ój vfcË®©g 13 Ë−Î This code uses three multiplications for the multiprecision arithmetic required to implement 128-bit arithmetic on a 64-bit machine. Describe the algorithm used to compute the product, and annotate the assembly code to show how it realizes your algorithm. Hint: When extending arguments of x and y to 128 bits, they can be rewritten as x = 264 . xh + xl and y = 264 . yh + yl, where xh, xl, yh, and yl are 64- bit values. Similarly, the 128-bit product can be written as p = 264 . ph + pl, where ph and pl are 64-bit values. Show how the code computes the values of ph and pl in terms of xh, xl, yh, and yl. 3.60 ◆◆ Consider the following assembly code: long loop(long x, int n) x in %rdi, n in %esi 1 ÄÇÇÉx 2 ÀÇÌÄ c−Í©j c−²Ó 3 ÀÇÌÄ boj c−®Ó 4 ÀÇÌÄ bnj c−þÓ 5 ÁÀÉ l‹p 6 l‹qx 7 ÀÇÌÊ cË®©j cËv 8 þÅ®Ê cË®Ój cËv 9 ÇËÊ cËvj cËþÓ 10 ÍþÄÊ c²Äj cË®Ó 11 l‹px 12 Î−ÍÎÊ cË®Ój cË®Ó 13 ÁÅ− l‹q 14 Ë−Éy Ë−Î The preceding code was generated by compiling C code that had the following overall form: Homework Problems 349 1 ÄÇÅ× ÄÇÇÉfÄÇÅ× Ój ÄÇÅ× Åg 2 Õ 3 ÄÇÅ× Ë−ÍÏÄÎ { y 4 ÄÇÅ× ÀþÍÂy 5 ðÇË fÀþÍÂ { y ÀþÍÂ y ÀþÍÂ { gÕ 6 Ë−ÍÏÄÎ Ú{ y 7 Û 8 Ë−ÎÏËÅ Ë−ÍÏÄÎy 9 Û Your task is to ﬁll in the missing parts of the C code to get a program equivalent to the generated assembly code. Recall that the result of the function is returned in register cËþÓ. You will ﬁnd it helpful to examine the assembly code before, during, and after the loop to form a consistent mapping between the registers and the program variables. A. Which registers hold program values Ó, Å, Ë−ÍÏÄÎ, and ÀþÍÂ? B. What are the initial values of Ë−ÍÏÄÎ and ÀþÍÂ? C. What is the test condition for ÀþÍÂ? D. How does ÀþÍÂ get updated? E. How does Ë−ÍÏÄÎ get updated? F. Fill in all the missing parts of the C code. 3.61 ◆◆ In Section 3.6.6, we examined the following code as a candidate for the use of conditional data transfer: ÄÇÅ× ²Ë−þ®fÄÇÅ× hÓÉg Õ Ë−ÎÏËÅ fÓÉ } hÓÉ x ngy Û We showed a trial implementation using a conditional move instruction but argued that it was not valid, since it could attempt to read from a null address. Write a C function ²Ë−þ®ˆþÄÎ that has the same behavior as ²Ë−þ®, except that it can be compiled to use conditional data transfer. When compiled, the generated code should use a conditional move instruction rather than one of the jump instructions. 3.62 ◆◆ The code that follows shows an example of branching on an enumerated type value in a switch statement. Recall that enumerated types in C are simply a way to introduce a set of names having associated integer values. By default, the values assigned to the names count from zero upward. In our code, the actions associated with the different case labels have been omitted. 350 Chapter 3 Machine-Level Representation of Programs 1 mh ¥ÅÏÀ−ËþÎ−® ÎÔÉ− ²Ë−þÎ−Í Í−Î Çð ²ÇÅÍÎþÅÎÍ ÅÏÀ¾−Ë−® n þÅ® ÏÉÑþË® hm 2 ÎÔÉ−®−ð −ÅÏÀ Õ›ﬂ⁄¥ˆ¡j ›ﬂ⁄¥ˆ¢j ›ﬂ⁄¥ˆ£j ›ﬂ⁄¥ˆ⁄j ›ﬂ⁄¥ˆ¥Û ÀÇ®−ˆÎy 3 4 ÄÇÅ× ÍÑ©Î²³qfÄÇÅ× hÉoj ÄÇÅ× hÉpj ÀÇ®−ˆÎ þ²Î©ÇÅg 5 Õ 6 ÄÇÅ× Ë−ÍÏÄÎ { ny 7 ÍÑ©Î²³fþ²Î©ÇÅg Õ 8 ²þÍ− ›ﬂ⁄¥ˆ¡x 9 10 ²þÍ− ›ﬂ⁄¥ˆ¢x 11 12 ²þÍ− ›ﬂ⁄¥ˆ£x 13 14 ²þÍ− ›ﬂ⁄¥ˆ⁄x 15 16 ²þÍ− ›ﬂ⁄¥ˆ¥x 17 18 ®−ðþÏÄÎx 19 20 Û 21 Ë−ÎÏËÅ Ë−ÍÏÄÎy 22 Û The part of the generated assembly code implementing the different actions is shown in Figure 3.52. The annotations indicate the argument locations, the register values, and the case labels for the different jump destinations. Fill in the missing parts of the C code. It contained one case that fell through to another—try to reconstruct this. 3.63 ◆◆ This problem will give you a chance to reverse engineer a ÍÑ©Î²³ statement from disassembled machine code. In the following procedure, the body of the ÍÑ©Î²³ statement has been omitted: 1 ÄÇÅ× ÍÑ©Î²³ˆÉËÇ¾fÄÇÅ× Ój ÄÇÅ× Åg Õ 2 ÄÇÅ× Ë−ÍÏÄÎ { Óy 3 ÍÑ©Î²³fÅg Õ 4 mh ƒ©ÄÄ ©Å ²Ç®− ³−Ë− hm 5 6 Û 7 Ë−ÎÏËÅ Ë−ÍÏÄÎy 8 Û Homework Problems 351 p1 in %rdi, p2 in %rsi, action in %edx 1 l‹vx MODE_E 2 ÀÇÌÄ bpuj c−þÓ 3 Ë−Î 4 l‹qx MODE_A 5 ÀÇÌÊ fcËÍ©gj cËþÓ 6 ÀÇÌÊ fcË®©gj cË®Ó 7 ÀÇÌÊ cË®Ój fcËÍ©g 8 Ë−Î 9 l‹sx MODE_B 10 ÀÇÌÊ fcË®©gj cËþÓ 11 þ®®Ê fcËÍ©gj cËþÓ 12 ÀÇÌÊ cËþÓj fcË®©g 13 Ë−Î 14 l‹tx MODE_C 15 ÀÇÌÊ bswj fcË®©g 16 ÀÇÌÊ fcËÍ©gj cËþÓ 17 Ë−Î 18 l‹ux MODE_D 19 ÀÇÌÊ fcËÍ©gj cËþÓ 20 ÀÇÌÊ cËþÓj fcË®©g 21 ÀÇÌÄ bpuj c−þÓ 22 Ë−Î 23 l‹wx default 24 ÀÇÌÄ bopj c−þÓ 25 Ë−Î Figure 3.52 Assembly code for Problem 3.62. This code implements the different branches of a ÍÑ©Î²³ statement. Figure 3.53 shows the disassembled machine code for the procedure. The jump table resides in a different area of memory. We can see from the indirect jump on line 5 that the jump table begins at address nÓrnntðv. Using the gdb debugger, we can examine the six 8-byte words of memory compris- ing the jump table with the command Ómt×Ó nÓrnntðv. Gdb prints the following: f×®¾g x/6gx 0x4006f8 nÓrnntðvx nÓnnnnnnnnnnrnnsþo nÓnnnnnnnnnnrnns²q nÓrnnunvx nÓnnnnnnnnnnrnnsþo nÓnnnnnnnnnnrnnsþþ nÓrnnuovx nÓnnnnnnnnnnrnns¾p nÓnnnnnnnnnnrnns¾ð Fill in the body of the ÍÑ©Î²³ statement with C code that will have the same behavior as the machine code. 352 Chapter 3 Machine-Level Representation of Programs long switch_prob(long x, long n) x in %rdi, n in %rsi 1 nnnnnnnnnnrnnswn zÍÑ©Î²³ˆÉËÇ¾|x 2 rnnswnx rv vq −− q² ÍÏ¾ bnÓq²jcËÍ© 3 rnnswrx rv vq ð− ns ²ÀÉ bnÓsjcËÍ© 4 rnnswvx uu pw Áþ rnns²q zÍÑ©Î²³ˆÉËÇ¾inÓqq| 5 rnnswþx ðð pr ðs ðv nt rn nn ÁÀÉÊ hnÓrnntðvfjcËÍ©jvg 6 rnnsþox rv v® nr ð® nn nn nn Ä−þ nÓnfjcË®©jvgjcËþÓ 7 rnnsþvx nn 8 rnnsþwx ²q Ë−ÎÊ 9 rnnsþþx rv vw ðv ÀÇÌ cË®©jcËþÓ 10 rnnsþ®x rv ²o ðv nq ÍþË bnÓqjcËþÓ 11 rnns¾ox ²q Ë−ÎÊ 12 rnns¾px rv vw ðv ÀÇÌ cË®©jcËþÓ 13 rnns¾sx rv ²o −n nr Í³Ä bnÓrjcËþÓ 14 rnns¾wx rv pw ðv ÍÏ¾ cË®©jcËþÓ 15 rnns¾²x rv vw ²u ÀÇÌ cËþÓjcË®© 16 rnns¾ðx rv nð þð ðð ©ÀÏÄ cË®©jcË®© 17 rnns²qx rv v® ru r¾ Ä−þ nÓr¾fcË®©gjcËþÓ 18 rnns²ux ²q Ë−ÎÊ Figure 3.53 Disassembled code for Problem 3.63. 3.64 ◆◆◆ Consider the following source code, where R, S, and T are constants declared with a®−ð©Å−: 1 ÄÇÅ× ¡‰‡`‰·`‰¶`y 2 3 ÄÇÅ× ÍÎÇË−ˆ−Ä−fÄÇÅ× ©j ÄÇÅ× Áj ÄÇÅ× Âj ÄÇÅ× h®−ÍÎg 4 Õ 5 h®−ÍÎ { ¡‰©`‰Á`‰Â`y 6 Ë−ÎÏËÅ Í©Ö−Çðf¡gy 7 Û In compiling this program, gcc generates the following assembly code: long store_ele(long i, long j, long k, long *dest) i in %rdi, j in %rsi, k in %rdx, dest in %rcx 1 ÍÎÇË−ˆ−Ä−x 2 Ä−þÊ fcËÍ©jcËÍ©jpgj cËþÓ 3 Ä−þÊ fcËÍ©jcËþÓjrgj cËþÓ 4 ÀÇÌÊ cË®©j cËÍ© 5 ÍþÄÊ btj cËÍ© 6 þ®®Ê cËÍ©j cË®© 7 þ®®Ê cËþÓj cË®© Homework Problems 353 8 þ®®Ê cË®©j cË®Ó 9 ÀÇÌÊ ¡fjcË®Ójvgj cËþÓ 10 ÀÇÌÊ cËþÓj fcË²Óg 11 ÀÇÌÄ bqtrnj c−þÓ 12 Ë−Î A. Extend Equation 3.1 from two dimensions to three to provide a formula for the location of array element ¡‰©`‰Á`‰Â`. B. Use your reverse engineering skills to determine the values of R, S, and T based on the assembly code. 3.65 ◆ The following code transposes the elements of an M × M array, where M is a constant deﬁned by a®−ð©Å−: 1 ÌÇ©® ÎËþÅÍÉÇÍ−fÄÇÅ× ¡‰›`‰›`g Õ 2 ÄÇÅ× ©j Áy 3 ðÇËf©{ny©z›y ©iig 4 ðÇËfÁ{nyÁz©y Áiig Õ 5 ÄÇÅ× Î { ¡‰©`‰Á`y 6 ¡‰©`‰Á` { ¡‰Á`‰©`y 7 ¡‰Á`‰©` { Îy 8 Û 9 Û When compiled with optimization level kﬂo, gcc generates the following code for the inner loop of the function: 1 l‹tx 2 ÀÇÌÊ fcË®Ógj cË²Ó 3 ÀÇÌÊ fcËþÓgj cËÍ© 4 ÀÇÌÊ cËÍ©j fcË®Óg 5 ÀÇÌÊ cË²Ój fcËþÓg 6 þ®®Ê bvj cË®Ó 7 þ®®Ê bopnj cËþÓ 8 ²ÀÉÊ cË®©j cËþÓ 9 ÁÅ− l‹t We can see that gcc has converted the array indexing to pointer code. A. Which register holds a pointer to array element ¡‰©`‰Á`? B. Which register holds a pointer to array element ¡‰Á`‰©`? C. What is the value of M? 3.66 ◆ Consider the following source code, where ﬁ‡ and ﬁ£ are macro expressions de- clared with a®−ð©Å− that compute the dimensions of array ¡ in terms of parame- ter n. This code computes the sum of the elements of column j of the array. 354 Chapter 3 Machine-Level Representation of Programs 1 ÄÇÅ× ÍÏÀˆ²ÇÄfÄÇÅ× Åj ÄÇÅ× ¡‰ﬁ‡fÅg`‰ﬁ£fÅg`j ÄÇÅ× Ág Õ 2 ÄÇÅ× ©y 3 ÄÇÅ× Ë−ÍÏÄÎ { ny 4 ðÇË f© { ny © z ﬁ‡fÅgy ©iig 5 Ë−ÍÏÄÎ i{ ¡‰©`‰Á`y 6 Ë−ÎÏËÅ Ë−ÍÏÄÎy 7 Û In compiling this program, gcc generates the following assembly code: long sum_col(long n, long A[NR(n)][NC(n)], long j) n in %rdi, A in %rsi, j in %rdx 1 ÍÏÀˆ²ÇÄx 2 Ä−þÊ ofjcË®©jrgj cËv 3 Ä−þÊ fcË®©jcË®©jpgj cËþÓ 4 ÀÇÌÊ cËþÓj cË®© 5 Î−ÍÎÊ cËþÓj cËþÓ 6 ÁÄ− l‹r 7 ÍþÄÊ bqj cËv 8 Ä−þÊ fcËÍ©jcË®Ójvgj cË²Ó 9 ÀÇÌÄ bnj c−þÓ 10 ÀÇÌÄ bnj c−®Ó 11 l‹qx 12 þ®®Ê fcË²Ógj cËþÓ 13 þ®®Ê boj cË®Ó 14 þ®®Ê cËvj cË²Ó 15 ²ÀÉÊ cË®©j cË®Ó 16 ÁÅ− l‹q 17 Ë−Éy Ë−Î 18 l‹rx 19 ÀÇÌÄ bnj c−þÓ 20 Ë−Î Use your reverse engineering skills to determine the deﬁnitions of ﬁ‡ and ﬁ£. 3.67 ◆◆ For this exercise, we will examine the code generated by gcc for functions that have structures as arguments and return values, and from this see how these language features are typically implemented. The following C code has a function ÉËÇ²−ÍÍ having structures as argument and return values, and a function −ÌþÄ that calls ÉËÇ²−ÍÍ: 1 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 2 ÄÇÅ× þ‰p`y 3 ÄÇÅ× hÉy 4 Û ÍÎË¡y 5 Homework Problems 355 6 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 7 ÄÇÅ× Ï‰p`y 8 ÄÇÅ× Êy 9 Û ÍÎË¢y 10 11 ÍÎË¢ ÉËÇ²−ÍÍfÍÎË¡ Íg Õ 12 ÍÎË¢ Ëy 13 ËlÏ‰n` { Ílþ‰o`y 14 ËlÏ‰o` { Ílþ‰n`y 15 ËlÊ { hÍlÉy 16 Ë−ÎÏËÅ Ëy 17 Û 18 19 ÄÇÅ× −ÌþÄfÄÇÅ× Ój ÄÇÅ× Ôj ÄÇÅ× Ög Õ 20 ÍÎË¡ Íy 21 Ílþ‰n` { Óy 22 Ílþ‰o` { Ôy 23 ÍlÉ { dÖy 24 ÍÎË¢ Ë { ÉËÇ²−ÍÍfÍgy 25 Ë−ÎÏËÅ ËlÏ‰n` i ËlÏ‰o` i ËlÊy 26 Û Gcc generates the following code for these two functions: strB process(strA s) 1 ÉËÇ²−ÍÍx 2 ÀÇÌÊ cË®©j cËþÓ 3 ÀÇÌÊ prfcËÍÉgj cË®Ó 4 ÀÇÌÊ fcË®Ógj cË®Ó 5 ÀÇÌÊ otfcËÍÉgj cË²Ó 6 ÀÇÌÊ cË²Ój fcË®©g 7 ÀÇÌÊ vfcËÍÉgj cË²Ó 8 ÀÇÌÊ cË²Ój vfcË®©g 9 ÀÇÌÊ cË®Ój otfcË®©g 10 Ë−Î long eval(long x, long y, long z) x in %rdi, y in %rsi, z in %rdx 1 −ÌþÄx 2 ÍÏ¾Ê bonrj cËÍÉ 3 ÀÇÌÊ cË®Ój prfcËÍÉg 4 Ä−þÊ prfcËÍÉgj cËþÓ 5 ÀÇÌÊ cË®©j fcËÍÉg 6 ÀÇÌÊ cËÍ©j vfcËÍÉg 7 ÀÇÌÊ cËþÓj otfcËÍÉg 8 Ä−þÊ trfcËÍÉgj cË®© 9 ²þÄÄ ÉËÇ²−ÍÍ 356 Chapter 3 Machine-Level Representation of Programs 10 ÀÇÌÊ upfcËÍÉgj cËþÓ 11 þ®®Ê trfcËÍÉgj cËþÓ 12 þ®®Ê vnfcËÍÉgj cËþÓ 13 þ®®Ê bonrj cËÍÉ 14 Ë−Î A. We can see on line 2 of function −ÌþÄ that it allocates 104 bytes on the stack. Diagram the stack frame for −ÌþÄ, showing the values that it stores on the stack prior to calling ÉËÇ²−ÍÍ. B. What value does −ÌþÄ pass in its call to ÉËÇ²−ÍÍ? C. How does the code for ÉËÇ²−ÍÍ access the elements of structure argument Í? D. How does the code for ÉËÇ²−ÍÍ set the ﬁelds of result structure Ë? E. Complete your diagram of the stack frame for −ÌþÄ, showing how −ÌþÄ accesses the elements of structure Ë following the return from ÉËÇ²−ÍÍ. F. What general principles can you discern about how structure values are passed as function arguments and how they are returned as function results? 3.68 ◆◆◆ In the following code, A and B are constants deﬁned with a®−ð©Å−: 1 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 2 ©ÅÎ Ó‰¡`‰¢`y mh •ÅÂÅÇÑÅ ²ÇÅÍÎþÅÎÍ ¡ þÅ® ¢ hm 3 ÄÇÅ× Ôy 4 Û ÍÎËoy 5 6 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 7 ²³þË þËËþÔ‰¢`y 8 ©ÅÎ Îy 9 Í³ÇËÎ Í‰¡`y 10 ÄÇÅ× Ïy 11 Û ÍÎËpy 12 13 ÌÇ©® Í−Î‚þÄfÍÎËo hÉj ÍÎËp hÊg Õ 14 ÄÇÅ× Ìo { Êk|Îy 15 ÄÇÅ× Ìp { Êk|Ïy 16 Ék|Ô { ÌoiÌpy 17 Û Gcc generates the following code for Í−Î‚þÄ: void setVal(str1 *p, str2 *q) p in %rdi, q in %rsi 1 Í−Î‚þÄx 2 ÀÇÌÍÄÊ vfcËÍ©gj cËþÓ 3 þ®®Ê qpfcËÍ©gj cËþÓ Homework Problems 357 4 ÀÇÌÊ cËþÓj ovrfcË®©g 5 Ë−Î What are the values of A and B? (The solution is unique.) 3.69 ◆◆◆ You are charged with maintaining a large C program, and you come across the following code: 1 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 2 ©ÅÎ ð©ËÍÎy 3 þˆÍÎËÏ²Î þ‰£ﬁ¶`y 4 ©ÅÎ ÄþÍÎy 5 Û ¾ˆÍÎËÏ²Îy 6 7 ÌÇ©® Î−ÍÎfÄÇÅ× ©j ¾ˆÍÎËÏ²Î h¾Ég 8 Õ 9 ©ÅÎ Å { ¾Ék|ð©ËÍÎ i ¾Ék|ÄþÍÎy 10 þˆÍÎËÏ²Î hþÉ { d¾Ék|þ‰©`y 11 þÉk|Ó‰þÉk|©®Ó` { Åy 12 Û The declarations of the compile-time constant £ﬁ¶ and the structure þˆÍÎËÏ²Î are in a ﬁle for which you do not have the necessary access privilege. Fortunately, you have a copy of the lÇ version of code, which you are able to disassemble with the objdump program, yielding the following disassembly: void test(long i, b_struct *bp) i in %rdi, bp in %rsi 1 nnnnnnnnnnnnnnnn zÎ−ÍÎ|x 2 nx v¾ v− pn no nn nn ÀÇÌ nÓopnfcËÍ©gjc−²Ó 3 tx nq n− þ®® fcËÍ©gjc−²Ó 4 vx rv v® nr ¾ð Ä−þ fcË®©jcË®©jrgjcËþÓ 5 ²x rv v® nr ²t Ä−þ fcËÍ©jcËþÓjvgjcËþÓ 6 onx rv v¾ sn nv ÀÇÌ nÓvfcËþÓgjcË®Ó 7 orx rv tq ²w ÀÇÌÍÄÊ c−²ÓjcË²Ó 8 oux rv vw r² ®n on ÀÇÌ cË²ÓjnÓonfcËþÓjcË®Ójvg 9 o²x ²q Ë−ÎÊ Using your reverse engineering skills, deduce the following: A. The value of £ﬁ¶. B. A complete declaration of structure þˆÍÎËÏ²Î. Assume that the only ﬁelds in this structure are ©®Ó and Ó, and that both of these contain signed values. 358 Chapter 3 Machine-Level Representation of Programs 3.70 ◆◆◆ Consider the following union declaration: 1 ÏÅ©ÇÅ −Ä− Õ 2 ÍÎËÏ²Î Õ 3 ÄÇÅ× hÉy 4 ÄÇÅ× Ôy 5 Û −oy 6 ÍÎËÏ²Î Õ 7 ÄÇÅ× Óy 8 ÏÅ©ÇÅ −Ä− hÅ−ÓÎy 9 Û −py 10 Ûy This declaration illustrates that structures can be embedded within unions. The following function (with some expressions omitted) operates on a linked list having these unions as list elements: 1 ÌÇ©® ÉËÇ² fÏÅ©ÇÅ −Ä− hÏÉg Õ 2 ÏÉk| {hf gk y 3 Û A. What are the offsets (in bytes) of the following ﬁelds: −olÉ −olÔ −plÓ −plÅ−ÓÎ B. How many total bytes does the structure require? C. The compiler generates the following assembly code for ÉËÇ²: void proc (union ele *up) up in %rdi 1 ÉËÇ²x 2 ÀÇÌÊ vfcË®©gj cËþÓ 3 ÀÇÌÊ fcËþÓgj cË®Ó 4 ÀÇÌÊ fcË®Ógj cË®Ó 5 ÍÏ¾Ê vfcËþÓgj cË®Ó 6 ÀÇÌÊ cË®Ój fcË®©g 7 Ë−Î On the basis of this information, ﬁll in the missing expressions in the code for ÉËÇ². Hint: Some union references can have ambiguous interpretations. These ambiguities get resolved as you see where the references lead. There Homework Problems 359 is only one answer that does not perform any casting and does not violate any type constraints. 3.71 ◆ Write a function ×ÇÇ®ˆ−²³Ç that reads a line from standard input and writes it to standard output. Your implementation should work for an input line of arbitrary length. You may use the library function ð×−ÎÍ, but you must make sure your function works correctly even when the input line requires more space than you have allocated for your buffer. Your code should also check for error conditions and return when one is encountered. Refer to the deﬁnitions of the standard I/O functions for documentation [45, 61]. 3.72 ◆◆ Figure 3.54(a) shows the code for a function that is similar to function ÌðÏÅ²Î (Figure 3.43(a)). We used ÌðÏÅ²Î to illustrate the use of a frame pointer in man- aging variable-size stack frames. The new function þðËþÀ− allocates space for local (a) C code 1 a©Å²ÄÏ®− zþÄÄÇ²þl³| 2 3 ÄÇÅ× þðËþÀ−fÄÇÅ× Åj ÄÇÅ× ©®Ój ÄÇÅ× hÊg Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× hhÉ { þÄÄÇ²þfÅ h Í©Ö−ÇðfÄÇÅ× hggy 6 É‰n` { d©y 7 ðÇËf©{oy©zÅy ©iig 8 É‰©` { Êy 9 Ë−ÎÏËÅ hÉ‰©®Ó`y 10 Û (b) Portions of generated assembly code long aframe(long n, long idx, long *q) n in %rdi, idx in %rsi, q in %rdx 1 þðËþÀ−x 2 ÉÏÍ³Ê cË¾É 3 ÀÇÌÊ cËÍÉj cË¾É 4 ÍÏ¾Ê botj cËÍÉ Allocate space for i (%rsp = s1) 5 Ä−þÊ qnfjcË®©jvgj cËþÓ 6 þÅ®Ê bkotj cËþÓ 7 ÍÏ¾Ê cËþÓj cËÍÉ Allocate space for array p (%rsp = s2) 8 Ä−þÊ osfcËÍÉgj cËv 9 þÅ®Ê bkotj cËv Set %r8 to &p[0] l l l Figure 3.54 Code for Problem 3.72. This function is similar to that of Figure 3.43. 360 Chapter 3 Machine-Level Representation of Programs array É by calling library function þÄÄÇ²þ. This function is similar to the more com- monly used function ÀþÄÄÇ², except that it allocates space on the run-time stack. The space is automatically deallocated when the executing procedure returns. Figure 3.54(b) shows the part of the assembly code that sets up the frame pointer and allocates space for local variables © and É. It is very similar to the corresponding code for ÌðËþÀ−. Let us use the same notation as in Problem 3.49: The stack pointer is set to values s1 at line 4 and s2 at line 7. The start address of array É is set to value p at line 9. Extra space e2 may arise between s2 and p, and extra space e1 may arise between the end of array É and s1. A. Explain, in mathematical terms, the logic in the computation of s2. B. Explain, in mathematical terms, the logic in the computation of p. C. Find values of n and s1 that lead to minimum and maximum values of e1. D. What alignment properties does this code guarantee for the values of s2 and p? 3.73 ◆ Write a function in assembly code that matches the behavior of the function ð©Å®ˆ ËþÅ×− in Figure 3.51. Your code should contain only one ﬂoating-point comparison instruction, and then it should use conditional branches to generate the correct result. Test your code on all 232 possible argument values. Web Aside asm:easm on page 214 describes how to incorporate functions written in assembly code into C programs. 3.74 ◆◆ Write a function in assembly code that matches the behavior of the function ð©Å®ˆ ËþÅ×− in Figure 3.51. Your code should contain only one ﬂoating-point comparison instruction, and then it should use conditional moves to generate the correct result. You might want to make use of the instruction ²ÀÇÌÉ (move if even parity). Test your code on all 232 possible argument values. Web Aside asm:easm on page 214 describes how to incorporate functions written in assembly code into C programs. 3.75 ◆ ISO C99 includes extensions to support complex numbers. Any ﬂoating-point type can be modiﬁed with the keyword ²ÇÀÉÄ−Ó. Here are some sample functions that work with complex data and that call some of the associated library functions: 1 a©Å²ÄÏ®− z²ÇÀÉÄ−Ól³| 2 3 ®ÇÏ¾Ä− ²ˆ©Àþ×f®ÇÏ¾Ä− ²ÇÀÉÄ−Ó Óg Õ 4 Ë−ÎÏËÅ ²©Àþ×fÓgy 5 Û 6 7 ®ÇÏ¾Ä− ²ˆË−þÄf®ÇÏ¾Ä− ²ÇÀÉÄ−Ó Óg Õ 8 Ë−ÎÏËÅ ²Ë−þÄfÓgy 9 Û 10 Solutions to Practice Problems 361 11 ®ÇÏ¾Ä− ²ÇÀÉÄ−Ó ²ˆÍÏ¾f®ÇÏ¾Ä− ²ÇÀÉÄ−Ó Ój ®ÇÏ¾Ä− ²ÇÀÉÄ−Ó Ôg Õ 12 Ë−ÎÏËÅÓkÔy 13 Û When compiled, gcc generates the following assembly code for these func- tions: double c_imag(double complex x) 1 ²ˆ©Àþ×x 2 ÀÇÌþÉ® cÓÀÀoj cÓÀÀn 3 Ë−Î double c_real(double complex x) 4 ²ˆË−þÄx 5 Ë−Éy Ë−Î double complex c_sub(double complex x, double complex y) 6 ²ˆÍÏ¾x 7 ÍÏ¾Í® cÓÀÀpj cÓÀÀn 8 ÍÏ¾Í® cÓÀÀqj cÓÀÀo 9 Ë−Î Based on these examples, determine the following: A. How are complex arguments passed to a function? B. How are complex values returned from a function? Solutions to Practice Problems Solution to Problem 3.1 (page 218) This exercise gives you practice with the different operand forms. Operand Value Comment cËþÓ nÓonn Register nÓonr nÓ¡¢ Absolute address bnÓonv nÓonv Immediate fcËþÓg nÓƒƒ Address nÓonn rfcËþÓg nÓ¡¢ Address nÓonr wfcËþÓjcË®Óg nÓoo Address nÓon£ ptnfcË²ÓjcË®Óg nÓoq Address nÓonv nÓƒ£fjcË²Ójrg nÓƒƒ Address nÓonn fcËþÓjcË®Ójrg nÓoo Address nÓon£ Solution to Problem 3.2 (page 221) As we have seen, the assembly code generated by gcc includes sufﬁxes on the instructions, while the disassembler does not. Being able to switch between these 362 Chapter 3 Machine-Level Representation of Programs two forms is an important skill to learn. One important feature is that memory references in x86-64 are always given with quad word registers, such as cËþÓ, even if the operand is a byte, single word, or double word. Here is the code written with sufﬁxes: ÀÇÌÄ c−þÓj fcËÍÉg ÀÇÌÑ fcËþÓgj c®Ó ÀÇÌ¾ bnÓƒƒj c¾Ä ÀÇÌ¾ fcËÍÉjcË®Ójrgj c®Ä ÀÇÌÊ fcË®Ógj cËþÓ ÀÇÌÑ c®Ój fcËþÓg Solution to Problem 3.3 (page 222) Since we will rely on gcc to generate most of our assembly code, being able to write correct assembly code is not a critical skill. Nonetheless, this exercise will help you become more familiar with the different instruction and operand types. Here is the code with explanations of the errors: ÀÇÌ¾ bnÓƒj fc−¾Óg Cannot use %ebx as address register ÀÇÌÄ cËþÓj fcËÍÉg Mismatch between instruction suffix and register ID ÀÇÌÑ fcËþÓgjrfcËÍÉg Cannot have both source and destination be memory references ÀÇÌ¾ cþÄjcÍÄ No register named %sl ÀÇÌÄ c−þÓjbnÓopq Cannot have immediate as destination ÀÇÌÄ c−þÓjc®Ó Destination operand incorrect size ÀÇÌ¾ cÍ©j vfcË¾Ég Mismatch between instruction suffix and register ID Solution to Problem 3.4 (page 223) This exercise gives you more experience with the different data movement in- structions and how they relate to the data types and conversion rules of C. The nuances of conversions of both signedness and size, as well as integral promotion, add challenge to this problem. ÍË²ˆÎ ®−ÍÎˆÎ Instruction Comments ÄÇÅ× ÄÇÅ× ÀÇÌÊ fcË®©gj cËþÓ Read 8 bytes ÀÇÌÊ cËþÓj fcËÍ©g Store 8 bytes ²³þË ©ÅÎ ÀÇÌÍ¾Ä fcË®©gj c−þÓ Convert char to int ÀÇÌÄ c−þÓj fcËÍ©g Store 4 bytes ²³þË ÏÅÍ©×Å−® ÀÇÌÍ¾Ä fcË®©gj c−þÓ Convert char to int ÀÇÌÄ c−þÓj fcËÍ©g Store 4 bytes ÏÅÍ©×Å−® ²³þË ÄÇÅ× ÀÇÌÖ¾Ä fcË®©gj c−þÓ Read byte and zero-extend ÀÇÌÊ cËþÓj fcËÍ©g Store 8 bytes Solutions to Practice Problems 363 ©ÅÎ ²³þË ÀÇÌÄ fcË®©gj c−þÓ Read 4 bytes ÀÇÌ¾ cþÄj fcËÍ©g Store low-order byte ÏÅÍ©×Å−® ÏÅÍ©×Å−® ÀÇÌÄ fcË®©gj c−þÓ Read 4 bytes ²³þË ÀÇÌ¾ cþÄj fcËÍ©g Store low-order byte ²³þË Í³ÇËÎ ÀÇÌÍ¾Ñ fcË®©gj cþÓ Read byte and sign-extend ÀÇÌÑ cþÓj fcËÍ©g Store 2 bytes Solution to Problem 3.5 (page 225) Reverse engineering is a good way to understand systems. In this case, we want to reverse the effect of the C compiler to determine what C code gave rise to this assembly code. The best way is to run a “simulation,” starting with values Ó, Ô, and Ö at the locations designated by pointers ÓÉ, ÔÉ, and ÖÉ, respectively. We would then get the following behavior: void decode1(long *xp, long *yp, long *zp) xp in %rdi, yp in %rsi, zp in %rdx ®−²Ç®−ox ÀÇÌÊ fcË®©gj cËv Get x = *xp ÀÇÌÊ fcËÍ©gj cË²Ó Get y = *yp ÀÇÌÊ fcË®Ógj cËþÓ Get z = *zp ÀÇÌÊ cËvj fcËÍ©g Store x at yp ÀÇÌÊ cË²Ój fcË®Óg Store y at zp ÀÇÌÊ cËþÓj fcË®©g Store z at xp Ë−Î From this, we can generate the following C code: ÌÇ©® ®−²Ç®−ofÄÇÅ× hÓÉj ÄÇÅ× hÔÉj ÄÇÅ× hÖÉg Õ ÄÇÅ× Ó { hÓÉy ÄÇÅ× Ô { hÔÉy ÄÇÅ× Ö { hÖÉy hÔÉ{Óy hÖÉ{Ôy hÓÉ{Öy Û Solution to Problem 3.6 (page 228) This exercise demonstrates the versatility of the Ä−þÊ instruction and gives you more practice in deciphering the different operand forms. Although the operand forms are classiﬁed as type “Memory” in Figure 3.3, no memory access occurs. 364 Chapter 3 Machine-Level Representation of Programs Instruction Result Ä−þÊ wfcË®Ógj cËþÓ 9 + q Ä−þÊ fcË®ÓjcË¾Ógj cËþÓ q + p Ä−þÊ fcË®ÓjcË¾Ójqgj cËþÓ q + 3p Ä−þÊ pfcË¾ÓjcË¾Ójugj cËþÓ 2 + 8p Ä−þÊ nÓ¥fjcË®Ójqgj cËþÓ 14 + 3q Ä−þÊ tfcË¾ÓjcË®Ójugj cË®Ó 6 + p + 7q Solution to Problem 3.7 (page 229) Again, reverse engineering proves to be a useful way to learn the relationship between C code and the generated assembly code. The best way to solve problems of this type is to annotate the lines of assembly code with information about the operations being performed. Here is a sample: short scale3(short x, short y, short z) x in %rdi, y in %rsi, z in %rdx Í²þÄ−qx Ä−þÊ fcËÍ©jcËÍ©jwgj cË¾Ó 10*y Ä−þÊ fcË¾ÓjcË®Ógj cË¾Ó 10*y+z Ä−þÊ fcË¾ÓjcË®©jcËÍ©gj cË¾Ó 10*y+z+y*x Ë−Î From this, it is easy to generate the missing expression: Í³ÇËÎÎ{onhÔiÖiÔhÓy Solution to Problem 3.8 (page 230) This problem gives you a chance to test your understanding of operands and the arithmetic instructions. The instruction sequence is designed so that the result of each instruction does not affect the behavior of subsequent ones. Instruction Destination Value þ®®Ê cË²ÓjfcËþÓg nÓonn nÓonn ÍÏ¾Ê cË®ÓjvfcËþÓg nÓonv nÓ¡v ©ÀÏÄÊ botjfcËþÓjcË®Ójvg nÓoov nÓoon ©Å²Ê otfcËþÓg nÓoon nÓor ®−²Ê cË²Ó cË²Ó nÓn ÍÏ¾Ê cË®ÓjcËþÓ cËþÓ nÓƒ⁄ Solution to Problem 3.9 (page 231) This exercise gives you a chance to generate a little bit of assembly code. The solution code was generated by gcc. By loading parameter Å in register c−²Ó,it can then use byte register c²Ä to specify the shift amount for the ÍþËÊ instruction. It might seem odd to use a ÀÇÌÄ instruction, given that Å is eight bytes long, but keep in mind that only the least signiﬁcant byte is required to specify the shift amount. Solutions to Practice Problems 365 long shift_left4_rightn(long x, long n) x in %rdi, n in %rsi Í³©ðÎˆÄ−ðÎrˆË©×³ÎÅx ÀÇÌÊ cË®©j cËþÓ Get x ÍþÄÊ brj cËþÓ x <<= 4 ÀÇÌÄ c−Í©j c−²Ó Get n (4 bytes) ÍþËÊ c²Äj cËþÓ x >>= n Solution to Problem 3.10 (page 232) This problem is fairly straightforward, since the assembly code follows the struc- ture of the C code closely. Í³ÇËÎ Éo{ÔÚÖy Í³ÇËÎ Ép { Éo || wy Í³ÇËÎ Éq { ÜÉpy Í³ÇËÎ Ér{ÔkÉqy Solution to Problem 3.11 (page 233) A. This instruction is used to set register cË²Ó to zero, exploiting the property that x ´ x = 0 for any x. It corresponds to the C statement Ó{n. B. A more direct way of setting register cË²Ó to zero is with the instruction ÀÇÌÊ bnjcË²Ó. C. Assembling and disassembling this code, however, we ﬁnd that the version with ÓÇËÊ requires only 3 bytes, while the version with ÀÇÌÊ requires 7. Other ways to set cË²Ó to zero rely on the property that any instruction that updates the lower 4 bytes will cause the high-order bytes to be set to zero. Thus, we could use either ÓÇËÄ c−²Ójc−²Ó (2 bytes) or ÀÇÌÄ bnjc−²Ó (5 bytes). Solution to Problem 3.12 (page 236) We can simply replace the ²ÊÎÇ instruction with one that sets register cË®Ó to zero, and use ®©ÌÊ rather than ©®©ÌÊ as our division instruction, yielding the following code: void uremdiv(unsigned long x, unsigned long y, unsigned long *qp, unsigned long *rp) x in %rdi, y in %rsi, qp in %rdx, rp in %rcx 1 ÏË−À®©Ìx 2 ÀÇÌÊ cË®Ój cËv Copy qp 3 ÀÇÌÊ cË®©j cËþÓ Move x to lower 8 bytes of dividend 4 ÀÇÌÄ bnj c−®Ó Set upper 8 bytes of dividend to 0 5 ®©ÌÊ cËÍ© Divide by y 6 ÀÇÌÊ cËþÓj fcËvg Store quotient at qp 7 ÀÇÌÊ cË®Ój fcË²Óg Store remainder at rp 8 Ë−Î 366 Chapter 3 Machine-Level Representation of Programs Solution to Problem 3.13 (page 240) It is important to understand that assembly code does not keep track of the type of a program value. Instead, the different instructions determine the operand sizes and whether they are signed or unsigned. When mapping from instruction sequences back to C code, we must do a bit of detective work to infer the data types of the program values. A. The sufﬁx ‘Ä’ and the register identiﬁers indicate 32-bit operands, while the comparison is for a two’s-complement z. We can infer that ®þÎþˆÎ must be ©ÅÎ. B. The sufﬁx ‘Ñ’ and the register identiﬁers indicate 16-bit operands, while the comparison is for a two’s-complement |{. We can infer that ®þÎþˆÎ must be Í³ÇËÎ. C. The sufﬁx ‘¾’ and the register identiﬁers indicate 8-bit operands, while the comparison is for an unsigned z{. We can infer that ®þÎþˆÎ must be ÏÅÍ©×Å−® ²³þË. D. The sufﬁx ‘Ê’ and the register identiﬁers indicate 64-bit operands, while the comparison is for _{, which is the same whether the arguments are signed, unsigned, or pointers. We can infer that ®þÎþˆÎ could be either ÄÇÅ×, ÏÅÍ©×Å−® ÄÇÅ×, or some form of pointer. Solution to Problem 3.14 (page 241) This problem is similar to Problem 3.13, except that it involves test instructions rather than cmp instructions. A. The sufﬁx ‘Ê’ and the register identiﬁers indicate a 64-bit operand, while the comparison is for |{, which must be signed. We can infer that ®þÎþˆÎ must be ÄÇÅ×. B. The sufﬁx ‘Ñ’ and the register identiﬁer indicate a 16-bit operand, while the comparison is for {{, which is the same for signed or unsigned. We can infer that ®þÎþˆÎ must be either Í³ÇËÎ or ÏÅÍ©×Å−® Í³ÇËÎ. C. The sufﬁx ‘¾’ and the register identiﬁer indicate an 8-bit operand, while the comparison is for unsigned |. We can infer that ®þÎþˆÎ must be ÏÅÍ©×Å−® ²³þË. D. The sufﬁx ‘Ä’ and the register identiﬁer indicate 32-bit operands, while the comparison is for z. We can infer that ®þÎþˆÎ must be ©ÅÎ. Solution to Problem 3.15 (page 245) This exercise requires you to examine disassembled code in detail and reason about the encodings for jump targets. It also gives you practice in hexadecimal arithmetic. A. The Á− instruction has as its target nÓrnnqð² + nÓnp. As the original disas- sembled code shows, this is nÓrnnqð−: rnnqðþx ur np Á− rnnqð− rnnqð²x ðð ®n ²þÄÄÊ hcËþÓ Solutions to Practice Problems 367 B. The Á− instruction has as its target nÓnÓrnnrqo − 12 (since nÓðr is the 1- byte two’s-complement representation of −12). As the original disassembled code shows, this is nÓrnnrps: rnnrpðx ur ðr Á− rnnrps rnnrqox s® ÉÇÉ cË¾É C. According to the annotation produced by the disassembler, the jump target is at absolute address nÓrnnsru. According to the byte encoding, this must be at an address nÓp bytes beyond that of the ÉÇÉ instruction. Subtracting these gives address nÓrnnsrs. Noting that the encoding of the Áþ instruction requires 2 bytes, it must be located at address nÓrnnsrq. These are conﬁrmed by examining the original disassembly: rnnsrqx uu np Áþ rnnsru rnnsrsx s® ÉÇÉ cË¾É D. Reading the bytes in reverse order, we can see that the target offset is nÓððððððuq, or decimal −141. Adding this to nÓnÓrnns−® (the address of the ÅÇÉ instruction) gives address nÓrnnstn: rnns−vx −w uq ðð ðð ðð ÁÀÉÊ rnnstn rnns−®x wn ÅÇÉ Solution to Problem 3.16 (page 248) Annotating assembly code and writing C code that mimics its control ﬂow are good ﬁrst steps in understanding assembly-language programs. This problem gives you practice for an example with simple control ﬂow. It also gives you a chance to examine the implementation of logical operations. A. Here is the C code: ÌÇ©® ×ÇÎÇˆ²ÇÅ®fÍ³ÇËÎ þj Í³ÇËÎ hÉg Õ ©ð fþ {{ ng ×ÇÎÇ ®ÇÅ−y ©ð fþ |{ hÉg ×ÇÎÇ ®ÇÅ−y hÉ{þy ®ÇÅ−x Ë−ÎÏËÅy Û B. The ﬁrst conditional branch is part of the implementation of the dd expres- sion. If the test for þ being non-null fails, the code will skip the test of þ|{hÉ. Solution to Problem 3.17 (page 248) This is an exercise to help you think about the idea of a general translation rule and how to apply it. A. Converting to this alternate form involves only switching around a few lines of the code: 368 Chapter 3 Machine-Level Representation of Programs ÄÇÅ× ×ÇÎÇ®©ððˆÍ−ˆþÄÎfÄÇÅ× Ój ÄÇÅ× Ôg Õ ÄÇÅ× Ë−ÍÏÄÎy ©ð fÓ z Ôg ×ÇÎÇ ÓˆÄÎˆÔy ×−ˆ²ÅÎiiy Ë−ÍÏÄÎ{ÓkÔy Ë−ÎÏËÅ Ë−ÍÏÄÎy ÓˆÄÎˆÔx ÄÎˆ²ÅÎiiy Ë−ÍÏÄÎ { Ô k Óy Ë−ÎÏËÅ Ë−ÍÏÄÎy Û B. In most respects, the choice is arbitrary. But the original rule works better for the common case where there is no −ÄÍ− statement. For this case, we can simply modify the translation rule to be as follows: Î{ test-expry ©ð f_Îg ×ÇÎÇ ®ÇÅ−y then-statement ®ÇÅ−x A translation based on the alternate rule is more cumbersome. Solution to Problem 3.18 (page 249) This problem requires that you work through a nested branch structure, where you will see how our rule for translating ©ð statements has been applied. On the whole, the machine code is a straightforward translation of the C code. Í³ÇËÎ Î−ÍÎfÍ³ÇËÎ Ój Í³ÇËÎ Ôj Í³ÇËÎ Ög Õ Í³ÇËÎ ÌþÄ { ÖiÔkÓy ©ðfÖ|sgÕ ©ð fÔ | pg ÌþÄ { ÓmÖy −ÄÍ− ÌþÄ { ÓmÔy Û −ÄÍ− ©ð fÖ z qg ÌþÄ { ÖmÔy Ë−ÎÏËÅ ÌþÄy Û Solution to Problem 3.19 (page 252) This problem reinforces our method of computing the misprediction penalty. A. We can apply our formula directly to get TMP = 2(45 − 25) = 40. Solutions to Practice Problems 369 B. When misprediction occurs, the function will require around 25 + 40 = 65 cycles. Solution to Problem 3.20 (page 255) This problem provides a chance to study the use of conditional moves. A. The operator is ‘m’. We see this is an example of dividing by a power of 4 by right shifting (see Section 2.3.7). Before shifting by k = 4, we must add a bias of 2k − 1 = 15 when the dividend is negative. B. Here is an annotated version of the assembly code: short arith(short x) x in %rdi þË©Î³x Ä−þÊ osfcË®©gj cË¾Ó temp = x+15 Î−ÍÎÊ cË®©j cË®© Text x ²ÀÇÌÅÍ cË®©j cË¾Ó If x>= 0, temp = x ÍþËÊ brj cË¾Ó result = temp >> 4 (= x/16) Ë−Î The program creates a temporary value equal to x + 15, in anticipation of x being negative and therefore requiring biasing. The ²ÀÇÌÅÍ instruction conditionally changes this number to x when x ≥ 0, and then it is shifted by 4 to generate x/16. Solution to Problem 3.21 (page 255) This problem is similar to Problem 3.18, except that some of the conditionals have been implemented by conditional data transfers. Although it might seem daunting to ﬁt this code into the framework of the original C code, you will ﬁnd that it follows the translation rules fairly closely. Í³ÇËÎ Î−ÍÎfÍ³ÇËÎ Ój Í³ÇËÎ Ôg Õ Í³ÇËÎ ÌþÄ{Ôiopy ©ðfÓzngÕ ©ð fÓ z Ôg ÌþÄ{ÓhÔy −ÄÍ− ÌþÄ{ÓÚÔy Û −ÄÍ− ©ð fÔ | ong ÌþÄ{ÓmÔy Ë−ÎÏËÅ ÌþÄy Û Solution to Problem 3.22 (page 257) A. The computation of 14! would overﬂow with a 32-bit ©ÅÎ. As we learned in Problem 2.35, when we get value x while attempting to compute n!, we can test for overﬂow by computing x/n and seeing whether it equals (n − 1)! 370 Chapter 3 Machine-Level Representation of Programs (assuming that we have already ensured that the computation of (n − 1)! did not overﬂow). In this case we get 1,278,945,280/14 = 91353234.286. As a second test, we can see that any factorial beyond 10! must be a multiple of 100 and therefore have zeros for the last two digits. The correct value of 14! is 87,178,291,200. Further, we can build up a table of factorials computed through 14! with data type ©ÅÎ, as shown below: nn! OK? 11 Y 22 Y 36 Y 424 Y 5 120 Y 6 720 Y 7 5,040 Y 8 40,320 Y 9 362,880 Y 10 3,628,800 Y 11 39,916,800 Y 12 479,001,600 Y 13 1,932,053,504 N 14 1,278,945,280 N B. Doing the computation with data type ÄÇÅ× lets us go up to 20!, thus the 14! computation does not overﬂow. Solution to Problem 3.23 (page 258) The code generated when compiling loops can be tricky to analyze, because the compiler can perform many different optimizations on loop code, and because it can be difﬁcult to match program variables with registers. This particular example demonstrates several places where the assembly code is not just a direct translation of the C code. A. Although parameter Ó is passed to the function in register cË®©, we can see that the register is never referenced once the loop is entered. Instead, we can see that registers cË¾Ó, cË²Ó, and cË®Ó are initialized in lines 2–5 to Ó, Ómw, and rhÓ. We can conclude, therefore, that these registers contain the program variables. B. The compiler determines that pointer É always points to Ó, and hence the expression fhÉgi{s simply increments Ó. It combines this incrementing by 5 with the increment of Ô, via the Ä−þÊ instruction of line 7. C. The annotated code is as follows: Solutions to Practice Problems 371 short dw_loop(short x) x initially in %rdi 1 ®ÑˆÄÇÇÉx 2 ÀÇÌÊ cË®©j cË¾Ó Copy x to %rbx 3 ÀÇÌÊ cË®©j cË²Ó 4 ©®©ÌÊ bwj cË²Ó Computey=x/9 5 Ä−þÊ fjcË®©jrgj cË®Ó Computen=4*x 6 l‹px loop: 7 Ä−þÊ sfcË¾ÓjcË²Ógj cË²Ó Compute y +=x+5 8 ÍÏ¾Ê bpj cË®Ó Decrement n by 2 9 Î−ÍÎÊ cË®Ój cË®Ó Test n 10 Á× l‹p If > 0, goto loop 11 Ë−Éy Ë−Î Return Solution to Problem 3.24 (page 260) This assembly code is a fairly straightforward translation of the loop using the jump-to-middle method. The full C code is as follows: Í³ÇËÎ ÄÇÇÉˆÑ³©Ä−fÍ³ÇËÎ þj Í³ÇËÎ ¾g Õ Í³ÇËÎ Ë−ÍÏÄÎ { ny Ñ³©Ä− fþ | ¾g Õ Ë−ÍÏÄÎ { Ë−ÍÏÄÎ i fþh¾gy þ { þkoy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û Solution to Problem 3.25 (page 262) While the generated code does not follow the exact pattern of the guarded-do translation, we can see that it is equivalent to the following C code: ÄÇÅ× ÄÇÇÉˆÑ³©Ä−pfÄÇÅ× þj ÄÇÅ× ¾g Õ ÄÇÅ× Ë−ÍÏÄÎ { ¾y Ñ³©Ä− f¾ | ng Õ Ë−ÍÏÄÎ { Ë−ÍÏÄÎ h þy ¾ { ¾kþy Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û We will often see cases, especially when compiling with higher levels of opti- mization, where gcc takes some liberties in the exact form of the code it generates, while preserving the required functionality. 372 Chapter 3 Machine-Level Representation of Programs Solution to Problem 3.26 (page 264) Being able to work backward from assembly code to C code is a prime example of reverse engineering. A. We can see that the code uses the jump-to-middle translation, using the ÁÀÉ instruction on line 3. B. Here is the original C code: Í³ÇËÎ Î−ÍÎˆÇÅ−fÏÅÍ©×Å−® Í³ÇËÎ Óg Õ Í³ÇËÎ ÌþÄ { oy Ñ³©Ä− fÓg Õ ÌþÄ ´{ Óy Ó ||{ oy Û Ë−ÎÏËÅ ÌþÄ d ny Û C. This code computes the parity of argument Ó. That is, it returns 1 if there is an odd number of ones in Ó and 0 if there is an even number. Solution to Problem 3.27 (page 267) This exercise is intended to reinforce your understanding of how loops are imple- mented. ÄÇÅ× ð©¾ÇÅþ²²©ˆ×®ˆ×ÇÎÇfÄÇÅ× Åg Õ ÄÇÅ×©{py ÄÇÅ× Å−ÓÎj ð©ËÍÎ { nj Í−²ÇÅ® { oy ©ð fÅ z{ og ×ÇÎÇ ®ÇÅ−y ÄÇÇÉx Å−ÓÎ { ð©ËÍÎ i Í−²ÇÅ®y ð©ËÍÎ { Í−²ÇÅ®y Í−²ÇÅ® { Å−ÓÎy ©iiy ©ð f© z{ Åg ×ÇÎÇ ÄÇÇÉy ®ÇÅ−x Ë−ÎÏËÅ Åy Û Solution to Problem 3.28 (page 267) This problem is trickier than Problem 3.26, since the code within the loop is more complex and the overall operation is less familiar. A. Here is the original C code: ÄÇÅ× ðÏÅˆ¾fÏÅÍ©×Å−® ÄÇÅ× Óg Õ ÄÇÅ× ÌþÄ { ny ÄÇÅ× ©y Solutions to Practice Problems 373 ðÇË f© { try © _{ ny ©kkg Õ ÌþÄ { fÌþÄ zz og Ú fÓ d nÓogy Ó ||{ oy Û Ë−ÎÏËÅ ÌþÄy Û B. The code was generated using the guarded-do transformation, but the com- piler detected that, since i is initialized to 64, it will satisfy the test i ̸= 0, and therefore the initial test is not required. C. This code reverses the bits in Ó, creating a mirror image. It does this by shifting the bits of Ó from left to right, and then ﬁlling these bits in as it shifts ÌþÄ from right to left. Solution to Problem 3.29 (page 268) Our stated rule for translating a ðÇË loop into a Ñ³©Ä− loop is just a bit too simplistic—this is the only aspect that requires special consideration. A. Applying our translation rule would yield the following code: mh ﬁþ©Ì− ÎËþÅÍÄþÎ©ÇÅ Çð ðÇË ÄÇÇÉ ©ÅÎÇ Ñ³©Ä− ÄÇÇÉ hm mh „¡‡ﬁ'ﬁ§x ¶³©Í ©Í ¾Ï××Ô ²Ç®− hm ÄÇÅ× ÍÏÀ { ny ÄÇÅ×©{ny Ñ³©Ä− f© z ong Õ ©ð f© d og mh ¶³©Í Ñ©ÄÄ ²þÏÍ− þÅ ©Åð©Å©Î− ÄÇÇÉ hm ²ÇÅÎ©ÅÏ−y ÍÏÀ i{ ©y ©iiy Û This code has an inﬁnite loop, since the ²ÇÅÎ©ÅÏ− statement would prevent index variable © from being updated. B. The general solution is to replace the ²ÇÅÎ©ÅÏ− statement with a ×ÇÎÇ statement that skips the rest of the loop body and goes directly to the update portion: mh £ÇËË−²Î ÎËþÅÍÄþÎ©ÇÅ Çð ðÇË ÄÇÇÉ ©ÅÎÇ Ñ³©Ä− ÄÇÇÉ hm ÄÇÅ× ÍÏÀ { ny ÄÇÅ×©{ny Ñ³©Ä− f© z ong Õ ©ð f© d og ×ÇÎÇ ÏÉ®þÎ−y ÍÏÀ i{ ©y ÏÉ®þÎ−x ©iiy Û 374 Chapter 3 Machine-Level Representation of Programs Solution to Problem 3.30 (page 272) This problem gives you a chance to reason about the control ﬂow of a ÍÑ©Î²³ statement. Answering the questions requires you to combine information from several places in the assembly code. . Line 2 of the assembly code adds 2 to x to set the lower range of the cases to zero. That means that the minimum case label is −2. . Lines 3 and 4 cause the program to jump to the default case when the adjusted case value is greater than 8. This implies that the maximum case label is −2 + 8 = 6. . In the jump table, we see that the entry on lines 6 (case value 2) and 9 (case value 5) have the same destination (l‹p) as the jump instruction on line 4, indicating the default case behavior. Thus, case labels 2 and 5 are missing in the ÍÑ©Î²³ statement body. . In the jump table, we see that the entries on lines 3 and 10 have the same destination. These correspond to cases −1 and 6. . In the jump table, we see that the entries on lines 5 and 7 have the same destination. These correspond to cases 1 and 3. From this reasoning, we draw the following conclusions: A. The case labels in the ÍÑ©Î²³ statement body have values −2, −1, 0, 1, 3, 4, and 6. B. The case with destination l‹s has labels −1 and 6. C. The case with destination l‹u has labels 1 and 3. Solution to Problem 3.31 (page 273) The key to reverse engineering compiled ÍÑ©Î²³ statements is to combine the information from the assembly code and the jump table to sort out the different cases. We can see from the Áþ instruction (line 3) that the code for the default case has label l‹p. We can see that the only other repeated label in the jump table is l‹s, and so this must be the code for the cases C and D. We can see that the code falls through at line 8, and so label l‹u must match case A and label l‹q must match case B. That leaves only label l‹t to match case E. The original C code is as follows: ÌÇ©® ÍÑ©Î²³−ËfÄÇÅ× þj ÄÇÅ× ¾j ÄÇÅ× ²j ÄÇÅ× h®−ÍÎg Õ ÄÇÅ× ÌþÄy ÍÑ©Î²³fþg Õ ²þÍ− sx ²{¾´osy mh ƒþÄÄ Î³ËÇÏ×³ hm ²þÍ− nx ÌþÄ{²i oopy ¾Ë−þÂy Solutions to Practice Problems 375 ²þÍ− px ²þÍ− ux ÌþÄ{f²i¾gzzpy ¾Ë−þÂy ²þÍ− rx ÌþÄ{þy ¾Ë−þÂy ®−ðþÏÄÎx ÌþÄ{¾y Û h®−ÍÎ { ÌþÄy Û Solution to Problem 3.32 (page 280) Tracing through the program execution at this level of detail reinforces many aspects of procedure call and return. We can see clearly how control is passed to the function when it is called, and how the calling function resumes upon return. We can also see how arguments get passed through registers cË®© and cËÍ©, and how results are returned via register cËþÓ. Instruction State values (at beginning) Label PC Instruction cË®© cËÍ© cËþÓ cËÍÉ hcËÍÉ Description M1 nÓrnnstn ²þÄÄÊ 10—— nÓuððððððð−vpn — Call ð©ËÍÎfong F1 nÓrnnsrv Ä−þ 10—— nÓuððððððð−vov nÓrnnsts Entry of ð©ËÍÎ F2 nÓrnnsr² ÍÏ¾ 10 11 — nÓuððððððð−vov nÓrnnsts F3 nÓrnnssn ²þÄÄÊ 911 — nÓuððððððð−vov nÓrnnsts Call ÄþÍÎfwj oog L1 nÓrnnsrn ÀÇÌ 911 — nÓuððððððð−von nÓrnnsss Entry of ÄþÍÎ L2 nÓrnnsrq ©ÀÏÄ 9119 nÓuððððððð−von nÓrnnsss L3 nÓrnnsru Ë−ÎÊ 911 99 nÓuððððððð−von nÓrnnsss Return 99 from ÄþÍÎ F4 nÓrnnsss Ë−ÉÖ Ë−ÉÊ 911 99 nÓuððððððð−vov nÓrnnsts Return 99 from ð©ËÍÎ M2 nÓrnnsts ÀÇÌ 911 99 nÓuððððððð−vpn — Resume Àþ©Å Solution to Problem 3.33 (page 282) This problem is a bit tricky due to the mixing of different data sizes. Let us ﬁrst describe one answer and then explain the second possibility. If we assume the ﬁrst addition (line 3) implements hÏ i{ þ, while the second (line 4) implements Ìi{¾, then we can see that þ was passed as the ﬁrst argument in c−®© and converted from 4 bytes to 8 before adding it to the 8 bytes pointed to by cË®Ó. This implies that þ must be of type ©ÅÎ and Ï must be of type ÄÇÅ× h. We can also see that the low-order byte of argument ¾ is added to the byte pointed to by cË²Ó. This implies that Ì must be of type ²³þË h, but the type of ¾ is ambiguous—it could be 1, 2, 4, or 8 bytes long. This ambiguity is resolved by noting the return value of 376 Chapter 3 Machine-Level Representation of Programs 6, computed as the sum of the sizes of þ and ¾. Since we know þ is 4 bytes long, we can deduce that ¾ must be 2. An annotated version of this function explains these details: int procprobl(int a, short b, long *u, char *v) a in %edi, b in %si, u in %rdx, v in %rcx 1 ÉËÇ²ÉËÇ¾x 2 ÀÇÌÍÄÊ c−®©j cË®© Convert a to long 3 þ®®Ê cË®©j fcË®Óg Add to *u (long) 4 þ®®¾ cÍ©Äj fcË²Óg Add low-order byte of b to *v 5 ÀÇÌÄ btj c−þÓ Return 4+2 6 Ë−Î Alternatively, we can see that the same assembly code would be valid if the two sums were computed in the assembly code in the opposite ordering as they are in the C code. This would result in interchanging arguments þ and ¾ and arguments Ï and Ì, yielding the following prototype: ©ÅÎ ÉËÇ²ÉËÇ¾f©ÅÎ ¾j Í³ÇËÎ þj ÄÇÅ× hÌj ²³þË hÏgy Solution to Problem 3.34 (page 288) This example demonstrates the use of callee-saved registers as well as the stack for holding local data. A. We can see that lines 9–14 save local values þn–þs into callee-saved registers cË¾Ó, cËos, cËor, cËoq, cËop, and cË¾É, respectively. B. Local values þt and þu are stored on the stack at offsets 0 and 8 relative to the stack pointer (lines 16 and 18). C. After storing six local variables, the program has used up the supply of callee- saved registers. It stores the remaining two local values on the stack. Solution to Problem 3.35 (page 290) This problem provides a chance to examine the code for a recursive function. An important lesson to learn is that recursive code has the exact same structure as the other functions we have seen. The stack and register-saving disciplines sufﬁce to make recursive functions operate correctly. A. Register cË¾Ó holds the value of parameter Ó, so that it can be used to compute the result expression. B. The assembly code was generated from the following C code: ÄÇÅ× ËðÏÅfÏÅÍ©×Å−® ÄÇÅ× Óg Õ ©ð fÓ {{ ng Ë−ÎÏËÅ ny ÏÅÍ©×Å−® ÄÇÅ× ÅÓ { Ó||py ÄÇÅ× ËÌ { ËðÏÅfÅÓgy Ë−ÎÏËÅÓiËÌy Û Solutions to Practice Problems 377 Solution to Problem 3.36 (page 292) This exercise tests your understanding of data sizes and array indexing. Observe that a pointer of any kind is 8 bytes long. Data type Í³ÇËÎ requires 2 bytes, while ©ÅÎ requires 4. Array Element size Total size Start address Element i – 420 x– x– + 4i † 24 x† x† + 2i ‡ 872 x‡ x‡ + 8i · 880 x· x· + 8i ¶ 816 x¶ x¶ + 8i Solution to Problem 3.37 (page 294) This problem is a variant of the one shown for integer array ¥. It is important to understand the difference between a pointer and the object being pointed to. Since data type Í³ÇËÎ requires 2 bytes, all of the array indices are scaled by a factor of 2. Rather than using ÀÇÌÄ, as before, we now use ÀÇÌÑ. Expression Type Value Assembly Code –‰o` Í³ÇËÎ M[x– + 2] ÀÇÌÑ pfcË®ÓgjcþÓ –iqi© Í³ÇËÎ h x– + 6 + 2i Ä−þÊ tfcË®ÓjcË²ÓjpgjcËþÓ –‰©htks` Í³ÇËÎ M[x– + 12i − 10] ÀÇÌÑ konfcË®ÓjcË²ÓjopgjcþÓ –‰p` Í³ÇËÎ M[x– + 4] ÀÇÌÑ rfcË®ÓgjcþÓ d–‰©ip` Í³ÇËÎ h x– + 2i + 4 Ä−þÊ rfcË®ÓjcË²ÓjpgjcËþÓ Solution to Problem 3.38 (page 295) This problem requires you to work through the scaling operations to determine the address computations, and to apply Equation 3.1 for row-major indexing. The ﬁrst step is to annotate the assembly code to determine how the address references are computed: long sum_element(long i, long j) i in %rdi, j in %rsi 1 ÍÏÀˆ−Ä−À−ÅÎx 2 Ä−þÊ nfjcË®©jvgj cË®Ó Compute 8i 3 ÍÏ¾Ê cË®©j cË®Ó Compute 7i 4 þ®®Ê cËÍ©j cË®Ó Compute 7i + j 5 Ä−þÊ fcËÍ©jcËÍ©jrgj cËþÓ Compute 5j 6 þ®®Ê cËþÓj cË®© Compute i + 5j 7 ÀÇÌÊ †fjcË®©jvgj cËþÓ Retrieve M[x† + 8 (5j + i)] 8 þ®®Ê –fjcË®Ójvgj cËþÓ Add M[x– + 8 (7i + j)] 9 Ë−Î We can see that the reference to matrix – is at byte offset 8 . (7i + j), while the reference to matrix † is at byte offset 8 . (5j + i). From this, we can determine that – has 7 columns, while † has 5, giving M = 5 and N = 7. 378 Chapter 3 Machine-Level Representation of Programs Solution to Problem 3.39 (page 298) These computations are direct applications of Equation 3.1: . For L = 4, C = 16, and j = 0, pointer ¡ÉÎË is computed as x¡ + 4 . (16i + 0) = x¡ + 64i. . For L = 4, C = 16, i = 0, and j = k, ¢ÉÎË is computed as x¢ + 4 . (16 . 0 + k) = x¢ + 4k. . For L = 4, C = 16, i = 16, and j = k, ¢−Å® is computed as x¢ + 4 . (16 . 16 + k) = x¢ + 1,024 + 4k. Solution to Problem 3.40 (page 298) This exercise requires that you be able to study compiler-generated assembly code to understand what optimizations have been performed. In this case, the compiler was clever in its optimizations. Let us ﬁrst study the following C code, and then see how it is derived from the assembly code generated for the original function. mh ·−Î þÄÄ ®©þ×ÇÅþÄ −Ä−À−ÅÎÍ ÎÇ ÌþÄ hm ÌÇ©® ð©ÓˆÍ−Îˆ®©þ×ˆÇÉÎfð©ÓˆÀþÎË©Ó ¡j ©ÅÎ ÌþÄg Õ ©ÅÎ h¡¾þÍ− { d¡‰n`‰n`y ÄÇÅ×©{ny ÄÇÅ× ©−Å® { ﬁhfﬁiogy ®Ç Õ ¡¾þÍ−‰©` { ÌþÄy © i{ fﬁiogy Û Ñ³©Ä− f© _{ ©−Å®gy Û This function introduces a variable ¡¾þÍ−, of type ©ÅÎ h, pointing to the start of array ¡. This pointer designates a sequence of 4-byte integers consisting of elements of ¡ in row-major order. We introduce an integer variable ©Å®−Ó that steps through the diagonal elements of ¡, with the property that diagonal elements i and i + 1 are spaced N + 1 elements apart in the sequence, and that once we reach diagonal element N (index value N(N + 1)), we have gone beyond the end. The actual assembly code follows this general form, but now the pointer increments must be scaled by a factor of 4. We label register cËþÓ as holding a value ©Å®−Ór equal to ©Å®−Ó in our C version but scaled by a factor of 4. For N = 16, we can see that our stopping point for ©Å®−Ór will be 4 . 16(16 + 1) = 1,088. 1 ð©ÓˆÍ−Îˆ®©þ×x void fix_set_diag(fix_matrix A, int val) A in %rdi, val in %rsi 2 ÀÇÌÄ bnj c−þÓ Set index4 = 0 3 l‹oqx loop: 4 ÀÇÌÄ c−Í©j fcË®©jcËþÓg Set Abase[index4/4] to val 5 þ®®Ê btvj cËþÓ Increment index4 += 4(N+1) Solutions to Practice Problems 379 6 ²ÀÉÊ bonvvj cËþÓ Compare index4: 4N(N+1) 7 ÁÅ− l‹oq If !=, goto loop 8 Ë−Éy Ë−Î Return Solution to Problem 3.41 (page 304) This problem gets you to think about structure layout and the code used to access structure ﬁelds. The structure declaration is a variant of the example shown in the text. It shows that nested structures are allocated by embedding the inner structures within the outer ones. A. The layout of the structure is as follows: Offset Contents p 010812 20 s.x s.y next B. It uses 20 bytes. C. As always, we start by annotating the assembly code: void st_init(struct test *st) st in %rdi 1 ÍÎˆ©Å©Îx 2 ÀÇÌÄ vfcË®©gj c−þÓ Get st->s.x 3 ÀÇÌÄ c−þÓj onfcË®©g Save in st->s.y 4 Ä−þÊ onfcË®©gj cËþÓ Compute &(st->s.y) 5 ÀÇÌÊ cËþÓj fcË®©g Store in st->p 6 ÀÇÌÊ cË®©j opfcË®©g Store st in st->next 7 Ë−Î From this, we can generate C code as follows: ÌÇ©® ÍÎˆ©Å©ÎfÍÎËÏ²Î Î−ÍÎ hÍÎg Õ ÍÎk|ÍlÔ { ÍÎk|ÍlÓy ÍÎk|É { dfÍÎk|ÍlÔgy ÍÎk|Å−ÓÎ { ÍÎy Û Solution to Problem 3.42 (page 305) This problem demonstrates how a very common data structure and operation on it is implemented in machine code. We solve the problem by ﬁrst annotating the assembly code, recognizing that the two ﬁelds of the structure are at offsets 0 (for Ì) and 2 (for É). short test(struct ACE *ptr) ptr in %rdi 1 Î−ÍÎx 2 ÀÇÌÄ boj c−þÓ result = 1 3 ÁÀÉ l‹p Goto middle 380 Chapter 3 Machine-Level Representation of Programs 4 l‹qx loop: 5 ©ÀÏÄÊ fcË®©gj cËþÓ result *= ptr->v 6 ÀÇÌÊ pfcË®©gj cË®© ptr = ptr->p 7 l‹px middle: 8 Î−ÍÎÊ cË®©j cË®© Test ptr 9 ÁÅ− l‹q If != NULL, goto loop 10 Ë−Éy Ë−Î A. Based on the annotated code, we can generate a C version: Í³ÇËÎ Î−ÍÎfÍÎËÏ²Î ¡£¥ hÉÎËg Õ Í³ÇËÎ ÌþÄ { oy Ñ³©Ä− fÉÎËg Õ ÌþÄ h{ ÉÎËk|Ìy ÉÎË { ÉÎËk|Éy Û Ë−ÎÏËÅ ÌþÄy Û B. We can see that each structure is an element in a singly linked list, with ﬁeld Ì being the value of the element and É being a pointer to the next element. Function ðÏÅ computes the sum of the element values in the list. Solution to Problem 3.43 (page 308) Structures and unions involve a simple set of concepts, but it takes practice to be comfortable with the different referencing patterns and their implementations. ¥”–‡ ¶»–¥ Code ÏÉk|ÎolÏ ÄÇÅ× ÀÇÌÊ fcË®©gj cËþÓ ÀÇÌÊ cËþÓj fcËÍ©g ÏÉk|ÎolÌ Í³ÇËÎ ÀÇÌÑ vfcË®©gj cþÓ ÀÇÌÑ cþÓj fcËÍ©g dÏÉk|ÎolÑ ²³þË h þ®®Ê bonj cË®© ÀÇÌÊ cË®©j fcËÍ©g ÏÉk|Îplþ ©ÅÎ h ÀÇÌÊ cË®©j fcËÍ©g ÏÉk|Îplþ‰ÏÉk|ÎolÏ` ©ÅÎ ÀÇÌÊ fcË®©gj cËþÓ ÀÇÌÄ fcË®©jcËþÓjrgj c−þÓ ÀÇÌÄ c−þÓj fcËÍ©g hÏÉk|ÎplÉ ²³þË ÀÇÌÊ vfcË®©gj cËþÓ ÀÇÌ¾ fcËþÓgj cþÄ ÀÇÌ¾ cþÄj fcËÍ©g Solutions to Practice Problems 381 Solution to Problem 3.44 (page 311) Understanding structure layout and alignment is very important for understand- ing how much storage different data structures require and for understanding the code generated by the compiler for accessing structures. This problem lets you work out the details of some example structures. A. ÍÎËÏ²Î –o Õ Í³ÇËÎ ©y ©ÅÎ ²y ©ÅÎ hÁy Í³ÇËÎ h®y Ûy ©²Á ® Total Alignment 0 2 6 14 16 8 B. ÍÎËÏ²Î –p Õ ©ÅÎ ©‰p`y ²³þË ²‰v`y Í³ÇËÎ ‰r`y ÄÇÅ× hÁy Ûy ©² ® Á Total Alignment 0 8 16 24 32 8 C. ÍÎËÏ²Î –q Õ ÄÇÅ× Ñ‰p`y ©ÅÎ h²‰p` Ûy Ñ² Total Alignment 016 32 8 D. ÍÎËÏ²Î –r Õ ²³þË Ñ‰ot`y ²³þË h²‰p` Ûy Ñ² Total Alignment 016 32 8 E. ÍÎËÏ²Î –s Õ ÍÎËÏ²Î –r þ‰p`y ÍÎËÏ²Î –o Î Ûy þÎ Total Alignment 024 40 8 Solution to Problem 3.45 (page 311) This is an exercise in understanding structure layout and alignment. A. Here are the object sizes and byte offsets: Field þ ¾ ²®−ð×³ Size 8 4 128848 Offset 0 8 12 16 24 32 40 48 B. The structure is a total of 56 bytes long. The end of the structure does not require padding to satisfy the 8-byte alignment requirement. C. One strategy that works, when all data elements have a length equal to a power of 2, is to order the structure elements in descending order of size. This leads to a declaration: 382 Chapter 3 Machine-Level Representation of Programs ÍÎËÏ²Î Õ ©ÅÎ hþy ²³þË h³y ®ÇÏ¾Ä− ðy ÄÇÅ× −y ðÄÇþÎ ¾y ©ÅÎ ×y Í³ÇËÎ ®y ²³þË ²y Û Ë−²y with the following offsets: Field þ ³ ð−¾×®² Size 8 8 884421 Offset 0 8 16 24 32 36 40 42 The structure must be padded by 5 bytes to satisfy the 8-byte alignment requirement, giving a total of 48 bytes. Solution to Problem 3.46 (page 318) This problem covers a wide range of topics, such as stack frames, string represen- tations, ASCII code, and byte ordering. It demonstrates the dangers of out-of- bounds memory references and the basic ideas behind buffer overﬂow. A. Stack after line 3: 00 00 00 00 00 40 00 76 01 23 45 67 89 AB CD EF buf = %rsp Return address Saved %rbx B. Stack after line 5: 00 00 00 00 00 40 00 34 33 32 31 30 39 38 37 36 35 34 33 32 31 30 39 38 37 36 35 34 33 32 31 30 buf = %rsp Return address Saved %rbx C. The program is attempting to return to address nÓnrnnqr. The low-order 2 bytes were overwritten by the code for character ‘r’ and the terminating null character. D. The saved value of register cË¾Ó was set to nÓqqqpqoqnqwqvquqt. This value will be loaded into the register before ×−ÎˆÄ©Å− returns. Solutions to Practice Problems 383 E. The call to ÀþÄÄÇ² should have had ÍÎËÄ−Åf¾Ïðgio as its argument, and the code should also check that the returned value is not equal to ﬁ•‹‹. Solution to Problem 3.47 (page 322) A. This corresponds to a range of around 213 addresses. B. A 128-byte nop sled would cover 27 addresses with each test, and so we would only require around 26 = 64 attempts. This example clearly shows that the degree of randomization in this version of Linux would provide only minimal deterrence against an overﬂow attack. Solution to Problem 3.48 (page 324) This problem gives you another chance to see how x86-64 code manages the stack, and to also better understand how to defend against buffer overﬂow attacks. A. For the unprotected code, we can see that lines 4 and 5 compute the positions of Ì and ¾Ïð to be at offsets 24 and 0 relative to cËÍÉ. In the protected code, the canary is stored at offset 40 (line 4), while Ì and ¾Ïð are at offsets 8 and 16 (lines 7 and 8). B. In the protected code, local variable Ì is positioned closer to the top of the stack than ¾Ïð, and so an overrun of ¾Ïð will not corrupt the value of Ì. Solution to Problem 3.49 (page 329) This code combines many of the tricks we have seen for performing bit-level arithmetic. It requires careful study to make any sense of it. A. The Ä−þÊ instruction of line 5 computes the value 8n + 22, which is then rounded down to the nearest multiple of 16 by the þÅ®Ê instruction of line 6. The resulting value will be 8n + 8 when n is odd and 8n + 16 when n is even, and this value is subtracted from s1 to give s2. B. The three instructions in this sequence round s2 up to the nearest multiple of 8. They make use of the combination of biasing and shifting that we saw for dividing by a power of 2 in Section 2.3.7. C. These two examples can be seen as the cases that minimize and maximize the values of e1 and e2. ns1 s2 pe1 e2 5 2,065 2,017 2,024 1 7 6 2,064 2,000 2,000 16 0 D. We can see that s2 is computed in a way that preserves whatever offset s1 has with the nearest multiple of 16. We can also see that p will be aligned on a multiple of 8, as is recommended for an array of 8-byte elements. Solution to Problem 3.50 (page 336) This exercise requires that you step through the code, paying careful attention to which conversion and data movement instructions are used. We can see the values being retrieved and converted as follows: 384 Chapter 3 Machine-Level Representation of Programs . The value at ®É is retrieved, converted to an ©ÅÎ (line 4), and then stored at ©É. We can therefore infer that ÌþÄo is ®. . The value at ©É is retrieved, converted to a ðÄÇþÎ (line 6), and then stored at ðÉ. We can therefore infer that ÌþÄp is ©. . The value of Ä is converted to a ®ÇÏ¾Ä− (line 8) and stored at ®É.Wecan therefore infer that ÌþÄq is Ä. . The value at ðÉ is retrieved on line 3. The two instructions at lines 10–11 convert this to double precision as the value returned in register cÓÀÀn.We can therefore infer that ÌþÄr is ð. Solution to Problem 3.51 (page 336) These cases can be handled by selecting the appropriate entries from the tables in Figures 3.47 and 3.48, or using one of the code sequences for converting between ﬂoating-point formats. TÓ TÔ Instruction(s) ÄÇÅ× ®ÇÏ¾Ä− Ì²ÌÎÍ©pÍ®Ê cË®©j cÓÀÀnj cÓÀÀn ®ÇÏ¾Ä− ©ÅÎ Ì²ÌÎÎÍ®pÍ© cÓÀÀnj c−þÓ ðÄÇþÎ ®ÇÏ¾Ä− ÌÏÅÉ²ÂÄÉ® cÓÀÀnj cÓÀÀnj cÓÀÀn Ì²ÌÎÉ®pÉÍ cÓÀÀnj cÓÀÀn ÄÇÅ× ðÄÇþÎ Ì²ÌÎÍ©pÍÍÊ cË®©j cÓÀÀnj cÓÀÀn ðÄÇþÎ ÄÇÅ× Ì²ÌÎÎÍÍpÍ©Ê cÓÀÀnj cËþÓ Solution to Problem 3.52 (page 337) The basic rules for mapping arguments to registers are fairly simple (although they become much more complex with more and other types of arguments [77]). A. ®ÇÏ¾Ä− ×of®ÇÏ¾Ä− þj ÄÇÅ× ¾j ðÄÇþÎ ²j ©ÅÎ ®gy Registers: þ in cÓÀÀn, ¾ in cË®© ² in cÓÀÀo, ® in c−Í© B. ®ÇÏ¾Ä− ×pf©ÅÎ þj ®ÇÏ¾Ä− h¾j ðÄÇþÎ h²j ÄÇÅ× ®gy Registers: þ in c−®©, ¾ in cËÍ©, ² in cË®Ó, ® in cË²Ó C. ®ÇÏ¾Ä− ×qf®ÇÏ¾Ä− hþj ®ÇÏ¾Ä− ¾j ©ÅÎ ²j ðÄÇþÎ ®gy Registers: þ in cË®©, ¾ in cÓÀÀn, ² in c−Í©, ® in cÓÀÀo D. ®ÇÏ¾Ä− ×rfðÄÇþÎ þj ©ÅÎ h¾j ðÄÇþÎ ²j ®ÇÏ¾Ä− ®gy Registers: þ in cÓÀÀn, ¾ in cË®©, ² in cÓÀÀo, ® in cÓÀÀp Solution to Problem 3.53 (page 339) We can see from the assembly code that there are two integer arguments, passed in registers cË®© and cËÍ©. Let us name these ©o and ©p. Similarly, there are two ﬂoating-point arguments, passed in registers cÓÀÀn and cÓÀÀo, which we name ðo and ðp. We can then annotate the assembly code: Solutions to Practice Problems 385 Refer to arguments as i1 (%rdi), i2 (%esi) f1 (%xmm0), and f2 (%xmm1) double funct1(arg1_t p, arg2_t q, arg3_t r, arg4_t s) 1 ðÏÅ²Îox 2 Ì²ÌÎÍ©pÍÍÊ cËÍ©j cÓÀÀpj cÓÀÀp Get i2 and convert from long to float 3 Ìþ®®ÍÍ cÓÀÀnj cÓÀÀpj cÓÀÀn Add f1 (type float) 4 Ì²ÌÎÍ©pÍÍ c−®©j cÓÀÀpj cÓÀÀp Get i1 and convert from int to float 5 Ì®©ÌÍÍ cÓÀÀnj cÓÀÀpj cÓÀÀn Compute i1 / (i2 + f1) 6 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn 7 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn Convert to double 8 ÌÍÏ¾Í® cÓÀÀoj cÓÀÀnj cÓÀÀn Compute i1 / (i2 + f1) - f2 (double) 9 Ë−Î From this we see that the code computes the value ©omf©piðogkðp. We can also see that ©o has type ©ÅÎ, ©p has type ÄÇÅ×, ðo has type ðÄÇþÎ, and ðp has type ®ÇÏ¾Ä−. The only ambiguity in matching arguments to the named values stems from the commutativity of multiplication—yielding two possible results: ®ÇÏ¾Ä− ðÏÅ²Îoþf©ÅÎ Éj ðÄÇþÎ Êj ÄÇÅ× Ëj ®ÇÏ¾Ä− Így ®ÇÏ¾Ä− ðÏÅ²Îo¾f©ÅÎ Éj ÄÇÅ× Êj ðÄÇþÎ Ëj ®ÇÏ¾Ä− Így Solution to Problem 3.54 (page 339) This problem can readily be solved by stepping through the assembly code and determining what is computed on each step, as shown with the annotations below: double funct2(double w, int x, float y, long z) w in %xmm0, x in %edi, y in %xmm1, z in %rsi 1 ðÏÅ²Îpx 2 Ì²ÌÎÍ©pÍÍ c−®©j cÓÀÀpj cÓÀÀp Convert x to float 3 ÌÀÏÄÍÍ cÓÀÀoj cÓÀÀpj cÓÀÀo Multiply by y 4 ÌÏÅÉ²ÂÄÉÍ cÓÀÀoj cÓÀÀoj cÓÀÀo 5 Ì²ÌÎÉÍpÉ® cÓÀÀoj cÓÀÀp Convert x*y to double 6 Ì²ÌÎÍ©pÍ®Ê cËÍ©j cÓÀÀoj cÓÀÀo Convert z to double 7 Ì®©ÌÍ® cÓÀÀoj cÓÀÀnj cÓÀÀn Compute w/z 8 ÌÍÏ¾Í® cÓÀÀnj cÓÀÀpj cÓÀÀn Subtract from x*y 9 Ë−Î Return We can conclude from this analysis that the function computes Ô ∗ Ó − Ñ/Ö. Solution to Problem 3.55 (page 341) This problem involves the same reasoning as was required to see that numbers declared at label l‹£p encode 1.8, but with a simpler example. We see that the two values are 0 and 1077936128 (nÓrnrnnnnn). From the high-order bytes, we can extract an exponent ﬁeld of nÓrnr (1028), from which we subtract a bias of 1023 to get an exponent of 5. Concatenating the fraction bits of the two values, we get a fraction ﬁeld of 0, but with the implied leading value giving value 1.0. The constant is therefore 1.0 × 25 = 32.0. 386 Chapter 3 Machine-Level Representation of Programs Solution to Problem 3.56 (page 341) A. We see here that the 16 bytes starting at address l‹£o form a mask, where the low-order 8 bytes contain all ones, except for the most signiﬁcant bit, which is the sign bit of a double-precision value. When we compute the and of this mask with cÓÀÀn, it will clear the sign bit of Ó, yielding the absolute value. In fact, we generated this code by deﬁning ¥”–‡fÓg to be ðþ¾ÍfÓg, where ðþ¾Í is deﬁned in zÀþÎ³l³|. B. We see that the ÌÓÇËÉ® instruction sets the entire register to zero, and so this is a way to generate ﬂoating-point constant 0.0. C. We see that the 16 bytes starting at address l‹£p form a mask with a single 1 bit, at the position of the sign bit for the low-order value in the XMM register. When we compute the exclusive-or of this mask with cÓÀÀn,we change the sign of Ó, computing the expression kÓ. Solution to Problem 3.57 (page 344) Again, we annotate the code, including dealing with the conditional branch: double funct3(int *ap, double b, long c, float *dp) ap in %rdi, b in %xmm0, c in %rsi, dp in %rdx 1 ðÏÅ²Îqx 2 ÌÀÇÌÍÍ fcË®Ógj cÓÀÀo Get d = *dp 3 Ì²ÌÎÍ©pÍ® fcË®©gj cÓÀÀpj cÓÀÀp Get a = *ap and convert to double 4 ÌÏ²ÇÀ©Í® cÓÀÀpj cÓÀÀn Compare b:a 5 Á¾− l‹v If <=, goto lesseq 6 Ì²ÌÎÍ©pÍÍÊ cËÍ©j cÓÀÀnj cÓÀÀn Convert c to float 7 ÌÀÏÄÍÍ cÓÀÀoj cÓÀÀnj cÓÀÀo Multiply by d 8 ÌÏÅÉ²ÂÄÉÍ cÓÀÀoj cÓÀÀoj cÓÀÀo 9 Ì²ÌÎÉÍpÉ® cÓÀÀoj cÓÀÀn Convert to double 10 Ë−Î Return 11 l‹vx lesseq: 12 Ìþ®®ÍÍ cÓÀÀoj cÓÀÀoj cÓÀÀo Compute d+d = 2.0 * d 13 Ì²ÌÎÍ©pÍÍÊ cËÍ©j cÓÀÀnj cÓÀÀn Convert c to float 14 Ìþ®®ÍÍ cÓÀÀoj cÓÀÀnj cÓÀÀn Computec+2*d 15 ÌÏÅÉ²ÂÄÉÍ cÓÀÀnj cÓÀÀnj cÓÀÀn 16 Ì²ÌÎÉÍpÉ® cÓÀÀnj cÓÀÀn Convert to double 17 Ë−Î Return From this, we can write the following code for ðÏÅ²Îq: ®ÇÏ¾Ä− ðÏÅ²Îqf©ÅÎ hþÉj ®ÇÏ¾Ä− ¾j ÄÇÅ× ²j ðÄÇþÎ h®Ég Õ ©ÅÎ þ { hþÉy ðÄÇþÎ ® { h®Éy ©ð fþ z ¾g Ë−ÎÏËÅ ²h®y −ÄÍ− Ë−ÎÏËÅ ²iph®y Û CHAPTER 4 Processor Architecture 4.1 The Y86-64 Instruction Set Architecture 391 4.2 Logic Design and the Hardware Control Language HCL 408 4.3 Sequential Y86-64 Implementations 420 4.4 General Principles of Pipelining 448 4.5 Pipelined Y86-64 Implementations 457 4.6 Summary 506 Bibliographic Notes 509 Homework Problems 509 Solutions to Practice Problems 516 387 388 Chapter 4 Processor Architecture M odern microprocessors are among the most complex systems ever created by humans. A single silicon chip, roughly the size of a ﬁngernail, can con- tain several high-performance processors, large cache memories, and the logic required to interface them to external devices. In terms of performance, the pro- cessors implemented on a single chip today dwarf the room-size supercomputers that cost over $10 million just 20 years ago. Even the embedded processors found in everyday appliances such as cell phones, navigation systems, and programmable thermostats are far more powerful than the early developers of computers could ever have envisioned. So far, we have only viewed computer systems down to the level of machine- language programs. We have seen that a processor must execute a sequence of instructions, where each instruction performs some primitive operation, such as adding two numbers. An instruction is encoded in binary form as a sequence of 1 or more bytes. The instructions supported by a particular processor and their byte-level encodings are known as its instruction set architecture (ISA). Different “families” of processors, such as Intel IA32 and x86-64, IBM/Freescale Power, and the ARM processor family, have different ISAs. A program compiled for one type of machine will not run on another. On the other hand, there are many dif- ferent models of processors within a single family. Each manufacturer produces processors of ever-growing performance and complexity, but the different models remain compatible at the ISA level. Popular families, such as x86-64, have pro- cessors supplied by multiple manufacturers. Thus, the ISA provides a conceptual layer of abstraction between compiler writers, who need only know what instruc- tions are permitted and how they are encoded, and processor designers, who must build machines that execute those instructions. In this chapter, we take a brief look at the design of processor hardware. We study the way a hardware system can execute the instructions of a particular ISA. This view will give you a better understanding of how computers work and the technological challenges faced by computer manufacturers. One important con- cept is that the actual way a modern processor operates can be quite different from the model of computation implied by the ISA. The ISA model would seem to imply sequential instruction execution, where each instruction is fetched and executed to completion before the next one begins. By executing different parts of multiple instructions simultaneously, the processor can achieve higher perfor- mance than if it executed just one instruction at a time. Special mechanisms are used to make sure the processor computes the same results as it would with se- quential execution. This idea of using clever tricks to improve performance while maintaining the functionality of a simpler and more abstract model is well known in computer science. Examples include the use of caching in Web browsers and information retrieval data structures such as balanced binary trees and hash tables. Chances are you will never design your own processor. This is a task for experts working at fewer than 100 companies worldwide. Why, then, should you learn about processor design? . It is intellectually interesting and important. There is an intrinsic value in learn- ing how things work. It is especially interesting to learn the inner workings of Chapter 4 Processor Architecture 389 Aside The progress of computer technology To get a sense of how much computer technology has improved over the past four decades, consider the following two processors. The ﬁrst Cray 1 supercomputer was delivered to Los Alamos National Laboratory in 1976. It was the fastest computer in the world, able to perform as many as 250 million arithmetic operations per second. It came with 8 megabytes of random access memory, the maximum conﬁguration allowed by the hardware. The machine was also very large—it weighed 5,000 kg, consumed 115 kilowatts, and cost $9 million. In total, around 80 of them were manufactured. The Apple ARM A7 microprocessor chip, introduced in 2013 to power the iPhone 5S, contains two CPUs, each of which can perform several billion arithmetic operations per second, and 1 gigabyte of random access memory. The entire phone weighs just 112 grams, consumes around 1 watt, and costs less than $800. Over 9 million units were sold in the ﬁrst weekend of its introduction. In addition to being a powerful computer, it can be used to take pictures, to place phone calls, and to provide driving directions, features never considered for the Cray 1. These two systems, spaced just 37 years apart, demonstrate the tremendous progress of semicon- ductor technology. Whereas the Cray 1’s CPU was constructed using around 100,000 semiconductor chips, each containing less than 20 transistors, the Apple A7 has over 1 billion transistors on its single chip. The Cray 1’s 8-megabyte memory required 8,192 chips, whereas the iPhone’s gigabyte memory is contained in a single chip. a system that is such a part of the daily lives of computer scientists and engi- neers and yet remains a mystery to many. Processor design embodies many of the principles of good engineering practice. It requires creating a simple and regular structure to perform a complex task. . Understanding how the processor works aids in understanding how the overall computer system works. In Chapter 6, we will look at the memory system and the techniques used to create an image of a very large memory with a very fast access time. Seeing the processor side of the processor–memory interface will make this presentation more complete. . Although few people design processors, many design hardware systems that contain processors. This has become commonplace as processors are embed- ded into real-world systems such as automobiles and appliances. Embedded- system designers must understand how processors work, because these sys- tems are generally designed and programmed at a lower level of abstraction than is the case for desktop and server-based systems. . You just might work on a processor design. Although the number of compa- nies producing microprocessors is small, the design teams working on those processors are already large and growing. There can be over 1,000 people involved in the different aspects of a major processor design. In this chapter, we start by deﬁning a simple instruction set that we use as a running example for our processor implementations. We call this the “Y86-64” 390 Chapter 4 Processor Architecture instruction set, because it was inspired by the x86-64 instruction set. Compared with x86-64, the Y86-64 instruction set has fewer data types, instructions, and addressing modes. It also has a simple byte-level encoding, making the machine code less compact than the comparable x86-64 code, but also much easier to design the CPU’s decoding logic. Even though the Y86-64 instruction set is very simple, it is sufﬁciently complete to allow us to write programs manipulating integer data. Designing a processor to implement Y86-64 requires us to deal with many of the challenges faced by processor designers. We then provide some background on digital hardware design. We describe the basic building blocks used in a processor and how they are connected together and operated. This presentation builds on our discussion of Boolean algebra and bit-level operations from Chapter 2. We also introduce a simple language, HCL (for “hardware control language”), to describe the control portions of hardware systems. We will later use this language to describe our processor designs. Even if you already have some background in logic design, read this section to understand our particular notation. As a ﬁrst step in designing a processor, we present a functionally correct, but somewhat impractical, Y86-64 processor based on sequential operation. This processor executes a complete Y86-64 instruction on every clock cycle. The clock must run slowly enough to allow an entire series of actions to complete within one cycle. Such a processor could be implemented, but its performance would be well below what could be achieved for this much hardware. With the sequential design as a basis, we then apply a series of transforma- tions to create a pipelined processor. This processor breaks the execution of each instruction into ﬁve steps, each of which is handled by a separate section or stage of the hardware. Instructions progress through the stages of the pipeline, with one in- struction entering the pipeline on each clock cycle. As a result, the processor can be executing the different steps of up to ﬁve instructions simultaneously. Mak- ing this processor preserve the sequential behavior of the Y86-64 ISA requires handling a variety of hazard conditions, where the location or operands of one instruction depend on those of other instructions that are still in the pipeline. We have devised a variety of tools for studying and experimenting with our processor designs. These include an assembler for Y86-64, a simulator for running Y86-64 programs on your machine, and simulators for two sequential and one pipelined processor design. The control logic for these designs is described by ﬁles in HCL notation. By editing these ﬁles and recompiling the simulator, you can alter and extend the simulator’s behavior. A number of exercises are provided that involve implementing new instructions and modifying how the machine processes instructions. Testing code is provided to help you evaluate the correctness of your modiﬁcations. These exercises will greatly aid your understanding of the material and will give you an appreciation for the many different design alternatives faced by processor designers. Web Aside arch:vlog on page 503 presents a representation of our pipelined Y86-64 processor in the Verilog hardware description language. This involves creating modules for the basic hardware building blocks and for the overall pro- cessor structure. We automatically translate the HCL description of the control Section 4.1 The Y86-64 Instruction Set Architecture 391 logic into Verilog. By ﬁrst debugging the HCL description with our simulators, we eliminate many of the tricky bugs that would otherwise show up in the hardware design. Given a Verilog description, there are commercial and open-source tools to support simulation and logic synthesis, generating actual circuit designs for the microprocessors. So, although much of the effort we expend here is to create picto- rial and textual descriptions of a system, much as one would when writing software, the fact that these designs can be automatically synthesized demonstrates that we are indeed creating a system that can be realized as hardware. 4.1 The Y86-64 Instruction Set Architecture Deﬁning an instruction set architecture, such as Y86-64, includes deﬁning the different components of its state, the set of instructions and their encodings, a set of programming conventions, and the handling of exceptional events. 4.1.1 Programmer-Visible State As Figure 4.1 illustrates, each instruction in a Y86-64 program can read and modify some part of the processor state. This is referred to as the programmer-visible state, where the “programmer” in this case is either someone writing programs in assembly code or a compiler generating machine-level code. We will see in our processor implementations that we do not need to represent and organize this state in exactly the manner implied by the ISA, as long as we can make sure that machine-level programs appear to have access to the programmer-visible state. The state for Y86-64 is similar to that for x86-64. There are 15 program registers: cËþÓ, cË²Ó, cË®Ó, cË¾Ó, cËÍÉ, cË¾É, cËÍ©, cË®©, and cËv through cËor. (We omit the x86-64 register cËos to simplify the instruction encoding.) Each of these stores a 64-bit word. Register cËÍÉ is used as a stack pointer by the push, pop, call, and return instructions. Otherwise, the registers have no ﬁxed meanings or values. There are three single-bit condition codes, …ƒ, ·ƒ, and ﬂƒ, storing information Figure 4.1 Y86-64 programmer- visible state. As with x86-64, programs for Y86- 64 access and modify the program registers, the condition codes, the program counter (PC), and the memory. The status code indicates whether the program is running normally or some special event has occurred. RF: Program registers Stat: Program status DMEM: Memory CC: Condition codes %rax %rcx %rdx %rbx %rsp %rbp %rsi %rdi %r8 %r9 %r10 %r11 %r12 %r13 %r14 PC ZF SF OF 392 Chapter 4 Processor Architecture about the effect of the most recent arithmetic or logical instruction. The program counter (PC) holds the address of the instruction currently being executed. The memory is conceptually a large array of bytes, holding both program and data. Y86-64 programs reference memory locations using virtual addresses. A combination of hardware and operating system software translates these into the actual, or physical, addresses indicating where the values are actually stored in memory. We will study virtual memory in more detail in Chapter 9. For now, we can think of the virtual memory system as providing Y86-64 programs with an image of a monolithic byte array. A ﬁnal part of the program state is a status code Stat, indicating the overall state of program execution. It will indicate either normal operation or that some sort of exception has occurred, such as when an instruction attempts to read from an invalid memory address. The possible status codes and the handling of exceptions is described in Section 4.1.4. 4.1.2 Y86-64 Instructions Figure 4.2 gives a concise description of the individual instructions in the Y86-64 ISA. We use this instruction set as a target for our processor implementations. The set of Y86-64 instructions is largely a subset of the x86-64 instruction set. It includes only 8-byte integer operations, has fewer addressing modes, and includes a smaller set of operations. Since we only use 8-byte data, we can refer to these as “words” without any ambiguity. In this ﬁgure, we show the assembly-code representation of the instructions on the left and the byte encodings on the right. Figure 4.3 shows further details of some of the instructions. The assembly-code format is similar to the ATT format for x86-64. Here are some details about the Y86-64 instructions. . The x86-64 ÀÇÌÊ instruction is split into four different instructions: ©ËÀÇÌÊ, ËËÀÇÌÊ, ÀËÀÇÌÊ, and ËÀÀÇÌÊ, explicitly indicating the form of the source and destination. The source is either immediate (©), register (Ë), or memory (À). It is designated by the ﬁrst character in the instruction name. The destination is either register (Ë) or memory (À). It is designated by the second character in the instruction name. Explicitly identifying the four types of data transfer will prove helpful when we decide how to implement them. The memory references for the two memory movement instructions have a simple base and displacement format. We do not support the second index register or any scaling of a register’s value in the address computation. As with x86-64, we do not allow direct transfers from one memory loca- tion to another. In addition, we do not allow a transfer of immediate data to memory. . There are four integer operation instructions, shown in Figure 4.2 as ﬂ–Ê. These are þ®®Ê, ÍÏ¾Ê, þÅ®Ê, and ÓÇËÊ. They operate only on register data, whereas x86-64 also allows operations on memory data. These instructions set the three condition codes …ƒ, ·ƒ, and ﬂƒ (zero, sign, and overﬂow). Section 4.1 The Y86-64 Instruction Set Architecture 393 halt nop rrmovq rA, rB irmovq V, rB rmmovq rA, D(rB) mrmovq D(rB), rA OPq rA, rB jXX Dest cmovXX rA, rB call Dest ret pushq rA popq rA 0 1 2 3 4 5 6 7 2 8 9 A B rB rB rB rB rB rB V D D Dest Dest 0Byte 1 2 3 4 56789 rA rA 0 0 0 0 0 0 fn fn fn 0 0 0 0 F F rA F rA rA rA rA Figure 4.2 Y86-64 instruction set. Instruction encodings range between 1 and 10 bytes. An instruction consists of a 1-byte instruction speciﬁer, possibly a 1-byte register speciﬁer, and possibly an 8-byte constant word. Field ðÅ speciﬁes a particular integer operation (ﬂ–Ê), data movement condition (²ÀÇÌ””), or branch condition (Á””). All numeric values are shown in hexadecimal. . The seven jump instructions (shown in Figure 4.2 as Á””) are ÁÀÉ, ÁÄ−, ÁÄ, Á−, ÁÅ−, Á×−, and Á×. Branches are taken according to the type of branch and the settings of the condition codes. The branch conditions are the same as with x86-64 (Figure 3.15). . There are six conditional move instructions (shown in Figure 4.2 as ²ÀÇÌ””): ²ÀÇÌÄ−, ²ÀÇÌÄ, ²ÀÇÌ−, ²ÀÇÌÅ−, ²ÀÇÌ×−, and ²ÀÇÌ×. These have the same format as the register–register move instruction ËËÀÇÌÊ, but the destination register is updated only if the condition codes satisfy the required constraints. . The ²þÄÄ instruction pushes the return address on the stack and jumps to the destination address. The Ë−Î instruction returns from such a call. . The ÉÏÍ³Ê and ÉÇÉÊ instructions implement push and pop, just as they do in x86-64. . The ³þÄÎ instruction stops instruction execution. x86-64 has a comparable instruction, called ³ÄÎ. x86-64 application programs are not permitted to use 394 Chapter 4 Processor Architecture this instruction, since it causes the entire system to suspend operation. For Y86-64, executing the ³þÄÎ instruction causes the processor to stop, with the status code set to ¤‹¶. (See Section 4.1.4.) 4.1.3 Instruction Encoding Figure 4.2 also shows the byte-level encoding of the instructions. Each instruction requires between 1 and 10 bytes, depending on which ﬁelds are required. Every instruction has an initial byte identifying the instruction type. This byte is split into two 4-bit parts: the high-order, or code, part, and the low-order, or function, part. As can be seen in Figure 4.2, code values range from n to nÓ¢. The function values are signiﬁcant only for the cases where a group of related instructions share a common code. These are given in Figure 4.3, showing the speciﬁc encodings of the integer operation, branch, and conditional move instructions. Observe that ËËÀÇÌÊ has the same instruction code as the conditional moves. It can be viewed as an “unconditional move” just as the ÁÀÉ instruction is an unconditional jump, both having function code n. As shown in Figure 4.4, each of the 15 program registers has an associated register identiﬁer (ID) ranging from n to nÓ¥. The numbering of registers in Y86- 64 matches what is used in x86-64. The program registers are stored within the CPU in a register ﬁle, a small random access memory where the register IDs serve as addresses. ID value nÓƒ is used in the instruction encodings and within our hardware designs when we need to indicate that no register should be accessed. Some instructions are just 1 byte long, but those that require operands have longer encodings. First, there can be an additional register speciﬁer byte, specifying either one or two registers. These register ﬁelds are called rA and rB in Figure 4.2. As the assembly-code versions of the instructions show, they can specify the registers used for data sources and destinations, as well as the base register used in an address computation, depending on the instruction type. Instructions that have no register operands, such as branches and ²þÄÄ, do not have a register speciﬁer byte. Those that require just one register operand (©ËÀÇÌÊ, ÉÏÍ³Ê, and ÉÇÉÊ) have Operations Branches 6addq 0 6subq 1 6andq 2 6xorq 3 7 7 jmp 0 jle 1 7jl 2 7je 3 7jne 4 7jge 5 7jg 6 Moves 2 2 rrmovq 0 cmovle 1 2cmovl 2 2cmove 3 2cmovne 4 2cmovge 5 2cmovg 6 Figure 4.3 Function codes for Y86-64 instruction set. The code speciﬁes a particular integer operation, branch condition, or data transfer condition. These instructions are shown as ﬂ–Ê, Á””, and ²ÀÇÌ”” in Figure 4.2. Section 4.1 The Y86-64 Instruction Set Architecture 395 Number Register name Number Register name n cËþÓ v cËv o cË²Ó w cËw p cË®Ó ¡ cËon q cË¾Ó ¢ cËoo r cËÍÉ £ cËop s cË¾É ⁄ cËoq t cËÍ© ¥ cËor u cË®© ƒ No register Figure 4.4 Y86-64 program register identiﬁers. Each of the 15 program registers has an associated identiﬁer (ID) ranging from n to nÓ¥.ID nÓƒ in a register ﬁeld of an instruction indicates the absence of a register operand. the other register speciﬁer set to value nÓƒ. This convention will prove useful in our processor implementation. Some instructions require an additional 8-byte constant word. This word can serve as the immediate data for ©ËÀÇÌÊ, the displacement for ËÀÀÇÌÊ and ÀËÀÇÌÊ address speciﬁers, and the destination of branches and calls. Note that branch and call destinations are given as absolute addresses, rather than using the PC-relative addressing seen in x86-64. Processors use PC-relative addressing to give more compact encodings of branch instructions and to allow code to be shifted from one part of memory to another without the need to update all of the branch target addresses. Since we are more concerned with simplicity in our presentation, we use absolute addressing. As with x86-64, all integers have a little-endian encoding. When the instruction is written in disassembled form, these bytes appear in reverse order. As an example, let us generate the byte encoding of the instruction ËÀÀÇÌÊ cËÍÉjnÓopqrstuvwþ¾²®fcË®Óg in hexadecimal. From Figure 4.2, we can see that ËÀÀÇÌÊ has initial byte rn. We can also see that source register cËÍÉ should be encoded in the rA ﬁeld, and base register cË®Ó should be encoded in the rB ﬁeld. Using the register numbers in Figure 4.4, we get a register speciﬁer byte of rp. Finally, the displacement is encoded in the 8-byte constant word. We ﬁrst pad nÓopqrstuvwþ¾²® with leading zeros to ﬁll out 8 bytes, giving a byte sequence of nn no pq rs tu vw þ¾ ²®. We write this in byte-reversed order as ²® þ¾ vw tu rs pq no nn. Combining these, we get an instruction encoding of rnrp²®þ¾vwturspqnonn. One important property of any instruction set is that the byte encodings must have a unique interpretation. An arbitrary sequence of bytes either encodes a unique instruction sequence or is not a legal byte sequence. This property holds for Y86-64, because every instruction has a unique combination of code and function in its initial byte, and given this byte, we can determine the length and meaning of any additional bytes. This property ensures that a processor can execute an object- code program without any ambiguity about the meaning of the code. Even if the code is embedded within other bytes in the program, we can readily determine 396 Chapter 4 Processor Architecture Aside Comparing x86-64 to Y86-64 instruction encodings Compared with the instruction encodings used in x86-64, the encoding of Y86-64 is much simpler but also less compact. The register ﬁelds occur only in ﬁxed positions in all Y86-64 instructions, whereas they are packed into various positions in the different x86-64 instructions. An x86-64 instruction can encode constant values in 1, 2, 4, or 8 bytes, whereas Y86-64 always requires 8 bytes. the instruction sequence as long as we start from the ﬁrst byte in the sequence. On the other hand, if we do not know the starting position of a code sequence, we cannot reliably determine how to split the sequence into individual instructions. This causes problems for disassemblers and other tools that attempt to extract machine-level programs directly from object-code byte sequences. Practice Problem 4.1 (solution page 516) Determine the byte encoding of the Y86-64 instruction sequence that follows. The line lÉÇÍ nÓonn indicates that the starting address of the object code should be nÓonn. lÉÇÍ nÓonn a ·ÎþËÎ ²Ç®− þÎ þ®®Ë−ÍÍ nÓonn ©ËÀÇÌÊ bosjcË¾Ó ËËÀÇÌÊ cË¾ÓjcË²Ó ÄÇÇÉx ËÀÀÇÌÊ cË²ÓjkqfcË¾Óg þ®®Ê cË¾ÓjcË²Ó ÁÀÉ ÄÇÇÉ Practice Problem 4.2 (solution page 517) For each byte sequence listed, determine the Y86-64 instruction sequence it en- codes. If there is some invalid byte in the sequence, show the instruction sequence up to that point and indicate where the invalid value occurs. For each sequence, we show the starting address, then a colon, and then the byte sequence. A. nÓonnx qnðqð²ððððððððððððððrntqnnnvnnnnnnnnnnnn B. nÓpnnx þntðvnn²npnnnnnnnnnnnnnnqnðqnþnnnnnnnnnnnnnn C. nÓqnnx snsrnunnnnnnnnnnnnnnonðn¾noð D. nÓrnnx tooquqnnnrnnnnnnnnnnnnnn E. nÓsnnx tqtpþnðn Section 4.1 The Y86-64 Instruction Set Architecture 397 Aside RISC and CISC instruction sets x86-64 is sometimes labeled as a “complex instruction set computer” (CISC—pronounced “sisk”), and is deemed to be the opposite of ISAs that are classiﬁed as “reduced instruction set computers” (RISC—pronounced “risk”). Historically, CISC machines came ﬁrst, having evolved from the earliest computers. By the early 1980s, instruction sets for mainframe and minicomputers had grown quite large, as machine designers incorporated new instructions to support high-level tasks, such as manipulating circular buffers, performing decimal arithmetic, and evaluating polynomials. The ﬁrst microprocessors appeared in the early 1970s and had limited instruction sets, because the integrated-circuit technology then posed severe constraints on what could be implemented on a single chip. Microprocessors evolved quickly and, by the early 1980s, were following the same path of increasing instruction set complexity that had been the case for mainframes and minicomputers. The x86 family took this path, evolving into IA32, and more recently into x86-64. The x86 line continues to evolve as new classes of instructions are added based on the needs of emerging applications. The RISC design philosophy developed in the early 1980s as an alternative to these trends. A group of hardware and compiler experts at IBM, strongly inﬂuenced by the ideas of IBM researcher John Cocke, recognized that they could generate efﬁcient code for a much simpler form of instruction set. In fact, many of the high-level instructions that were being added to instruction sets were very difﬁcult to generate with a compiler and were seldom used. A simpler instruction set could be implemented with much less hardware and could be organized in an efﬁcient pipeline structure, similar to those described later in this chapter. IBM did not commercialize this idea until many years later, when it developed the Power and PowerPC ISAs. The RISC concept was further developed by Professors David Patterson, of the University of California at Berkeley, and John Hennessy, of Stanford University. Patterson gave the name RISC to this new class of machines, and CISC to the existing class, since there had previously been no need to have a special designation for a nearly universal form of instruction set. When comparing CISC with the original RISC instruction sets, we ﬁnd the following general characteristics: CISC Early RISC A large number of instructions. The Intel document describing the complete set of instructions [51] is over 1,200 pages long. Many fewer instructions—typically less than 100. Some instructions with long execution times. These include instructions that copy an entire block from one part of memory to another and others that copy multiple registers to and from memory. No instruction with a long execution time. Some early RISC machines did not even have an integer multiply instruction, requiring compilers to implement multiplication as a sequence of additions. Variable-size encodings. x86-64 instructions can range from 1 to 15 bytes. Fixed-length encodings. Typically all instructions are encoded as 4 bytes. 398 Chapter 4 Processor Architecture Aside RISC and CISC instruction sets (continued) CISC Early RISC Multiple formats for specifying operands. In x86- 64, a memory operand speciﬁer can have many different combinations of displacement, base and index registers, and scale factors. Simple addressing formats. Typically just base and displacement addressing. Arithmetic and logical operations can be applied to both memory and register operands. Arithmetic and logical operations only use register operands. Memory referencing is only allowed by load instructions, reading from memory into a register, and store instructions, writing from a register to memory. This convention is referred to as a load/store architecture. Implementation artifacts hidden from machine- level programs. The ISA provides a clean abstraction between programs and how they get executed. Implementation artifacts exposed to machine- level programs. Some RISC machines prohibit particular instruction sequences and have jumps that do not take effect until the following instruction is executed. The compiler is given the task of optimizing performance within these constraints. Condition codes. Special ﬂags are set as a side effect of instructions and then used for conditional branch testing. No condition codes. Instead, explicit test instructions store the test results in normal registers for use in conditional evaluation. Stack-intensive procedure linkage. The stack is used for procedure arguments and return addresses. Register-intensive procedure linkage. Registers are used for procedure arguments and return addresses. Some procedures can thereby avoid any memory references. Typically, the processor has many more (up to 32) registers. The Y86-64 instruction set includes attributes of both CISC and RISC instruction sets. On the CISC side, it has condition codes and variable-length instructions, and it uses the stack to store return addresses. On the RISC side, it uses a load/store architecture and a regular instruction encoding, and it passes procedure arguments through registers. It can be viewed as taking a CISC instruction set (x86) and simplifying it by applying some of the principles of RISC. Section 4.1 The Y86-64 Instruction Set Architecture 399 Aside The RISC versus CISC controversy Through the 1980s, battles raged in the computer architecture community regarding the merits of RISC versus CISC instruction sets. Proponents of RISC claimed they could get more computing power for a given amount of hardware through a combination of streamlined instruction set design, advanced compiler technology, and pipelined processor implementation. CISC proponents countered that fewer CISC instructions were required to perform a given task, and so their machines could achieve higher overall performance. Major companies introduced RISC processor lines, including Sun Microsystems (SPARC), IBM and Motorola (PowerPC), and Digital Equipment Corporation (Alpha). A British company, Acorn Computers Ltd., developed its own architecture, ARM (originally an acronym for “Acorn RISC machine”), which has become widely used in embedded applications, such as cell phones. In the early 1990s, the debate diminished as it became clear that neither RISC nor CISC in their purest forms were better than designs that incorporated the best ideas of both. RISC machines evolved and introduced more instructions, many of which take multiple cycles to execute. RISC machines today have hundreds of instructions in their repertoire, hardly ﬁtting the name “reduced instruction set machine.” The idea of exposing implementation artifacts to machine-level programs proved to be shortsighted. As new processor models were developed using more advanced hardware structures, many of these artifacts became irrelevant, but they still remained part of the instruction set. Still, the core of RISC design is an instruction set that is well suited to execution on a pipelined machine. More recent CISC machines also take advantage of high-performance pipeline structures. As we will discuss in Section 5.7, they fetch the CISC instructions and dynamically translate them into a sequence of simpler, RISC-like operations. For example, an instruction that adds a register to memory is translated into three operations: one to read the original memory value, one to perform the addition, and a third to write the sum to memory. Since the dynamic translation can generally be performed well in advance of the actual instruction execution, the processor can sustain a very high execution rate. Marketing issues, apart from technological ones, have also played a major role in determining the success of different instruction sets. By maintaining compatibility with its existing processors, Intel with x86 made it easy to keep moving from one generation of processor to the next. As integrated-circuit technology improved, Intel and other x86 processor manufacturers could overcome the inefﬁciencies created by the original 8086 instruction set design, using RISC techniques to produce performance comparable to the best RISC machines. As we saw in Section 3.1, the evolution of IA32 into x86-64 provided an opportunity to incorporate several features of RISC into the x86 family. In the areas of desktop, laptop, and server-based computing, x86 has achieved near total domination. RISC processors have done very well in the market for embedded processors, controlling such systems as cellular telephones, automobile brakes, and Internet appliances. In these applications, saving on cost and power is more important than maintaining backward compatibility. In terms of the number of processors sold, this is a very large and growing market. 4.1.4 Y86-64 Exceptions The programmer-visible state for Y86-64 (Figure 4.1) includes a status code Stat describing the overall state of the executing program. The possible values for this code are shown in Figure 4.5. Code value 1, named ¡ﬂ«, indicates that the program 400 Chapter 4 Processor Architecture Value Name Meaning 1 ¡ﬂ« Normal operation 2 ¤‹¶ ³þÄÎ instruction encountered 3 ¡⁄‡ Invalid address encountered 4 'ﬁ· Invalid instruction encountered Figure 4.5 Y86-64 status codes. In our design, the processor halts for any code other than ¡ﬂ«. is executing normally, while the other codes indicate that some type of exception has occurred. Code 2, named ¤‹¶, indicates that the processor has executed a ³þÄÎ instruction. Code 3, named ¡⁄‡, indicates that the processor attempted to read from or write to an invalid memory address, either while fetching an instruction or while reading or writing data. We limit the maximum address (the exact limit varies by implementation), and any access to an address beyond this limit will trigger an ¡⁄‡ exception. Code 4, named 'ﬁ·, indicates that an invalid instruction code has been encountered. For Y86-64, we will simply have the processor stop executing instructions when it encounters any of the exceptions listed. In a more complete design, the processor would typically invoke an exception handler, a procedure designated to handle the speciﬁc type of exception encountered. As described in Chapter 8, exception handlers can be conﬁgured to have different effects, such as aborting the program or invoking a user-deﬁned signal handler. 4.1.5 Y86-64 Programs Figure 4.6 shows x86-64 and Y86-64 assembly code for the following C function: 1 ÄÇÅ× ÍÏÀfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg 2 Õ 3 ÄÇÅ× ÍÏÀ { ny 4 Ñ³©Ä− f²ÇÏÅÎg Õ 5 ÍÏÀ i{ hÍÎþËÎy 6 ÍÎþËÎiiy 7 ²ÇÏÅÎkky 8 Û 9 Ë−ÎÏËÅ ÍÏÀy 10 Û The x86-64 code was generated by the gcc compiler. The Y86-64 code is similar, but with the following differences: . The Y86-64 code loads constants into registers (lines 2–3), since it cannot use immediate data in arithmetic instructions. Section 4.1 The Y86-64 Instruction Set Architecture 401 x86-64 code long sum(long *start, long count) start in %rdi, count in %rsi 1 ÍÏÀx 2 ÀÇÌÄ bnj c−þÓ sum=0 3 ÁÀÉ l‹p Goto test 4 l‹qx loop: 5 þ®®Ê fcË®©gj cËþÓ Add *start to sum 6 þ®®Ê bvj cË®© start++ 7 ÍÏ¾Ê boj cËÍ© count-- 8 l‹px test: 9 Î−ÍÎÊ cËÍ©j cËÍ© Test sum 10 ÁÅ− l‹q If !=0, goto loop 11 Ë−Éy Ë−Î Return Y86-64 code long sum(long *start, long count) start in %rdi, count in %rsi 1 ÍÏÀx 2 ©ËÀÇÌÊ bvjcËv Constant 8 3 ©ËÀÇÌÊ bojcËw Constant 1 4 ÓÇËÊ cËþÓjcËþÓ sum=0 5 þÅ®Ê cËÍ©jcËÍ© Set CC 6 ÁÀÉ Î−ÍÎ Goto test 7 ÄÇÇÉx 8 ÀËÀÇÌÊ fcË®©gjcËon Get *start 9 þ®®Ê cËonjcËþÓ Add to sum 10 þ®®Ê cËvjcË®© start++ 11 ÍÏ¾Ê cËwjcËÍ© count--. Set CC 12 Î−ÍÎx 13 ÁÅ− ÄÇÇÉ Stop when 0 14 Ë−Î Return Figure 4.6 Comparison of Y86-64 and x86-64 assembly programs. The ÍÏÀ function computes the sum of an integer array. The Y86-64 code follows the same general pattern as the x86-64 code. 402 Chapter 4 Processor Architecture . The Y86-64 code requires two instructions (lines 8–9) to read a value from memory and add it to a register, whereas the x86-64 code can do this with a single þ®®Ê instruction (line 5). . Our hand-coded Y86-64 implementation takes advantage of the property that the ÍÏ¾Ê instruction (line 11) also sets the condition codes, and so the Î−ÍÎÊ instruction of the gcc-generated code (line 9) is not required. For this to work, though, the Y86-64 code must set the condition codes prior to entering the loop with an þÅ®Ê instruction (line 5). Figure 4.7 shows an example of a complete program ﬁle written in Y86- 64 assembly code. The program contains both data and instructions. Directives indicate where to place code or data and how to align it. The program speciﬁes issues such as stack placement, data initialization, program initialization, and program termination. In this program, words beginning with ‘l’ are assembler directives telling the assembler to adjust the address at which it is generating code or to insert some words of data. The directive lÉÇÍ n (line 2) indicates that the assembler should begin generating code starting at address n. This is the starting address for all Y86-64 programs. The next instruction (line 3) initializes the stack pointer. We can see that the label ÍÎþ²Â is declared at the end of the program (line 40), to indicate address nÓpnn using a lÉÇÍ directive (line 39). Our stack will therefore start at this address and grow toward lower addresses. We must ensure that the stack does not grow so large that it overwrites the code or other program data. Lines 8 to 13 of the program declare an array of four words, having the values nÓnnn®nnn®nnn®nnn®, nÓnn²nnn²nnn²nnn²n, nÓn¾nnn¾nnn¾nnn¾nn, nÓþnnnþnnnþnnnþnnn The label þËËþÔ denotes the start of this array, and is aligned on an 8-byte boundary (using the lþÄ©×Å directive). Lines 16 to 19 show a “main” procedure that calls the function ÍÏÀ on the four-word array and then halts. As this example shows, since our only tool for creating Y86-64 code is an assembler, the programmer must perform tasks we ordinarily delegate to the compiler, linker, and run-time system. Fortunately, we only do this for small programs, for which simple mechanisms sufﬁce. Figure 4.8 shows the result of assembling the code shown in Figure 4.7 by an assembler we call yas. The assembler output is in ASCII format to make it more readable. On lines of the assembly ﬁle that contain instructions or data, the object code contains an address, followed by the values of between 1 and 10 bytes. We have implemented an instruction set simulator we call yis, the purpose of which is to model the execution of a Y86-64 machine-code program without attempting to model the behavior of any speciﬁc processor implementation. This form of simulation is useful for debugging programs before actual hardware is available, and for checking the result of either simulating the hardware or running Section 4.1 The Y86-64 Instruction Set Architecture 403 1 a ¥Ó−²ÏÎ©ÇÅ ¾−×©ÅÍ þÎ þ®®Ë−ÍÍ n 2 lÉÇÍ n 3 ©ËÀÇÌÊ ÍÎþ²Âj cËÍÉ a ·−Î ÏÉ ÍÎþ²Â ÉÇ©ÅÎ−Ë 4 ²þÄÄ Àþ©Å a ¥Ó−²ÏÎ− Àþ©Å ÉËÇ×ËþÀ 5 ³þÄÎ a ¶−ËÀ©ÅþÎ− ÉËÇ×ËþÀ 6 7 a ¡ËËþÔ Çð r −Ä−À−ÅÎÍ 8 lþÄ©×Å v 9 þËËþÔx 10 lÊÏþ® nÓnnn®nnn®nnn® 11 lÊÏþ® nÓnn²nnn²nnn²n 12 lÊÏþ® nÓn¾nnn¾nnn¾nn 13 lÊÏþ® nÓþnnnþnnnþnnn 14 15 Àþ©Åx 16 ©ËÀÇÌÊ þËËþÔjcË®© 17 ©ËÀÇÌÊ brjcËÍ© 18 ²þÄÄ ÍÏÀ a ÍÏÀfþËËþÔj rg 19 Ë−Î 20 21 a ÄÇÅ× ÍÏÀfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg 22 a ÍÎþËÎ ©Å cË®©j ²ÇÏÅÎ ©Å cËÍ© 23 ÍÏÀx 24 ©ËÀÇÌÊ bvjcËv a £ÇÅÍÎþÅÎ v 25 ©ËÀÇÌÊ bojcËw a £ÇÅÍÎþÅÎ o 26 ÓÇËÊ cËþÓjcËþÓ a ÍÏÀ { n 27 þÅ®Ê cËÍ©jcËÍ© a ·−Î ££ 28 ÁÀÉ Î−ÍÎ a §ÇÎÇ Î−ÍÎ 29 ÄÇÇÉx 30 ÀËÀÇÌÊ fcË®©gjcËon a §−Î hÍÎþËÎ 31 þ®®Ê cËonjcËþÓ a ¡®® ÎÇ ÍÏÀ 32 þ®®Ê cËvjcË®© a ÍÎþËÎii 33 ÍÏ¾Ê cËwjcËÍ© a ²ÇÏÅÎkkl ·−Î ££ 34 Î−ÍÎx 35 ÁÅ− ÄÇÇÉ a ·ÎÇÉ Ñ³−Å n 36 Ë−Î a ‡−ÎÏËÅ 37 38 a ·Îþ²Â ÍÎþËÎÍ ³−Ë− þÅ® ×ËÇÑÍ ÎÇ ÄÇÑ−Ë þ®®Ë−ÍÍ−Í 39 lÉÇÍ nÓpnn 40 ÍÎþ²Âx Figure 4.7 Sample program written in Y86-64 assembly code. The ÍÏÀ function is called to compute the sum of a four-element array. 404 Chapter 4 Processor Architecture Ú a ¥Ó−²ÏÎ©ÇÅ ¾−×©ÅÍ þÎ þ®®Ë−ÍÍ n nÓnnnx Ú lÉÇÍ n nÓnnnx qnðrnnnpnnnnnnnnnnnn Ú ©ËÀÇÌÊ ÍÎþ²Âj cËÍÉ a ·−Î ÏÉ ÍÎþ²Â ÉÇ©ÅÎ−Ë nÓnnþx vnqvnnnnnnnnnnnnnn Ú ²þÄÄ Àþ©Å a ¥Ó−²ÏÎ− Àþ©Å ÉËÇ×ËþÀ nÓnoqx nn Ú ³þÄÎ a ¶−ËÀ©ÅþÎ− ÉËÇ×ËþÀ Ú Ú a ¡ËËþÔ Çð r −Ä−À−ÅÎÍ nÓnovx Ú lþÄ©×Å v nÓnovx Ú þËËþÔx nÓnovx n®nnn®nnn®nnnnnn Ú lÊÏþ® nÓnnn®nnn®nnn® nÓnpnx ²nnn²nnn²nnnnnnn Ú lÊÏþ® nÓnn²nnn²nnn²n nÓnpvx nnn¾nnn¾nnn¾nnnn Ú lÊÏþ® nÓn¾nnn¾nnn¾nn nÓnqnx nnþnnnþnnnþnnnnn Ú lÊÏþ® nÓþnnnþnnnþnnn Ú nÓnqvx Ú Àþ©Åx nÓnqvx qnðuovnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ þËËþÔjcË®© nÓnrpx qnðtnrnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ brjcËÍ© nÓnr²x vnstnnnnnnnnnnnnnn Ú ²þÄÄ ÍÏÀ a ÍÏÀfþËËþÔj rg nÓnssx wn Ú Ë−Î Ú Ú a ÄÇÅ× ÍÏÀfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg Ú a ÍÎþËÎ ©Å cË®©j ²ÇÏÅÎ ©Å cËÍ© nÓnstx Ú ÍÏÀx nÓnstx qnðvnvnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bvjcËv a £ÇÅÍÎþÅÎ v nÓntnx qnðwnonnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bojcËw a £ÇÅÍÎþÅÎ o nÓntþx tqnn Ú ÓÇËÊ cËþÓjcËþÓ a ÍÏÀ { n nÓnt²x tptt Ú þÅ®Ê cËÍ©jcËÍ© a ·−Î ££ nÓnt−x unvunnnnnnnnnnnnnn Ú ÁÀÉ Î−ÍÎ a §ÇÎÇ Î−ÍÎ nÓnuux Ú ÄÇÇÉx nÓnuux snþunnnnnnnnnnnnnnnn Ú ÀËÀÇÌÊ fcË®©gjcËon a §−Î hÍÎþËÎ nÓnvox tnþn Ú þ®®Ê cËonjcËþÓ a ¡®® ÎÇ ÍÏÀ nÓnvqx tnvu Ú þ®®Ê cËvjcË®© a ÍÎþËÎii nÓnvsx towt Ú ÍÏ¾Ê cËwjcËÍ© a ²ÇÏÅÎkkl ·−Î ££ nÓnvux Ú Î−ÍÎx nÓnvux uruunnnnnnnnnnnnnn Ú ÁÅ− ÄÇÇÉ a ·ÎÇÉ Ñ³−Å n nÓnwnx wn Ú Ë−Î a ‡−ÎÏËÅ Ú Ú a ·Îþ²Â ÍÎþËÎÍ ³−Ë− þÅ® ×ËÇÑÍ ÎÇ ÄÇÑ−Ë þ®®Ë−ÍÍ−Í nÓpnnx Ú lÉÇÍ nÓpnn nÓpnnx Ú ÍÎþ²Âx Figure 4.8 Output of yas assembler. Each line includes a hexadecimal address and between 1 and 10 bytes of object code. Section 4.1 The Y86-64 Instruction Set Architecture 405 the program on the hardware itself. Running on our sample object code, yis generates the following output: ·ÎÇÉÉ−® ©Å qr ÍÎ−ÉÍ þÎ –£ { nÓoql ·ÎþÎÏÍ ’¤‹¶’j ££ …{o ·{n ﬂ{n £³þÅ×−Í ÎÇ Ë−×©ÍÎ−ËÍx cËþÓx nÓnnnnnnnnnnnnnnnn nÓnnnnþ¾²®þ¾²®þ¾²® cËÍÉx nÓnnnnnnnnnnnnnnnn nÓnnnnnnnnnnnnnpnn cË®©x nÓnnnnnnnnnnnnnnnn nÓnnnnnnnnnnnnnnqv cËvx nÓnnnnnnnnnnnnnnnn nÓnnnnnnnnnnnnnnnv cËwx nÓnnnnnnnnnnnnnnnn nÓnnnnnnnnnnnnnnno cËonx nÓnnnnnnnnnnnnnnnn nÓnnnnþnnnþnnnþnnn £³þÅ×−Í ÎÇ À−ÀÇËÔx nÓnoðnx nÓnnnnnnnnnnnnnnnn nÓnnnnnnnnnnnnnnss nÓnoðvx nÓnnnnnnnnnnnnnnnn nÓnnnnnnnnnnnnnnoq The ﬁrst line of the simulation output summarizes the execution and the resulting values of the PC and program status. In printing register and memory values, it only prints out words that change during simulation, either in registers or in memory. The original values (here they are all zero) are shown on the left, and the ﬁnal values are shown on the right. We can see in this output that register cËþÓ contains nÓþ¾²®þ¾²®þ¾²®þ¾²®, the sum of the 4-element array passed to procedure ÍÏÀ. In addition, we can see that the stack, which starts at address nÓpnn and grows toward lower addresses, has been used, causing changes to words of memory at addresses nÓoðn–nÓoðv. The maximum address for executable code is nÓnwn, and so the pushing and popping of values on the stack did not corrupt the executable code. Practice Problem 4.3 (solution page 518) One common pattern in machine-level programs is to add a constant value to a register. With the Y86-64 instructions presented thus far, this requires ﬁrst using an ©ËÀÇÌÊ instruction to set a register to the constant, and then an þ®®Ê instruction to add this value to the destination register. Suppose we want to add a new instruction ©þ®®Ê with the following format: 0 C0 Byte iaddq V, rB 1 F rB V 23456789 This instruction adds the constant value V to register rB. Rewrite the Y86-64 ÍÏÀ function of Figure 4.6 to make use of the ©þ®®Ê instruction. In the original version, we dedicated registers cËv and cËw to hold constant values. Now, we can avoid using those registers altogether. 406 Chapter 4 Processor Architecture Practice Problem 4.4 (solution page 518) Write Y86-64 code to implement a recursive product function ËÉËÇ®Ï²Î, based on the following C code: ÄÇÅ× ËÉËÇ®Ï²ÎfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg Õ ©ð f²ÇÏÅÎ z{ og Ë−ÎÏËÅ oy Ë−ÎÏËÅ hÍÎþËÎ h ËÉËÇ®Ï²ÎfÍÎþËÎioj ²ÇÏÅÎkogy Û Use the same argument passing and register saving conventions as x86-64 code does. You might ﬁnd it helpful to compile the C code on an x86-64 machine and then translate the instructions to Y86-64. Practice Problem 4.5 (solution page 519) Modify the Y86-64 code for the ÍÏÀ function (Figure 4.6) to implement a function þ¾Í·ÏÀ that computes the sum of absolute values of an array. Use a conditional jump instruction within your inner loop. Practice Problem 4.6 (solution page 519) Modify the Y86-64 code for the ÍÏÀ function (Figure 4.6) to implement a function þ¾Í·ÏÀ that computes the sum of absolute values of an array. Use a conditional move instruction within your inner loop. 4.1.6 Some Y86-64 Instruction Details Most Y86-64 instructions transform the program state in a straightforward man- ner, and so deﬁning the intended effect of each instruction is not difﬁcult. Two unusual instruction combinations, however, require special attention. The ÉÏÍ³Ê instruction both decrements the stack pointer by 8 and writes a register value to memory. It is therefore not totally clear what the processor should do when executing the instruction ÉÏÍ³Ê cËÍÉ, since the register being pushed is being changed by the same instruction. Two different conventions are possible: (1) push the original value of cËÍÉ, or (2) push the decremented value of cËÍÉ. For the Y86-64 processor, let us adopt the same convention as is used with x86-64, as determined in the following problem. Practice Problem 4.7 (solution page 520) Let us determine the behavior of the instruction ÉÏÍ³Ê cËÍÉ for an x86-64 pro- cessor. We could try reading the Intel documentation on this instruction, but a Section 4.1 The Y86-64 Instruction Set Architecture 407 simpler approach is to conduct an experiment on an actual machine. The C com- piler would not normally generate this instruction, so we must use hand-generated assembly code for this task. Here is a test function we have written (Web Aside asm:easm on page 214 describes how to write programs that combine C code with handwritten assembly code): 1 lÎ−ÓÎ 2 l×ÄÇ¾Ä ÉÏÍ³Î−ÍÎ 3 ÉÏÍ³Î−ÍÎx 4 ÀÇÌÊ cËÍÉj cËþÓ Copy stack pointer 5 ÉÏÍ³Ê cËÍÉ Push stack pointer 6 ÉÇÉÊ cË®Ó Pop it back 7 ÍÏ¾Ê cË®Ój cËþÓ Return 0 or 4 8 Ë−Î In our experiments, we ﬁnd that function ÉÏÍ³Î−ÍÎ always returns 0. What does this imply about the behavior of the instruction ÉÏÍ³Ê cËÍÉ under x86-64? A similar ambiguity occurs for the instruction ÉÇÉÊ cËÍÉ. It could either set cËÍÉ to the value read from memory or to the incremented stack pointer. As with Problem 4.7, let us run an experiment to determine how an x86-64 machine would handle this instruction, and then design our Y86-64 machine to follow the same convention. Practice Problem 4.8 (solution page 520) The following assembly-code function lets us determine the behavior of the in- struction ÉÇÉÊ cËÍÉ for x86-64: 1 lÎ−ÓÎ 2 l×ÄÇ¾Ä ÉÇÉÎ−ÍÎ 3 ÉÇÉÎ−ÍÎx 4 ÀÇÌÊ cËÍÉj cË®© Save stack pointer 5 ÉÏÍ³Ê bnÓþ¾²® Push test value 6 ÉÇÉÊ cËÍÉ Pop to stack pointer 7 ÀÇÌÊ cËÍÉj cËþÓ Set popped value as return value 8 ÀÇÌÊ cË®©j cËÍÉ Restore stack pointer 9 Ë−Î We ﬁnd this function always returns nÓþ¾²®. What does this imply about the behavior of ÉÇÉÊ cËÍÉ? What other Y86-64 instruction would have the exact same behavior? 408 Chapter 4 Processor Architecture Aside Getting the details right: Inconsistencies across x86 models Practice Problems 4.7 and 4.8 are designed to help us devise a consistent set of conventions for instruc- tions that push or pop the stack pointer. There seems to be little reason why one would want to perform either of these operations, and so a natural question to ask is, “Why worry about such picky details?” Several useful lessons can be learned about the importance of consistency from the following excerpt from the Intel documentation of the push instruction [51]: For IA-32 processors from the Intel 286 on, the PUSH ESP instruction pushes the value of the ESP register as it existed before the instruction was executed. (This is also true for Intel 64 architecture, real-address and virtual-8086 modes of IA-32 architecture.) For the Intel® 8086 processor, the PUSH SP instruction pushes the new value of the SP register (that is the value after it has been decremented by 2). (PUSH ESP instruction. Intel Corporation. 50.) Although the exact details of this note may be difﬁcult to follow, we can see that it states that, depending on what mode an x86 processor operates under, it will do different things when instructed to push the stack pointer register. Some modes push the original value, while others push the decremented value. (Interestingly, there is no corresponding ambiguity about popping to the stack pointer register.) There are two drawbacks to this inconsistency: . It decreases code portability. Programs may have different behavior depending on the processor mode. Although the particular instruction is not at all common, even the potential for incompati- bility can have serious consequences. . It complicates the documentation. As we see here, a special note is required to try to clarify the differences. The documentation for x86 is already complex enough without special cases such as this one. We conclude, therefore, that working out details in advance and striving for complete consistency can save a lot of trouble in the long run. 4.2 Logic Design and the Hardware Control Language HCL In hardware design, electronic circuits are used to compute functions on bits and to store bits in different kinds of memory elements. Most contemporary circuit technology represents different bit values as high or low voltages on signal wires. In current technology, logic value 1 is represented by a high voltage of around 1.0 volt, while logic value 0 is represented by a low voltage of around 0.0 volts. Three major components are required to implement a digital system: combinational logic to compute functions on the bits, memory elements to store bits, and clock signals to regulate the updating of the memory elements. In this section, we provide a brief description of these different components. We also introduce HCL (for “hardware control language”), the language that we use to describe the control logic of the different processor designs. We only describe HCL informally here. A complete reference for HCL can be found in Web Aside arch:hcl on page 508. Section 4.2 Logic Design and the Hardware Control Language HCL 409 Aside Modern logic design At one time, hardware designers created circuit designs by drawing schematic diagrams of logic circuits (ﬁrst with paper and pencil, and later with computer graphics terminals). Nowadays, most designs are expressed in a hardware description language (HDL), a textual notation that looks similar to a programming language but that is used to describe hardware structures rather than program behaviors. The most commonly used languages are Verilog, having a syntax similar to C, and VHDL, having a syntax similar to the Ada programming language. These languages were originally designed for creating simulation models of digital circuits. In the mid-1980s, researchers developed logic synthesis programs that could generate efﬁcient circuit designs from HDL descriptions. There are now a number of commercial synthesis programs, and this has become the dominant technique for generating digital circuits. This shift from hand-designed circuits to synthesized ones can be likened to the shift from writing programs in assembly code to writing them in a high-level language and having a compiler generate the machine code. Our HCL language expresses only the control portions of a hardware design, with only a limited set of operations and with no modularity. As we will see, however, the control logic is the most difﬁcult part of designing a microprocessor. We have developed tools that can directly translate HCL into Verilog, and by combining this code with Verilog code for the basic hardware units, we can generate HDL descriptions from which actual working microprocessors can be synthesized. By carefully separating out, designing, and testing the control logic, we can create a working microprocessor with reasonable effort. Web Aside arch:vlog on page 503 describes how we can generate Verilog versions of a Y86-64 processor. Figure 4.9 Logic gate types. Each gate generates output equal to some Boolean function of its inputs. AND outout out OR NOT a a b a b out = a && b out = a || b out = !a 4.2.1 Logic Gates Logic gates are the basic computing elements for digital circuits. They generate an output equal to some Boolean function of the bit values at their inputs. Figure 4.9 shows the standard symbols used for Boolean functions and, or, and not. HCL expressions are shown below the gates for the operators in C (Section 2.1.8): dd for and, ÚÚ for or, and _ for not. We use these instead of the bit-level C operators d, Ú, and Ü, because logic gates operate on single-bit quantities, not entire words. Although the ﬁgure illustrates only two-input versions of the and and or gates, it is common to see these being used as n-way operations for n> 2. We still write these in HCL using binary operators, though, so the operation of a three-input and gate with inputs a, b, and c is described with the HCL expression þdd¾dd². Logic gates are always active. If some input to a gate changes, then within some small amount of time, the output will change accordingly. 410 Chapter 4 Processor Architecture Figure 4.10 Combinational circuit to test for bit equality. The output will equal 1 when both inputs are 0 or both are 1. a b eq Bit equal 4.2.2 Combinational Circuits and HCL Boolean Expressions By assembling a number of logic gates into a network, we can construct computa- tional blocks known as combinational circuits. Several restrictions are placed on how the networks are constructed: . Every logic gate input must be connected to exactly one of the following: (1) one of the system inputs (known as a primary input), (2) the output connection of some memory element, or (3) the output of some logic gate. . The outputs of two or more logic gates cannot be connected together. Oth- erwise, the two could try to drive the wire toward different voltages, possibly causing an invalid voltage or a circuit malfunction. . The network must be acyclic. That is, there cannot be a path through a series of gates that forms a loop in the network. Such loops can cause ambiguity in the function computed by the network. Figure 4.10 shows an example of a simple combinational circuit that we will ﬁnd useful. It has two inputs, a and b. It generates a single output eq, such that the output will equal 1 if either a and b are both 1 (detected by the upper and gate) or are both 0 (detected by the lower and gate). We write the function of this network in HCL as ¾ÇÇÄ −Ê { fþ dd ¾g ÚÚ f_þ dd _¾gy This code simply deﬁnes the bit-level (denoted by data type ¾ÇÇÄ) signal eq as a function of inputs a and b. As this example shows, HCL uses C-style syntax, with ‘{’ associating a signal name with an expression. Unlike C, however, we do not view this as performing a computation and assigning the result to some memory location. Instead, it is simply a way to give a name to an expression. Practice Problem 4.9 (solution page 520) Write an HCL expression for a signal xor, equal to the exclusive-or of inputs a and b. What is the relation between the signals xor and eq deﬁned above? Figure 4.11 shows another example of a simple but useful combinational circuit known as a multiplexor (commonly referred to as a “MUX”). A multiplexor Section 4.2 Logic Design and the Hardware Control Language HCL 411 Figure 4.11 Single-bit multiplexor circuit. The output will equal input a if the control signal s is 1 and will equal input b when s is 0. s b a Bit MUX out selects a value from among a set of different data signals, depending on the value of a control input signal. In this single-bit multiplexor, the two data signals are the input bits a and b, while the control signal is the input bit s. The output will equal a when s is 1, and it will equal b when s is 0. In this circuit, we can see that the two and gates determine whether to pass their respective data inputs to the or gate. The upper and gate passes signal b when s is 0 (since the other input to the gate is _s), while the lower and gate passes signal a when s is 1. Again, we can write an HCL expression for the output signal, using the same operations as are present in the combinational circuit: ¾ÇÇÄ ÇÏÎ { fÍ dd þg ÚÚ f_Í dd ¾gy Our HCL expressions demonstrate a clear parallel between combinational logic circuits and logical expressions in C. They both use Boolean operations to compute functions over their inputs. Several differences between these two ways of expressing computation are worth noting: . Since a combinational circuit consists of a series of logic gates, it has the property that the outputs continually respond to changes in the inputs. If some input to the circuit changes, then after some delay, the outputs will change accordingly. By contrast, a C expression is only evaluated when it is encountered during the execution of a program. . Logical expressions in C allow arguments to be arbitrary integers, interpreting 0as false and anything else as true. In contrast, our logic gates only operate over the bit values 0 and 1. . Logical expressions in C have the property that they might only be partially evaluated. If the outcome of an and or or operation can be determined by just evaluating the ﬁrst argument, then the second argument will not be evaluated. For example, with the C expression fþ dd _þg dd ðÏÅ²f¾j²g the function ðÏÅ² will not be called, because the expression fþ dd _þg evalu- ates to 0. In contrast, combinational logic does not have any partial evaluation rules. The gates simply respond to changing inputs. 412 Chapter 4 Processor Architecture (a) Bit-level implementation Bit equal Bit equal Bit equal Bit equal b63 a63 b62 a62 b1 a1 b0 a0 eq63 eq1 eq0 eq62 Eq. . .. . . (b) Word-level abstraction A B A == B = Figure 4.12 Word-level equality test circuit. The output will equal 1 when each bit from word A equals its counterpart from word B. Word-level equality is one of the operations in HCL. 4.2.3 Word-Level Combinational Circuits and HCL Integer Expressions By assembling large networks of logic gates, we can construct combinational circuits that compute much more complex functions. Typically, we design circuits that operate on data words. These are groups of bit-level signals that represent an integer or some control pattern. For example, our processor designs will contain numerous words, with word sizes ranging between 4 and 64 bits, representing integers, addresses, instruction codes, and register identiﬁers. Combinational circuits that perform word-level computations are constructed using logic gates to compute the individual bits of the output word, based on the individual bits of the input words. For example, Figure 4.12 shows a combinational circuit that tests whether two 64-bit words A and B are equal. That is, the output will equal 1 if and only if each bit of A equals the corresponding bit of B. This circuit is implemented using 64 of the single-bit equality circuits shown in Figure 4.10. The outputs of these single-bit circuits are combined with an and gate to form the circuit output. In HCL, we will declare any word-level signal as an ©ÅÎ, without specifying the word size. This is done for simplicity. In a full-featured hardware description language, every word can be declared to have a speciﬁc number of bits. HCL allows words to be compared for equality, and so the functionality of the circuit shown in Figure 4.12 can be expressed at the word level as ¾ÇÇÄ ¥Ê { f¡ {{ ¢gy where arguments A and B are of type ©ÅÎ. Note that we use the same syntax conventions as in C, where ‘{’ denotes assignment and ‘{{’ denotes the equality operator. Section 4.2 Logic Design and the Hardware Control Language HCL 413 As is shown on the right side of Figure 4.12, we will draw word-level circuits using medium-thickness lines to represent the set of wires carrying the individual bits of the word, and we will show a single-bit signal as a dashed line. Practice Problem 4.10 (solution page 520) Suppose you want to implement a word-level equality circuit using the exclusive- or circuits from Problem 4.9 rather than from bit-level equality circuits. Design such a circuit for a 64-bit word consisting of 64 bit-level exclusive-or circuits and two additional logic gates. Figure 4.13 shows the circuit for a word-level multiplexor. This circuit gener- ates a 64-bit word Out equal to one of the two input words, A or B, depending on the control input bit s. The circuit consists of 64 identical subcircuits, each hav- ing a structure similar to the bit-level multiplexor from Figure 4.11. Rather than replicating the bit-level multiplexor 64 times, the word-level version reduces the number of inverters by generating _s once and reusing it at each bit position. (a) Bit-level implementation (b) Word-level abstraction out63 out62 out0 s !s s B A OutMUX int Out = [ s : A; l : B; ]; b63 a63 b62 a62 b0 a0. . . Figure 4.13 Word-level multiplexor circuit. The output will equal input word A when the control signal s is 1, and it will equal B otherwise. Multiplexors are described in HCL using case expressions. 414 Chapter 4 Processor Architecture We will use many forms of multiplexors in our processor designs. They allow us to select a word from a number of sources depending on some control condi- tion. Multiplexing functions are described in HCL using case expressions. A case expression has the following general form: ‰ select1 x expr1y select2 x expr2y l l l selectk x exprky ` The expression contains a series of cases, where each case i consists of a Boolean expression selecti, indicating when this case should be selected, and an integer expression expri, indicating the resulting value. Unlike the ÍÑ©Î²³ statement of C, we do not require the different selection expressions to be mutually exclusive. Logically, the selection expressions are eval- uated in sequence, and the case for the ﬁrst one yielding 1 is selected. For example, the word-level multiplexor of Figure 4.13 can be described in HCL as ÑÇË® ﬂÏÎ { ‰ Íx ¡y ox ¢y `y In this code, the second selection expression is simply o, indicating that this case should be selected if no prior one has been. This is the way to specify a default case in HCL. Nearly all case expressions end in this manner. Allowing nonexclusive selection expressions makes the HCL code more read- able. An actual hardware multiplexor must have mutually exclusive signals con- trolling which input word should be passed to the output, such as the signals s and _s in Figure 4.13. To translate an HCL case expression into hardware, a logic syn- thesis program would need to analyze the set of selection expressions and resolve any possible conﬂicts by making sure that only the ﬁrst matching case would be selected. The selection expressions can be arbitrary Boolean expressions, and there can be an arbitrary number of cases. This allows case expressions to describe blocks where there are many choices of input signals with complex selection criteria. For example, consider the diagram of a 4-way multiplexor shown in Figure 4.14. This circuit selects from among the four input words A, B, C, and D based on the control signals s1 and s0, treating the controls as a 2-bit binary number. We can express this in HCL using Boolean expressions to describe the different combinations of control bit patterns: ÑÇË® ﬂÏÎr { ‰ _Ío dd _Ín x ¡y a nn Section 4.2 Logic Design and the Hardware Control Language HCL 415 Figure 4.14 Four-way multiplexor. The different combinations of control signals s1 and s0 determine which data input is transmitted to the output. D s1 s0 Out4 C B A MUX4 _Ío x¢yano _Ín x£yaon o x⁄yaoo `y The comments on the right (any text starting with a and running for the rest of the line is a comment) show which combination of s1 and s0 will cause the case to be selected. Observe that the selection expressions can sometimes be simpliﬁed, since only the ﬁrst matching case is selected. For example, the second expression can be written _s1, rather than the more complete _s1dd s0, since the only other possibility having s1 equal to 0 was given as the ﬁrst selection expression. Similarly, the third expression can be written as _s0, while the fourth can simply be written as o. As a ﬁnal example, suppose we want to design a logic circuit that ﬁnds the minimum value among a set of words A, B, and C, diagrammed as follows: C B A MIN3 Min3 We can express this using an HCL case expression as ÑÇË® ›©Åq { ‰ ¡z{¢dd¡z{£x¡y ¢z{¡dd¢z{£x¢y ox £y `y Practice Problem 4.11 (solution page 520) The HCL code given for computing the minimum of three words contains four comparison expressions of the form X<= Y . Rewrite the code to compute the same result, but using only three comparisons. 416 Chapter 4 Processor Architecture 0 Y X X \u0003 Y A L U A B 1 Y X X \u0002 Y A L U A B 2 Y X X & Y A L U A B 3 Y X X ^ Y A L U A B Figure 4.15 Arithmetic/logic unit (ALU). Depending on the setting of the function input, the circuit will perform one of four different arithmetic and logical operations. Practice Problem 4.12 (solution page 520) Write HCL code describing a circuit that for word inputs A, B, and C selects the median of the three values. That is, the output equals the word lying between the minimum and maximum of the three inputs. Combinational logic circuits can be designed to perform many different types of operations on word-level data. The detailed design of these is beyond the scope of our presentation. One important combinational circuit, known as an arithmetic/logic unit (ALU), is diagrammed at an abstract level in Figure 4.15. In our version, the circuit has three inputs: two data inputs labeled A and B and a control input. Depending on the setting of the control input, the circuit will perform different arithmetic or logical operations on the data inputs. Observe that the four operations diagrammed for this ALU correspond to the four different integer operations supported by the Y86-64 instruction set, and the control values match the function codes for these instructions (Figure 4.3). Note also the ordering of operands for subtraction, where the A input is subtracted from the B input. This ordering is chosen in anticipation of the ordering of arguments in the ÍÏ¾Ê instruction. 4.2.4 Set Membership In our processor designs, we will ﬁnd many examples where we want to compare one signal against a number of possible matching signals, such as to test whether the code for some instruction being processed matches some category of instruc- tion codes. As a simple example, suppose we want to generate the signals s1 and s0 for the 4-way multiplexor of Figure 4.14 by selecting the high- and low-order bits from a 2-bit signal code, as follows: code s1 s0 D C B A Control MUX4 Out4 Section 4.2 Logic Design and the Hardware Control Language HCL 417 In this circuit, the 2-bit signal code would then control the selection among the four data words A, B, C, and D. We can express the generation of signals s1 and s0 using equality tests based on the possible values of code: ¾ÇÇÄ Ío { ²Ç®− {{ p ÚÚ ²Ç®− {{ qy ¾ÇÇÄ Ín { ²Ç®− {{ o ÚÚ ²Ç®− {{ qy A more concise expression can be written that expresses the property that s1 is 1 when code is in the set {2, 3}, and s0 is 1 when code is in the set {1, 3}: ¾ÇÇÄ Ío { ²Ç®− ©Å Õ pj q Ûy ¾ÇÇÄ Ín { ²Ç®− ©Å Õ oj q Ûy The general form of a set membership test is iexpr ©Å Õiexpr1j iexpr2j ... j iexprkÛ where the value being tested (iexpr) and the candidate matches (iexpr1 through iexprk) are all integer expressions. 4.2.5 Memory and Clocking Combinational circuits, by their very nature, do not store any information. Instead, they simply react to the signals at their inputs, generating outputs equal to some function of the inputs. To create sequential circuits—that is, systems that have state and perform computations on that state—we must introduce devices that store information represented as bits. Our storage devices are all controlled by a single clock, a periodic signal that determines when new values are to be loaded into the devices. We consider two classes of memory devices: Clocked registers (or simply registers) store individual bits or words. The clock signal controls the loading of the register with the value at its input. Random access memories (or simply memories) store multiple words, using an address to select which word should be read or written. Examples of random access memories include (1) the virtual memory system of a processor, where a combination of hardware and operating system software make it appear to a processor that it can access any word within a large address space; and (2) the register ﬁle, where register identiﬁers serve as the addresses. In a Y86-64 processor, the register ﬁle holds the 15 program registers (cËþÓ through cËor). As we can see, the word “register” means two slightly different things when speaking of hardware versus machine-language programming. In hardware, a register is directly connected to the rest of the circuit by its input and output wires. In machine-level programming, the registers represent a small collection of addressable words in the CPU, where the addresses consist of register IDs. These words are generally stored in the register ﬁle, although we will see that the hardware can sometimes pass a word directly from one instruction to another to 418 Chapter 4 Processor Architecture State = x State = y Input = y Output = x Output = yRising clock x y Figure 4.16 Register operation. The register outputs remain held at the current register state until the clock signal rises. When the clock rises, the values at the register inputs are captured to become the new register state. avoid the delay of ﬁrst writing and then reading the register ﬁle. When necessary to avoid ambiguity, we will call the two classes of registers “hardware registers” and “program registers,” respectively. Figure 4.16 gives a more detailed view of a hardware register and how it operates. For most of the time, the register remains in a ﬁxed state (shown as x), generating an output equal to its current state. Signals propagate through the combinational logic preceding the register, creating a new value for the register input (shown as y), but the register output remains ﬁxed as long as the clock is low. As the clock rises, the input signals are loaded into the register as its next state (y), and this becomes the new register output until the next rising clock edge. A key point is that the registers serve as barriers between the combinational logic in different parts of the circuit. Values only propagate from a register input to its output once every clock cycle at the rising clock edge. Our Y86-64 processors will use clocked registers to hold the program counter (PC), the condition codes (CC), and the program status (Stat). The following diagram shows a typical register ﬁle: Register file A B valA valW dstW srcA valB srcB clock Write portWRead ports This register ﬁle has two read ports, named A and B, and one write port, named W. Such a multiported random access memory allows multiple read and write operations to take place simultaneously. In the register ﬁle diagrammed, the circuit can read the values of two program registers and update the state of a third. Each port has an address input, indicating which program register should be selected, and a data output or input giving a value for that program register. The addresses are register identiﬁers, using the encoding shown in Figure 4.4. The two read ports have address inputs srcA and srcB (short for “source A” and “source B”) and data Section 4.2 Logic Design and the Hardware Control Language HCL 419 outputs valA and valB (short for “value A” and “value B”). The write port has address input dstW (short for “destination W”) and data input valW (short for “value W”). The register ﬁle is not a combinational circuit, since it has internal storage. In our implementation, however, data can be read from the register ﬁle as if it were a block of combinational logic having addresses as inputs and the data as outputs. When either srcA or srcB is set to some register ID, then, after some delay, the value stored in the corresponding program register will appear on either valA or valB. For example, setting srcA to 3 will cause the value of program register cË¾Ó to be read, and this value will appear on output valA. The writing of words to the register ﬁle is controlled by the clock signal in a manner similar to the loading of values into a clocked register. Every time the clock rises, the value on input valW is written to the program register indicated by the register ID on input dstW. When dstW is set to the special ID value nÓƒ,no program register is written. Since the register ﬁle can be both read and written, a natural question to ask is, “What happens if the circuit attempts to read and write the same register simultaneously?” The answer is straightforward: if the same register ID is used for both a read port and the write port, then, as the clock rises, there will be a transition on the read port’s data output from the old value to the new. When we incorporate the register ﬁle into our processor design, we will make sure that we take this property into consideration. Our processor has a random access memory for storing program data, as illustrated below: Data memory data out data inaddress error read write clock This memory has a single address input, a data input for writing, and a data output for reading. Like the register ﬁle, reading from our memory operates in a manner similar to combinational logic: If we provide an address on the address input and set the write control signal to 0, then after some delay, the value stored at that address will appear on data out.The error signal will be set to 1 if the address is out of range, and to 0 otherwise. Writing to the memory is controlled by the clock: We set address to the desired address, data in to the desired value, and write to 1. When we then operate the clock, the speciﬁed location in the memory will be updated, as long as the address is valid. As with the read operation, the error signal will be set to 1 if the address is invalid. This signal is generated by combinational logic, since the required bounds checking is purely a function of the address input and does not involve saving any state. 420 Chapter 4 Processor Architecture Aside Real-life memory design The memory system in a full-scale microprocessor is far more complex than the simple one we assume in our design. It consists of several forms of hardware memories, including several random access memories, plus nonvolatile memory or magnetic disk, as well as a variety of hardware and software mechanisms for managing these devices. The design and characteristics of the memory system are described in Chapter 6. Nonetheless, our simple memory design can be used for smaller systems, and it provides us with an abstraction of the interface between the processor and memory for more complex systems. Our processor includes an additional read-only memory for reading instruc- tions. In most actual systems, these memories are merged into a single memory with two ports: one for reading instructions, and the other for reading or writ- ing data. 4.3 Sequential Y86-64 Implementations Now we have the components required to implement a Y86-64 processor. As a ﬁrst step, we describe a processor called SEQ (for “sequential” processor). On each clock cycle, SEQ performs all the steps required to process a complete instruction. This would require a very long cycle time, however, and so the clock rate would be unacceptably low. Our purpose in developing SEQ is to provide a ﬁrst step toward our ultimate goal of implementing an efﬁcient pipelined processor. 4.3.1 Organizing Processing into Stages In general, processing an instruction involves a number of operations. We organize them in a particular sequence of stages, attempting to make all instructions follow a uniform sequence, even though the instructions differ greatly in their actions. The detailed processing at each step depends on the particular instruction being executed. Creating this framework will allow us to design a processor that makes best use of the hardware. The following is an informal description of the stages and the operations performed within them: Fetch. The fetch stage reads the bytes of an instruction from memory, using the program counter (PC) as the memory address. From the instruction it extracts the two 4-bit portions of the instruction speciﬁer byte, referred to as icode (the instruction code) and ifun (the instruction function). It possibly fetches a register speciﬁer byte, giving one or both of the register operand speciﬁers rA and rB. It also possibly fetches an 8-byte constant word valC. It computes valP to be the address of the instruction following the current one in sequential order. That is, valP equals the value of the PC plus the length of the fetched instruction. Section 4.3 Sequential Y86-64 Implementations 421 Decode. The decode stage reads up to two operands from the register ﬁle, giving values valA and/or valB. Typically, it reads the registers designated by instruction ﬁelds rA and rB, but for some instructions it reads register cËÍÉ. Execute. In the execute stage, the arithmetic/logic unit (ALU) either performs the operation speciﬁed by the instruction (according to the value of ifun), computes the effective address of a memory reference, or increments or decrements the stack pointer. We refer to the resulting value as valE.The condition codes are possibly set. For a conditional move instruction, the stage will evaluate the condition codes and move condition (given by ifun) and enable the updating of the destination register only if the condition holds. Similarly, for a jump instruction, it determines whether or not the branch should be taken. Memory. The memory stage may write data to memory, or it may read data from memory. We refer to the value read as valM. Write back. The write-back stage writes up to two results to the register ﬁle. PC update. The PC is set to the address of the next instruction. The processor loops indeﬁnitely, performing these stages. In our simpliﬁed im- plementation, the processor will stop when any exception occurs—that is, when it executes a ³þÄÎ or invalid instruction, or it attempts to read or write an invalid ad- dress. In a more complete design, the processor would enter an exception-handling mode and begin executing special code determined by the type of exception. As can be seen by the preceding description, there is a surprising amount of processing required to execute a single instruction. Not only must we perform the stated operation of the instruction, we must also compute addresses, update stack pointers, and determine the next instruction address. Fortunately, the overall ﬂow can be similar for every instruction. Using a very simple and uniform struc- ture is important when designing hardware, since we want to minimize the total amount of hardware and we must ultimately map it onto the two-dimensional surface of an integrated-circuit chip. One way to minimize the complexity is to have the different instructions share as much of the hardware as possible. For example, each of our processor designs contains a single arithmetic/logic unit that is used in different ways depending on the type of instruction being exe- cuted. The cost of duplicating blocks of logic in hardware is much higher than the cost of having multiple copies of code in software. It is also more difﬁcult to deal with many special cases and idiosyncrasies in a hardware system than with software. Our challenge is to arrange the computing required for each of the different instructions to ﬁt within this general framework. We will use the code shown in Figure 4.17 to illustrate the processing of different Y86-64 instructions. Figures 4.18 through 4.21 contain tables describing how the different Y86-64 instructions proceed through the stages. It is worth the effort to study these tables carefully. They are in a form that enables a straightforward mapping into the hardware. Each line in these tables describes an assignment to some signal or stored state 422 Chapter 4 Processor Architecture 1 nÓnnnx qnðpnwnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bwj cË®Ó 2 nÓnnþx qnðqosnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bpoj cË¾Ó 3 nÓnorx topq Ú ÍÏ¾Ê cË®Ój cË¾Ó a ÍÏ¾ÎËþ²Î 4 nÓnotx qnðrvnnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bopvjcËÍÉ a –ËÇ¾Ä−À rloq 5 nÓnpnx rnrqtrnnnnnnnnnnnnnn Ú ËÀÀÇÌÊ cËÍÉj onnfcË¾Óg a ÍÎÇË− 6 nÓnpþx þnpð Ú ÉÏÍ³Ê cË®Ó a ÉÏÍ³ 7 nÓnp²x ¾nnð Ú ÉÇÉÊ cËþÓ a –ËÇ¾Ä−À rlor 8 nÓnp−x uqrnnnnnnnnnnnnnnn Ú Á− ®ÇÅ− a ﬁÇÎ ÎþÂ−Å 9 nÓnqux vnronnnnnnnnnnnnnn Ú ²þÄÄ ÉËÇ² a –ËÇ¾Ä−À rlov 10 nÓnrnx Ú ®ÇÅ−x 11 nÓnrnx nn Ú ³þÄÎ 12 nÓnrox Ú ÉËÇ²x 13 nÓnrox wn Ú Ë−Î a ‡−ÎÏËÅ 14 Ú Figure 4.17 Sample Y86-64 instruction sequence. We will trace the processing of these instructions through the different stages. (indicated by the assignment operation ‘←’). These should be read as if they were evaluated in sequence from top to bottom. When we later map the computations to hardware, we will ﬁnd that we do not need to perform these evaluations in strict sequential order. Figure 4.18 shows the processing required for instruction types ﬂ–Ê (integer and logical operations), ËËÀÇÌÊ (register-register move), and ©ËÀÇÌÊ (immediate- register move). Let us ﬁrst consider the integer operations. Examining Figure 4.2, we can see that we have carefully chosen an encoding of instructions so that the four integer operations (þ®®Ê, ÍÏ¾Ê, þÅ®Ê, and ÓÇËÊ) all have the same value of icode. We can handle them all by an identical sequence of steps, except that the ALU computation must be set according to the particular instruction operation, encoded in ifun. The processing of an integer-operation instruction follows the general pattern listed above. In the fetch stage, we do not require a constant word, and so valP is computed as PC + 2. During the decode stage, we read both operands. These are supplied to the ALU in the execute stage, along with the function speciﬁer ifun, so that valE becomes the instruction result. This computation is shown as the expression valB OP valA, where OP indicates the operation speciﬁed by ifun. Note the ordering of the two arguments—this order is consistent with the conventions of Y86-64 (and x86-64). For example, the instruction ÍÏ¾Ê cËþÓjcË®Ó is supposed to compute the value R[cË®Ó] − R[cËþÓ]. Nothing happens in the memory stage for these instructions, but valE is written to register rB in the write-back stage, and the PC is set to valP to complete the instruction execution. Executing an ËËÀÇÌÊ instruction proceeds much like an arithmetic operation. We do not need to fetch the second register operand, however. Instead, we set the second ALU input to zero and add this to the ﬁrst, giving valE = valA, which is Section 4.3 Sequential Y86-64 Implementations 423 Stage ﬂ–Ê rAj rB ËËÀÇÌÊ rAj rB ©ËÀÇÌÊ Vj rB Fetch icode : ifun ← M1[PC] icode : ifun ← M1[PC] icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] rA : rB ← M1[PC + 1] rA : rB ← M1[PC + 1] valC ← M8[PC + 2] valP ← PC + 2 valP ← PC + 2 valP ← PC + 10 Decode valA ← R[rA] valA ← R[rA] valB ← R[rB] Execute valE ← valB OP valA valE ← 0 + valA valE ← 0 + valC Set CC Memory Write back R[rB] ← valE R[rB] ← valE R[rB] ← valE PC update PC ← valP PC ← valP PC ← valP Figure 4.18 Computations in sequential implementation of Y86-64 instructions ﬂ–Ê, ËËÀÇÌÊ, and ©ËÀÇÌÊ. These instructions compute a value and store the result in a register. The notation icode x ifun indicates the two components of the instruction byte, while rA x rB indicates the two components of the register speciﬁer byte. The notation M1[x] indicates accessing (either reading or writing) 1 byte at memory location x, while M8[x] indicates accessing 8 bytes. then written to the register ﬁle. Similar processing occurs for ©ËÀÇÌÊ, except that we use constant value valC for the ﬁrst ALU input. In addition, we must increment the program counter by 10 for ©ËÀÇÌÊ due to the long instruction format. Neither of these instructions changes the condition codes. Practice Problem 4.13 (solution page 521) Fill in the right-hand column of the following table to describe the processing of the ©ËÀÇÌÊ instruction on line 4 of the object code in Figure 4.17: Generic Speciﬁc Stage ©ËÀÇÌÊ Vj rB ©ËÀÇÌÊ bopvj cËÍÉ Fetch icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] valC ← M8[PC + 2] valP ← PC + 10 Decode Execute valE ← 0 + valC 424 Chapter 4 Processor Architecture Aside Tracing the execution of a ÍÏ¾Ê instruction As an example, let us follow the processing of the ÍÏ¾Ê instruction on line 3 of the object code shown in Figure 4.17. We can see that the previous two instructions initialize registers cË®Ó and cË¾Ó to 9 and 21, respectively. We can also see that the instruction is located at address nÓnor and consists of 2 bytes, having values nÓto and nÓpq. The stages would proceed as shown in the following table, which lists the generic rule for processing an ﬂ–Ê instruction (Figure 4.18) on the left, and the computations for this speciﬁc instruction on the right. Stage ﬂ–Ê rAj rB ÍÏ¾Ê cË®Ój cË¾Ó Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnor] = txo rA : rB ← M1[PC + 1] rA : rB ← M1[nÓnos] = pxq valP ← PC + 2 valP ← nÓnor + 2 = nÓnot Decode valA ← R[rA] valA ← R[cË®Ó] = w valB ← R[rB] valB ← R[cË¾Ó] = po Execute valE ← valB OP valA valE ← po − w = op Set CC …ƒ ← n, ·ƒ ← n, ﬂƒ ← n Memory Write back R[rB] ← valE R[cË¾Ó] ← valE = op PC update PC ← valP PC ← valP = nÓnot As this trace shows, we achieve the desired effect of setting register cË¾Ó to 12, setting all three condition codes to zero, and incrementing the PC by 2. Generic Speciﬁc Stage ©ËÀÇÌÊ Vj rB ©ËÀÇÌÊ bopvj cËÍÉ Memory Write back R[rB] ← valE PC update PC ← valP How does this instruction execution modify the registers and the PC? Figure 4.19 shows the processing required for the memory write and read in- structions ËÀÀÇÌÊ and ÀËÀÇÌÊ. We see the same basic ﬂow as before, but using the ALU to add valC to valB, giving the effective address (the sum of the displacement and the base register value) for the memory operation. In the memory stage, we either write the register value valA to memory or read valM from memory. Section 4.3 Sequential Y86-64 Implementations 425 Stage ËÀÀÇÌÊ rAj DfrBg ÀËÀÇÌÊ DfrBgj rA Fetch icode : ifun ← M1[PC] icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] rA : rB ← M1[PC + 1] valC ← M8[PC + 2] valC ← M8[PC + 2] valP ← PC + 10 valP ← PC + 10 Decode valA ← R[rA] valB ← R[rB] valB ← R[rB] Execute valE ← valB + valC valE ← valB + valC Memory M8[valE] ← valA valM ← M8[valE] Write back R[rA] ← valM PC update PC ← valP PC ← valP Figure 4.19 Computations in sequential implementation of Y86-64 instructions ËÀÀÇÌÊ and ÀËÀÇÌÊ. These instructions read or write memory. Figure 4.20 shows the steps required to process ÉÏÍ³Ê and ÉÇÉÊ instructions. These are among the most difﬁcult Y86-64 instructions to implement, because they involve both accessing memory and incrementing or decrementing the stack pointer. Although the two instructions have similar ﬂows, they have important differences. The ÉÏÍ³Ê instruction starts much like our previous instructions, but in the decode stage we use cËÍÉ as the identiﬁer for the second register operand, giving the stack pointer as value valB. In the execute stage, we use the ALU to decrement the stack pointer by 8. This decremented value is used for the memory write address and is also stored back to cËÍÉ in the write-back stage. By using valE as the address for the write operation, we adhere to the Y86-64 (and x86-64) convention that ÉÏÍ³Ê should decrement the stack pointer before writing, even though the actual updating of the stack pointer does not occur until after the memory operation has completed. The ÉÇÉÊ instruction proceeds much like ÉÏÍ³Ê, except that we read two copies of the stack pointer in the decode stage. This is clearly redundant, but we will see that having the stack pointer as both valA and valB makes the subsequent ﬂow more similar to that of other instructions, enhancing the overall uniformity of the design. We use the ALU to increment the stack pointer by 8 in the execute stage, but use the unincremented value as the address for the memory operation. In the write-back stage, we update both the stack pointer register with the incre- mented stack pointer and register rA with the value read from memory. Using the unincremented stack pointer as the memory read address preserves the Y86-64 426 Chapter 4 Processor Architecture Aside Tracing the execution of an ËÀÀÇÌÊ instruction Let us trace the processing of the ËÀÀÇÌÊ instruction on line 5 of the object code shown in Figure 4.17. We can see that the previous instruction initialized register cËÍÉ to 128, while cË¾Ó still holds 12, as computed by the ÍÏ¾Ê instruction (line 3). We can also see that the instruction is located at address nÓnpn and consists of 10 bytes. The ﬁrst 2 bytes have values nÓrn and nÓrq, while the ﬁnal 8 bytes are a byte-reversed version of the number nÓnnnnnnnnnnnnnntr (decimal 100). The stages would proceed as follows: Generic Speciﬁc Stage ËÀÀÇÌÊ rAj DfrBg ËÀÀÇÌÊ cËÍÉj onnfcË¾Óg Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnpn] = rxn rA : rB ← M1[PC + 1] rA : rB ← M1[nÓnpo] = rxq valC ← M8[PC + 2] valC ← M8[nÓnpp] = onn valP ← PC + 10 valP ← nÓnpn + 10 = nÓnpþ Decode valA ← R[rA] valA ← R[cËÍÉ] = opv valB ← R[rB] valB ← R[cË¾Ó]= op Execute valE ← valB + valC valE ← op + onn = oop Memory M8[valE] ← valA M8[oop] ← opv Write back PC update PC ← valP PC ← nÓnpþ As this trace shows, the instruction has the effect of writing 128 to memory address 112 and incrementing the PC by 10. (and x86-64) convention that ÉÇÉÊ should ﬁrst read memory and then increment the stack pointer. Practice Problem 4.14 (solution page 522) Fill in the right-hand column of the following table to describe the processing of the ÉÇÉÊ instruction on line 7 of the object code in Figure 4.17. Generic Speciﬁc Stage ÉÇÉÊ rA ÉÇÉÊ cËþÓ Fetch icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] valP ← PC + 2 Section 4.3 Sequential Y86-64 Implementations 427 Stage ÉÏÍ³Ê rA ÉÇÉÊ rA Fetch icode : ifun ← M1[PC] icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] rA : rB ← M1[PC + 1] valP ← PC + 2 valP ← PC + 2 Decode valA ← R[rA] valA ← R[cËÍÉ] valB ← R[cËÍÉ] valB ← R[cËÍÉ] Execute valE ← valB + (−8) valE ← valB + 8 Memory M8[valE] ← valA valM ← M8[valA] Write back R[cËÍÉ] ← valE R[cËÍÉ] ← valE R[rA] ← valM PC update PC ← valP PC ← valP Figure 4.20 Computations in sequential implementation of Y86-64 instructions ÉÏÍ³Ê and ÉÇÉÊ. These instructions push and pop the stack. Generic Speciﬁc Stage ÉÇÉÊ rA ÉÇÉÊ cËþÓ Decode valA ← R[cËÍÉ] valB ← R[cËÍÉ] Execute valE ← valB + 8 Memory valM ← M8[valA] Write back R[cËÍÉ] ← valE R[rA] ← valM PC update PC ← valP What effect does this instruction execution have on the registers and the PC? Practice Problem 4.15 (solution page 522) What would be the effect of the instruction ÉÏÍ³Ê cËÍÉ according to the steps listed in Figure 4.20? Does this conform to the desired behavior for Y86-64, as determined in Problem 4.7? 428 Chapter 4 Processor Architecture Aside Tracing the execution of a ÉÏÍ³Ê instruction Let us trace the processing of the ÉÏÍ³Ê instruction on line 6 of the object code shown in Figure 4.17. At this point, we have 9 in register cË®Ó and 128 in register cËÍÉ. We can also see that the instruction is located at address nÓnpþ and consists of 2 bytes having values nÓþn and nÓpð. The stages would proceed as follows: Generic Speciﬁc Stage ÉÏÍ³Ê rA ÉÏÍ³Ê cË®Ó Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnpþ] = þxn rA : rB ← M1[PC + 1] rA : rB ← M1[nÓnp¾] = pxð valP ← PC + 2 valP ← nÓnpþ + 2 = nÓnp² Decode valA ← R[rA] valA ← R[cË®Ó] = w valB ← R[cËÍÉ] valB ← R[cËÍÉ] = opv Execute valE ← valB + (−8) valE ← opv + (−8) = opn Memory M8[valE] ← valA M8[opn] ← w Write back R[cËÍÉ] ← valE R[cËÍÉ] ← opn PC update PC ← valP PC ← nÓnp² As this trace shows, the instruction has the effect of setting cËÍÉ to 120, writing 9 to address 120, and incrementing the PC by 2. Practice Problem 4.16 (solution page 522) Assume the two register writes in the write-back stage for ÉÇÉÊ occur in the order listed in Figure 4.20. What would be the effect of executing ÉÇÉÊ cËÍÉ? Does this conform to the desired behavior for Y86-64, as determined in Problem 4.8? Figure 4.21 indicates the processing of our three control transfer instructions: the different jumps, ²þÄÄ, and Ë−Î. We see that we can implement these instruc- tions with the same overall ﬂow as the preceding ones. As with integer operations, we can process all of the jumps in a uniform manner, since they differ only when determining whether or not to take the branch. A jump instruction proceeds through fetch and decode much like the previous instructions, except that it does not require a register speciﬁer byte. In the execute stage, we check the condition codes and the jump condition to de- termine whether or not to take the branch, yielding a 1-bit signal Cnd. During the PC update stage, we test this ﬂag and set the PC to valC (the jump target) if the ﬂag is 1 and to valP (the address of the following instruction) if the ﬂag is 0. Our notation x ? a : b is similar to the conditional expression in C—it yields a when x is 1 and b when x is 0. Section 4.3 Sequential Y86-64 Implementations 429 Stage Á”” Dest ²þÄÄ Dest Ë−Î Fetch icode : ifun ← M1[PC] icode : ifun ← M1[PC] icode : ifun ← M1[PC] valC ← M8[PC + 1] valC ← M8[PC + 1] valP ← PC + 9 valP ← PC + 9 valP ← PC + 1 Decode valA ← R[cËÍÉ] valB ← R[cËÍÉ] valB ← R[cËÍÉ] Execute valE ← valB + (−8) valE ← valB + 8 Cnd ← Cond(CC, ifun) Memory M8[valE] ← valP valM ← M8[valA] Write back R[cËÍÉ] ← valE R[cËÍÉ] ← valE PC update PC ← Cnd ? valC : valP PC ← valC PC ← valM Figure 4.21 Computations in sequential implementation of Y86-64 instructions Á””, ²þÄÄ, and Ë−Î. These instructions cause control transfers. Practice Problem 4.17 (solution page 522) We can see by the instruction encodings (Figures 4.2 and 4.3) that the ËËÀÇÌÊ instruction is the unconditional version of a more general class of instructions that include the conditional moves. Show how you would modify the steps for the ËËÀÇÌÊ instruction below to also handle the six conditional move instructions. You may ﬁnd it useful to see how the implementation of the Á”” instructions (Figure 4.21) handles conditional behavior. Stage ²ÀÇÌ”” rAj rB Fetch icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] valP ← PC + 2 Decode valA ← R[rA] Execute valE ← 0 + valA Memory Write back R[rB] ← valE PC update PC ← valP 430 Chapter 4 Processor Architecture Aside Tracing the execution of a Á− instruction Let us trace the processing of the Á− instruction on line 8 of the object code shown in Figure 4.17. The condition codes were all set to zero by the ÍÏ¾Ê instruction (line 3), and so the branch will not be taken. The instruction is located at address nÓnp− and consists of 9 bytes. The ﬁrst has value nÓuq, while the remaining 8 bytes are a byte-reversed version of the number nÓnnnnnnnnnnnnnnrn, the jump target. The stages would proceed as follows: Generic Speciﬁc Stage Á”” Dest Á− nÓnrn Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnp−] = uxq valC ← M8[PC + 1] valC ← M8[nÓnpð] = nÓnrn valP ← PC + 9 valP ← nÓnp− + 9 = nÓnqu Decode Execute Cnd ← Cond(CC, ifun) Cnd ← Cond(⟨0, 0, 0⟩, q) = 0 Memory Write back PC update PC ← Cnd ? valC : valP PC ← 0? nÓnrn : nÓnqu = nÓnqu As this trace shows, the instruction has the effect of incrementing the PC by 9. Instructions ²þÄÄ and Ë−Î bear some similarity to instructions ÉÏÍ³Ê and ÉÇÉÊ, except that we push and pop program counter values. With instruction ²þÄÄ,we push valP, the address of the instruction that follows the ²þÄÄ instruction. During the PC update stage, we set the PC to valC, the call destination. With instruction Ë−Î, we assign valM, the value popped from the stack, to the PC in the PC update stage. Practice Problem 4.18 (solution page 523) Fill in the right-hand column of the following table to describe the processing of the ²þÄÄ instruction on line 9 of the object code in Figure 4.17: Generic Speciﬁc Stage ²þÄÄ Dest ²þÄÄ nÓnro Fetch icode : ifun ← M1[PC] valC ← M8[PC + 1] valP ← PC + 9 Section 4.3 Sequential Y86-64 Implementations 431 Aside Tracing the execution of a Ë−Î instruction Let us trace the processing of the Ë−Î instruction on line 13 of the object code shown in Figure 4.17. The instruction address is nÓnro and is encoded by a single byte nÓwn. The previous ²þÄÄ instruction set cËÍÉ to 120 and stored the return address nÓnrn at memory address 120. The stages would proceed as follows: Generic Speciﬁc Stage Ë−Î Ë−Î Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnro] = wxn valP ← PC + 1 valP ← nÓnro + 1 = nÓnrp Decode valA ← R[cËÍÉ] valA ← R[cËÍÉ] = opn valB ← R[cËÍÉ] valB ← R[cËÍÉ] = opn Execute valE ← valB + 8 valE ← opn + 8 = opv Memory valM ← M8[valA] valM ← M8[opn] = nÓnrn Write back R[cËÍÉ] ← valE R[cËÍÉ] ← opv PC update PC ← valM PC ← nÓnrn As this trace shows, the instruction has the effect of setting the PC to nÓnrn, the address of the ³þÄÎ instruction. It also sets cËÍÉ to 128. Generic Speciﬁc Stage ²þÄÄ Dest ²þÄÄ nÓnro Decode valB ← R[cËÍÉ] Execute valE ← valB + (−8) Memory M8[valE] ← valP Write back R[cËÍÉ] ← valE PC update PC ← valC What effect would this instruction execution have on the registers, the PC, and the memory? We have created a uniform framework that handles all of the different types of Y86-64 instructions. Even though the instructions have widely varying behavior, we can organize the processing into six stages. Our task now is to create a hardware design that implements the stages and connects them together. 432 Chapter 4 Processor Architecture 4.3.2 SEQ Hardware Structure The computations required to implement all of the Y86-64 instructions can be or- ganized as a series of six basic stages: fetch, decode, execute, memory, write back, and PC update. Figure 4.22 shows an abstract view of a hardware structure that can perform these computations. The program counter is stored in a register, shown in the lower left-hand corner (labeled “PC”). Information then ﬂows along wires (shown grouped together as a heavy gray line), ﬁrst upward and then around to the right. Processing is performed by hardware units associated with the different stages. The feedback paths coming back down on the right-hand side contain the updated values to write to the register ﬁle and the updated program counter. In SEQ, all of the processing by the hardware units occurs within a single clock cycle, as is discussed in Section 4.3.3. This diagram omits some small blocks of combi- national logic as well as all of the control logic needed to operate the different hardware units and to route the appropriate values to the units. We will add this detail later. Our method of drawing processors with the ﬂow going from bottom to top is unconventional. We will explain the reason for this convention when we start designing pipelined processors. The hardware units are associated with the different processing stages: Fetch. Using the program counter register as an address, the instruction mem- ory reads the bytes of an instruction. The PC incrementer computes valP, the incremented program counter. Decode. The register ﬁle has two read ports, A and B, via which register values valA and valB are read simultaneously. Execute. The execute stage uses the arithmetic/logic (ALU) unit for different purposes according to the instruction type. For integer operations, it per- forms the speciﬁed operation. For other instructions, it serves as an adder to compute an incremented or decremented stack pointer, to compute an effective address, or simply to pass one of its inputs to its outputs by adding zero. The condition code register (CC) holds the three condition code bits. New values for the condition codes are computed by the ALU. When executing a conditional move instruction, the decision as to whether or not to update the destination register is computed based on the condition codes and move condition. Similarly, when executing a jump instruction, the branch signal Cnd is computed based on the condition codes and the jump type. Memory. The data memory reads or writes a word of memory when executing a memory instruction. The instruction and data memories access the same memory locations, but for different purposes. Write back. The register ﬁle has two write ports. Port E is used to write values computed by the ALU, while port M is used to write values read from the data memory. Section 4.3 Sequential Y86-64 Implementations 433 AB M E PC update Write back Memory Execute newPC valE, valM valM Data memory Addr, Data valE CC ALUCnd aluA, aluB valA, valB srcA, srcB dstE, dstM Register file valP Decode icode, ifun rA, rB valC Fetch Instruction memory PC increment PC Figure 4.22 Abstract view of SEQ, a sequential implementation. The information processed during execution of an instruction follows a clockwise ﬂow starting with an instruction fetch using the program counter (PC), shown in the lower left-hand corner of the ﬁgure. 434 Chapter 4 Processor Architecture PC update. The new value of the program counter is selected to be either valP, the address of the next instruction, valC, the destination address speciﬁed by a call or jump instruction, or valM, the return address read from memory. Figure 4.23 gives a more detailed view of the hardware required to implement SEQ (although we will not see the complete details until we examine the individual stages). We see the same set of hardware units as earlier, but now the wires are shown explicitly. In this ﬁgure, as well as in our other hardware diagrams, we use the following drawing conventions: . Clocked registers are shown as white rectangles. The program counter PC is the only clocked register in SEQ. . Hardware units are shown as light blue boxes. These include the memories, the ALU, and so forth. We will use the same basic set of units for all of our processor implementations. We will treat these units as “black boxes” and not go into their detailed designs. . Control logic blocks are drawn as gray rounded rectangles. These blocks serve to select from among a set of signal sources or to compute some Boolean func- tion. We will examine these blocks in complete detail, including developing HCL descriptions. . Wire names are indicated in white circles. These are simply labels on the wires, not any kind of hardware element. . Word-wide data connections are shown as medium lines. Each of these lines actually represents a bundle of 64 wires, connected in parallel, for transferring a word from one part of the hardware to another. . Byte and narrower data connections are shown as thin lines. Each of these lines actually represents a bundle of four or eight wires, depending on what type of values must be carried on the wires. . Single-bit connections are shown as dotted lines.These represent control values passed between the units and blocks on the chip. All of the computations we have shown in Figures 4.18 through 4.21 have the property that each line represents either the computation of a speciﬁc value, such as valP, or the activation of some hardware unit, such as the memory. These com- putations and actions are listed in the second column of Figure 4.24. In addition to the signals we have already described, this list includes four register ID signals: srcA, the source of valA; srcB, the source of valB; dstE, the register to which valE gets written; and dstM, the register to which valM gets written. The two right-hand columns of this ﬁgure show the computations for the ﬂ–Ê and ÀËÀÇÌÊ instructions to illustrate the values being computed. To map the computations into hardware, we want to implement control logic that will transfer the data between the different hardware units and operate these units in such a way that the speciﬁed operations are performed for each of the different instruction types. That is the purpose of the control logic blocks, shown as gray rounded boxes Section 4.3 Sequential Y86-64 Implementations 435 Stat PC update Memory Execute Decode Fetch newPC New PC data out dmem_error read write Data memory Addr Data Mem. control Cnd valE valM Stat CC ALU ALU fun. ALU A ALU B valA valB dstE dstM srcA srcB dstE dstM srcA srcB Register file Write back AB E M icode instr_valid imem_error ifun rA rB valC valP PC increment Instruction memory PC Figure 4.23 Hardware structure of SEQ, a sequential implementation. Some of the control signals, as well as the register and control word connections, are not shown. 436 Chapter 4 Processor Architecture Stage Computation ﬂ–Ê rAj rB ÀËÀÇÌÊ DfrBgj rA Fetch icode, ifun icode : ifun ← M1[PC] icode : ifun ← M1[PC] rA, rB rA : rB ← M1[PC + 1] rA : rB ← M1[PC + 1] valC valC ← M8[PC + 2] valP valP ← PC + 2 valP ← PC + 10 Decode valA, srcA valA ← R[rA] valB, srcB valB ← R[rB] valB ← R[rB] Execute valE valE ← valB OP valA valE ← valB + valC Cond. codes Set CC Memory Read/write valM ← M8[valE] Write back E port, dstE R[rB] ← valE M port, dstM R[rA] ← valM PC update PC PC ← valP PC ← valP Figure 4.24 Identifying the different computation steps in the sequential imple- mentation. The second column identiﬁes the value being computed or the operation being performed in the stages of SEQ. The computations for instructions ﬂ–Ê and ÀËÀÇÌÊ are shown as examples of the computations. in Figure 4.23. Our task is to proceed through the individual stages and create detailed designs for these blocks. 4.3.3 SEQ Timing In introducing the tables of Figures 4.18 through 4.21, we stated that they should be read as if they were written in a programming notation, with the assignments performed in sequence from top to bottom. On the other hand, the hardware structure of Figure 4.23 operates in a fundamentally different way, with a single clock transition triggering a ﬂow through combinational logic to execute an entire instruction. Let us see how the hardware can implement the behavior listed in these tables. Our implementation of SEQ consists of combinational logic and two forms of memory devices: clocked registers (the program counter and condition code register) and random access memories (the register ﬁle, the instruction memory, and the data memory). Combinational logic does not require any sequencing or control—values propagate through a network of logic gates whenever the inputs change. As we have described, we also assume that reading from a random access memory operates much like combinational logic, with the output word generated based on the address input. This is a reasonable assumption for smaller Section 4.3 Sequential Y86-64 Implementations 437 memories (such as the register ﬁle), and we can mimic this effect for larger circuits using special clock circuits. Since our instruction memory is only used to read instructions, we can therefore treat this unit as if it were combinational logic. We are left with just four hardware units that require an explicit control over their sequencing—the program counter, the condition code register, the data memory, and the register ﬁle. These are controlled via a single clock signal that triggers the loading of new values into the registers and the writing of values to the random access memories. The program counter is loaded with a new instruction address every clock cycle. The condition code register is loaded only when an integer operation instruction is executed. The data memory is written only when an ËÀÀÇÌÊ, ÉÏÍ³Ê,or ²þÄÄ instruction is executed. The two write ports of the register ﬁle allow two program registers to be updated on every cycle, but we can use the special register ID nÓƒ as a port address to indicate that no write should be performed for this port. This clocking of the registers and memories is all that is required to control the sequencing of activities in our processor. Our hardware achieves the same effect as would a sequential execution of the assignments shown in the tables of Figures 4.18 through 4.21, even though all of the state updates actually occur simultaneously and only as the clock rises to start the next cycle. This equivalence holds because of the nature of the Y86-64 instruction set, and because we have organized the computations in such a way that our design obeys the following principle: principle: No reading back The processor never needs to read back the state updated by an instruction in order to complete the processing of this instruction. This principle is crucial to the success of our implementation. As an illustra- tion, suppose we implemented the ÉÏÍ³Ê instruction by ﬁrst decrementing cËÍÉ by 8 and then using the updated value of cËÍÉ as the address of a write operation. This approach would violate the principle stated above. It would require reading the updated stack pointer from the register ﬁle in order to perform the memory operation. Instead, our implementation (Figure 4.20) generates the decremented value of the stack pointer as the signal valE and then uses this signal both as the data for the register write and the address for the memory write. As a result, it can perform the register and memory writes simultaneously as the clock rises to begin the next clock cycle. As another illustration of this principle, we can see that some instructions (the integer operations) set the condition codes, and some instructions (the conditional move and jump instructions) read these condition codes, but no instruction must both set and then read the condition codes. Even though the condition codes are not set until the clock rises to begin the next clock cycle, they will be updated before any instruction attempts to read them. Figure 4.25 shows how the SEQ hardware would process the instructions at lines 3 and 4 in the following code sequence, shown in assembly code with the instruction addresses listed on the left: 438 Chapter 4 Processor Architecture 1 nÓnnnx ©ËÀÇÌÊ bnÓonnjcË¾Ó a cË¾Ó zkk nÓonn 2 nÓnnþx ©ËÀÇÌÊ bnÓpnnjcË®Ó a cË®Ó zkk nÓpnn 3 nÓnorx þ®®Ê cË®ÓjcË¾Ó a cË¾Ó zkk nÓqnn ££ zkk nnn 4 nÓnotx Á− ®−ÍÎ a ﬁÇÎ ÎþÂ−Å 5 nÓnoðx ËÀÀÇÌÊ cË¾ÓjnfcË®Óg a ›‰nÓpnn` zkk nÓqnn 6 nÓnpwx ®−ÍÎx ³þÄÎ Each of the diagrams labeled 1 through 4 shows the four state elements plus the combinational logic and the connections among the state elements. We show the combinational logic as being wrapped around the condition code register, because some of the combinational logic (such as the ALU) generates the input to the condition code register, while other parts (such as the branch computation and the PC selection logic) have the condition code register as input. We show the register ﬁle and the data memory as having separate connections for reading and writing, since the read operations propagate through these units as if they were combinational logic, while the write operations are controlled by the clock. The color coding in Figure 4.25 indicates how the circuit signals relate to the different instructions being executed. We assume the processing starts with the condition codes, listed in the order …ƒ, ·ƒ, and ﬂƒ, set to onn. At the beginning of clock cycle 3 (point 1), the state elements hold the state as updated by the second ©ËÀÇÌÊ instruction (line 2 of the listing), shown in light gray. The combinational logic is shown in white, indicating that it has not yet had time to react to the changed state. The clock cycle begins with address nÓnor loaded into the program counter. This causes the þ®®Ê instruction (line 3 of the listing), shown in blue, to be fetched and processed. Values ﬂow through the combinational logic, including the reading of the random access memories. By the end of the cycle (point 2), the combinational logic has generated new values (nnn) for the condition codes, an update for program register cË¾Ó, and a new value (nÓnot) for the program counter. At this point, the combinational logic has been updated according to the þ®®Ê instruction (shown in blue), but the state still holds the values set by the second ©ËÀÇÌÊ instruction (shown in light gray). As the clock rises to begin cycle 4 (point 3), the updates to the program counter, the register ﬁle, and the condition code register occur, and so we show these in blue, but the combinational logic has not yet reacted to these changes, and so we show this in white. In this cycle, the Á− instruction (line 4 in the listing), shown in dark gray, is fetched and executed. Since condition code …ƒ is 0, the branch is not taken. By the end of the cycle (point 4), a new value of nÓnoð has been generated for the program counter. The combinational logic has been updated according to the Á− instruction (shown in dark gray), but the state still holds the values set by the þ®®Ê instruction (shown in blue) until the next cycle begins. As this example illustrates, the use of a clock to control the updating of the state elements, combined with the propagation of values through combinational logic, sufﬁces to control the computations performed for each instruction in our implementation of SEQ. Every time the clock transitions from low to high, the processor begins executing a new instruction. Section 4.3 Sequential Y86-64 Implementations 439 Clock Cycle 1 Cycle 1: Cycle 2: Cycle 3: Cycle 4: Cycle 5: Beginning of cycle 3 End of cycle 3 Cycle 2 Cycle 3 Cycle 4 1 1 2 2 34 0x000: irmovq $0x100,%rbx # %rbx <-- 0x100 0x00a: irmovq $0x200,%rdx # %rdx <-- 0x200 0x014: addq %rdx,%rbx # %rbx <-- 0x300 CC <-- 000 0x016: je dest # Not taken 0x01f: rmmovq %rbx,0(%rdx) # M[0x200] <-- 0x300 Combinational logic Read Read ports Write Data memory Combinational logic Read Read ports Write ports Write %rbx 0x300 Beginning of cycle 4 End of cycle 43 4 Combinational logic CC 000 Read Read ports Write ports Write Combinational logic CC 000 Read Read ports Write ports Write 000 0x016 0x01f Write ports Register file %rbx = 0x100 PC 0x014 CC 100 PC 0x016 CC 100 PC 0x014 Register file %rbx = 0x100 Data memory Data memory Register file %rbx = 0x300 PC 0x016 Register file %rbx = 0x300 Data memory Figure 4.25 Tracing two cycles of execution by SEQ. Each cycle begins with the state elements (program counter, condition code register, register ﬁle, and data memory) set according to the previous instruction. Signals propagate through the combinational logic, creating new values for the state elements. These values are loaded into the state elements to start the next cycle. 440 Chapter 4 Processor Architecture 4.3.4 SEQ Stage Implementations In this section, we devise HCL descriptions for the control logic blocks required to implement SEQ. A complete HCL description for SEQ is given in Web Aside arch:hcl on page 508. We show some example blocks here, and others are given as practice problems. We recommend that you work these problems as a way to check your understanding of how the blocks relate to the computational requirements of the different instructions. Part of the HCL description of SEQ that we do not include here is a deﬁnition of the different integer and Boolean signals that can be used as arguments to the HCL operations. These include the names of the different hardware signals, as well as constant values for the different instruction codes, function codes, register names, ALU operations, and status codes. Only those that must be explicitly Name Value (hex) Meaning '¤¡‹¶ n Code for ³þÄÎ instruction 'ﬁﬂ– o Code for ÅÇÉ instruction '‡‡›ﬂ‚† p Code for ËËÀÇÌÊ instruction ''‡›ﬂ‚† q Code for ©ËÀÇÌÊ instruction '‡››ﬂ‚† r Code for ËÀÀÇÌÊ instruction '›‡›ﬂ‚† s Code for ÀËÀÇÌÊ instruction 'ﬂ–‹ t Code for integer operation instructions '“”” u Code for jump instructions '£¡‹‹ v Code for ²þÄÄ instruction '‡¥¶ w Code for Ë−Î instruction '–•·¤† ¡ Code for ÉÏÍ³Ê instruction '–ﬂ–† ¢ Code for ÉÇÉÊ instruction ƒﬁﬂﬁ¥ n Default function code ‡¥·– r Register ID for cËÍÉ ‡ﬁﬂﬁ¥ ƒ Indicates no register ﬁle access ¡‹•¡⁄⁄ n Function for addition operation ·¡ﬂ« o Status code for normal operation ·¡⁄‡ p Status code for address exception ·'ﬁ· q Status code for illegal instruction exception ·¤‹¶ r Status code for ³þÄÎ Figure 4.26 Constant values used in HCL descriptions. These values represent the encodings of the instructions, function codes, register IDs, ALU operations, and status codes. Section 4.3 Sequential Y86-64 Implementations 441 Figure 4.27 SEQ fetch stage. Six bytes are read from the instruction memory using the PC as the starting address. From these bytes, we generate the different instruction ﬁelds. The PC increment block computes signal valP. icode ifun rA rB valC valP Need valC Need regids PC increment Align Bytes 1–9Byte 0 imem_error Instruction memory PC Split Instr valid icode ifun referenced in the control logic are shown. The constants we use are documented in Figure 4.26. By convention, we use uppercase names for constant values. In addition to the instructions shown in Figures 4.18 to 4.21, we include the processing for the ÅÇÉ and ³þÄÎ instructions. The ÅÇÉ instruction simply ﬂows through stages without much processing, except to increment the PC by 1. The ³þÄÎ instruction causes the processor status to be set to ¤‹¶, causing it to halt operation. Fetch Stage As shown in Figure 4.27, the fetch stage includes the instruction memory hardware unit. This unit reads 10 bytes from memory at a time, using the PC as the address of the ﬁrst byte (byte 0). This byte is interpreted as the instruction byte and is split (by the unit labeled “Split”) into two 4-bit quantities. The control logic blocks labeled “icode” and “ifun” then compute the instruction and function codes as equaling either the values read from memory or, in the event that the instruction address is not valid (as indicated by the signal imem_error), the values corresponding to a ÅÇÉ instruction. Based on the value of icode, we can compute three 1-bit signals (shown as dashed lines): instr_valid. Does this byte correspond to a legal Y86-64 instruction? This signal is used to detect an illegal instruction. need_regids. Does this instruction include a register speciﬁer byte? need_valC. Does this instruction include a constant word? The signals instr_valid and imem_error (generated when the instruction address is out of bounds) are used to generate the status code in the memory stage. 442 Chapter 4 Processor Architecture As an example, the HCL description for need_regids simply determines whether the value of icode is one of the instructions that has a register speci- ﬁer byte: ¾ÇÇÄ Å−−®ˆË−×©®Í { ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j 'ﬂ–†j '–•·¤†j '–ﬂ–†j ''‡›ﬂ‚†j '‡››ﬂ‚†j '›‡›ﬂ‚† Ûy Practice Problem 4.19 (solution page 523) Write HCL code for the signal Å−−®ˆÌþÄ£ in the SEQ implementation. As Figure 4.27 shows, the remaining 9 bytes read from the instruction memory encode some combination of the register speciﬁer byte and the constant word. These bytes are processed by the hardware unit labeled “Align” into the register ﬁelds and the constant word. Byte 1 is split into register speciﬁers rA and rB when the computed signal need_regids is 1. If need_regids is 0, both register speciﬁers are set to nÓƒ (‡ﬁﬂﬁ¥), indicating there are no registers speciﬁed by this instruction. Recall also (Figure 4.2) that for any instruction having only one register operand, the other ﬁeld of the register speciﬁer byte will be nÓƒ (‡ﬁﬂﬁ¥). Thus, we can assume that the signals rA and rB either encode registers we want to access or indicate that register access is not required. The unit labeled “Align” also generates the constant word valC. This will either be bytes 1–8 or bytes 2–9, depending on the value of signal need_regids. The PC incrementer hardware unit generates the signal valP, based on the current value of the PC, and the two signals need_regids and need_valC. For PC value p, need_regids value r, and need_valC value i, the incrementer generates the value p + 1 + r + 8i. Decode and Write-Back Stages Figure 4.28 provides a detailed view of logic that implements both the decode and write-back stages in SEQ. These two stages are combined because they both access the register ﬁle. The register ﬁle has four ports. It supports up to two simultaneous reads (on ports A and B) and two simultaneous writes (on ports E and M). Each port has both an address connection and a data connection, where the address connection is a register ID, and the data connection is a set of 64 wires serving as either an output word (for a read port) or an input word (for a write port) of the register ﬁle. The two read ports have address inputs srcA and srcB, while the two write ports have address inputs dstE and dstM. The special identiﬁer nÓƒ (‡ﬁﬂﬁ¥)onan address port indicates that no register should be accessed. The four blocks at the bottom of Figure 4.28 generate the four different register IDs for the register ﬁle, based on the instruction code icode, the register speciﬁers rA and rB, and possibly the condition signal Cnd computed in the execute stage. Register ID srcA indicates which register should be read to generate valA. Section 4.3 Sequential Y86-64 Implementations 443 Figure 4.28 SEQ decode and write-back stage. The instruction ﬁelds are decoded to generate register identiﬁers for four addresses (two read and two write) used by the register ﬁle. The values read from the register ﬁle become the signals valA and valB. The two write-back values valE and valM serve as the data for the writes. valACnd valB valM valE Register file A dstE dstM srcA srcB dstE dstM srcA srcB rAicode rB B M E The desired value depends on the instruction type, as shown in the ﬁrst row for the decode stage in Figures 4.18 to 4.21. Combining all of these entries into a single computation gives the following HCL description of srcA (recall that ‡¥·– is the register ID of cËÍÉ): ÑÇË® ÍË²¡ { ‰ ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j '‡››ﬂ‚†j 'ﬂ–†j '–•·¤† Û x Ë¡y ©²Ç®− ©Å Õ '–ﬂ–†j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î Å−−® Ë−×©ÍÎ−Ë `y Practice Problem 4.20 (solution page 524) The register signal srcB indicates which register should be read to generate the signal valB. The desired value is shown as the second step in the decode stage in Figures 4.18 to 4.21. Write HCL code for srcB. Register ID dstE indicates the destination register for write port E, where the computed value valE is stored. This is shown in Figures 4.18 to 4.21 as the ﬁrst step in the write-back stage. If we ignore for the moment the conditional move instructions, then we can combine the destination registers for all of the different instructions to give the following HCL description of dstE: a „¡‡ﬁ'ﬁ§x £ÇÅ®©Î©ÇÅþÄ ÀÇÌ− ÅÇÎ ©ÀÉÄ−À−ÅÎ−® ²ÇËË−²ÎÄÔ ³−Ë− ÑÇË® ®ÍÎ¥ { ‰ ©²Ç®− ©Å Õ '‡‡›ﬂ‚†ÛxË¢y ©²Ç®− ©Å Õ ''‡›ﬂ‚†j 'ﬂ–†Û x Ë¢y ©²Ç®− ©Å Õ '–•·¤†j '–ﬂ–†j '£¡‹‹j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î ÑË©Î− þÅÔ Ë−×©ÍÎ−Ë `y We will revisit this signal and how to implement conditional moves when we examine the execute stage. 444 Chapter 4 Processor Architecture Practice Problem 4.21 (solution page 524) Register ID dstM indicates the destination register for write port M, where valM, the value read from memory, is stored. This is shown in Figures 4.18 to 4.21 as the second step in the write-back stage. Write HCL code for dstM. Practice Problem 4.22 (solution page 524) Only the ÉÇÉÊ instruction uses both register ﬁle write ports simultaneously. For the instruction ÉÇÉÊ cËÍÉ, the same address will be used for both the E and M write ports, but with different data. To handle this conﬂict, we must establish a priority among the two write ports so that when both attempt to write the same register on the same cycle, only the write from the higher-priority port takes place. Which of the two ports should be given priority in order to implement the desired behavior, as determined in Practice Problem 4.8? Execute Stage The execute stage includes the arithmetic/logic unit (ALU). This unit performs the operation add, subtract, and,or exclusive-or on inputs aluA and aluB based on the setting of the alufun signal. These data and control signals are generated by three control blocks, as diagrammed in Figure 4.29. The ALU output becomes the signal valE. In Figures 4.18 to 4.21, the ALU computation for each instruction is shown as the ﬁrst step in the execute stage. The operands are listed with aluB ﬁrst, followed by aluA to make sure the ÍÏ¾Ê instruction subtracts valA from valB. We can see that the value of aluA can be valA, valC, or either −8or +8, depending on the instruction type. We can therefore express the behavior of the control block that generates aluA as follows: ÑÇË® þÄÏ¡ { ‰ ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j 'ﬂ–† Û x ÌþÄ¡y ©²Ç®− ©Å Õ ''‡›ﬂ‚†j '‡››ﬂ‚†j '›‡›ﬂ‚† Û x ÌþÄ£y Figure 4.29 SEQ execute stage. The ALU either performs the operation for an integer operation instruction or acts as an adder. The condition code registers are set according to the ALU value. The condition code values are tested to determine whether a branch should be taken. Cnd valE cond CC Set CC ALU ALU fun. ALU B ALU A valC valA valBicode ifun Section 4.3 Sequential Y86-64 Implementations 445 ©²Ç®− ©Å Õ '£¡‹‹j '–•·¤†Ûxkvy ©²Ç®− ©Å Õ '‡¥¶j '–ﬂ–†Ûxvy a ﬂÎ³−Ë ©ÅÍÎËÏ²Î©ÇÅÍ ®ÇÅ’Î Å−−® ¡‹• `y Practice Problem 4.23 (solution page 524) Based on the ﬁrst operand of the ﬁrst step of the execute stage in Figures 4.18 to 4.21, write an HCL description for the signal aluB in SEQ. Looking at the operations performed by the ALU in the execute stage, we can see that it is mostly used as an adder. For the ﬂ–Ê instructions, however, we want it to use the operation encoded in the ifun ﬁeld of the instruction. We can therefore write the HCL description for the ALU control as follows: ÑÇË® þÄÏðÏÅ { ‰ ©²Ç®− {{ 'ﬂ–† x ©ðÏÅy o x ¡‹•¡⁄⁄y `y The execute stage also includes the condition code register. Our ALU gen- erates the three signals on which the condition codes are based—zero, sign, and overﬂow—every time it operates. However, we only want to set the condition codes when an ﬂ–Ê instruction is executed. We therefore generate a signal set_cc that controls whether or not the condition code register should be updated: ¾ÇÇÄ Í−Îˆ²² { ©²Ç®− ©Å Õ 'ﬂ–† Ûy The hardware unit labeled “cond” uses a combination of the condition codes and the function code to determine whether a conditional branch or data transfer should take place (Figure 4.3). It generates the Cnd signal used both for the setting of dstE with conditional moves and in the next PC logic for conditional branches. For other instructions, the Cnd signal may be set to either 1 or 0, depending on the instruction’s function code and the setting of the condition codes, but it will be ignored by the control logic. We omit the detailed design of this unit. Practice Problem 4.24 (solution page 524) The conditional move instructions, abbreviated ²ÀÇÌ””, have instruction code '‡‡›ﬂ‚†. As Figure 4.28 shows, we can implement these instructions by making use of the Cnd signal, generated in the execute stage. Modify the HCL code for dstE to implement these instructions. Memory Stage The memory stage has the task of either reading or writing program data. As shown in Figure 4.30, two control blocks generate the values for the memory 446 Chapter 4 Processor Architecture Figure 4.30 SEQ memory stage. The data memory can either write or read memory values. The value read from memory forms the signal valM. Stat Stat valM data out Mem. read Mem. write write read dmem_error imem_error instr_valid Mem. addr Mem. data icode valE valA valP data in Data memory address and the memory input data (for write operations). Two other blocks generate the control signals indicating whether to perform a read or a write operation. When a read operation is performed, the data memory generates the value valM. The desired memory operation for each instruction type is shown in the memory stage of Figures 4.18 to 4.21. Observe that the address for memory reads and writes is always valE or valA. We can describe this block in HCL as follows: ÑÇË® À−Àˆþ®®Ë { ‰ ©²Ç®− ©Å Õ '‡››ﬂ‚†j '–•·¤†j '£¡‹‹j '›‡›ﬂ‚† Û x ÌþÄ¥y ©²Ç®− ©Å Õ '–ﬂ–†j '‡¥¶ Û x ÌþÄ¡y a ﬂÎ³−Ë ©ÅÍÎËÏ²Î©ÇÅÍ ®ÇÅ’Î Å−−® þ®®Ë−ÍÍ `y Practice Problem 4.25 (solution page 524) Looking at the memory operations for the different instructions shown in Fig- ures 4.18 to 4.21, we can see that the data for memory writes are always either valA or valP. Write HCL code for the signal mem_data in SEQ. We want to set the control signal mem_read only for instructions that read data from memory, as expressed by the following HCL code: ¾ÇÇÄ À−ÀˆË−þ® { ©²Ç®− ©Å Õ '›‡›ﬂ‚†j '–ﬂ–†j '‡¥¶ Ûy Practice Problem 4.26 (solution page 525) We want to set the control signal mem_write only for instructions that write data to memory. Write HCL code for the signal mem_write in SEQ. Section 4.3 Sequential Y86-64 Implementations 447 Figure 4.31 SEQ PC update stage. The next value of the PC is selected from among the signals valC, valM, and valP, depending on the instruction code and the branch ﬂag. PC New PC icode Cnd valC valM valP A ﬁnal function for the memory stage is to compute the status code Stat resulting from the instruction execution according to the values of icode, imem_ error, and instr_valid generated in the fetch stage and the signal dmem_error generated by the data memory. Practice Problem 4.27 (solution page 525) Write HCL code for Stat, generating the four status codes ·¡ﬂ«, ·¡⁄‡, ·'ﬁ·, and ·¤‹¶ (see Figure 4.26). PC Update Stage The ﬁnal stage in SEQ generates the new value of the program counter (see Figure 4.31). As the ﬁnal steps in Figures 4.18 to 4.21 show, the new PC will be valC, valM, or valP, depending on the instruction type and whether or not a branch should be taken. This selection can be described in HCL as follows: ÑÇË® Å−ÑˆÉ² { ‰ a £þÄÄl •Í− ©ÅÍÎËÏ²Î©ÇÅ ²ÇÅÍÎþÅÎ ©²Ç®− {{ '£¡‹‹ x ÌþÄ£y a ¶þÂ−Å ¾ËþÅ²³l •Í− ©ÅÍÎËÏ²Î©ÇÅ ²ÇÅÍÎþÅÎ ©²Ç®− {{ '“”” dd £Å® x ÌþÄ£y a £ÇÀÉÄ−Î©ÇÅ Çð ‡¥¶ ©ÅÍÎËÏ²Î©ÇÅl •Í− ÌþÄÏ− ðËÇÀ ÍÎþ²Â ©²Ç®− {{ '‡¥¶ x ÌþÄ›y a ⁄−ðþÏÄÎx •Í− ©Å²Ë−À−ÅÎ−® –£ o x ÌþÄ–y `y Surveying SEQ We have now stepped through a complete design for a Y86-64 processor. We have seen that by organizing the steps required to execute each of the different instructions into a uniform ﬂow, we can implement the entire processor with a small number of different hardware units and with a single clock to control the sequencing of computations. The control logic must then route the signals between these units and generate the proper control signals based on the instruction types and the branch conditions. 448 Chapter 4 Processor Architecture The only problem with SEQ is that it is too slow. The clock must run slowly enough so that signals can propagate through all of the stages within a single cycle. As an example, consider the processing of a Ë−Î instruction. Starting with an updated program counter at the beginning of the clock cycle, the instruction must be read from the instruction memory, the stack pointer must be read from the register ﬁle, the ALU must increment the stack pointer by 8, and the return address must be read from the memory in order to determine the next value for the program counter. All of these must be completed by the end of the clock cycle. This style of implementation does not make very good use of our hardware units, since each unit is only active for a fraction of the total clock cycle. We will see that we can achieve much better performance by introducing pipelining. 4.4 General Principles of Pipelining Before attempting to design a pipelined Y86-64 processor, let us consider some general properties and principles of pipelined systems. Such systems are familiar to anyone who has been through the serving line at a cafeteria or run a car through an automated car wash. In a pipelined system, the task to be performed is divided into a series of discrete stages. In a cafeteria, this involves supplying salad, a main dish, dessert, and beverage. In a car wash, this involves spraying water and soap, scrubbing, applying wax, and drying. Rather than having one customer run through the entire sequence from beginning to end before the next can begin, we allow multiple customers to proceed through the system at once. In a traditional cafeteria line, the customers maintain the same order in the pipeline and pass through all stages, even if they do not want some of the courses. In the case of the car wash, a new car is allowed to enter the spraying stage as the preceding car moves from the spraying stage to the scrubbing stage. In general, the cars must move through the system at the same rate to avoid having one car crash into the next. A key feature of pipelining is that it increases the throughput of the system (i.e., the number of customers served per unit time), but it may also slightly increase the latency (i.e., the time required to service an individual customer). For example, a customer in a cafeteria who only wants a dessert could pass through a nonpipelined system very quickly, stopping only at the dessert stage. A customer in a pipelined system who attempts to go directly to the dessert stage risks incurring the wrath of other customers. 4.4.1 Computational Pipelines Shifting our focus to computational pipelines, the “customers” are instructions and the stages perform some portion of the instruction execution. Figure 4.32(a) shows an example of a simple nonpipelined hardware system. It consists of some logic that performs a computation, followed by a register to hold the results of this computation. A clock signal controls the loading of the register at some regular time interval. An example of such a system is the decoder in a compact disk (CD) player. The incoming signals are the bits read from the surface of the CD, and Section 4.4 General Principles of Pipelining 449 Figure 4.32 Unpipelined computation hardware. On each 320 ps cycle, the system spends 300 ps evaluating a combinational logic function and 20 ps storing the results in an output register. (a) Hardware: Unpipelined (b) Pipeline diagram 300 ps 20 ps Delay \u0005 320 ps Throughput \u0005 3.12 GIPS Combinational logic R e g Clock I1 I2 I3 Time the logic decodes these to generate audio signals. The computational block in the ﬁgure is implemented as combinational logic, meaning that the signals will pass through a series of logic gates, with the outputs becoming some function of the inputs after some time delay. In contemporary logic design, we measure circuit delays in units of picosec- onds (abbreviated “ps”), or 10−12 seconds. In this example, we assume the com- binational logic requires 300 ps, while the loading of the register requires 20 ps. Figure 4.32 shows a form of timing diagram known as a pipeline diagram. In this diagram, time ﬂows from left to right. A series of instructions (here named 'o, 'p, and 'q) are written from top to bottom. The solid rectangles indicate the times during which these instructions are executed. In this implementation, we must complete one instruction before beginning the next. Hence, the boxes do not over- lap one another vertically. The following formula gives the maximum rate at which we could operate the system: Throughput = 1 instruction (20 + 300) picoseconds . 1,000 picoseconds 1 nanosecond ≈ 3.12 GIPS We express throughput in units of giga-instructions per second (abbreviated GIPS), or billions of instructions per second. The total time required to perform a single instruction from beginning to end is known as the latency. In this system, the latency is 320 ps, the reciprocal of the throughput. Suppose we could divide the computation performed by our system into three stages, A, B, and C, where each requires 100 ps, as illustrated in Figure 4.33. Then we could put pipeline registers between the stages so that each instruction moves through the system in three steps, requiring three complete clock cycles from beginning to end. As the pipeline diagram in Figure 4.33 illustrates, we could allow 'p to enter stage A as soon as 'o moves from A to B, and so on. In steady state, all three stages would be active, with one instruction leaving and a new one entering the system every clock cycle. We can see this during the third clock cycle in the pipeline diagram where 'o is in stage C, 'p is in stage B, and 'q is in stage A. In 450 Chapter 4 Processor Architecture Clock Comb. logic A R e g (a) Hardware: Three-stage pipeline 100 ps 20 ps Comb. logic B R e g 100 ps 20 ps Comb. logic C R e g 100 ps 20 ps (b) Pipeline diagram Time Delay \u0005 360 ps Throughput \u0005 8.33 GIPS I1 I2 I3 ABC ABC ABC Figure 4.33 Three-stage pipelined computation hardware. The computation is split into stages A, B, and C. On each 120 ps cycle, each instruction progresses through one stage. Figure 4.34 Three-stage pipeline timing. The rising edge of the clock signal controls the movement of instructions from one pipeline stage to the next. Clock I1 I2 I3 1200 240 360 Time 480 600 ABC ABC ABC this system, we could cycle the clocks every 100 + 20 = 120 picoseconds, giving a throughput of around 8.33 GIPS. Since processing a single instruction requires 3 clock cycles, the latency of this pipeline is 3 × 120 = 360 ps. We have increased the throughput of the system by a factor of 8.33/3.12 = 2.67 at the expense of some added hardware and a slight increase in the latency (360/320 = 1.12). The increased latency is due to the time overhead of the added pipeline registers. 4.4.2 A Detailed Look at Pipeline Operation To better understand how pipelining works, let us look in some detail at the timing and operation of pipeline computations. Figure 4.34 shows the pipeline diagram for the three-stage pipeline we have already looked at (Figure 4.33). The transfer of the instructions between pipeline stages is controlled by a clock signal, as shown above the pipeline diagram. Every 120 ps, this signal rises from 0 to 1, initiating the next set of pipeline stage evaluations. Figure 4.35 traces the circuit activity between times 240 and 360, as instruc- tion 'o (shown in dark gray) propagates through stage C, 'p (shown in blue) Section 4.4 General Principles of Pipelining 451 B A C B A Clock Clock Clock Clock Clock I1 I2 I3 Time Time \u0005 239 120 240 360 21 1 Time \u0005 2412 Time \u0005 3003 Time \u0005 3594 3 4 Comb. logic A R e g 100 ps 20 ps Comb. logic B R e g 100 ps 20 ps Comb. logic C R e g 100 ps 20 ps Comb. logic A R e g 100 ps 20 ps Comb. logic B R e g 100 ps 20 ps Comb. logic C R e g 100 ps 20 ps Comb. logic A R e g 100 ps 20 ps Comb. logic B R e g 100 ps 20 ps Comb. logic C R e g 100 ps 20 ps Comb. logic A R e g 100 ps 20 ps Comb. logic B R e g 100 ps 20 ps Comb. logic C R e g 100 ps 20 ps Figure 4.35 One clock cycle of pipeline operation. Just before the clock rises at time 240 (point 1), instructions 'o (shown in dark gray) and 'p (shown in blue) have completed stages B and A. After the clock rises, these instructions begin propagating through stages C and B, while instruction 'q (shown in light gray) begins propagating through stage A (points 2 and 3). Just before the clock rises again, the results for the instructions have propagated to the inputs of the pipeline registers (point 4). 452 Chapter 4 Processor Architecture propagates through stage B, and 'q (shown in light gray) propagates through stage A. Just before the rising clock at time 240 (point 1), the values computed in stage A for instruction 'p have reached the input of the ﬁrst pipeline register, but its state and output remain set to those computed during stage A for instruction 'o.The values computed in stage B for instruction 'o have reached the input of the sec- ond pipeline register. As the clock rises, these inputs are loaded into the pipeline registers, becoming the register outputs (point 2). In addition, the input to stage A is set to initiate the computation of instruction 'q. The signals then propagate through the combinational logic for the different stages (point 3). As the curved wave fronts in the diagram at point 3 suggest, signals can propagate through differ- ent sections at different rates. Before time 360, the result values reach the inputs of the pipeline registers (point 4). When the clock rises at time 360, each of the instructions will have progressed through one pipeline stage. We can see from this detailed view of pipeline operation that slowing down the clock would not change the pipeline behavior. The signals propagate to the pipeline register inputs, but no change in the register states will occur until the clock rises. On the other hand, we could have disastrous effects if the clock were run too fast. The values would not have time to propagate through the combinational logic, and so the register inputs would not yet be valid when the clock rises. As with our discussion of the timing for the SEQ processor (Section 4.3.3), we see that the simple mechanism of having clocked registers between blocks of combinational logic sufﬁces to control the ﬂow of instructions in the pipeline. As the clock rises and falls repeatedly, the different instructions ﬂow through the stages of the pipeline without interfering with one another. 4.4.3 Limitations of Pipelining The example of Figure 4.33 shows an ideal pipelined system in which we are able to divide the computation into three independent stages, each requiring one-third of the time required by the original logic. Unfortunately, other factors often arise that diminish the effectiveness of pipelining. Nonuniform Partitioning Figure 4.36 shows a system in which we divide the computation into three stages as before, but the delays through the stages range from 50 to 150 ps. The sum of the delays through all of the stages remains 300 ps. However, the rate at which we can operate the clock is limited by the delay of the slowest stage. As the pipeline diagram in this ﬁgure shows, stage A will be idle (shown as a white box) for 100 ps every clock cycle, while stage C will be idle for 50 ps every clock cycle. Only stage B will be continuously active. We must set the clock cycle to 150 + 20 = 170 picoseconds, giving a throughput of 5.88 GIPS. In addition, the latency would increase to 510 ps due to the slower clock rate. Devising a partitioning of the system computation into a series of stages having uniform delays can be a major challenge for hardware designers. Often, Section 4.4 General Principles of Pipelining 453 I1 I2 I3 AB C AB C AB C Time Clock Comb. logic A R e g (a) Hardware: Three-stage pipeline, nonuniform stage delays 50 ps 20 ps Comb. logic B R e g 150 ps 20 ps Comb. logic C R e g 100 ps 20 ps (b) Pipeline diagram Delay \u0005 510 ps Throughput \u0005 5.88 GIPS Figure 4.36 Limitations of pipelining due to nonuniform stage delays. The system throughput is limited by the speed of the slowest stage. some of the hardware units in a processor, such as the ALU and the memories, cannot be subdivided into multiple units with shorter delay. This makes it difﬁcult to create a set of balanced stages. We will not concern ourselves with this level of detail in designing our pipelined Y86-64 processor, but it is important to appreciate the importance of timing optimization in actual system design. Practice Problem 4.28 (solution page 525) Suppose we analyze the combinational logic of Figure 4.32 and determine that it can be separated into a sequence of six blocks, named A to F, having delays of 80, 30, 60, 50, 70, and 10 ps, respectively, illustrated as follows: 80 ps 30 ps 60 ps 50 ps 70 ps 10 ps AE FCB D 20 ps Clock R e g We can create pipelined versions of this design by inserting pipeline registers between pairs of these blocks. Different combinations of pipeline depth (how many stages) and maximum throughput arise, depending on where we insert the pipeline registers. Assume that a pipeline register has a delay of 20 ps. A. Inserting a single register gives a two-stage pipeline. Where should the register be inserted to maximize throughput? What would be the throughput and latency? 454 Chapter 4 Processor Architecture B. Where should two registers be inserted to maximize the throughput of a three-stage pipeline? What would be the throughput and latency? C. Where should three registers be inserted to maximize the throughput of a 4-stage pipeline? What would be the throughput and latency? D. What is the minimum number of stages that would yield a design with the maximum achievable throughput? Describe this design, its throughput, and its latency. Diminishing Returns of Deep Pipelining Figure 4.37 illustrates another limitation of pipelining. In this example, we have divided the computation into six stages, each requiring 50 ps. Inserting a pipeline register between each pair of stages yields a six-stage pipeline. The minimum clock period for this system is 50 + 20 = 70 picoseconds, giving a throughput of 14.29 GIPS. Thus, in doubling the number of pipeline stages, we improve the performance by a factor of 14.29/8.33 = 1.71. Even though we have cut the time required for each computation block by a factor of 2, we do not get a doubling of the throughput, due to the delay through the pipeline registers. This delay becomes a limiting factor in the throughput of the pipeline. In our new design, this delay consumes 28.6% of the total clock period. Modern processors employ very deep pipelines (15 or more stages) in an attempt to maximize the processor clock rate. The processor architects divide the instruction execution into a large number of very simple steps so that each stage can have a very small delay. The circuit designers carefully design the pipeline registers to minimize their delay. The chip designers must also carefully design the clock distribution network to ensure that the clock changes at the exact same time across the entire chip. All of these factors contribute to the challenge of designing high-speed microprocessors. Practice Problem 4.29 (solution page 526) Suppose we could take the system of Figure 4.32 and divide it into an arbitrary number of pipeline stages k, each having a delay of 300/k, and with each pipeline register having a delay of 20 ps. Comb. logic Comb. logic Comb. logic Comb. logic Comb. logic Comb. logic 50 ps R e g 50 ps20 ps 20 ps 20 ps 20 ps 20 ps 20 ps50 ps R e g 50 ps Delay = 420 ps, throughput = 14.29 GIPSClock R e g 50 ps R e g 50 ps R e g R e g Figure 4.37 Limitations of pipelining due to overhead. As the combinational logic is split into shorter blocks, the delay due to register updating becomes a limiting factor. Section 4.4 General Principles of Pipelining 455 A. What would be the latency and the throughput of the system, as functions of k? B. What would be the ultimate limit on the throughput? 4.4.4 Pipelining a System with Feedback Up to this point, we have considered only systems in which the objects passing through the pipeline—whether cars, people, or instructions—are completely in- dependent of one another. For a system that executes machine programs such as x86-64 or Y86-64, however, there are potential dependencies between successive instructions. For example, consider the following Y86-64 instruction sequence: 1 irmovq $50, %rax 2 addq %rax , %rbx 3 mrmovq 100( %rbx ), %rdx In this three-instruction sequence, there is a data dependency between each successive pair of instructions, as indicated by the circled register names and the arrows between them. The ©ËÀÇÌÊ instruction (line 1) stores its result in cËþÓ, which then must be read by the þ®®Ê instruction (line 2); and this instruction stores its result in cË¾Ó, which must then be read by the ÀËÀÇÌÊ instruction (line 3). Another source of sequential dependencies occurs due to the instruction control ﬂow. Consider the following Y86-64 instruction sequence: 1 ÄÇÇÉx 2 ÍÏ¾Ê cË®ÓjcË¾Ó 3 ÁÅ− ÎþË× 4 ©ËÀÇÌÊ bonjcË®Ó 5 ÁÀÉ ÄÇÇÉ 6 ÎþË×x 7 ³þÄÎ The ÁÅ− instruction (line 3) creates a control dependency since the outcome of the conditional test determines whether the next instruction to execute will be the ©ËÀÇÌÊ instruction (line 4) or the ³þÄÎ instruction (line 7). In our design for SEQ, these dependencies were handled by the feedback paths shown on the right-hand side of Figure 4.22. This feedback brings the updated register values down to the register ﬁle and the new PC value down to the PC register. Figure 4.38 illustrates the perils of introducing pipelining into a system con- taining feedback paths. In the original system (Figure 4.38(a)), the result of each 456 Chapter 4 Processor Architecture Time Clock Time (a) Hardware: Unpipelined with feedback (b) Pipeline diagram (d) Pipeline diagram (c) Hardware: Three-stage pipeline with feedback Combinational logic R e g Clock Comb. logic A R e g Comb. logic B R e g Comb. logic C R e g I1 I2 I3 I1 I2 I3 ABC ABC ABC I4 ABC Figure 4.38 Limitations of pipelining due to logical dependencies. In going from an unpipelined system with feedback (a) to a pipelined one (c), we change its computational behavior, as can be seen by the two pipeline diagrams (b and d). instruction is fed back around to the next instruction. This is illustrated by the pipeline diagram (Figure 4.38(b)), where the result of 'o becomes an input to 'p, and so on. If we attempt to convert this to a three-stage pipeline in the most straightforward manner (Figure 4.38(c)), we change the behavior of the system. As Figure 4.38(c) shows, the result of 'o becomes an input to 'r. In attempting to speed up the system via pipelining, we have changed the system behavior. Section 4.5 Pipelined Y86-64 Implementations 457 When we introduce pipelining into a Y86-64 processor, we must deal with feedback effects properly. Clearly, it would be unacceptable to alter the system behavior as occurred in the example of Figure 4.38. Somehow we must deal with the data and control dependencies between instructions so that the resulting behavior matches the model deﬁned by the ISA. 4.5 Pipelined Y86-64 Implementations We are ﬁnally ready for the major task of this chapter—designing a pipelined Y86- 64 processor. We start by making a small adaptation of the sequential processor SEQ to shift the computation of the PC into the fetch stage. We then add pipeline registers between the stages. Our ﬁrst attempt at this does not handle the different data and control dependencies properly. By making some modiﬁcations, however, we achieve our goal of an efﬁcient pipelined processor that implements the Y86- 64 ISA. 4.5.1 SEQ+: Rearranging the Computation Stages As a transitional step toward a pipelined design, we must slightly rearrange the order of the ﬁve stages in SEQ so that the PC update stage comes at the beginning of the clock cycle, rather than at the end. This transformation requires only minimal change to the overall hardware structure, and it will work better with the sequencing of activities within the pipeline stages. We refer to this modiﬁed design as SEQ+. We can move the PC update stage so that its logic is active at the beginning of the clock cycle by making it compute the PC value for the current instruction. Figure 4.39 shows how SEQ and SEQ+ differ in their PC computation. With SEQ (Figure 4.39(a)), the PC computation takes place at the end of the clock cycle, computing the new value for the PC register based on the values of signals computed during the current clock cycle. With SEQ+ (Figure 4.39(b)), we create state registers to hold the signals computed during an instruction. Then, as a new clock cycle begins, the values propagate through the exact same logic to compute the PC for the now-current instruction. We label the registers “pIcode,” PC New PC icode Cnd valC (a) SEQ new PC computation (b) SEQ+ PC selection valM valP PC PC plcode pValCpValMpCnd pValP Figure 4.39 Shifting the timing of the PC computation. With SEQ+, we compute the value of the program counter for the current state as the ﬁrst step in instruction execution. 458 Chapter 4 Processor Architecture Aside Where is the PC in SEQ+? One curious feature of SEQ+ is that there is no hardware register storing the program counter. Instead, the PC is computed dynamically based on some state information stored from the previous instruction. This is a small illustration of the fact that we can implement a processor in a way that differs from the conceptual model implied by the ISA, as long as the processor correctly executes arbitrary machine- language programs. We need not encode the state in the form indicated by the programmer-visible state, as long as the processor can generate correct values for any part of the programmer-visible state (such as the program counter). We will exploit this principle even more in creating a pipelined design. Out- of-order processing techniques, as described in Section 5.7, take this idea to an extreme by executing instructions in a completely different order than they occur in the machine-level program. “pCnd,” and so on, to indicate that on any given cycle, they hold the control signals generated during the previous cycle. Figure 4.40 shows a more detailed view of the SEQ+ hardware. We can see that it contains the exact same hardware units and control blocks that we had in SEQ (Figure 4.23), but with the PC logic shifted from the top, where it was active at the end of the clock cycle, to the bottom, where it is active at the beginning. The shift of state elements from SEQ to SEQ+ is an example of a general transformation known as circuit retiming [68]. Retiming changes the state repre- sentation for a system without changing its logical behavior. It is often used to balance the delays between the different stages of a pipelined system. 4.5.2 Inserting Pipeline Registers In our ﬁrst attempt at creating a pipelined Y86-64 processor, we insert pipeline registers between the stages of SEQ+ and rearrange signals somewhat, yielding the PIPE− processor, where the “−” in the name signiﬁes that this processor has somewhat less performance than our ultimate processor design. The structure of PIPE− is illustrated in Figure 4.41. The pipeline registers are shown in this ﬁgure as blue boxes, each containing different ﬁelds that are shown as white boxes. As indicated by the multiple ﬁelds, each pipeline register holds multiple bytes and words. Unlike the labels shown in rounded boxes in the hardware structure of the two sequential processors (Figures 4.23 and 4.40), these white boxes represent actual hardware components. Observe that PIPE− uses nearly the same set of hardware units as our sequen- tial design SEQ (Figure 4.40), but with the pipeline registers separating the stages. The differences between the signals in the two systems is discussed in Section 4.5.3. The pipeline registers are labeled as follows: F holds a predicted value of the program counter, as will be discussed shortly. D sits between the fetch and decode stages. It holds information about the most recently fetched instruction for processing by the decode stage. Section 4.5 Pipelined Y86-64 Implementations 459 Memory Execute Decode Fetch PC valM data out read write Data memory Addr Data Mem. control Cnd valE CC ALU ALU fun. ALU A ALU B valA valB dstE dstM srcA srcB dstE dstM srcA srcB Register file Write back AB E M icode ifun rA rB valC valP PC increment Instruction memory PC PC plcode pValCpValMpCnd pValP Stat dmem_error Stat instr_valid imem_error Figure 4.40 SEQ+ hardware structure. Shifting the PC computation from the end of the clock cycle to the beginning makes it more suitable for pipelining. 460 Chapter 4 Processor Architecture Stat Stat valA Stat Stat Write back W icode valE valM dstE dstM ALU A ALU B ALU fun. M icode Cnd valE valA dstE dstM E icode ifun valC valA valB dstM srcA srcBdstE D icodestat stat stat ifun valC valPrBrA F predPC data out data in M_Cnd e_Cnd Memory ALU Execute dstE dstM srcA srcBSelect A Predict PC Select PC d_srcA d_rvalA d_srcB W_valM M_valA W_valE M_valA f_pc f_stat D_stat E_stat M_stat m_stat W_stat imem_error instr_valid W_valM CC Decode Fetch read dmem_error write Addr Mem. control Register file AB E M PC increment Instruction memory dstE Data memory stat Figure 4.41 Hardware structure of PIPE−, an initial pipelined implementation. By inserting pipeline registers into SEQ+ (Figure 4.40), we create a ﬁve-stage pipeline. There are several shortcomings of this version that we will deal with shortly. Section 4.5 Pipelined Y86-64 Implementations 461 E sits between the decode and execute stages. It holds information about the most recently decoded instruction and the values read from the register ﬁle for processing by the execute stage. M sits between the execute and memory stages. It holds the results of the most recently executed instruction for processing by the memory stage. It also holds information about branch conditions and branch targets for processing conditional jumps. W sits between the memory stage and the feedback paths that supply the computed results to the register ﬁle for writing and the return address to the PC selection logic when completing a Ë−Î instruction. Figure 4.42 shows how the following code sequence would ﬂow through our ﬁve-stage pipeline, where the comments identify the instructions as 'o to 's for reference: 1 ©ËÀÇÌÊ bojcËþÓ a 'o 2 ©ËÀÇÌÊ bpjcË¾Ó a 'p 3 ©ËÀÇÌÊ bqjcË²Ó a 'q 4 ©ËÀÇÌÊ brjcË®Ó a 'r 5 ³þÄÎ a 's irmovq $1,%rax #Il irmovq $2,%rbx #I2 irmovq $3,%rcx #I3 irmovq $4,%rdx #I4 halt #I5 FD E M W 12345 FD E M W 6 FD E M W 7 FD E M W 8 FD E M W 9 Cycle 5 W Il M I2 E I3 D I4 F I5 Figure 4.42 Example of instruction ﬂow through pipeline. 462 Chapter 4 Processor Architecture The right side of the ﬁgure shows a pipeline diagram for this instruction sequence. As with the pipeline diagrams for the simple pipelined computation units of Section 4.4, this diagram shows the progression of each instruction through the pipeline stages, with time increasing from left to right. The numbers along the top identify the clock cycles at which the different stages occur. For example, in cycle 1, instruction 'o is fetched, and it then proceeds through the pipeline stages, with its result being written to the register ﬁle after the end of cycle 5. Instruction 'p is fetched in cycle 2, and its result is written back after the end of cycle 6, and so on. At the bottom, we show an expanded view of the pipeline for cycle 5. At this point, there is an instruction in each of the pipeline stages. From Figure 4.42, we can also justify our convention of drawing processors so that the instructions ﬂow from bottom to top. The expanded view for cycle 5 shows the pipeline stages with the fetch stage on the bottom and the write-back stage on the top, just as do our diagrams of the pipeline hardware (Figure 4.41). If we look at the ordering of instructions in the pipeline stages, we see that they appear in the same order as they do in the program listing. Since normal program ﬂow goes from top to bottom of a listing, we preserve this ordering by having the pipeline ﬂow go from bottom to top. This convention is particularly useful when working with the simulators that accompany this text. 4.5.3 Rearranging and Relabeling Signals Our sequential implementations SEQ and SEQ+ only process one instruction at a time, and so there are unique values for signals such as valC, srcA, and valE.In our pipelined design, there will be multiple versions of these values associated with the different instructions ﬂowing through the system. For example, in the detailed structure of PIPE−, there are four white boxes labeled “Stat” that hold the status codes for four different instructions (see Figure 4.41). We need to take great care to make sure we use the proper version of a signal, or else we could have serious errors, such as storing the result computed for one instruction at the destination register speciﬁed by another instruction. We adopt a naming scheme where a signal stored in a pipeline register can be uniquely identiﬁed by preﬁxing its name with that of the pipe register written in uppercase. For example, the four status codes are named D_stat, E_stat, M_stat, and W_stat. We also need to refer to some signals that have just been computed within a stage. These are labeled by preﬁxing the signal name with the ﬁrst character of the stage name, written in lowercase. Using the status codes as examples, we can see control logic blocks labeled “Stat” in the fetch and memory stages. The outputs of these blocks are therefore named f_stat and m_stat. We can also see that the actual status of the overall processor Stat is computed by a block in the write-back stage, based on the status value in pipeline register W. The decode stages of SEQ+ and PIPE− both generate signals dstE and dstM indicating the destination register for values valE and valM. In SEQ+, we could connect these signals directly to the address inputs of the register ﬁle write ports. With PIPE−, these signals are carried along in the pipeline through the execute and memory stages and are directed to the register ﬁle only once they reach Section 4.5 Pipelined Y86-64 Implementations 463 Aside What is the difference between signals M_stat and m_stat? With our naming system, the uppercase preﬁxes ‘D’, ‘E’, ‘M’, and ‘W’ refer to pipeline registers, and so M_stat refers to the status code ﬁeld of pipeline register M. The lowercase preﬁxes ‘f’, ‘d’, ‘e’, ‘m’, and ‘w’ refer to the pipeline stages, and so m_stat refers to the status signal generated in the memory stage by a control logic block. Understanding this naming convention is critical to understanding the operation of our pipelined processors. the write-back stage (shown in the more detailed views of the stages). We do this to make sure the write port address and data inputs hold values from the same instruction. Otherwise, the write back would be writing the values for the instruction in the write-back stage, but with register IDs from the instruction in the decode stage. As a general principle, we want to keep all of the information about a particular instruction contained within a single pipeline stage. One block of PIPE− that is not present in SEQ+ in the exact same form is the block labeled “Select A” in the decode stage. We can see that this block generates the value valA for the pipeline register E by choosing either valP from pipeline register D or the value read from the A port of the register ﬁle. This block is included to reduce the amount of state that must be carried forward to pipeline registers E and M. Of all the different instructions, only the ²þÄÄ requires valP in the memory stage. Only the jump instructions require the value of valP in the execute stage (in the event the jump is not taken). None of these instructions requires a value read from the register ﬁle. Therefore, we can reduce the amount of pipeline register state by merging these two signals and carrying them through the pipeline as a single signal valA. This eliminates the need for the block labeled “Data” in SEQ (Figure 4.23) and SEQ+ (Figure 4.40), which served a similar purpose. In hardware design, it is common to carefully identify how signals get used and then reduce the amount of register state and wiring by merging signals such as these. As shown in Figure 4.41, our pipeline registers include a ﬁeld for the status code stat, initially computed during the fetch stage and possibly modiﬁed during the memory stage. We will discuss how to implement the processing of exceptional events in Section 4.5.6, after we have covered the implementation of normal in- struction execution. Sufﬁce it to say at this point that the most systematic approach is to associate a status code with each instruction as it passes through the pipeline, as we have indicated in the ﬁgure. 4.5.4 Next PC Prediction We have taken some measures in the design of PIPE− to properly handle control dependencies. Our goal in the pipelined design is to issue a new instruction on every clock cycle, meaning that on each clock cycle, a new instruction proceeds into the execute stage and will ultimately be completed. Achieving this goal would 464 Chapter 4 Processor Architecture Aside Other branch prediction strategies Our design uses an always taken branch prediction strategy. Studies show this strategy has around a 60% success rate [44, 122]. Conversely, a never taken (NT) strategy has around a 40% success rate. A slightly more sophisticated strategy, known as backward taken, forward not taken (BTFNT), predicts that branches to lower addresses than the next instruction will be taken, while those to higher addresses will not be taken. This strategy has a success rate of around 65%. This improvement stems from the fact that loops are closed by backward branches and loops are generally executed multiple times. Forward branches are used for conditional operations, and these are less likely to be taken. In Problems 4.55 and 4.56, you can modify the Y86-64 pipeline processor to implement the NT and BTFNT branch prediction strategies. As we saw in Section 3.6.6, mispredicted branches can degrade the performance of a program considerably, thus motivating the use of conditional data transfer rather than conditional control transfer when possible. yield a throughput of one instruction per cycle. To do this, we must determine the location of the next instruction right after fetching the current instruction. Unfortunately, if the fetched instruction is a conditional branch, we will not know whether or not the branch should be taken until several cycles later, after the instruction has passed through the execute stage. Similarly, if the fetched instruction is a Ë−Î, we cannot determine the return location until the instruction has passed through the memory stage. With the exception of conditional jump instructions and Ë−Î, we can deter- mine the address of the next instruction based on information computed during the fetch stage. For ²þÄÄ and ÁÀÉ (unconditional jump), it will be valC, the con- stant word in the instruction, while for all others it will be valP, the address of the next instruction. We can therefore achieve our goal of issuing a new instruction every clock cycle in most cases by predicting the next value of the PC. For most in- struction types, our prediction will be completely reliable. For conditional jumps, we can predict either that a jump will be taken, so that the new PC value would be valC, or that it will not be taken, so that the new PC value would be valP. In either case, we must somehow deal with the case where our prediction was incorrect and therefore we have fetched and partially executed the wrong instructions. We will return to this matter in Section 4.5.8. This technique of guessing the branch direction and then initiating the fetching of instructions according to our guess is known as branch prediction. It is used in some form by virtually all processors. Extensive experiments have been conducted on effective strategies for predicting whether or not branches will be taken [46, Section 2.3]. Some systems devote large amounts of hardware to this task. In our design, we will use the simple strategy of predicting that conditional branches are always taken, and so we predict the new value of the PC to be valC. We are still left with predicting the new PC value resulting from a Ë−Î in- struction. Unlike conditional jumps, we have a nearly unbounded set of possible Section 4.5 Pipelined Y86-64 Implementations 465 Aside Return address prediction with a stack With most programs, it is very easy to predict return addresses, since procedure calls and returns occur in matched pairs. Most of the time that a procedure is called, it returns to the instruction following the call. This property is exploited in high-performance processors by including a hardware stack within the instruction fetch unit that holds the return address generated by procedure call instructions. Every time a procedure call instruction is executed, its return address is pushed onto the stack. When a return instruction is fetched, the top value is popped from this stack and used as the predicted return address. Like branch prediction, a mechanism must be provided to recover when the prediction was incorrect, since there are times when calls and returns do not match. In general, the prediction is highly reliable. This hardware stack is not part of the programmer-visible state. results, since the return address will be whatever word is on the top of the stack. In our design, we will not attempt to predict any value for the return address. Instead, we will simply hold off processing any more instructions until the Ë−Î instruction passes through the write-back stage. We will return to this part of the implementation in Section 4.5.8. The PIPE− fetch stage, diagrammed at the bottom of Figure 4.41, is respon- sible for both predicting the next value of the PC and selecting the actual PC for the instruction fetch. We can see the block labeled “Predict PC” can choose either valP (as computed by the PC incrementer) or valC (from the fetched instruction). This value is stored in pipeline register F as the predicted value of the program counter. The block labeled “Select PC” is similar to the block labeled “PC” in the SEQ+ PC selection stage (Figure 4.40). It chooses one of three values to serve as the address for the instruction memory: the predicted PC, the value of valP for a not-taken branch instruction that reaches pipeline register M (stored in regis- ter M_valA), or the value of the return address when a Ë−Î instruction reaches pipeline register W (stored in W_valM). 4.5.5 Pipeline Hazards Our structure PIPE− is a good start at creating a pipelined Y86-64 processor. Recall from our discussion in Section 4.4.4, however, that introducing pipelining into a system with feedback can lead to problems when there are dependencies between successive instructions. We must resolve this issue before we can com- plete our design. These dependencies can take two forms: (1) data dependencies, where the results computed by one instruction are used as the data for a follow- ing instruction, and (2) control dependencies, where one instruction determines the location of the following instruction, such as when executing a jump, call, or return. When such dependencies have the potential to cause an erroneous com- putation by the pipeline, they are called hazards. Like dependencies, hazards can be classiﬁed as either data hazards or control hazards. We ﬁrst concern ourselves with data hazards and then consider control hazards. 466 Chapter 4 Processor Architecture F0x000: irmovq $10,%rdx # progl# progl 0x00a: irmovq $3,%rax 0x014: nop 0x015: nop 0x016: nop 0x017: addq %rdx,%rax 0x019: halt DE M W FD E M W FD E M W FD E M W FD E M W FD E M W FD E M W Cycle 6 Cycle 7 12345 6 7 8 910 11 W D R[%rax] 3 valA R[%rdx] = 10 valB R[%rax] = 3 Figure 4.43 Pipelined execution of ÉËÇ×o without special pipeline control. In cycle 6, the second ©ËÀÇÌÊ writes its result to program register cËþÓ. The þ®®Ê instruction reads its source operands in cycle 7, so it gets correct values for both cË®Ó and cËþÓ. Figure 4.43 illustrates the processing of a sequence of instructions we refer to as ÉËÇ×o by the PIPE− processor. Let us assume in this example and successive ones that the program registers initially all have value 0. The code loads values 10 and 3 into program registers cË®Ó and cËþÓ, executes three ÅÇÉ instructions, and then adds register cË®Ó to cËþÓ. We focus our attention on the potential data hazards resulting from the data dependencies between the two ©ËÀÇÌÊ instructions and the þ®®Ê instruction. On the right-hand side of the ﬁgure, we show a pipeline diagram for the instruction sequence. The pipeline stages for cycles 6 and 7 are shown highlighted in the pipeline diagram. Below this, we show an expanded view of the write-back activity in cycle 6 and the decode activity during cycle 7. After the start of cycle 7, both of the ©ËÀÇÌÊ instructions have passed through the write- back stage, and so the register ﬁle holds the updated values of cË®Ó and cËþÓ. As the þ®®Ê instruction passes through the decode stage during cycle 7, it will therefore read the correct values for its source operands. The data dependencies between the two ©ËÀÇÌÊ instructions and the þ®®Ê instruction have not created data hazards in this example. We saw that ÉËÇ×o will ﬂow through our pipeline and get the correct results, because the three ÅÇÉ instructions create a delay between instructions with data Section 4.5 Pipelined Y86-64 Implementations 467 D valA R[%rdx] = 10 valB R[%rax] = 0 F0x000: irmovq $10,%rdx # prog2# prog2 0x00a: irmovq $3,%rax 0x014: nop 0x015: nop 0x016: addq %rdx,%rax 0x018: halt DE M W FD E M W FD E M W FD E M W FD E M W FD E M W Cycle 6 12345 6 7 8 910 W R[%rax] 3 Error. . . Figure 4.44 Pipelined execution of ÉËÇ×p without special pipeline control. The write to program register cËþÓ does not occur until the start of cycle 7, and so the þ®®Ê instruction gets the incorrect value for this register in the decode stage. dependencies. Let us see what happens as these ÅÇÉ instructions are removed. Figure 4.44 illustrates the pipeline ﬂow of a program, named ÉËÇ×p, containing two ÅÇÉ instructions between the two ©ËÀÇÌÊ instructions generating values for registers cË®Ó and cËþÓ and the þ®®Ê instruction having these two registers as operands. In this case, the crucial step occurs in cycle 6, when the þ®®Ê instruc- tion reads its operands from the register ﬁle. An expanded view of the pipeline activities during this cycle is shown at the bottom of the ﬁgure. The ﬁrst ©ËÀÇÌÊ instruction has passed through the write-back stage, and so program register cË®Ó has been updated in the register ﬁle. The second ©ËÀÇÌÊ instruction is in the write- back stage during this cycle, and so the write to program register cËþÓ only occurs at the start of cycle 7 as the clock rises. As a result, the incorrect value zero would be read for register cËþÓ (recall that we assume all registers are initially zero), since the pending write for this register has not yet occurred. Clearly, we will have to adapt our pipeline to handle this hazard properly. Figure 4.45 shows what happens when we have only one ÅÇÉ instruction between the ©ËÀÇÌÊ instructions and the þ®®Ê instruction, yielding a program ÉËÇ×q. Now we must examine the behavior of the pipeline during cycle 5 as the þ®®Ê instruction passes through the decode stage. Unfortunately, the pending 468 Chapter 4 Processor Architecture M M_valE = 3 M_dstE = %rax D valA R[%rdx] = 0 valB R[%rax] = 0 F0x000: irmovq $10,%rdx # prog3# prog3 0x00a: irmovq $3,%rax 0x014: nop 0x015: addq %rdx,%rax 0x017: halt DE M W FD E M W FD E M W FD E M W FD E M W Cycle 5 12345 6 7 8 9 W R[%rdx] 10 Error. . . Figure 4.45 Pipelined execution of ÉËÇ×q without special pipeline control. In cycle 5, the þ®®Ê instruction reads its source operands from the register ﬁle. The pending write to register cË®Ó is still in the write-back stage, and the pending write to register cËþÓ is still in the memory stage. Both operands valA and valB get incorrect values. write to register cË®Ó is still in the write-back stage, and the pending write to cËþÓ is still in the memory stage. Therefore, the þ®®Ê instruction would get the incorrect values for both operands. Figure 4.46 shows what happens when we remove all of the ÅÇÉ instructions between the ©ËÀÇÌÊ instructions and the þ®®Ê instruction, yielding a program ÉËÇ×r. Now we must examine the behavior of the pipeline during cycle 4 as the þ®®Ê instruction passes through the decode stage. Unfortunately, the pending write to register cË®Ó is still in the memory stage, and the new value for cËþÓ is just being computed in the execute stage. Therefore, the þ®®Ê instruction would get the incorrect values for both operands. These examples illustrate that a data hazard can arise for an instruction when one of its operands is updated by any of the three preceding instructions. These hazards occur because our pipelined processor reads the operands for an instruction from the register ﬁle in the decode stage but does not write the results for the instruction to the register ﬁle until three cycles later, after the instruction passes through the write-back stage. Section 4.5 Pipelined Y86-64 Implementations 469 e_valE 0 + 3 = 3 E_dstE = %rax M_valE = 10 M_dstE = %rdx D valA R[%rdx] = 0 valB R[%rax] = 0 F0x000: irmovq $10,%rdx # prog4# prog4 0x00a: irmovq $3,%rax 0x014: addq %rdx,%rax 0x016: halt DE M W FD E M W FD E M W FD E M W Cycle 4 12345 6 7 8 M E Error Figure 4.46 Pipelined execution of ÉËÇ×r without special pipeline control. In cycle 4, the þ®®Ê instruction reads its source operands from the register ﬁle. The pending write to register cË®Ó is still in the memory stage, and the new value for register cËþÓ is just being computed in the execute stage. Both operands valA and valB get incorrect values. Avoiding Data Hazards by Stalling One very general technique for avoiding hazards involves stalling, where the processor holds back one or more instructions in the pipeline until the hazard condition no longer holds. Our processor can avoid data hazards by holding back an instruction in the decode stage until the instructions generating its source op- erands have passed through the write-back stage. The details of this mechanism will be discussed in Section 4.5.8. It involves simple enhancements to the pipeline control logic. The effect of stalling is diagrammed in Figure 4.47 (ÉËÇ×p) and Fig- ure 4.48 (ÉËÇ×r). (We omit ÉËÇ×q from this discussion, since it operates similarly to the other two examples.) When the þ®®Ê instruction is in the decode stage, the pipeline control logic detects that at least one of the instructions in the exe- cute, memory, or write-back stage will update either register cË®Ó or register cËþÓ. Rather than letting the þ®®Ê instruction pass through the stage with the incorrect results, it stalls the instruction, holding it back in the decode stage for either one (for ÉËÇ×p) or three (for ÉËÇ×r) extra cycles. For all three programs, the þ®®Ê in- struction ﬁnally gets correct values for its two source operands in cycle 7 and then proceeds down the pipeline. 470 Chapter 4 Processor Architecture F0x000: irmovq $10,%rdx # prog2# prog2 0x00a: irmovq $3,%rax 0x014: nop 0x015: nop bubble bubble 0x016: addlq %rdx,%rax 0x018: halt DE M W FD E M W FD E M W FD E M W EM W FD D E WM FF D E WM 12345 6 7 8 910 11 Figure 4.47 Pipelined execution of ÉËÇ×p using stalls. After decoding the þ®®Ê instruction in cycle 6, the stall control logic detects a data hazard due to the pending write to register cËþÓ in the write-back stage. It injects a bubble into the execute stage and repeats the decoding of the þ®®Ê instruction in cycle 7. In effect, the machine has dynamically inserted a ÅÇÉ instruction, giving a ﬂow similar to that shown for ÉËÇ×o (Figure 4.43). F0x000: irmovq $10,%rdx # prog4# prog4 0x00a: irmovq $3,%rax bubblebubble bubblebubble bubblebubble 0x014: addq %rdx,%rax 0x016: halt DE M W FD E M W EM W EM W EM W DDF FF DD E WM FF D E WM 12345 6 7 8 910 11 Figure 4.48 Pipelined execution of ÉËÇ×r using stalls. After decoding the þ®®Ê instruction in cycle 4, the stall control logic detects data hazards for both source registers. It injects a bubble into the execute stage and repeats the decoding of the þ®®Ê instruction on cycle 5. It again detects hazards for both source registers, injects a bubble into the execute stage, and repeats the decoding of the þ®®Ê instruction on cycle 6. Still, it detects a hazard for source register cËþÓ, injects a bubble into the execute stage, and repeats the decoding of the þ®®Ê instruction on cycle 7. In effect, the machine has dynamically inserted three ÅÇÉ instructions, giving a ﬂow similar to that shown for ÉËÇ×o (Figure 4.43). In holding back the þ®®Ê instruction in the decode stage, we must also hold back the ³þÄÎ instruction following it in the fetch stage. We can do this by keeping the program counter at a ﬁxed value, so that the ³þÄÎ instruction will be fetched repeatedly until the stall has completed. Stalling involves holding back one group of instructions in their stages while allowing other instructions to continue ﬂowing through the pipeline. What then should we do in the stages that would normally be processing the þ®®Ê instruction? We handle these by injecting a bubble into the execute stage each time we hold an instruction back in the decode stage. A bubble is like a dynamically generated ÅÇÉ instruction—it does not cause any changes to the registers, the memory, the Section 4.5 Pipelined Y86-64 Implementations 471 Aside Enumerating classes of data hazards Hazards can potentially occur when one instruction updates part of the program state that will be read by a later instruction. For Y86-64, the program state includes the program registers, the program counter, the memory, the condition code register, and the status register. Let us look at the hazard possibilities in our proposed design for each of these forms of state. Program registers. These are the hazards we have already identiﬁed. They arise because the register ﬁle is read in one stage and written in another, leading to possible unintended interactions between different instructions. Program counter. Conﬂicts between updating and reading the program counter give rise to control hazards. No hazard arises when our fetch-stage logic correctly predicts the new value of the program counter before fetching the next instruction. Mispredicted branches and Ë−Î instructions require special handling, as will be discussed in Section 4.5.5. Memory. Writes and reads of the data memory both occur in the memory stage. By the time an instruction reading memory reaches this stage, any preceding instructions writing memory will have already done so. On the other hand, there can be interference between instructions writing data in the memory stage and the reading of instructions in the fetch stage, since the instruction and data memories reference a single address space. This can only happen with programs containing self-modifying code, where instructions write to a portion of memory from which instructions are later fetched. Some systems have complex mechanisms to detect and avoid such hazards, while others simply mandate that programs should not use self- modifying code. We will assume for simplicity that programs do not modify themselves, and therefore we do not need to take special measures to update the instruction memory based on updates to the data memory during program execution. Condition code register. These are written by integer operations in the execute stage. They are read by conditional moves in the execute stage and by conditional jumps in the memory stage. By the time a conditional move or jump reaches the execute stage, any preceding integer operation will have already completed this stage. No hazards can arise. Status register. The program status can be affected by instructions as they ﬂow through the pipeline. Our mechanism of associating a status code with each instruction in the pipeline enables the processor to come to an orderly halt when an exception occurs, as will be discussed in Section 4.5.6. This analysis shows that we only need to deal with register data hazards, control hazards, and making sure exceptions are handled properly. A systematic analysis of this form is important when designing a complex system. It can identify the potential difﬁculties in implementing the system, and it can guide the generation of test programs to be used in checking the correctness of the system. condition codes, or the program status. These are shown as white boxes in the pipeline diagrams of Figures 4.47 and 4.48. In these ﬁgures the arrow between the box labeled “D” for the þ®®Ê instruction and the box labeled “E” for one of the pipeline bubbles indicates that a bubble was injected into the execute stage in place of the þ®®Ê instruction that would normally have passed from the decode to 472 Chapter 4 Processor Architecture the execute stage. We will look at the detailed mechanisms for making the pipeline stall and for injecting bubbles in Section 4.5.8. In using stalling to handle data hazards, we effectively execute programs ÉËÇ×p and ÉËÇ×r by dynamically generating the pipeline ﬂow seen for ÉËÇ×o (Fig- ure 4.43). Injecting one bubble for ÉËÇ×p and three for ÉËÇ×r has the same effect as having three ÅÇÉ instructions between the second ©ËÀÇÌÊ instruction and the þ®®Ê instruction. This mechanism can be implemented fairly easily (see Problem 4.53), but the resulting performance is not very good. There are numerous cases in which one instruction updates a register and a closely following instruction uses the same register. This will cause the pipeline to stall for up to three cycles, reduc- ing the overall throughput signiﬁcantly. Avoiding Data Hazards by Forwarding Our design for PIPE− reads source operands from the register ﬁle in the decode stage, but there can also be a pending write to one of these source registers in the write-back stage. Rather than stalling until the write has completed, it can simply pass the value that is about to be written to pipeline register E as the source operand. Figure 4.49 shows this strategy with an expanded view of the pipeline diagram for cycle 6 of ÉËÇ×p. The decode-stage logic detects that register. . . 0x000: irmovq $10,%rdx # prog2# prog2 0x00a: irmovq $3,%rax 0x014: nop 0x015: nop 0x016: addq %rdx,%rax 0x018: halt srcA = %rdx srcB = %rax W_dstE = %rax W_valE = 3 D valA R[%rdx] = 10 valB W_valE = 3 W Cycle 6 R[%rax] 3 F 12345 6 7 8 910 FD E M W FD E M W FD E M W DE M W FD E M W FD E M W Figure 4.49 Pipelined execution of ÉËÇ×p using forwarding. In cycle 6, the decode- stage logic detects the presence of a pending write to register cËþÓ in the write-back stage. It uses this value for source operand valB rather than the value read from the register ﬁle. Section 4.5 Pipelined Y86-64 Implementations 473. . . F0x000: irmovq $10,%rdx # prog3# prog3 0x00a: irmovq $3,%rax 0x014: nop 0x005: addq %rdx,%rax 0x017: halt DE M W FD E M W FD E M W FD E M W FD E M W 12345 6 7 8 9 srcA = %rdx srcB = %rax W_dstE = %rdx W_valE = 10 valA W_valE = 10 valB M_valE = 3 Cycle 5 R[%rdx] 10 D W M_dstE = %rax M_valE = 3 M Figure 4.50 Pipelined execution of ÉËÇ×q using forwarding. In cycle 5, the decode- stage logic detects a pending write to register cË®Ó in the write-back stage and to register cËþÓ in the memory stage. It uses these as the values for valA and valB rather than the values read from the register ﬁle. cËþÓ is the source register for operand valB, and that there is also a pending write to cËþÓ on write port E. It can therefore avoid stalling by simply using the data word supplied to port E (signal W_valE) as the value for operand valB. This technique of passing a result value directly from one pipeline stage to an earlier one is commonly known as data forwarding (or simply forwarding, and sometimes bypassing). It allows the instructions of ÉËÇ×p to proceed through the pipeline without any stalling. Data forwarding requires adding additional data connections and control logic to the basic hardware structure. As Figure 4.50 illustrates, data forwarding can also be used when there is a pending write to a register in the memory stage, avoiding the need to stall for program ÉËÇ×q. In cycle 5, the decode-stage logic detects a pending write to register cË®Ó on port E in the write-back stage, as well as a pending write to register cËþÓ that is on its way to port E but is still in the memory stage. Rather than stalling until the writes have occurred, it can use the value in the write-back stage (signal W_valE) for operand valA and the value in the memory stage (signal M_valE) for operand valB. 474 Chapter 4 Processor Architecture F0x000: irmovq $10,%rdx # prog4# prog4 0x00a: irmovq $3,%rax 0x014: addq %rdx,%rax 0x016: halt DE M W FD E M W FD E M W FD E M W 12345 6 7 8 srcA = %rdx srcB = %rax M_dstE = %rdx M_valE = 10 valA M_valE = 10 valB e_valE = 3 Cycle 4 D M E_dstE = %rax e_valE 0 + 3 = 3 E Figure 4.51 Pipelined execution of ÉËÇ×r using forwarding. In cycle 4, the decode- stage logic detects a pending write to register cË®Ó in the memory stage. It also detects that a new value is being computed for register cËþÓ in the execute stage. It uses these as the values for valA and valB rather than the values read from the register ﬁle. To exploit data forwarding to its full extent, we can also pass newly computed values from the execute stage to the decode stage, avoiding the need to stall for program ÉËÇ×r, as illustrated in Figure 4.51. In cycle 4, the decode-stage logic detects a pending write to register cË®Ó in the memory stage, and also that the value being computed by the ALU in the execute stage will later be written to register cËþÓ. It can use the value in the memory stage (signal M_valE) for operand valA. It can also use the ALU output (signal e_valE) for operand valB. Note that using the ALU output does not introduce any timing problems. The decode stage only needs to generate signals valA and valB by the end of the clock cycle so that pipeline register E can be loaded with the results from the decode stage as the clock rises to start the next cycle. The ALU output will be valid before this point. The uses of forwarding illustrated in programs ÉËÇ×p to ÉËÇ×r all involve the forwarding of values generated by the ALU and destined for write port E. Forwarding can also be used with values read from the memory and destined for write port M. From the memory stage, we can forward the value that has just been read from the data memory (signal m_valM). From the write-back stage, we can forward the pending write to port M (signal W_valM). This gives a total of ﬁve different forwarding sources (e_valE, m_valM, M_valE, W_valM, and W_valE) and two different forwarding destinations (valA and valB). Section 4.5 Pipelined Y86-64 Implementations 475 The expanded diagrams of Figures 4.49 to 4.51 also show how the decode- stage logic can determine whether to use a value from the register ﬁle or to use a forwarded value. Associated with every value that will be written back to the register ﬁle is the destination register ID. The logic can compare these IDs with the source register IDs srcA and srcB to detect a case for forwarding. It is possible to have multiple destination register IDs match one of the source IDs. We must establish a priority among the different forwarding sources to handle such cases. This will be discussed when we look at the detailed design of the forwarding logic. Figure 4.52 shows the structure of PIPE, an extension of PIPE− that can handle data hazards by forwarding. Comparing this to the structure of PIPE− (Figure 4.41), we can see that the values from the ﬁve forwarding sources are fed back to the two blocks labeled “Sel+Fwd A” and “Fwd B” in the decode stage. The block labeled “Sel+Fwd A” combines the role of the block labeled “Select A” in PIPE− with the forwarding logic. It allows valA for pipeline register E to be either the incremented program counter valP, the value read from the A port of the register ﬁle, or one of the forwarded values. The block labeled “Fwd B” implements the forwarding logic for source operand valB. Load/Use Data Hazards One class of data hazards cannot be handled purely by forwarding, because mem- ory reads occur late in the pipeline. Figure 4.53 illustrates an example of a load/use hazard, where one instruction (the ÀËÀÇÌÊ at address nÓnpv) reads a value from memory for register cËþÓ while the next instruction (the þ®®Ê at address nÓnqp) needs this value as a source operand. Expanded views of cycles 7 and 8 are shown in the lower part of the ﬁgure, where we assume all program registers initially have value 0. The þ®®Ê instruction requires the value of the register in cycle 7, but it is not generated by the ÀËÀÇÌÊ instruction until cycle 8. In order to “forward” from the ÀËÀÇÌÊ to the þ®®Ê, the forwarding logic would have to make the value go backward in time! Since this is clearly impossible, we must ﬁnd some other mech- anism for handling this form of data hazard. (The data hazard for register cË¾Ó, with the value being generated by the ©ËÀÇÌÊ instruction at address nÓno− and used by the þ®®Ê instruction at address nÓnqp, can be handled by forwarding.) As Figure 4.54 demonstrates, we can avoid a load/use data hazard with a combination of stalling and forwarding. This requires modiﬁcations of the con- trol logic, but it can use existing bypass paths. As the ÀËÀÇÌÊ instruction passes through the execute stage, the pipeline control logic detects that the instruction in the decode stage (the þ®®Ê) requires the result read from memory. It stalls the instruction in the decode stage for one cycle, causing a bubble to be injected into the execute stage. As the expanded view of cycle 8 shows, the value read from memory can then be forwarded from the memory stage to the þ®®Ê instruction in the decode stage. The value for register cË¾Ó is also forwarded from the write- back to the memory stage. As indicated in the pipeline diagram by the arrow from the box labeled “D” in cycle 7 to the box labeled “E” in cycle 8, the injected bub- ble replaces the þ®®Ê instruction that would normally continue ﬂowing through the pipeline. 476 Chapter 4 Processor Architecture valA Fwd B W icode valE valM dstE dstM ALU A ALU B ALU fun. M icode Cnd valE valA dstE dstM E icode ifun valC valA valB dstM srcA srcBdstE D icode ifun valC valPrBrA F predPC data out data in M_Cnd dmem_error m_stat M_valE m_valM e_Cnd Memory ALU Execute dstE dstE dstM srcA srcB Sel+Fwd A Predict PC Select PC d_srcA d_srcB W_valM e_dstE M_valA W_valM W_valE W_valE M_valA W_valM CC Decode Fetch read write Data memory Addr Mem. control Register file AB E M PC increment Instruction memory f_pc Stat stat imem_error instr_valid Stat Stat Stat Write back stat stat stat Figure 4.52 Hardware structure of PIPE, our ﬁnal pipelined implementation. The additional bypassing paths enable forwarding the results from the three preceding instructions. This allows us to handle most forms of data hazards without stalling the pipeline. Section 4.5 Pipelined Y86-64 Implementations 477 M_dstE = %rbx M_valE = 10 M M_dstM = %rax m_valM M[128] = 3 M F0x000: irmovq $128,%rdx # prog5# prog5 0x00a: irmovq $3,%rcx 0x014: rmmovq %rcx, 0(%rdx) 0x01e: irmovq $10,%rbx 0x028: mrmovq 0(%rdx),%rax # Load %rax 0x032: addq %ebx,%eax # Use %rax 0x034: halt DE M W FD E M W FD E M W FD E M W FD E M W FD E M W FD E M W 12345 6 7 8 910 11 D valA M_valE = 10 valB R[%rax] = 0 Cycle 7 Cycle 8 Error. . . Figure 4.53 Example of load/use data hazard. The þ®®Ê instruction requires the value of register cËþÓ during the decode stage in cycle 7. The preceding ÀËÀÇÌÊ reads a new value for this register during the memory stage in cycle 8, which is too late for the þ®®Ê instruction. This use of a stall to handle a load/use hazard is called a load interlock. Load interlocks combined with forwarding sufﬁce to handle all possible forms of data hazards. Since only load interlocks reduce the pipeline throughput, we can nearly achieve our throughput goal of issuing one new instruction on every clock cycle. Avoiding Control Hazards Control hazards arise when the processor cannot reliably determine the address of the next instruction based on the current instruction in the fetch stage. As was discussed in Section 4.5.4, control hazards can only occur in our pipelined processor for Ë−Î and jump instructions. Moreover, the latter case only causes dif- ﬁculties when the direction of a conditional jump is mispredicted. In this section, we provide a high-level view of how these hazards can be handled. The detailed implementation will be presented in Section 4.5.8 as part of a more general dis- cussion of the pipeline control. For the Ë−Î instruction, consider the following example program. This pro- gram is shown in assembly code, but with the addresses of the different instructions on the left for reference: 478 Chapter 4 Processor Architecture W_dstE = %rbx W_valE = 10 W M_dstM = %rax m_valM M[128] = 3 M F0x000: irmovq $128,%rdx 0x00a: irmovq $3,%rcx 0x014: rmmovq %rcx, 0(%rdx) 0x01e: irmovq $10,%rbx 0x028: mrmovq 0(%rdx),%rax # Load %rax 0x032: addq %rbx,%rax # Use %rax 0x034: halt # prog5# prog5 DE M W FD E M W FD E M W FD E M W FD E M W EM W DFD E M W FFD E M W 12345 6 7 8 910 11 12 D valA W_valE = 10 valB m_valM = 3 Cycle 8. . . bubblebubble Figure 4.54 Handling a load/use hazard by stalling. By stalling the þ®®Ê instruction for one cycle in the decode stage, the value for valB can be forwarded from the ÀËÀÇÌÊ instruction in the memory stage to the þ®®Ê instruction in the decode stage. nÓnnnx ©ËÀÇÌÊ ÍÎþ²ÂjcËÍÉ a 'Å©Î©þÄ©Ö− ÍÎþ²Â ÉÇ©ÅÎ−Ë nÓnnþx ²þÄÄ ÉËÇ² a –ËÇ²−®ÏË− ²þÄÄ nÓnoqx ©ËÀÇÌÊ bonjcË®Ó a ‡−ÎÏËÅ ÉÇ©ÅÎ nÓno®x ³þÄÎ nÓnpnx lÉÇÍ nÓpn nÓnpnx ÉËÇ²x a ÉËÇ²x nÓnpnx Ë−Î a ‡−ÎÏËÅ ©ÀÀ−®©þÎ−ÄÔ nÓnpox ËËÀÇÌÊ cË®ÓjcË¾Ó a ﬁÇÎ −Ó−²ÏÎ−® nÓnqnx lÉÇÍ nÓqn nÓnqnx ÍÎþ²Âx a ÍÎþ²Âx ·Îþ²Â ÉÇ©ÅÎ−Ë Figure 4.55 shows how we want the pipeline to process the Ë−Î instruction. As with our earlier pipeline diagrams, this ﬁgure shows the pipeline activity with Section 4.5 Pipelined Y86-64 Implementations 479 FD E M W FD E M W FD EM W F DE M W FD E M W FD E M W FD E M W 0x000: irmovq Stack,%edx 0x00a: call proc 0x020: ret 0x013: irmovq $10,%edx # Return point bubblebubble bubblebubble bubblebubble # prog7# prog7 12345 6 7 8 910 11 Figure 4.55 Simpliﬁed view of Ë−Î instruction processing. The pipeline should stall while the Ë−Î passes through the decode, execute, and memory stages, injecting three bubbles in the process. The PC selection logic will choose the return address as the instruction fetch address once the Ë−Î reaches the write-back stage (cycle 7). time growing to the right. Unlike before, the instructions are not listed in the same order they occur in the program, since this program involves a control ﬂow where instructions are not executed in a linear sequence. It is useful to look at the instruction addresses to identify the different instructions in the program. As this diagram shows, the Ë−Î instruction is fetched during cycle 3 and proceeds down the pipeline, reaching the write-back stage in cycle 7. While it passes through the decode, execute, and memory stages, the pipeline cannot do any useful activity. Instead, we want to inject three bubbles into the pipeline. Once the Ë−Î instruction reaches the write-back stage, the PC selection logic will set the program counter to the return address, and therefore the fetch stage will fetch the ©ËÀÇÌÊ instruction at the return point (address nÓnoq). To handle a mispredicted branch, consider the following program, shown in assembly code but with the instruction addresses shown on the left for reference: nÓnnnx ÓÇËÊ cËþÓjcËþÓ nÓnnpx ÁÅ− ÎþË×−Î a ﬁÇÎ ÎþÂ−Å nÓnn¾x ©ËÀÇÌÊ boj cËþÓ a ƒþÄÄ Î³ËÇÏ×³ nÓnosx ³þÄÎ nÓnotx ÎþË×−Îx nÓnotx ©ËÀÇÌÊ bpj cË®Ó a ¶þË×−Î nÓnpnx ©ËÀÇÌÊ bqj cË¾Ó a ¶þË×−Îio nÓnpþx ³þÄÎ Figure 4.56 shows how these instructions are processed. As before, the instruc- tions are listed in the order they enter the pipeline, rather than the order they occur in the program. Since the jump instruction is predicted as being taken, the instruc- tion at the jump target will be fetched in cycle 3, and the instruction following this one will be fetched in cycle 4. By the time the branch logic detects that the jump should not be taken during cycle 4, two instructions have been fetched that should not continue being executed. Fortunately, neither of these instructions has caused a change in the programmer-visible state. That can only occur when an instruction 480 Chapter 4 Processor Architecture FD E M W FD E M W FD EM W F DE M W FD E M W FD E M W 0x000: xorq %rax,%rax 0x002: jne target # Not taken 0x016: irmovl $2,%rdx # Target 0x020: irmovl $3,%rbx # Target+1 0x00b: irmovq $1,%rax # Fall through 0x015: halt # prog7 12345 6 7 8 910 bubblebubble bubblebubble Figure 4.56 Processing mispredicted branch instructions. The pipeline predicts branches will be taken and so starts fetching instructions at the jump target. Two instructions are fetched before the misprediction is detected in cycle 4 when the jump instruction ﬂows through the execute stage. In cycle 5, the pipeline cancels the two target instructions by injecting bubbles into the decode and execute stages, and it also fetches the instruction following the jump. reaches the execute stage, where it can cause the condition codes to change. At this point, the pipeline can simply cancel (sometimes called instruction squashing) the two misfetched instructions by injecting bubbles into the decode and execute stages on the following cycle while also fetching the instruction following the jump instruction. The two misfetched instructions will then simply disappear from the pipeline and therefore not have any effect on the programmer-visible state. The only drawback is that two clock cycles’ worth of instruction processing capability have been wasted. This discussion of control hazards indicates that they can be handled by careful consideration of the pipeline control logic. Techniques such as stalling and injecting bubbles into the pipeline dynamically adjust the pipeline ﬂow when special conditions arise. As we will discuss in Section 4.5.8, a simple extension to the basic clocked register design will enable us to stall stages and to inject bubbles into pipeline registers as part of the pipeline control logic. 4.5.6 Exception Handling As we will discuss in Chapter 8, a variety of activities in a processor can lead to exceptional control ﬂow, where the normal chain of program execution gets broken. Exceptions can be generated either internally, by the executing program, or externally, by some outside signal. Our instruction set architecture includes three different internally generated exceptions, caused by (1) a ³þÄÎ instruction, (2) an instruction with an invalid combination of instruction and function code, and (3) an attempt to access an invalid address, either for instruction fetch or data read or write. A more complete processor design would also handle external exceptions, such as when the processor receives a signal that the network interface has received a new packet or the user has clicked a mouse button. Handling Section 4.5 Pipelined Y86-64 Implementations 481 exceptions correctly is a challenging aspect of any microprocessor design. They can occur at unpredictable times, and they require creating a clean break in the ﬂow of instructions through the processor pipeline. Our handling of the three internal exceptions gives just a glimpse of the true complexity of correctly detecting and handling exceptions. Let us refer to the instruction causing the exception as the excepting instruc- tion. In the case of an invalid instruction address, there is no actual excepting instruction, but it is useful to think of there being a sort of “virtual instruction” at the invalid address. In our simpliﬁed ISA model, we want the processor to halt when it reaches an exception and to set the appropriate status code, as listed in Fig- ure 4.5. It should appear that all instructions up to the excepting instruction have completed, but none of the following instructions should have any effect on the programmer-visible state. In a more complete design, the processor would con- tinue by invoking an exception handler, a procedure that is part of the operating system, but implementing this part of exception handling is beyond the scope of our presentation. In a pipelined system, exception handling involves several subtleties. First, it is possible to have exceptions triggered by multiple instructions simultaneously. For example, during one cycle of pipeline operation, we could have a ³þÄÎ instruction in the fetch stage, and the data memory could report an out-of-bounds data address for the instruction in the memory stage. We must determine which of these exceptions the processor should report to the operating system. The basic rule is to put priority on the exception triggered by the instruction that is furthest along the pipeline. In the example above, this would be the out-of-bounds address attempted by the instruction in the memory stage. In terms of the machine-language program, the instruction in the memory stage should appear to execute before one in the fetch stage, and therefore only this exception should be reported to the operating system. A second subtlety occurs when an instruction is ﬁrst fetched and begins execution, causes an exception, and later is canceled due to a mispredicted branch. The following is an example of such a program in its object-code form: nÓnnnx tqnn Ú ÓÇËÊ cËþÓjcËþÓ nÓnnpx urotnnnnnnnnnnnnnn Ú ÁÅ− ÎþË×−Î a ﬁÇÎ ÎþÂ−Å nÓnn¾x qnðnnonnnnnnnnnnnnnn Ú ©ËÀÇÌÊ boj cËþÓ a ƒþÄÄ Î³ËÇÏ×³ nÓnosx nn Ú ³þÄÎ nÓnotx Ú ÎþË×−Îx nÓnotx ðð Ú l¾ÔÎ− nÓƒƒ a 'ÅÌþÄ©® ©ÅÍÎËÏ²Î©ÇÅ ²Ç®− In this program, the pipeline will predict that the branch should be taken, and so it will fetch and attempt to use a byte with value nÓƒƒ as an instruction (generated in the assembly code using the l¾ÔÎ− directive). The decode stage will therefore detect an invalid instruction exception. Later, the pipeline will discover that the branch should not be taken, and so the instruction at address nÓnot should never even have been fetched. The pipeline control logic will cancel this instruction, but we want to avoid raising an exception. 482 Chapter 4 Processor Architecture A third subtlety arises because a pipelined processor updates different parts of the system state in different stages. It is possible for an instruction following one causing an exception to alter some part of the state before the excepting instruction completes. For example, consider the following code sequence, in which we assume that user programs are not allowed to access addresses at the upper end of the 64-bit range: 1 ©ËÀÇÌÊ bojcËþÓ 2 ÓÇËÊ cËÍÉjcËÍÉ a ·−Î ÍÎþ²Â ÉÇ©ÅÎ−Ë ÎÇ n þÅ® ££ ÎÇ onn 3 ÉÏÍ³Ê cËþÓ a ¡ÎÎ−ÀÉÎ ÎÇ ÑË©Î− ÎÇ nÓðððððððððððððððv 4 þ®®Ê cËþÓjcËþÓ a f·³ÇÏÄ® ÅÇÎ ¾− −Ó−²ÏÎ−®g „ÇÏÄ® Í−Î ££ ÎÇ nnn The ÉÏÍ³Ê instruction causes an address exception, because decrementing the stack pointer causes it to wrap around to nÓðððððððððððððððv. This exception is detected in the memory stage. On the same cycle, the þ®®Ê instruction is in the execute stage, and it will cause the condition codes to be set to new values. This would violate our requirement that none of the instructions following the excepting instruction should have had any effect on the system state. In general, we can both correctly choose among the different exceptions and avoid raising exceptions for instructions that are fetched due to mispredicted branches by merging the exception-handling logic into the pipeline structure. That is the motivation for us to include a status code stat in each of our pipeline registers (Figures 4.41 and 4.52). If an instruction generates an exception at some stage in its processing, the status ﬁeld is set to indicate the nature of the exception. The exception status propagates through the pipeline with the rest of the information for that instruction, until it reaches the write-back stage. At this point, the pipeline control logic detects the occurrence of the exception and stops execution. To avoid having any updating of the programmer-visible state by instructions beyond the excepting instruction, the pipeline control logic must disable any updating of the condition code register or the data memory when an instruction in the memory or write-back stages has caused an exception. In the example program above, the control logic will detect that the ÉÏÍ³Ê in the memory stage has caused an exception, and therefore the updating of the condition code register by the þ®®Ê instruction in the execute stage will be disabled. Let us consider how this method of handling exceptions deals with the sub- tleties we have mentioned. When an exception occurs in one or more stages of a pipeline, the information is simply stored in the status ﬁelds of the pipeline reg- isters. The event has no effect on the ﬂow of instructions in the pipeline until an excepting instruction reaches the ﬁnal pipeline stage, except to disable any updat- ing of the programmer-visible state (the condition code register and the memory) by later instructions in the pipeline. Since instructions reach the write-back stage in the same order as they would be executed in a nonpipelined processor, we are guaranteed that the ﬁrst instruction encountering an exception will arrive ﬁrst in the write-back stage, at which point program execution can stop and the status code in pipeline register W can be recorded as the program status. If some in- struction is fetched but later canceled, any exception status information about the Section 4.5 Pipelined Y86-64 Implementations 483 instruction gets canceled as well. No instruction following one that causes an ex- ception can alter the programmer-visible state. The simple rule of carrying the exception status together with all other information about an instruction through the pipeline provides a simple and reliable mechanism for handling exceptions. 4.5.7 PIPE Stage Implementations We have now created an overall structure for PIPE, our pipelined Y86-64 proces- sor with forwarding. It uses the same set of hardware units as the earlier sequential designs, with the addition of pipeline registers, some reconﬁgured logic blocks, and additional pipeline control logic. In this section, we go through the design of the different logic blocks, deferring the design of the pipeline control logic to the next section. Many of the logic blocks are identical to their counterparts in SEQ and SEQ+, except that we must choose proper versions of the different signals from the pipeline registers (written with the pipeline register name, written in upper- case, as a preﬁx) or from the stage computations (written with the ﬁrst character of the stage name, written in lowercase, as a preﬁx). As an example, compare the HCL code for the logic that generates the srcA signal in SEQ to the corresponding code in PIPE: a £Ç®− ðËÇÀ ·¥† ÑÇË® ÍË²¡ { ‰ ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j '‡››ﬂ‚†j 'ﬂ–†j '–•·¤† Û x Ë¡y ©²Ç®− ©Å Õ '–ﬂ–†j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î Å−−® Ë−×©ÍÎ−Ë `y a £Ç®− ðËÇÀ –'–¥ ÑÇË® ®ˆÍË²¡ { ‰ ⁄ˆ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j '‡››ﬂ‚†j 'ﬂ–†j '–•·¤† Û x ⁄ˆË¡y ⁄ˆ©²Ç®− ©Å Õ '–ﬂ–†j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î Å−−® Ë−×©ÍÎ−Ë `y They differ only in the preﬁxes added to the PIPE signals: ⁄ˆ for the source values, to indicate that the signals come from pipeline register D, and ®ˆ for the result value, to indicate that it is generated in the decode stage. To avoid repetition, we will not show the HCL code here for blocks that only differ from those in SEQ because of the preﬁxes on names. As a reference, the complete HCL code for PIPE is given in Web Aside arch:hcl on page 508. PC Selection and Fetch Stage Figure 4.57 provides a detailed view of the PIPE fetch stage logic. As discussed earlier, this stage must also select a current value for the program counter and predict the next PC value. The hardware units for reading the instruction from 484 Chapter 4 Processor Architecture Need valC Need regids Predict PC Select PC PC increment Align Bytes 1–9 f_pc Byte 0 imem_error Instruction memory Split Instr valid D icodestat ifun valC valPrBrA F predPC M_icode M_Cnd M_valA W_icode W_valM icode Stat ifun Figure 4.57 PIPE PC selection and fetch logic. Within the one cycle time limit, the processor can only predict the address of the next instruction. memory and for extracting the different instruction ﬁelds are the same as those we considered for SEQ (see the fetch stage in Section 4.3.4). The PC selection logic chooses between three program counter sources. As a mispredicted branch enters the memory stage, the value of valP for this instruction (indicating the address of the following instruction) is read from pipeline register M (signal M_valA). When a Ë−Î instruction enters the write-back stage, the return address is read from pipeline register W (signal W_valM). All other cases use the predicted value of the PC, stored in pipeline register F (signal F_predPC): ÑÇË® ðˆÉ² { ‰ a ›©ÍÉË−®©²Î−® ¾ËþÅ²³l ƒ−Î²³ þÎ ©Å²Ë−À−ÅÎ−® –£ ›ˆ©²Ç®− {{ '“”” dd _›ˆ£Å® x ›ˆÌþÄ¡y a £ÇÀÉÄ−Î©ÇÅ Çð ‡¥¶ ©ÅÍÎËÏ²Î©ÇÅ „ˆ©²Ç®− {{ '‡¥¶ x „ˆÌþÄ›y a ⁄−ðþÏÄÎx •Í− ÉË−®©²Î−® ÌþÄÏ− Çð –£ o x ƒˆÉË−®–£y `y Section 4.5 Pipelined Y86-64 Implementations 485 The PC prediction logic chooses valC for the fetched instruction when it is either a call or a jump, and valP otherwise: ÑÇË® ðˆÉË−®–£ { ‰ ðˆ©²Ç®− ©Å Õ '“””j '£¡‹‹ Û x ðˆÌþÄ£y o x ðˆÌþÄ–y `y The logic blocks labeled “Instr valid,” “Need regids,” and “Need valC” are the same as for SEQ, with appropriately named source signals. Unlike in SEQ, we must split the computation of the instruction status into two parts. In the fetch stage, we can test for a memory error due to an out-of-range instruction address, and we can detect an illegal instruction or a ³þÄÎ instruction. Detecting an invalid data address must be deferred to the memory stage. Practice Problem 4.30 (solution page 526) Write HCL code for the signal f_stat, providing the provisional status for the fetched instruction. Decode and Write-Back Stages Figure 4.58 gives a detailed view of the decode and write-back logic for PIPE. The blocks labeled dstE, dstM, srcA, and srcB are very similar to their counterparts in the implementation of SEQ. Observe that the register IDs supplied to the write ports come from the write-back stage (signals W_dstE and W_dstM), rather than from the decode stage. This is because we want the writes to occur to the destination registers speciﬁed by the instruction in the write-back stage. Practice Problem 4.31 (solution page 526) The block labeled “dstE” in the decode stage generates the register ID for the E port of the register ﬁle, based on ﬁelds from the fetched instruction in pipeline register D. The resulting signal is named d_dstE in the HCL description of PIPE. Write HCL code for this signal, based on the HCL description of the SEQ signal dstE. (See the decode stage for SEQ in Section 4.3.4.) Do not concern yourself with the logic to implement conditional moves yet. Most of the complexity of this stage is associated with the forwarding logic. As mentioned earlier, the block labeled “Sel+Fwd A” serves two roles. It merges the valP signal into the valA signal for later stages in order to reduce the amount of state in the pipeline register. It also implements the forwarding logic for source operand valA. The merging of signals valA and valP exploits the fact that only the ²þÄÄ and jump instructions need the value of valP in later stages, and these instructions 486 Chapter 4 Processor Architecture D icode ifun valC valPrBrA AB srcA srcB dstM M dstE E Register file e_dstE e_valE d_rvalA d_rvalB E icodestat stat ifun valC valA valB Sel+Fwd A Fwd B dstE dstM srcA srcB dstE dstM srcA srcB M_dstE M_valE M_dstM m_valM W_dstM W_valM W_dstE W_valE d_srcA d_srcB Figure 4.58 PIPE decode and write-back stage logic. No instruction requires both valP and the value read from register port A, and so these two can be merged to form the signal valA for later stages. The block labeled “Sel+Fwd A” performs this task and also implements the forwarding logic for source operand valA. The block labeled “Fwd B” implements the forwarding logic for source operand valB. The register write locations are speciﬁed by the dstE and dstM signals from the write-back stage rather than from the decode stage, since it is writing the results of the instruction currently in the write-back stage. do not need the value read from the A port of the register ﬁle. This selection is controlled by the icode signal for this stage. When signal D_icode matches the instruction code for either ²þÄÄ or Á””, this block should select D_valP as its output. As mentioned in Section 4.5.5, there are ﬁve different forwarding sources, each with a data word and a destination register ID: Section 4.5 Pipelined Y86-64 Implementations 487 Data word Register ID Source description e_valE e_dstE ALU output m_valM M_dstM Memory output M_valE M_dstE Pending write to port E in memory stage W_valM W_dstM Pending write to port M in write-back stage W_valE W_dstE Pending write to port E in write-back stage If none of the forwarding conditions hold, the block should select d_rvalA, the value read from register port A, as its output. Putting all of this together, we get the following HCL description for the new value of valA for pipeline register E: ÑÇË® ®ˆÌþÄ¡ { ‰ ⁄ˆ©²Ç®− ©Å Õ '£¡‹‹j '“”” Û x ⁄ˆÌþÄ–y a •Í− ©Å²Ë−À−ÅÎ−® –£ ®ˆÍË²¡ {{ −ˆ®ÍÎ¥ x −ˆÌþÄ¥y a ƒÇËÑþË® ÌþÄ¥ ðËÇÀ −Ó−²ÏÎ− ®ˆÍË²¡ {{ ›ˆ®ÍÎ› x ÀˆÌþÄ›y a ƒÇËÑþË® ÌþÄ› ðËÇÀ À−ÀÇËÔ ®ˆÍË²¡ {{ ›ˆ®ÍÎ¥ x ›ˆÌþÄ¥y a ƒÇËÑþË® ÌþÄ¥ ðËÇÀ À−ÀÇËÔ ®ˆÍË²¡ {{ „ˆ®ÍÎ› x „ˆÌþÄ›y a ƒÇËÑþË® ÌþÄ› ðËÇÀ ÑË©Î− ¾þ²Â ®ˆÍË²¡ {{ „ˆ®ÍÎ¥ x „ˆÌþÄ¥y a ƒÇËÑþË® ÌþÄ¥ ðËÇÀ ÑË©Î− ¾þ²Â o x ®ˆËÌþÄ¡y a •Í− ÌþÄÏ− Ë−þ® ðËÇÀ Ë−×©ÍÎ−Ë ð©Ä− `y The priority given to the ﬁve forwarding sources in the above HCL code is very important. This priority is determined in the HCL code by the order in which the ﬁve destination register IDs are tested. If any order other than the one shown were chosen, the pipeline would behave incorrectly for some programs. Figure 4.59 shows an example of a program that requires a correct setting of priority among the forwarding sources in the execute and memory stages. In this program, the ﬁrst two instructions write to register cË®Ó, while the third uses this register as its source operand. When the ËËÀÇÌÊ instruction reaches the decode stage in cycle 4, the forwarding logic must choose between two values destined for its source register. Which one should it choose? To set the priority, we must consider the behavior of the machine-language program when it is executed one instruction at a time. The ﬁrst ©ËÀÇÌÊ instruction would set register cË®Ó to 10, the second would set the register to 3, and then the ËËÀÇÌÊ instruction would read 3 from cË®Ó. To imitate this behavior, our pipelined implementation should always give priority to the forwarding source in the earliest pipeline stage, since it holds the latest instruction in the program sequence setting the register. Thus, the logic in the HCL code above ﬁrst tests the forwarding source in the execute stage, then those in the memory stage, and ﬁnally the sources in the write-back stage. The forwarding priority between the two sources in either the memory or the write-back stages is only a concern for the instruction ÉÇÉÊ cËÍÉ, since only this instruction can attempt two simultaneous writes to the same register. 488 Chapter 4 Processor Architecture W F0x000: irmovq $10,%rdx 0x00a: irmovq $3,%rdx 0x014: rrmovq %rdx,%rax 0x016: halt # prog8# prog8 DE M W FD E M W FD E M FD E M W 12345 6 7 8 D valA e_valE = 3 Cycle 4 M_dstE = %rdx M_valE = 10 srcA = %rdx M E_dstE = %rdx e_valE 0 + 3 = 3 E Figure 4.59 Demonstration of forwarding priority. In cycle 4, values for cË®Ó are available from both the execute and memory stages. The forwarding logic should choose the one in the execute stage, since it represents the most recently generated value for this register. Practice Problem 4.32 (solution page 526) Suppose the order of the third and fourth cases (the two forwarding sources from the memory stage) in the HCL code for d_valA were reversed. Describe the resulting behavior of the ËËÀÇÌÊ instruction (line 5) for the following program: 1 ©ËÀÇÌÊ bsj cË®Ó 2 ©ËÀÇÌÊ bnÓonnjcËÍÉ 3 ËÀÀÇÌÊ cË®ÓjnfcËÍÉg 4 ÉÇÉÊ cËÍÉ 5 ËËÀÇÌÊ cËÍÉjcËþÓ Practice Problem 4.33 (solution page 527) Suppose the order of the ﬁfth and sixth cases (the two forwarding sources from the write-back stage) in the HCL code for d_valA were reversed. Write a Y86-64 program that would be executed incorrectly. Describe how the error would occur and its effect on the program behavior. Section 4.5 Pipelined Y86-64 Implementations 489 Practice Problem 4.34 (solution page 527) Write HCL code for the signal d_valB, giving the value for source operand valB supplied to pipeline register E. One small part of the write-back stage remains. As shown in Figure 4.52, the overall processor status Stat is computed by a block based on the status value in pipeline register W. Recall from Section 4.1.1 that the code should indicate either normal operation (¡ﬂ«) or one of the three exception conditions. Since pipeline register W holds the state of the most recently completed instruction, it is natural to use this value as an indication of the overall processor status. The only special case to consider is when there is a bubble in the write-back stage. This is part of normal operation, and so we want the status code to be ¡ﬂ« for this case as well: ÑÇË® ·ÎþÎ { ‰ „ˆÍÎþÎ {{ ·¢•¢ x ·¡ﬂ«y o x „ˆÍÎþÎy `y Execute Stage Figure 4.60 shows the execute stage logic for PIPE. The hardware units and the logic blocks are identical to those in SEQ, with an appropriate renaming of signals. We can see the signals e_valE and e_dstE directed toward the decode stage as one of the forwarding sources. One difference is that the logic labeled “Set CC,” which determines whether or not to update the condition codes, has signals m_stat and W_stat as inputs. These signals are used to detect cases where an instruction e_Cnd W_stat m_stat e_valE e_dstE M icodestat stat Cnd valE valA dstE dstM E icode ifun valC valA valB dstM srcA srcBdstE ALU A Set CC ALU B ALU fun. ALUCC cond dstE Figure 4.60 PIPE execute stage logic. This part of the design is very similar to the logic in the SEQ implementation. 490 Chapter 4 Processor Architecture Stat Stat M_icode M_Cnd W_icode W_dstM m_valM M_dstE M_dstM M_valA M_valE W_dstE W_valM W_valE W icode valE valM dstE dstM M icode stat stat Cnd valE valA dstE dstM data out data in read m_stat dmem_error write Data memory Addr Stat Mem. read Mem. write Figure 4.61 PIPE memory stage logic. Many of the signals from pipeline registers M and W are passed down to earlier stages to provide write-back results, instruction addresses, and forwarded results. causing an exception is passing through later pipeline stages, and therefore any updating of the condition codes should be suppressed. This aspect of the design is discussed in Section 4.5.8. Practice Problem 4.35 (solution page 527) Our second case in the HCL code for d_valA uses signal e_dstE to see whether to select the ALU output e_valE as the forwarding source. Suppose instead that we use signal E_dstE, the destination register ID in pipeline register E for this selection. Write a Y86-64 program that would give an incorrect result with this modiﬁed forwarding logic. Memory Stage Figure 4.61 shows the memory stage logic for PIPE. Comparing this to the memory stage for SEQ (Figure 4.30), we see that, as noted before, the block labeled “Mem. data” in SEQ is not present in PIPE. This block served to select between data sources valP (for ²þÄÄ instructions) and valA, but this selection is now performed by the block labeled “Sel+Fwd A” in the decode stage. Most other blocks in this stage are identical to their counterparts in SEQ, with an appropriate renaming of the signals. In this ﬁgure, you can also see that many of the values in pipeline registers and M and W are supplied to other parts of the circuit as part of the forwarding and pipeline control logic. Section 4.5 Pipelined Y86-64 Implementations 491 Practice Problem 4.36 (solution page 528) In this stage, we can complete the computation of the status code Stat by detecting the case of an invalid address for the data memory. Write HCL code for the signal m_stat. 4.5.8 Pipeline Control Logic We are now ready to complete our design for PIPE by creating the pipeline control logic. This logic must handle the following four control cases for which other mechanisms, such as data forwarding and branch prediction, do not sufﬁce: Load/use hazards. The pipeline must stall for one cycle between an instruction that reads a value from memory and an instruction that uses this value. Processing Ë−Î. The pipeline must stall until the Ë−Î instruction reaches the write-back stage. Mispredicted branches. By the time the branch logic detects that a jump should not have been taken, several instructions at the branch target will have started down the pipeline. These instructions must be canceled, and fetch- ing should begin at the instruction following the jump instruction. Exceptions. When an instruction causes an exception, we want to disable the updating of the programmer-visible state by later instructions and halt execution once the excepting instruction reaches the write-back stage. We will go through the desired actions for each of these cases and then develop control logic to handle all of them. Desired Handling of Special Control Cases For a load/use hazard, we have described the desired pipeline operation in Section 4.5.5, as illustrated by the example of Figure 4.54. Only the ÀËÀÇÌÊ and ÉÇÉÊ instructions read data from memory. When (1) either of these is in the execute stage and (2) an instruction requiring the destination register is in the decode stage, we want to hold back the second instruction in the decode stage and inject a bubble into the execute stage on the next cycle. After this, the forwarding logic will resolve the data hazard. The pipeline can hold back an instruction in the decode stage by keeping pipeline register D in a ﬁxed state. In doing so, it should also keep pipeline register F in a ﬁxed state, so that the next instruction will be fetched a second time. In summary, implementing this pipeline ﬂow requires detecting the hazard condition, keeping pipeline registers F and D ﬁxed, and injecting a bubble into the execute stage. For the processing of a Ë−Î instruction, we have described the desired pipeline operation in Section 4.5.5. The pipeline should stall for three cycles until the return address is read as the Ë−Î instruction passes through the memory stage. 492 Chapter 4 Processor Architecture This was illustrated by a simpliﬁed pipeline diagram in Figure 4.55 for processing the following program: nÓnnnx ©ËÀÇÌÊ ÍÎþ²ÂjcËÍÉ a 'Å©Î©þÄ©Ö− ÍÎþ²Â ÉÇ©ÅÎ−Ë nÓnnþx ²þÄÄ ÉËÇ² a –ËÇ²−®ÏË− ²þÄÄ nÓnoqx ©ËÀÇÌÊ bonjcË®Ó a ‡−ÎÏËÅ ÉÇ©ÅÎ nÓno®x ³þÄÎ nÓnpnx lÉÇÍ nÓpn nÓnpnx ÉËÇ²x a ÉËÇ²x nÓnpnx Ë−Î a ‡−ÎÏËÅ ©ÀÀ−®©þÎ−ÄÔ nÓnpox ËËÀÇÌÊ cË®ÓjcË¾Ó a ﬁÇÎ −Ó−²ÏÎ−® nÓnqnx lÉÇÍ nÓqn nÓnqnx ÍÎþ²Âx a ÍÎþ²Âx ·Îþ²Â ÉÇ©ÅÎ−Ë Figure 4.62 provides a detailed view of the processing of the Ë−Î instruction for the example program. The key observation here is that there is no way to inject a bubble into the fetch stage of our pipeline. On every cycle, the fetch stage reads some instruction from the instruction memory. Looking at the HCL code for implementing the PC prediction logic in Section 4.5.7, we can see that for the Ë−Î instruction, the new value of the PC is predicted to be valP, the address of the following instruction. In our example program, this would be nÓnpo, the address of the ËËÀÇÌÊ instruction following the Ë−Î. This prediction is not correct for this example, nor would it be for most cases, but we are not attempting to predict return addresses correctly in our design. For three clock cycles, the fetch stage stalls, causing the ËËÀÇÌÊ instruction to be fetched but then replaced by a bubble in the decode stage. This process is illustrated in Figure 4.62 by the three fetches, with an arrow leading down to the bubbles passing through the remaining pipeline stages. Finally, the ©ËÀÇÌÊ instruction is fetched on cycle 7. Comparing Figure 4.62 with FD E M W FD E M W FD EM W F DE M W F D F EM W FD E M W 0x000: irmovq Stack,%rsp 0x00a: call proc 0x020: ret 0x021: rrmovq %rdx,%rbx # Not executed 0x021: rrmovq %rdx,%rbx # Not executed 0x021: rrmovq %rdx,%rbx # Not executed 0x013: irmovq $10,%rdx # Return point bubblebubble bubblebubble bubblebubble # prog6# prog6 12345 6 7 8 910 11 DE M W Figure 4.62 Detailed processing of the Ë−Î instruction. The fetch stage repeatedly fetches the ËËÀÇÌÊ instruction following the Ë−Î instruction, but then the pipeline control logic injects a bubble into the decode stage rather than allowing the ËËÀÇÌÊ instruction to proceed. The resulting behavior is equivalent to that shown in Figure 4.55. Section 4.5 Pipelined Y86-64 Implementations 493 Figure 4.55, we see that our implementation achieves the desired effect, but with a slightly peculiar fetching of an incorrect instruction for three consecutive cycles. When a mispredicted branch occurs, we have described the desired pipeline operation in Section 4.5.5 and illustrated it in Figure 4.56. The misprediction will be detected as the jump instruction reaches the execute stage. The control logic then injects bubbles into the decode and execute stages on the next cycle, causing the two incorrectly fetched instructions to be canceled. On the same cycle, the pipeline reads the correct instruction into the fetch stage. For an instruction that causes an exception, we must make the pipelined im- plementation match the desired ISA behavior, with all prior instructions complet- ing and with none of the following instructions having any effect on the program state. Achieving these effects is complicated by the facts that (1) exceptions are detected during two different stages (fetch and memory) of program execution, and (2) the program state is updated in three different stages (execute, memory, and write-back). Our stage designs include a status code stat in each pipeline register to track the status of each instruction as it passes through the pipeline stages. When an exception occurs, we record that information as part of the instruction’s status and continue fetching, decoding, and executing instructions as if nothing were amiss. As the excepting instruction reaches the memory stage, we take steps to prevent later instructions from modifying the programmer-visible state by (1) disabling the setting of condition codes by instructions in the execute stage, (2) injecting bubbles into the memory stage to disable any writing to the data memory, and (3) stalling the write-back stage when it has an excepting instruction, thus bringing the pipeline to a halt. The pipeline diagram in Figure 4.63 illustrates how our pipeline control han- dles the situation where an instruction causing an exception is followed by one that would change the condition codes. On cycle 6, the ÉÏÍ³Ê instruction reaches the memory stage and generates a memory error. On the same cycle, the þ®®Ê instruc- tion in the execute stage generates new values for the condition codes. We disable the setting of condition codes when an excepting instruction is in the memory or write-back stage (by examining the signals m_stat and W_stat and then setting the signal set_cc to zero). We can also see the combination of injecting bubbles into the memory stage and stalling the excepting instruction in the write-back stage in the example of Figure 4.63—the ÉÏÍ³Ê instruction remains stalled in the write-back stage, and none of the subsequent instructions get past the execute stage. By this combination of pipelining the status signals, controlling the setting of condition codes, and controlling the pipeline stages, we achieve the desired behav- ior for exceptions: all instructions prior to the excepting instruction are completed, while none of the following instructions has any effect on the programmer-visible state. Detecting Special Control Conditions Figure 4.64 summarizes the conditions requiring special pipeline control. It gives expressions describing the conditions under which the three special cases arise. 494 Chapter 4 Processor Architecture FD E M W FD E M W FD E FD E FD E MW W W W 0x000: irmovq $1,%rax 0x00a: xorq %rsp,%rsp #CC = 100 0x00c: pushq %rax 0x00e: addq %rax,%rax 0x010: irmovq $2,%rax # prog10# prog10 12345 6 7 8 910 . . . Cycle 6 mem_error = 1 set_cc ← 0 M New CC = 000 E Figure 4.63 Processing invalid memory reference exception. On cycle 6, the invalid memory reference by the ÉÏÍ³Ê instruction causes the updating of the condition codes to be disabled. The pipeline starts injecting bubbles into the memory stage and stalling the excepting instruction in the write-back stage. Condition Trigger Processing Ë−Î IRET ∈{D icode, E icode, M icode} Load/use hazard E icode ∈{IMRMOVQ, IPOPQ} dd E dstM ∈{d srcA, d srcB} Mispredicted branch E icode = IJXX dd _e Cnd Exception m stat ∈{SADR, SINS, SHLT} ÚÚ W stat ∈{SADR, SINS, SHLT} Figure 4.64 Detection conditions for pipeline control logic. Four different conditions require altering the pipeline ﬂow by either stalling the pipeline or canceling partially executed instructions. These expressions are implemented by simple blocks of combinational logic that must generate their results before the end of the clock cycle in order to control the action of the pipeline registers as the clock rises to start the next cycle. During a clock cycle, pipeline registers D, E, and M hold the states of the instructions that are in the decode, execute, and memory pipeline stages, respectively. As we approach the end of the clock cycle, signals d_srcA and d_srcB will be set to the register IDs of the source operands for the instruction in the decode stage. Detecting a Ë−Î instruction as it passes through the pipeline simply involves checking the instruction codes of the instructions in the decode, execute, and memory stages. Detecting a load/use hazard involves checking the instruction type (ÀËÀÇÌÊ or ÉÇÉÊ) of the instruction in the execute stage and comparing its destination register with the source registers of the instruction in the decode stage. The pipeline control logic should detect a mispredicted branch while the jump Section 4.5 Pipelined Y86-64 Implementations 495 instruction is in the execute stage, so that it can set up the conditions required to recover from the misprediction as the instruction enters the memory stage. When a jump instruction is in the execute stage, the signal e_Cnd indicates whether or not the jump should be taken. We detect an excepting instruction by examining the instruction status values in the memory and write-back stages. For the memory stage, we use the signal m_stat, computed within the stage, rather than M_stat from the pipeline register. This internal signal incorporates the possibility of a data memory address error. Pipeline Control Mechanisms Figure 4.65 shows low-level mechanisms that allow the pipeline control logic to hold back an instruction in a pipeline register or to inject a bubble into the pipeline. These mechanisms involve small extensions to the basic clocked register described x y x x n o p x State = x (a) Normal State = y Input = y stall = 0 bubble = 0 Output = x Output = yRising clock State = x (b) Stall State = x Input = y stall = 1 bubble = 0 Output = x Output = xRising clock State = x (c) Bubble State = nop Input = y stall = 0 bubble = 1 Output = x Output = nopRising clock Figure 4.65 Additional pipeline register operations. (a) Under normal conditions, the state and output of the register are set to the value at the input when the clock rises. (b) When operated in stall mode, the state is held ﬁxed at its previous value. (c) When operated in bubble mode, the state is overwritten with that of a ÅÇÉ operation. 496 Chapter 4 Processor Architecture Pipeline register Condition F D E M W Processing Ë−Î stall bubble normal normal normal Load/use hazard stall stall bubble normal normal Mispredicted branch normal bubble bubble normal normal Figure 4.66 Actions for pipeline control logic. The different conditions require altering the pipeline ﬂow by either stalling the pipeline or canceling partially executed instructions. in Section 4.2.5. Suppose that each pipeline register has two control inputs stall and bubble. The settings of these signals determine how the pipeline register is updated as the clock rises. Under normal operation (Figure 4.65(a)), both of these inputs are set to 0, causing the register to load its input as its new state. When the stall signal is set to 1 (Figure 4.65(b)), the updating of the state is disabled. Instead, the register will remain in its previous state. This makes it possible to hold back an instruction in some pipeline stage. When the bubble signal is set to 1 (Figure 4.65(c)), the state of the register will be set to some ﬁxed reset conﬁguration, giving a state equivalent to that of a ÅÇÉ instruction. The particular pattern of ones and zeros for a pipeline register’s reset conﬁguration depends on the set of ﬁelds in the pipeline register. For example, to inject a bubble into pipeline register D, we want the icode ﬁeld to be set to the constant value 'ﬁﬂ– (Figure 4.26). To inject a bubble into pipeline register E, we want the icode ﬁeld to be set to 'ﬁﬂ– and the dstE, dstM, srcA, and srcB ﬁelds to be set to the constant ‡ﬁﬂﬁ¥. Determining the reset conﬁguration is one of the tasks for the hardware designer in designing a pipeline register. We will not concern ourselves with the details here. We will consider it an error to set both the bubble and the stall signals to 1. The table in Figure 4.66 shows the actions the different pipeline stages should take for each of the three special conditions. Each involves some combination of normal, stall, and bubble operations for the pipeline registers. In terms of timing, the stall and bubble control signals for the pipeline registers are generated by blocks of combinational logic. These values must be valid as the clock rises, causing each of the pipeline registers to either load, stall, or bubble as the next clock cycle begins. With this small extension to the pipeline register designs, we can implement a complete pipeline, including all of its control, using the basic building blocks of combinational logic, clocked registers, and random access memories. Combinations of Control Conditions In our discussion of the special pipeline control conditions so far, we assumed that at most one special case could arise during any single clock cycle. A common bug in designing a system is to fail to handle instances where multiple special conditions arise simultaneously. Let us analyze such possibilities. We need not worry about combinations involving program exceptions, since we have carefully designed our exception-handling mechanism to consider other instructions in the pipeline. Figure 4.67 diagrams the pipeline states that cause the other three special control Section 4.5 Pipelined Y86-64 Implementations 497 Figure 4.67 Pipeline states for special control conditions. The two pairs indicated can arise simultaneously. Load/use M Mispredict ret 1 ret 2 ret 3 E D M E JXXLoad retUse bubble ret bubble bubble ret D M Combination A Combination B E D M E D M E D conditions. These diagrams show blocks for the decode, execute, and memory stages. The shaded boxes represent particular constraints that must be satisﬁed for the condition to arise. A load/use hazard requires that the instruction in the execute stage reads a value from memory into a register, and that the instruction in the decode stage has this register as a source operand. A mispredicted branch requires the instruction in the execute stage to have a jump instruction. There are three possible cases for Ë−Î—the instruction can be in either the decode, execute, or memory stage. As the Ë−Î instruction moves through the pipeline, the earlier pipeline stages will have bubbles. We can see by these diagrams that most of the control conditions are mutually exclusive. For example, it is not possible to have a load/use hazard and a mispre- dicted branch simultaneously, since one requires a load instruction (ÀËÀÇÌÊ or ÉÇÉÊ) in the execute stage, while the other requires a jump. Similarly, the second and third Ë−Î combinations cannot occur at the same time as a load/use hazard or a mispredicted branch. Only the two combinations indicated by arrows can arise simultaneously. Combination A involves a not-taken jump instruction in the execute stage and a Ë−Î instruction in the decode stage. Setting up this combination requires the Ë−Î to be at the target of a not-taken branch. The pipeline control logic should detect that the branch was mispredicted and therefore cancel the Ë−Î instruction. Practice Problem 4.37 (solution page 528) Write a Y86-64 assembly-language program that causes combination A to arise and determines whether the control logic handles it correctly. Combining the control actions for the combination A conditions (Figure 4.66), we get the following pipeline control actions (assuming that either a bubble or a stall overrides the normal case): Pipeline register Condition F D E M W Processing Ë−Î stall bubble normal normal normal Mispredicted branch normal bubble bubble normal normal Combination stall bubble bubble normal normal 498 Chapter 4 Processor Architecture That is, it would be handled like a mispredicted branch, but with a stall in the fetch stage. Fortunately, on the next cycle, the PC selection logic will choose the address of the instruction following the jump, rather than the predicted program counter, and so it does not matter what happens with the pipeline register F. We conclude that the pipeline will correctly handle this combination. Combination B involves a load/use hazard, where the loading instruction sets register cËÍÉ and the Ë−Î instruction then uses this register as a source operand, since it must pop the return address from the stack. The pipeline control logic should hold back the Ë−Î instruction in the decode stage. Practice Problem 4.38 (solution page 528) Write a Y86-64 assembly-language program that causes combination B to arise and completes with a ³þÄÎ instruction if the pipeline operates correctly. Combining the control actions for the combination B conditions (Figure 4.66), we get the following pipeline control actions: Pipeline register Condition F D E M W Processing Ë−Î stall bubble normal normal normal Load/use hazard stall stall bubble normal normal Combination stall bubble+stall bubble normal normal Desired stall stall bubble normal normal If both sets of actions were triggered, the control logic would try to stall the Ë−Î instruction to avoid the load/use hazard but also inject a bubble into the decode stage due to the Ë−Î instruction. Clearly, we do not want the pipeline to perform both sets of actions. Instead, we want it to just take the actions for the load/use hazard. The actions for processing the Ë−Î instruction should be delayed for one cycle. This analysis shows that combination B requires special handling. In fact, our original implementation of the PIPE control logic did not handle this combination correctly. Even though the design had passed many simulation tests, it had a subtle bug that was uncovered only by the analysis we have just shown. When a program having combination B was executed, the control logic would set both the bubble and the stall signals for pipeline register D to 1. This example shows the importance of systematic analysis. It would be unlikely to uncover this bug by just running normal programs. If left undetected, the pipeline would not faithfully implement the ISA behavior. Control Logic Implementation Figure 4.68 shows the overall structure of the pipeline control logic. Based on signals from the pipeline registers and pipeline stages, the control logic generates Section 4.5 Pipelined Y86-64 Implementations 499 F CC W icode valE valM dstEstat stat stat stat dstM M icode Cnd valE valA dstE dstM E icode ifun valC valA valB dstM srcA srcBdstE D icode ifun valC valPrBrA F predPC srcA srcB Stat Pipeline control logic M_icode W_stat e_Cnd m_stat E_dstM d_srcB d_srcA D_icode E_icode E_bubble set_cc W_stall M_bubble D_bubble D_stall F_stall Figure 4.68 PIPE pipeline control logic. This logic overrides the normal ﬂow of instructions through the pipeline to handle special conditions such as procedure returns, mispredicted branches, load/use hazards, and program exceptions. stall and bubble control signals for the pipeline registers and also determines whether the condition code registers should be updated. We can combine the detection conditions of Figure 4.64 with the actions of Figure 4.66 to create HCL descriptions for the different pipeline control signals. Pipeline register F must be stalled for either a load/use hazard or a Ë−Î instruction: ¾ÇÇÄ ƒˆÍÎþÄÄ { a £ÇÅ®©Î©ÇÅÍ ðÇË þ ÄÇþ®mÏÍ− ³þÖþË® ¥ˆ©²Ç®− ©Å Õ '›‡›ﬂ‚†j '–ﬂ–† Û dd ¥ˆ®ÍÎ› ©Å Õ ®ˆÍË²¡j ®ˆÍË²¢ Û ÚÚ a ·ÎþÄÄ©Å× þÎ ð−Î²³ Ñ³©Ä− Ë−Î ÉþÍÍ−Í Î³ËÇÏ×³ É©É−Ä©Å− '‡¥¶ ©Å Õ ⁄ˆ©²Ç®−j ¥ˆ©²Ç®−j ›ˆ©²Ç®− Ûy Practice Problem 4.39 (solution page 529) Write HCL code for the signal D_stall in the PIPE implementation. Pipeline register D must be set to bubble for a mispredicted branch or a Ë−Î instruction. As the analysis in the preceding section shows, however, it should 500 Chapter 4 Processor Architecture not inject a bubble when there is a load/use hazard in combination with a Ë−Î instruction: ¾ÇÇÄ ⁄ˆ¾Ï¾¾Ä− { a ›©ÍÉË−®©²Î−® ¾ËþÅ²³ f¥ˆ©²Ç®− {{ '“”” dd _−ˆ£Å®g ÚÚ a ·ÎþÄÄ©Å× þÎ ð−Î²³ Ñ³©Ä− Ë−Î ÉþÍÍ−Í Î³ËÇÏ×³ É©É−Ä©Å− a ¾ÏÎ ÅÇÎ ²ÇÅ®©Î©ÇÅ ðÇË þ ÄÇþ®mÏÍ− ³þÖþË® _f¥ˆ©²Ç®− ©Å Õ '›‡›ﬂ‚†j '–ﬂ–† Û dd ¥ˆ®ÍÎ› ©Å Õ ®ˆÍË²¡j ®ˆÍË²¢ Ûg dd '‡¥¶ ©Å Õ ⁄ˆ©²Ç®−j ¥ˆ©²Ç®−j ›ˆ©²Ç®− Ûy Practice Problem 4.40 (solution page 529) Write HCL code for the signal E_bubble in the PIPE implementation. Practice Problem 4.41 (solution page 529) Write HCL code for the signal set_cc in the PIPE implementation. This should only occur for ﬂ–Ê instructions, and should consider the effects of program excep- tions. Practice Problem 4.42 (solution page 529) Write HCL code for the signals M_bubble and W_stall in the PIPE implemen- tation. The latter signal requires modifying the exception condition listed in Fig- ure 4.64. This covers all of the special pipeline control signal values. In the complete HCL code for PIPE, all other pipeline control signals are set to zero. 4.5.9 Performance Analysis We can see that the conditions requiring special action by the pipeline control logic all cause our pipeline to fall short of the goal of issuing a new instruction on every clock cycle. We can measure this inefﬁciency by determining how often a bubble gets injected into the pipeline, since these cause unused pipeline cycles. A return instruction generates three bubbles, a load/use hazard generates one, and a mispredicted branch generates two. We can quantify the effect these penalties have on the overall performance by computing an estimate of the average number of clock cycles PIPE would require per instruction it executes, a measure known as the CPI (for “cycles per instruction”). This measure is the reciprocal of the average throughput of the pipeline, but with time measured in clock cycles rather than picoseconds. It is a useful measure of the architectural efﬁciency of a design. If we ignore the performance implications of exceptions (which, by deﬁnition, will only occur rarely), another way to think about CPI is to imagine we run the Section 4.5 Pipelined Y86-64 Implementations 501 Aside Testing the design As we have seen, there are many ways to introduce bugs into a design, even for a simple microprocessor. With pipelining, there are many subtle interactions between the instructions at different pipeline stages. We have seen that many of the design challenges involve unusual instructions (such as popping to the stack pointer) or unusual instruction combinations (such as a not-taken jump followed by a Ë−Î). We also see that exception handling adds an entirely new dimension to the possible pipeline behaviors. How, then, can we be sure that our design is correct? For hardware manufacturers, this is a dominant concern, since they cannot simply report an error and have users download code patches over the Internet. Even a simple logic design error can have serious consequences, especially as microprocessors are increasingly used to operate systems that are critical to our lives and health, such as automotive antilock braking systems, heart pacemakers, and aircraft control systems. Simply simulating a design while running a number of “typical” programs is not a sufﬁcient means of testing a system. Instead, thorough testing requires devising ways of systematically generating many tests that will exercise as many different instructions and instruction combinations as possible. In creating our Y86-64 processor designs, we also devised a number of testing scripts, each of which generates many different tests, runs simulations of the processor, and compares the resulting register and memory values to those produced by our yis instruction set simulator. Here is a brief description of the scripts: optest. Runs 49 tests of different Y86-64 instructions with different source and destination registers jtest. Runs 64 tests of the different jump and call instructions, with different combinations of whether or not the branches are taken cmtest. Runs 28 tests of the different conditional move instructions, with different control combi- nations htest. Runs 600 tests of different data hazard possibilities, with different combinations of source and destination instructions, and with different numbers of ÅÇÉ instructions between the instruction pairs ctest. Tests 22 different control combinations, based on an analysis similar to what we did in Sec- tion 4.5.8 etest. Tests 12 different combinations where an instruction causes an exception and the instructions following it could alter the programmer-visible state The key idea of this testing method is that we want to be as systematic as possible, generating tests that create the different conditions that are likely to cause pipeline errors. processor on some benchmark program and observe the operation of the execute stage. On each cycle, the execute stage either (1) processes an instruction and this instruction continues through the remaining stages to completion, or (2) processes a bubble injected due to one of the three special cases. If the stage processes a total of Ci instructions and Cb bubbles, then the processor has required around Ci + Cb total clock cycles to execute Ci instructions. We say “around” because we ignore 502 Chapter 4 Processor Architecture Aside Formally verifying our design Even when a design passes an extensive set of tests, we cannot be certain that it will operate correctly for all possible programs. The number of possible programs we could test is unimaginably large, even if we only consider tests consisting of short code segments. Newer methods of formal veriﬁcation, however, hold the promise that we can have tools that rigorously consider all possible behaviors of a system and determine whether or not there are any design errors. We were able to apply formal veriﬁcation to an earlier version of our Y86-64 processors [13]. We set up a framework to compare the behavior of the pipelined design PIPE to the unpipelined version SEQ. That is, it was able to prove that for an arbitrary machine-language program, the two processors would have identical effects on the programmer-visible state. Of course, our veriﬁer cannot actually run all possible programs, since there are an inﬁnite number of them. Instead, it uses a form of proof by induction, showing a consistency between the two processors on a cycle-by-cycle basis. Carrying out this analysis requires reasoning about the hardware using symbolic methods in which we consider all program values to be arbitrary integers, and we abstract the ALU as a sort of “black box,” computing some unspeciﬁed function over its arguments. We assume only that the ALUs for SEQ and PIPE compute identical functions. We used the HCL descriptions of the control logic to generate the control logic for our symbolic processor models, and so we could catch any bugs in the HCL code. Being able to show that SEQ and PIPE are identical does not guarantee that either of them faithfully implements the instruction set architecture. However, it would uncover any bug due to an incorrect pipeline design, and this is the major source of design errors. In our experiments, we veriﬁed not only a version of PIPE similar to the one we have presented in this chapter but also several variants that we give as homework problems, in which we add more instructions, modify the hardware capabilities, or use different branch prediction strategies. Interest- ingly, we found only one bug in all of our designs, involving control combination B (described in Section 4.5.8) for our solution to the variant described in Problem 4.58. This exposed a weakness in our testing regime that caused us to add additional cases to the ctest testing script. Formal veriﬁcation is still in an early stage of development. The tools are often difﬁcult to use, and they do not have the capacity to verify large-scale designs. We were able to verify our processors in part because of their relative simplicity. Even then, it required several weeks of effort and multiple runs of the tools, each requiring up to 8 hours of computer time. This is an active area of research, with some tools becoming commercially available and some in use at companies such as Intel, AMD, and IBM. the cycles required to start the instructions ﬂowing through the pipeline. We can then compute the CPI for this benchmark as follows: CPI = Ci + Cb Ci = 1.0 + Cb Ci That is, the CPI equals 1.0 plus a penalty term Cb/Ci indicating the average number of bubbles injected per instruction executed. Since only three different instruction types can cause a bubble to be injected, we can break this penalty term into three components: Section 4.5 Pipelined Y86-64 Implementations 503 Web Aside ARCH:VLOG Verilog implementation of a pipelined Y86-64 processor As we have mentioned, modern logic design involves writing textual representations of hardware designs in a hardware description language. The design can then be tested by both simulation and a variety of formal veriﬁcation tools. Once we have conﬁdence in the design, we can use logic synthesis tools to translate the design into actual logic circuits. We have developed models of our Y86-64 processor designs in the Verilog hardware description language. These designs combine modules implementing the basic building blocks of the processor, along with control logic generated directly from the HCL descriptions. We have been able to synthesize some of these designs, download the logic circuit descriptions onto ﬁeld-programmable gate array (FPGA) hardware, and run the processors on actual programs. CPI = 1.0 + lp + mp + rp where lp (for “load penalty”) is the average frequency with which bubbles are in- jected while stalling for load/use hazards, mp (for “mispredicted branch penalty”) is the average frequency with which bubbles are injected when canceling instruc- tions due to mispredicted branches, and rp (for “return penalty”) is the average frequency with which bubbles are injected while stalling for Ë−Î instructions. Each of these penalties indicates the total number of bubbles injected for the stated reason (some portion of Cb) divided by the total number of instructions that were executed (Ci.) To estimate each of these penalties, we need to know how frequently the relevant instructions (load, conditional branch, and return) occur, and for each of these how frequently the particular condition arises. Let us pick the following set of frequencies for our CPI computation (these are comparable to measurements reported in [44] and [46]): . Load instructions (ÀËÀÇÌÊ and ÉÇÉÊ) account for 25% of all instructions executed. Of these, 20% cause load/use hazards. . Conditional branches account for 20% of all instructions executed. Of these, 60% are taken and 40% are not taken. . Return instructions account for 2% of all instructions executed. We can therefore estimate each of our penalties as the product of the fre- quency of the instruction type, the frequency the condition arises, and the number of bubbles that get injected when the condition occurs: Instruction Condition Cause Name frequency frequency Bubbles Product Load/use lp 0.25 0.20 1 0.05 Mispredict mp 0.20 0.40 2 0.16 Return rp 0.02 1.00 3 0.06 Total penalty 0.27 504 Chapter 4 Processor Architecture The sum of the three penalties is 0.27, giving a CPI of 1.27. Our goal was to design a pipeline that can issue one instruction per cycle, giving a CPI of 1.0. We did not quite meet this goal, but the overall performance is still quite good. We can also see that any effort to reduce the CPI further should focus on mispredicted branches. They account for 0.16 of our total penalty of 0.27, because conditional branches are common, our prediction strategy often fails, and we cancel two instructions for every misprediction. Practice Problem 4.43 (solution page 530) Suppose we use a branch prediction strategy that achieves a success rate of 65%, such as backward taken, forward not taken (BTFNT), as described in Section 4.5.4. What would be the impact on CPI, assuming all of the other frequencies are not affected? Practice Problem 4.44 (solution page 530) Let us analyze the relative performance of using conditional data transfers versus conditional control transfers for the programs you wrote for Problems 4.5 and 4.6. Assume that we are using these programs to compute the sum of the absolute values of a very long array, and so the overall performance is determined largely by the number of cycles required by the inner loop. Assume that our jump instructions are predicted as being taken, and that around 50% of the array values are positive. A. On average, how many instructions are executed in the inner loops of the two programs? B. On average, how many bubbles would be injected into the inner loops of the two programs? C. What is the average number of clock cycles required per array element for the two programs? 4.5.10 Unﬁnished Business We have created a structure for the PIPE pipelined microprocessor, designed the control logic blocks, and implemented pipeline control logic to handle special cases where normal pipeline ﬂow does not sufﬁce. Still, PIPE lacks several key features that would be required in an actual microprocessor design. We highlight a few of these and discuss what would be required to add them. Multicycle Instructions All of the instructions in the Y86-64 instruction set involve simple operations such as adding numbers. These can be processed in a single clock cycle within the exe- cute stage. In a more complete instruction set, we would also need to implement instructions requiring more complex operations such as integer multiplication and Section 4.5 Pipelined Y86-64 Implementations 505 division and ﬂoating-point operations. In a medium-performance processor such as PIPE, typical execution times for these operations range from 3 or 4 cycles for ﬂoating-point addition up to 64 cycles for integer division. To implement these instructions, we require both additional hardware to perform the computations and a mechanism to coordinate the processing of these instructions with the rest of the pipeline. One simple approach to implementing multicycle instructions is to simply expand the capabilities of the execute stage logic with integer and ﬂoating-point arithmetic units. An instruction remains in the execute stage for as many clock cycles as it requires, causing the fetch and decode stages to stall. This approach is simple to implement, but the resulting performance is not very good. Better performance can be achieved by handling the more complex opera- tions with special hardware functional units that operate independently of the main pipeline. Typically, there is one functional unit for performing integer mul- tiplication and division, and another for performing ﬂoating-point operations. As an instruction enters the decode stage, it can be issued to the special unit. While the unit performs the operation, the pipeline continues processing other instructions. Typically, the ﬂoating-point unit is itself pipelined, and thus multiple operations can execute concurrently in the main pipeline and in the different units. The operations of the different units must be synchronized to avoid incorrect behavior. For example, if there are data dependencies between the different operations being handled by different units, the control logic may need to stall one part of the system until the results from an operation handled by some other part of the system have been completed. Often, different forms of forwarding are used to convey results from one part of the system to other parts, just as we saw between the different stages of PIPE. The overall design becomes more complex than we have seen with PIPE, but the same techniques of stalling, forwarding, and pipeline control can be used to make the overall behavior match the sequential ISA model. Interfacing with the Memory System In our presentation of PIPE, we assumed that both the instruction fetch unit and the data memory could read or write any memory location in one clock cycle. We also ignored the possible hazards caused by self-modifying code where one instruction writes to the region of memory from which later instructions are fetched. Furthermore, we reference memory locations according to their virtual addresses, and these require a translation into physical addresses before the actual read or write operation can be performed. Clearly, it is unrealistic to do all of this processing in a single clock cycle. Even worse, the memory values being accessed may reside on disk, requiring millions of clock cycles to read into the processor memory. As will be discussed in Chapters 6 and 9, the memory system of a processor uses a combination of multiple hardware memories and operating system soft- ware to manage the virtual memory system. The memory system is organized as a hierarchy, with faster but smaller memories holding a subset of the memory being 506 Chapter 4 Processor Architecture backed up by slower and larger memories. At the level closest to the processor, the cache memories provide fast access to the most heavily referenced memory locations. A typical processor has two ﬁrst-level caches—one for reading instruc- tions and one for reading and writing data. Another type of cache memory, known as a translation look-aside buffer, or TLB, provides a fast translation from virtual to physical addresses. Using a combination of TLBs and caches, it is indeed pos- sible to read instructions and read or write data in a single clock cycle most of the time. Thus, our simpliﬁed view of memory referencing by our processors is actually quite reasonable. Although the caches hold the most heavily referenced memory locations, there will be times when a cache miss occurs, where some reference is made to a location that is not held in the cache. In the best case, the missing data can be retrieved from a higher-level cache or from the main memory of the processor, requiring 3 to 20 clock cycles. Meanwhile, the pipeline simply stalls, holding the instruction in the fetch or memory stage until the cache can perform the read or write operation. In terms of our pipeline design, this can be implemented by adding more stall conditions to the pipeline control logic. A cache miss and the consequent synchronization with the pipeline is handled completely by hardware, keeping the time required down to a small number of clock cycles. In some cases, the memory location being referenced is actually stored in the disk or nonvolatile memory. When this occurs, the hardware signals a page fault exception. Like other exceptions, this will cause the processor to invoke the operating system’s exception handler code. This code will then set up a transfer from the disk to the main memory. Once this completes, the operating system will return to the original program, where the instruction causing the page fault will be re-executed. This time, the memory reference will succeed, although it might cause a cache miss. Having the hardware invoke an operating system routine, which then returns control back to the hardware, allows the hardware and system software to cooperate in the handling of page faults. Since accessing a disk can require millions of clock cycles, the several thousand cycles of processing performed by the OS page fault handler has little impact on performance. From the perspective of the processor, the combination of stalling to han- dle short-duration cache misses and exception handling to handle long-duration page faults takes care of any unpredictability in memory access times due to the structure of the memory hierarchy. 4.6 Summary We have seen that the instruction set architecture, or ISA, provides a layer of abstraction between the behavior of a processor—in terms of the set of instructions and their encodings—and how the processor is implemented. The ISA provides a very sequential view of program execution, with one instruction executed to completion before the next one begins. Section 4.6 Summary 507 Aside State-of-the-art microprocessor design A ﬁve-stage pipeline, such as we have shown with the PIPE processor, represented the state of the art in processor design in the mid-1980s. The prototype RISC processor developed by Patterson’s research group at Berkeley formed the basis for the ﬁrst SPARC processor, developed by Sun Microsystems in 1987. The processor developed by Hennessy’s research group at Stanford was commercialized by MIPS Technologies (a company founded by Hennessy) in 1986. Both of these used ﬁve-stage pipelines. The Intel i486 processor also uses a ﬁve-stage pipeline, although with a different partitioning of responsibilities among the stages, with two decode stages and a combined execute/memory stage [27]. These pipelined designs are limited to a throughput of at most one instruction per clock cycle. The CPI (for “cycles per instruction”) measure described in Section 4.5.9 can never be less than 1.0. The different stages can only process one instruction at a time. More recent processors support superscalar operation, meaning that they can achieve a CPI less than 1.0 by fetching, decoding, and executing multiple instructions in parallel. As superscalar processors have become widespread, the accepted performance measure has shifted from CPI to its reciprocal—the average number of instructions executed per cycle, or IPC. It can exceed 1.0 for superscalar processors. The most advanced designs use a technique known as out-of-order execution to execute multiple instructions in parallel, possibly in a totally different order than they occur in the program, while preserving the overall behavior implied by the sequential ISA model. This form of execution is described in Chapter 5 as part of our discussion of program optimization. Pipelined processors are not just historical artifacts, however. The majority of processors sold are used in embedded systems, controlling automotive functions, consumer products, and other devices where the processor is not directly visible to the system user. In these applications, the simplicity of a pipelined processor, such as the one we have explored in this chapter, reduces its cost and power requirements compared to higher-performance models. More recently, as multicore processors have gained a following, some have argued that we could get more overall computing power by integrating many simple processors on a single chip rather than a smaller number of more complex ones. This strategy is sometimes referred to as “many-core” processors [10]. We deﬁned the Y86-64 instruction set by starting with the x86-64 instructions and simplifying the data types, address modes, and instruction encoding consider- ably. The resulting ISA has attributes of both RISC and CISC instruction sets. We then organized the processing required for the different instructions into a series of ﬁve stages, where the operations at each stage vary according to the instruction being executed. From this, we constructed the SEQ processor, in which an entire instruction is executed every clock cycle by having it ﬂow through all ﬁve stages. Pipelining improves the throughput performance of a system by letting the different stages operate concurrently. At any given time, multiple operations are being processed by the different stages. In introducing this concurrency, we must be careful to provide the same program-level behavior as would a sequential execution of the program. We introduced pipelining by reordering parts of SEQ to get SEQ+ and then adding pipeline registers to create the PIPE− pipeline. 508 Chapter 4 Processor Architecture Web Aside ARCH:HCL HCL descriptions of Y86-64 processors In this chapter, we have looked at portions of the HCL code for several simple logic designs and for the control logic for Y86-64 processors SEQ and PIPE. For reference, we provide documentation of the HCL language and complete HCL descriptions for the control logic of the two processors. Each of these descriptions requires only ﬁve to seven pages of HCL code, and it is worthwhile to study them in their entirety. We enhanced the pipeline performance by adding forwarding logic to speed the sending of a result from one instruction to another. Several special cases require additional pipeline control logic to stall or cancel some of the pipeline stages. Our design included rudimentary mechanisms to handle exceptions, where we make sure that only instructions up to the excepting instruction affect the programmer-visible state. Implementing a complete handling of exceptions would be signiﬁcantly more challenging. Properly handling exceptions gets even more complex in systems that employ greater degrees of pipelining and parallelism. In this chapter, we have learned several important lessons about processor design: . Managing complexity is a top priority. We want to make optimum use of the hardware resources to get maximum performance at minimum cost. We did this by creating a very simple and uniform framework for processing all of the different instruction types. With this framework, we could share the hardware units among the logic for processing the different instruction types. . We do not need to implement the ISA directly. A direct implementation of the ISA would imply a very sequential design. To achieve higher performance, we want to exploit the ability in hardware to perform many operations si- multaneously. This led to the use of a pipelined design. By careful design and analysis, we can handle the various pipeline hazards, so that the overall effect of running a program exactly matches what would be obtained with the ISA model. . Hardware designers must be meticulous. Once a chip has been fabricated, it is nearly impossible to correct any errors. It is very important to get the design right on the ﬁrst try. This means carefully analyzing different instruction types and combinations, even ones that do not seem to make sense, such as popping to the stack pointer. Designs must be thoroughly tested with systematic simulation test programs. In developing the control logic for PIPE, our design had a subtle bug that was uncovered only after a careful and systematic analysis of control combinations. 4.6.1 Y86-64 Simulators The lab materials for this chapter include simulators for the SEQ and PIPE processors. Each simulator has two versions: Homework Problems 509 . The GUI (graphic user interface) version displays the memory, program code, and processor state in graphic windows. This provides a way to readily see how the instructions ﬂow through the processors. The control panel also allows you to reset, single-step, or run the simulator interactively. . The text version runs the same simulator, but it only displays information by printing to the terminal. This version is not as useful for debugging, but it allows automated testing of the processor. The control logic for the simulators is generated by translating the HCL declarations of the logic blocks into C code. This code is then compiled and linked with the rest of the simulation code. This combination makes it possible for you to test out variants of the original designs using the simulators. Testing scripts are also available that thoroughly exercise the different instructions and the different hazard possibilities. Bibliographic Notes For those interested in learning more about logic design, the Katz and Borriello logic design textbook [58] is a standard introductory text, emphasizing the use of hardware description languages. Hennessy and Patterson’s computer architecture textbook [46] provides extensive coverage of processor design, including both simple pipelines, such as the one we have presented here, and advanced processors that execute more instructions in parallel. Shriver and Smith [101] give a very thorough presentation of an Intel-compatible x86-64 processor manufactured by AMD. Homework Problems 4.45 ◆ In Section 3.4.2, the x86-64 ÉÏÍ³Ê instruction was described as decrementing the stack pointer and then storing the register at the stack pointer location. So, if we had an instruction of the form ÉÏÍ³Ê REG, for some register REG, it would be equivalent to the code sequence ÍÏ¾Ê bvjcËÍÉ Decrement stack pointer ÀÇÌÊ REGj fcËÍÉg Store REG on stack A. In light of analysis done in Practice Problem 4.7, does this code sequence correctly describe the behavior of the instruction ÉÏÍ³Ê cËÍÉ? Explain. B. How could you rewrite the code sequence so that it correctly describes both the cases where REG is cËÍÉ as well as any other register? 4.46 ◆ In Section 3.4.2, the x86-64 ÉÇÉÊ instruction was described as copying the result from the top of the stack to the destination register and then incrementing the stack pointer. So, if we had an instruction of the form ÉÇÉÊ REG, it would be equivalent to the code sequence 510 Chapter 4 Processor Architecture ÀÇÌÊ fcËÍÉgj REG Read REG from stack þ®®Ê bvjcËÍÉ Increment stack pointer A. In light of analysis done in Practice Problem 4.8, does this code sequence correctly describe the behavior of the instruction ÉÇÉÊ cËÍÉ? Explain. B. How could you rewrite the code sequence so that it correctly describes both the cases where REG is cËÍÉ as well as any other register? 4.47 ◆◆◆ Your assignment will be to write a Y86-64 program to perform bubblesort. For reference, the following C function implements bubblesort using array refer- encing: 1 mh ¢Ï¾¾Ä− ÍÇËÎx ¡ËËþÔ Ì−ËÍ©ÇÅ hm 2 ÌÇ©® ¾Ï¾¾Ä−ˆþfÄÇÅ× h®þÎþj ÄÇÅ× ²ÇÏÅÎg Õ 3 ÄÇÅ× ©j ÄþÍÎy 4 ðÇË fÄþÍÎ { ²ÇÏÅÎkoy ÄþÍÎ | ny ÄþÍÎkkg Õ 5 ðÇË f© { ny © z ÄþÍÎy ©iig 6 ©ð f®þÎþ‰©io` z ®þÎþ‰©`g Õ 7 mh ·ÑþÉ þ®Áþ²−ÅÎ −Ä−À−ÅÎÍ hm 8 ÄÇÅ× Î { ®þÎþ‰©io`y 9 ®þÎþ‰©io` { ®þÎþ‰©`y 10 ®þÎþ‰©` { Îy 11 Û 12 Û 13 Û A. Write and test a C version that references the array elements with pointers, rather than using array indexing. B. Write and test a Y86-64 program consisting of the function and test code. You may ﬁnd it useful to pattern your implementation after x86-64 code generated by compiling your C code. Although pointer comparisons are normally done using unsigned arithmetic, you can use signed arithmetic for this exercise. 4.48 ◆◆ Modify the code you wrote for Problem 4.47 to implement the test and swap in the bubblesort function (lines 6–11) using no jumps and at most three conditional moves. 4.49 ◆◆◆ Modify the code you wrote for Problem 4.47 to implement the test and swap in the bubblesort function (lines 6–11) using no jumps and just one conditional move. 4.50 ◆◆◆ In Section 3.6.8, we saw that a common way to implement ÍÑ©Î²³ statements is to create a set of code blocks and then index those blocks using a jump table. Consider Homework Problems 511 a©Å²ÄÏ®− zÍÎ®©Çl³| mh ¥ÓþÀÉÄ− ÏÍ− Çð ÍÑ©Î²³ ÍÎþÎ−À−ÅÎ hm ÄÇÅ× ÍÑ©Î²³ÌfÄÇÅ× ©®Óg Õ ÄÇÅ× Ë−ÍÏÄÎ { ny ÍÑ©Î²³f©®Óg Õ ²þÍ− nx Ë−ÍÏÄÎ { nÓþþþy ¾Ë−þÂy ²þÍ− px ²þÍ− sx Ë−ÍÏÄÎ { nÓ¾¾¾y ¾Ë−þÂy ²þÍ− qx Ë−ÍÏÄÎ { nÓ²²²y ¾Ë−þÂy ®−ðþÏÄÎx Ë−ÍÏÄÎ { nÓ®®®y Û Ë−ÎÏËÅ Ë−ÍÏÄÎy Û mh ¶−ÍÎ©Å× £Ç®− hm a®−ð©Å− £ﬁ¶ v a®−ð©Å− ›'ﬁ‚¡‹ ko ©ÅÎ Àþ©Åfg Õ ÄÇÅ× ÌþÄÍ‰£ﬁ¶`y ÄÇÅ× ©y ðÇË f© { ny © z £ﬁ¶y ©iig Õ ÌþÄÍ‰©` { ÍÑ©Î²³Ìf© i ›'ﬁ‚¡‹gy ÉË©ÅÎðf‘©®Ó { cÄ®j ÌþÄ { nÓcÄÓ¿Å‘j © i ›'ﬁ‚¡‹j ÌþÄÍ‰©`gy Û Ë−ÎÏËÅ ny Û Figure 4.69 Switch statements can be translated into Y86-64 code. This requires implementation of a jump table. the C code shown in Figure 4.69 for a function ÍÑ©Î²³Ì, along with associated test code. Implement ÍÑ©Î²³Ì in Y86-64 using a jump table. Although the Y86-64 in- struction set does not include an indirect jump instruction, you can get the same effect by pushing a computed address onto the stack and then executing the Ë−Î 512 Chapter 4 Processor Architecture instruction. Implement test code similar to what is shown in C to demonstrate that your implementation of ÍÑ©Î²³Ì will handle both the cases handled explicitly as well as those that trigger the ®−ðþÏÄÎ case. 4.51 ◆ Practice Problem 4.3 introduced the ©þ®®Ê instruction to add immediate data to a register. Describe the computations performed to implement this instruction. Use the computations for ©ËÀÇÌÊ and ﬂ–Ê (Figure 4.18) as a guide. 4.52 ◆◆ The ﬁle Í−ÊkðÏÄÄl³²Ä contains the HCL description for SEQ, along with the declaration of a constant ''¡⁄⁄† having hexadecimal value £, the instruction code for ©þ®®Ê. Modify the HCL descriptions of the control logic blocks to implement the ©þ®®Ê instruction, as described in Practice Problem 4.3 and Problem 4.51. See the lab material for directions on how to generate a simulator for your solution and how to test it. 4.53 ◆◆◆ Suppose we wanted to create a lower-cost pipelined processor based on the struc- ture we devised for PIPE− (Figure 4.41), without any bypassing. This design would handle all data dependencies by stalling until the instruction generating a needed value has passed through the write-back stage. The ﬁle É©É−kÍÎþÄÄl³²Ä contains a modiﬁed version of the HCL code for PIPE in which the bypassing logic has been disabled. That is, the signals e_valA and e_valB are simply declared as follows: aa ⁄ﬂ ﬁﬂ¶ ›ﬂ⁄'ƒ» ¶¤¥ ƒﬂ‹‹ﬂ„'ﬁ§ £ﬂ⁄¥l aa ﬁÇ ðÇËÑþË®©Å×l ÌþÄ¡ ©Í −©Î³−Ë ÌþÄ– ÇË ÌþÄÏ− ðËÇÀ Ë−×©ÍÎ−Ë ð©Ä− ÑÇË® ®ˆÌþÄ¡ { ‰ ⁄ˆ©²Ç®− ©Å Õ '£¡‹‹j '“”” Û x ⁄ˆÌþÄ–y a •Í− ©Å²Ë−À−ÅÎ−® –£ o x ®ˆËÌþÄ¡y a •Í− ÌþÄÏ− Ë−þ® ðËÇÀ Ë−×©ÍÎ−Ë ð©Ä− `y aa ﬁÇ ðÇËÑþË®©Å×l ÌþÄ¢ ©Í ÌþÄÏ− ðËÇÀ Ë−×©ÍÎ−Ë ð©Ä− ÑÇË® ®ˆÌþÄ¢ { ®ˆËÌþÄ¢y Modify the pipeline control logic at the end of this ﬁle so that it correctly han- dles all possible control and data hazards. As part of your design effort, you should analyze the different combinations of control cases, as we did in the design of the pipeline control logic for PIPE. You will ﬁnd that many different combinations can occur, since many more conditions require the pipeline to stall. Make sure your control logic handles each combination correctly. See the lab material for directions on how to generate a simulator for your solution and how to test it. 4.54 ◆◆ The ﬁle É©É−kðÏÄÄl³²Ä contains a copy of the PIPE HCL description, along with a declaration of the constant value ''¡⁄⁄†. Modify this ﬁle to implement the ©þ®®Ê instruction, as described in Practice Problem 4.3 and Problem 4.51. See the lab Homework Problems 513 material for directions on how to generate a simulator for your solution and how to test it. 4.55 ◆◆◆ The ﬁle É©É−kÅÎl³²Ä contains a copy of the HCL code for PIPE, plus a declaration of the constant “ˆ»¥· with value n, the function code for an unconditional jump instruction. Modify the branch prediction logic so that it predicts conditional jumps as being not taken while continuing to predict unconditional jumps and ²þÄÄ as being taken. You will need to devise a way to get valC, the jump target address, to pipeline register M to recover from mispredicted branches. See the lab material for directions on how to generate a simulator for your solution and how to test it. 4.56 ◆◆◆ The ﬁle É©É−k¾ÎðÅÎl³²Ä contains a copy of the HCL code for PIPE, plus a decla- ration of the constant “ˆ»¥· with value n, the function code for an unconditional jump instruction. Modify the branch prediction logic so that it predicts condi- tional jumps as being taken when valC < valP (backward branch) and as being not taken when valC ≥ valP (forward branch). (Since Y86-64 does not support unsigned arithmetic, you should implement this test using a signed comparison.) Continue to predict unconditional jumps and ²þÄÄ as being taken. You will need to devise a way to get both valC and valP to pipeline register M to recover from mispredicted branches. See the lab material for directions on how to generate a simulator for your solution and how to test it. 4.57 ◆◆◆ In our design of PIPE, we generate a stall whenever one instruction performs a load, reading a value from memory into a register, and the next instruction has this register as a source operand. When the source gets used in the execute stage, this stalling is the only way to avoid a hazard. For cases where the second instruction stores the source operand to memory, such as with an ËÀÀÇÌÊ or ÉÏÍ³Ê instruction, this stalling is not necessary. Consider the following code examples: 1 ÀËÀÇÌÊ nfcË²ÓgjcË®Ó a ‹Çþ® o 2 ÉÏÍ³Ê cË®Ó a ·ÎÇË− o 3 ÅÇÉ 4 ÉÇÉÊ cË®Ó a ‹Çþ® p 5 ËÀÀÇÌÊ cËþÓjnfcË®Óg a ·ÎÇË− p In lines 1 and 2, the ÀËÀÇÌÊ instruction reads a value from memory into cË®Ó, and the ÉÏÍ³Ê instruction then pushes this value onto the stack. Our design for PIPE would stall the ÉÏÍ³Ê instruction to avoid a load/use hazard. Observe, however, that the value of cË®Ó is not required by the ÉÏÍ³Ê instruction until it reaches the memory stage. We can add an additional bypass path, as diagrammed in Figure 4.70, to forward the memory output (signal m_valM)tothe valA ﬁeld in pipeline register M. On the next clock cycle, this forwarded value can then be written to memory. This technique is known as load forwarding. Note that the second example (lines 4 and 5) in the code sequence above cannot make use of load forwarding. The value loaded by the ÉÇÉÊ instruction is 514 Chapter 4 Processor Architecture e_Cnd E_icode W_stat m_stat M_dstM E_srcA m_valM E_valA W icodestat stat stat valE valM dstE dstM M icode Cnd valE valA dstE dstM E icode ifun valC valA valB dstM srcA srcB ALU A Set CC ALU B ALU fun.ALUCC cond data out data in read dmem_error write Addr Fwd A Mem. read Mem. write Stat dstE dstE Data memory Figure 4.70 Execute and memory stages capable of load forwarding. By adding a bypass path from the memory output to the source of valA in pipeline register M, we can use forwarding rather than stalling for one form of load/use hazard. This is the subject of Problem 4.57. used as part of the address computation by the next instruction, and this value is required in the execute stage rather than the memory stage. A. Write a logic formula describing the detection condition for a load/use haz- ard, similar to the one given in Figure 4.64, except that it will not cause a stall in cases where load forwarding can be used. B. The ﬁle É©É−kÄðl³²Ä contains a modiﬁed version of the control logic for PIPE. It contains the deﬁnition of a signal e_valA to implement the block labeled “Fwd A” in Figure 4.70. It also has the conditions for a load/use haz- ard in the pipeline control logic set to zero, and so the pipeline control logic will not detect any forms of load/use hazards. Modify this HCL description to implement load forwarding. See the lab material for directions on how to generate a simulator for your solution and how to test it. Homework Problems 515 4.58 ◆◆◆ Our pipelined design is a bit unrealistic in that we have two write ports for the register ﬁle, but only the ÉÇÉÊ instruction requires two simultaneous writes to the register ﬁle. The other instructions could therefore use a single write port, sharing this for writing valE and valM. The following ﬁgure shows a modiﬁed version of the write-back logic, in which we merge the write-back register IDs (W_dstE and W_dstM) into a single signal w_dstE and the write-back values (W_valE and W_valM) into a single signal w_valE: Stat Stat W icode valE valM dstE dstM valE dstE w_valE w_dstE W_icode stat The logic for performing the merges is written in HCL as follows: aa ·−Î ¥ ÉÇËÎ Ë−×©ÍÎ−Ë '⁄ ÑÇË® Ñˆ®ÍÎ¥ { ‰ aa ÑË©Î©Å× ðËÇÀ ÌþÄ› „ˆ®ÍÎ› _{ ‡ﬁﬂﬁ¥ x „ˆ®ÍÎ›y ox „ˆ®ÍÎ¥y `y aa ·−Î ¥ ÉÇËÎ ÌþÄÏ− ÑÇË® ÑˆÌþÄ¥ { ‰ „ˆ®ÍÎ› _{ ‡ﬁﬂﬁ¥ x „ˆÌþÄ›y ox „ˆÌþÄ¥y `y The control for these multiplexors is determined by dstE—when it indicates there is some register, then it selects the value for port E, and otherwise it selects the value for port M. In the simulation model, we can then disable register port M, as shown by the following HCL code: aa ⁄©Íþ¾Ä− Ë−×©ÍÎ−Ë ÉÇËÎ › aa ·−Î › ÉÇËÎ Ë−×©ÍÎ−Ë '⁄ ÑÇË® Ñˆ®ÍÎ› { ‡ﬁﬂﬁ¥y aa ·−Î › ÉÇËÎ ÌþÄÏ− ÑÇË® ÑˆÌþÄ› { ny The challenge then becomes to devise a way to handle ÉÇÉÊ. One method is to use the control logic to dynamically process the instruction ÉÇÉÊ rA so that it has the same effect as the two-instruction sequence 516 Chapter 4 Processor Architecture ©þ®®Ê bvj cËÍÉ ÀËÀÇÌÊ kvfcËÍÉgj rA (See Practice Problem 4.3 for a description of the ©þ®®Ê instruction.) Note the ordering of the two instructions to make sure ÉÇÉÊ cËÍÉ works properly. You can do this by having the logic in the decode stage treat ÉÇÉÊ the same as it would the ©þ®®Ê listed above, except that it predicts the next PC to be equal to the current PC. On the next cycle, the ÉÇÉÊ instruction is refetched, but the instruction code is converted to a special value '–ﬂ–p. This is treated as a special instruction that has the same behavior as the ÀËÀÇÌÊ instruction listed above. The ﬁle É©É−koÑl³²Ä contains the modiﬁed write port logic described above. It contains a declaration of the constant '–ﬂ–p having hexadecimal value ¥.It also contains the deﬁnition of a signal f_icode that generates the icode ﬁeld for pipeline register D. This deﬁnition can be modiﬁed to insert the instruction code '–ﬂ–p the second time the ÉÇÉÊ instruction is fetched. The HCL ﬁle also contains a declaration of the signal f_pc, the value of the program counter generated in the fetch stage by the block labeled “Select PC” (Figure 4.57). Modify the control logic in this ﬁle to process ÉÇÉÊ instructions in the manner we have described. See the lab material for directions on how to generate a simulator for your solution and how to test it. 4.59 ◆◆ Compare the performance of the three versions of bubblesort (Problems 4.47, 4.48, and 4.49). Explain why one version performs better than the other. Solutions to Practice Problems Solution to Problem 4.1 (page 396) Encoding instructions by hand is rather tedious, but it will solidify your under- standing of the idea that assembly code gets turned into byte sequences by the assembler. In the following output from our Y86-64 assembler, each line shows an address and a byte sequence that starts at that address: 1 nÓonnx Ú lÉÇÍ nÓonn a ·ÎþËÎ ²Ç®− þÎ þ®®Ë−ÍÍ nÓonn 2 nÓonnx qnðqnðnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bosjcË¾Ó 3 nÓonþx pnqo Ú ËËÀÇÌÊ cË¾ÓjcË²Ó 4 nÓon²x Ú ÄÇÇÉx 5 nÓon²x rnoqð®ðððððððððððððð Ú ËÀÀÇÌÊ cË²ÓjkqfcË¾Óg 6 nÓootx tnqo Ú þ®®Ê cË¾ÓjcË²Ó 7 nÓoovx unn²nonnnnnnnnnnnn Ú ÁÀÉ ÄÇÇÉ Several features of this encoding are worth noting: . Decimal 15 (line 2) has hex representation nÓnnnnnnnnnnnnnnnð. Writing the bytes in reverse order gives nð nn nn nn nn nn nn nn. Solutions to Practice Problems 517 . Decimal −3 (line 5) has hex representation nÓððððððððððððððð®. Writing the bytes in reverse order gives ð® ðð ðð ðð ðð ðð ðð ðð. . The code starts at address nÓonn. The ﬁrst instruction requires 10 bytes, while the second requires 2. Thus, the loop target will be nÓnnnnnon². Writing these bytes in reverse order gives n² no nn nn nn nn nn nn. Solution to Problem 4.2 (page 396) Decoding a byte sequence by hand helps you understand the task faced by a processor. It must read byte sequences and determine what instructions are to be executed. In the following, we show the assembly code used to generate each of the byte sequences. To the left of the assembly code, you can see the address and byte sequence for each instruction. A. Some operations with immediate data and address displacements: nÓonnx qnðqð²ðððððððððððððð Ú ©ËÀÇÌÊ bkrjcË¾Ó nÓonþx rntqnnnvnnnnnnnnnnnn Ú ËÀÀÇÌÊ cËÍ©jnÓvnnfcË¾Óg nÓoorx nn Ú ³þÄÎ B. Code including a function call: nÓpnnx þntð Ú ÉÏÍ³Ê cËÍ© nÓpnpx vnn²npnnnnnnnnnnnn Ú ²þÄÄ ÉËÇ² nÓpn¾x nn Ú ³þÄÎ nÓpn²x Ú ÉËÇ²x nÓpn²x qnðqnþnnnnnnnnnnnnnn Ú ©ËÀÇÌÊ bonjcË¾Ó nÓpotx wn Ú Ë−Î C. Code containing illegal instruction speciﬁer byte nÓðn: nÓqnnx snsrnunnnnnnnnnnnnnn Ú ÀËÀÇÌÊ ufcËÍÉgjcË¾É nÓqnþx on Ú ÅÇÉ nÓqn¾x ðn Ú l¾ÔÎ− nÓðn a 'ÅÌþÄ©® ©ÅÍÎËÏ²Î©ÇÅ ²Ç®− nÓqn²x ¾noð Ú ÉÇÉÊ cË²Ó D. Code containing a jump operation: nÓrnnx Ú ÄÇÇÉx nÓrnnx tooq Ú ÍÏ¾Ê cË²Ój cË¾Ó nÓrnpx uqnnnrnnnnnnnnnnnn Ú Á− ÄÇÇÉ nÓrn¾x nn Ú ³þÄÎ E. Code containing an invalid second byte in a ÉÏÍ³Ê instruction: nÓsnnx tqtp Ú ÓÇËÊ cËÍ©jcË®Ó nÓsnpx þn Ú l¾ÔÎ− nÓþn a ÉÏÍ³Ê ©ÅÍÎËÏ²Î©ÇÅ ²Ç®− nÓsnqx ðn Ú l¾ÔÎ− nÓðn a 'ÅÌþÄ©® Ë−×©ÍÎ−Ë ÍÉ−²©ð©−Ë ¾ÔÎ− 518 Chapter 4 Processor Architecture Solution to Problem 4.3 (page 405) Using the ©þ®®Ê instruction, we can rewrite the ÍÏÀ function as a ÄÇÅ× ÍÏÀfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg a ÍÎþËÎ ©Å cË®©j ²ÇÏÅÎ ©Å cËÍ© ÍÏÀx ÓÇËÊ cËþÓjcËþÓ a ÍÏÀ { n þÅ®Ê cËÍ©jcËÍ© a ·−Î ²ÇÅ®©Î©ÇÅ ²Ç®−Í ÁÀÉ Î−ÍÎ ÄÇÇÉx ÀËÀÇÌÊ fcË®©gjcËon a §−Î hÍÎþËÎ þ®®Ê cËonjcËþÓ a ¡®® ÎÇ ÍÏÀ ©þ®®Ê bvjcË®© a ÍÎþËÎii ©þ®®Ê bkojcËÍ© a ²ÇÏÅÎkk Î−ÍÎx ÁÅ− ÄÇÇÉ a ·ÎÇÉ Ñ³−Å n Ë−Î Solution to Problem 4.4 (page 406) Gcc, running on an x86-64 machine, produces the following code for ËÉËÇ®Ï²Î: long rproduct(long *start, long count) start in %rdi, count in %rsi ËÉËÇ®Ï²Îx ÀÇÌÄ boj c−þÓ Î−ÍÎÊ cËÍ©j cËÍ© ÁÄ− l‹w ÉÏÍ³Ê cË¾Ó ÀÇÌÊ fcË®©gj cË¾Ó ÍÏ¾Ê boj cËÍ© þ®®Ê bvj cË®© ²þÄÄ ËÉËÇ®Ï²Î ©ÀÏÄÊ cË¾Ój cËþÓ ÉÇÉÊ cË¾Ó l‹wx Ë−Éy Ë−Î This can easily be adapted to produce Y86-64 code: a ÄÇÅ× ËÉËÇ®Ï²ÎfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg a ÍÎþËÎ ©Å cË®©j ²ÇÏÅÎ ©Å cËÍ© ËÉËÇ®Ï²Îx ÓÇËÊ cËþÓjcËþÓ a ·−Î Ë−ÎÏËÅ ÌþÄÏ− ÎÇ o þÅ®Ê cËÍ©jcËÍ© a ·−Î ²ÇÅ®©Î©ÇÅ ²Ç®−Í Á− Ë−ÎÏËÅ a 'ð ²ÇÏÅÎ z{ nj Ë−ÎÏËÅ o ÉÏÍ³Ê cË¾Ó a ·þÌ− ²þÄÄ−−kÍþÌ−® Ë−×©ÍÎ−Ë Solutions to Practice Problems 519 ÀËÀÇÌÊ fcË®©gjcË¾Ó a §−Î hÍÎþËÎ ©ËÀÇÌÊ bkojcËon þ®®Ê cËonjcËÍ© a ²ÇÏÅÎkk ©ËÀÇÌÊ bvjcËon þ®®Ê cËonjcË®© a ÍÎþËÎii ²þÄÄ ËÉËÇ®Ï²Î ©ÀÏÄÊ cË¾ÓjcËþÓ a ›ÏÄÎ©ÉÄÔ hÍÎþËÎ ÎÇ ÉËÇ®Ï²Î ÉÇÉÊ cË¾Ó a ‡−ÍÎÇË− ²þÄÄ−−kÍþÌ−® Ë−×©ÍÎ−Ë Ë−ÎÏËÅx Ë−Î Solution to Problem 4.5 (page 406) This problem gives you a chance to try your hand at writing assembly code. 1 a ÄÇÅ× þ¾Í·ÏÀfÄÇÅ× hÍÎþËÎj ÄÇÅ× ²ÇÏÅÎg 2 a ÍÎþËÎ ©Å cË®©j ²ÇÏÅÎ ©Å cËÍ© 3 þ¾Í·ÏÀx 4 ©ËÀÇÌÊ bvjcËv a £ÇÅÍÎþÅÎ v 5 ©ËÀÇÌÊ bojcËw a £ÇÅÍÎþÅÎ o 6 ÓÇËÊ cËþÓjcËþÓ a ÍÏÀ { n 7 þÅ®Ê cËÍ©jcËÍ© a ·−Î ²ÇÅ®©Î©ÇÅ ²Ç®−Í 8 ÁÀÉ Î−ÍÎ 9 ÄÇÇÉx 10 ÀËÀÇÌÊ fcË®©gjcËon a Ó { hÍÎþËÎ 11 ÓÇËÊ cËoojcËoo a £ÇÅÍÎþÅÎ n 12 ÍÏ¾Ê cËonjcËoo a kÓ 13 ÁÄ− ÉÇÍ a ·Â©É ©ð kÓ z{ n 14 ËËÀÇÌÊ cËoojcËon aÓ{kÓ 15 ÉÇÍx 16 þ®®Ê cËonjcËþÓ a ¡®® ÎÇ ÍÏÀ 17 þ®®Ê cËvjcË®© a ÍÎþËÎii 18 ÍÏ¾Ê cËwjcËÍ© a ²ÇÏÅÎkk 19 Î−ÍÎx 20 ÁÅ− ÄÇÇÉ a ·ÎÇÉ Ñ³−Å n 21 Ë−Î Solution to Problem 4.6 (page 406) This problem gives you a chance to try your hand at writing assembly code with conditional moves. We show only the code for the loop. The rest is the same as for Problem 4.5: 9 ÄÇÇÉx 10 ÀËÀÇÌÊ fcË®©gjcËon a Ó { hÍÎþËÎ 11 ÓÇËÊ cËoojcËoo a £ÇÅÍÎþÅÎ n 12 ÍÏ¾Ê cËonjcËoo a kÓ 13 ²ÀÇÌ× cËoojcËon a 'ð kÓ|nÎ³−ÅÓ{kÓ 520 Chapter 4 Processor Architecture 14 þ®®Ê cËonjcËþÓ a ¡®® ÎÇ ÍÏÀ 15 þ®®Ê cËvjcË®© a ÍÎþËÎii 16 ÍÏ¾Ê cËwjcËÍ© a ²ÇÏÅÎkk 17 Î−ÍÎx 18 ÁÅ− ÄÇÇÉ a ·ÎÇÉ Ñ³−Å n Solution to Problem 4.7 (page 406) Although it is hard to imagine any practical use for this particular instruction, it is important when designing a system to avoid any ambiguities in the speciﬁcation. We want to determine a reasonable convention for the instruction’s behavior and to make sure each of our implementations adheres to this convention. The ÍÏ¾Ê instruction in this test compares the starting value of cËÍÉ to the value pushed onto the stack. The fact that the result of this subtraction is zero implies that the old value of cËÍÉ gets pushed. Solution to Problem 4.8 (page 407) It is even more difﬁcult to imagine why anyone would want to pop to the stack pointer. Still, we should decide on a convention and stick with it. This code sequence pushes nÓþ¾²® onto the stack, pops to cËÍÉ, and returns the popped value. Since the result equals nÓþ¾²®, we can deduce that ÉÇÉÊ cËÍÉ sets the stack pointer to the value read from memory. It is therefore equivalent to the instruction ÀËÀÇÌÊ fcËÍÉgjcËÍÉ. Solution to Problem 4.9 (page 410) The exclusive-or function requires that the 2 bits have opposite values: ¾ÇÇÄ ÓÇË { f_þ dd ¾g ÚÚ fþ dd _¾gy In general, the signals −Ê and ÓÇË will be complements of each other. That is, one will equal 1 whenever the other is 0. Solution to Problem 4.10 (page 413) The outputs of the exclusive-or circuits will be the complements of the bit equal- ity values. Using DeMorgan’s laws (Web Aside data:bool on page 88), we can implement and using or and not, yielding the circuit shown in Figure 4.71. Solution to Problem 4.11 (page 415) We can see that the second part of the case expression can be written as ¢z{£ x¢y Since the ﬁrst line will detect the case where ¡ is the minimum element, the second line need only determine whether ¢ or £ is minimum. Solution to Problem 4.12 (page 416) This design is a variant of the one to ﬁnd the minimum of the three inputs: Solutions to Practice Problems 521 Figure 4.71 Solution for Problem 4.10. Xor Xor Xor Xor b63 a63 b62 a62 b1 a1 b0 a0 ! eq63 ! eq1 ! eq0 ! eq62 Eq. . .. . . ÑÇË® ›−®q { ‰ ¡z{¢dd¢z{£x¢y £z{¢dd¢z{¡x¢y ¢z{¡dd¡z{£x¡y £z{¡dd¡z{¢x¡y ox £y `y Solution to Problem 4.13 (page 423) These exercises help make the stage computations more concrete. We can see from the object code that this instruction is located at address nÓnot. It consists of 10 bytes, with the ﬁrst two being nÓqn and nÓðr. The last 8 bytes are a byte-reversed version of nÓnnnnnnnnnnnnnnvn (decimal opv). Generic Speciﬁc Stage ©ËÀÇÌÊ Vj rB ©ËÀÇÌÊ bopvj cËÍÉ Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnot] = qxn rA : rB ← M1[PC + 1] rA : rB ← M1[nÓnou] = ðxr valC ← M8[PC + 2] valC ← M8[nÓnov] = opv valP ← PC + 10 valP ← nÓnot + 10 = nÓnpn Decode Execute valE ← 0 + valC valE ← 0 + opv = opv Memory Write back R[rB] ← valE R[cËÍÉ] ← valE = opv PC update PC ← valP PC ← valP = nÓnpn This instruction sets register cËÍÉ to 128 and increments the PC by 10. 522 Chapter 4 Processor Architecture Solution to Problem 4.14 (page 426) We can see that the instruction is located at address nÓnp² and consists of 2 bytes with values nÓ¾n and nÓnnð. Register cËÍÉ was set to 120 by the ÉÏÍ³Ê instruction (line 6), which also stored 9 at this memory location. Generic Speciﬁc Stage ÉÇÉÊ rA ÉÇÉÊ cËþÓ Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnp²] = ¾xn rA : rB ← M1[PC + 1] rA : rB ← M1[nÓnp®] = nxð valP ← PC + 2 valP ← nÓnp² + 2 = nÓnp− Decode valA ← R[cËÍÉ] valA ← R[cËÍÉ] = opn valB ← R[cËÍÉ] valB ← R[cËÍÉ] = opn Execute valE ← valB + 8 valE ← opn + 8 = opv Memory valM ← M8[valA] valM ← M8[opn] = w Write back R[cËÍÉ] ← valE R[cËÍÉ] ← opv R[rA] ← valM R[cËþÓ] ← w PC update PC ← valP PC ← nÓnp− The instruction sets cËþÓ to 9, sets cËÍÉ to 128, and increments the PC by 2. Solution to Problem 4.15 (page 427) Tracing the steps listed in Figure 4.20 with rA equal to cËÍÉ, we can see that in the memory stage the instruction will store valA, the original value of the stack pointer, to memory, just as we found for x86-64. Solution to Problem 4.16 (page 428) Tracing the steps listed in Figure 4.20 with rA equal to cËÍÉ, we can see that both of the write-back operations will update cËÍÉ. Since the one writing valM would occur last, the net effect of the instruction will be to write the value read from memory to cËÍÉ, just as we saw for x86-64. Solution to Problem 4.17 (page 429) Implementing conditional moves requires only minor changes from register-to- register moves. We simply condition the write-back step on the outcome of the conditional test: Stage ²ÀÇÌ”” rAj rB Fetch icode : ifun ← M1[PC] rA : rB ← M1[PC + 1] valP ← PC + 2 Decode valA ← R[rA] Solutions to Practice Problems 523 Stage ²ÀÇÌ”” rAj rB Execute valE ← 0 + valA Cnd ← Cond(CC, ifun) Memory Write back ©ð fCndg R[rB] ← valE PC update PC ← valP Solution to Problem 4.18 (page 430) We can see that this instruction is located at address nÓnqu and is 9 bytes long. The ﬁrst byte has value nÓvn, while the last 8 bytes are a byte-reversed version of nÓnnnnnnnnnnnnnnro, the call target. The stack pointer was set to 128 by the ÉÇÉÊ instruction (line 7). Generic Speciﬁc Stage ²þÄÄ Dest ²þÄÄ nÓnro Fetch icode : ifun ← M1[PC] icode : ifun ← M1[nÓnqu] = vxn valC ← M8[PC + 1] valC ← M8[nÓnqv] = nÓnro valP ← PC + 9 valP ← nÓnqu + 9 = nÓnrn Decode valB ← R[cËÍÉ] valB ← R[cËÍÉ] = opv Execute valE ← valB +−8 valE ← opv +−8 = opn Memory M8[valE] ← valP M8[opn] ← nÓnrn Write back R[cËÍÉ] ← valE R[cËÍÉ] ← opn PC update PC ← valC PC ← nÓnro The effect of this instruction is to set cËÍÉ to 120, to store nÓnrn (the return address) at this memory address, and to set the PC to nÓnro (the call target). Solution to Problem 4.19 (page 442) All of the HCL code in this and other practice problems is straightforward, but trying to generate it yourself will help you think about the different instructions and how they are processed. For this problem, we can simply look at the set of Y86-64 instructions (Figure 4.2) and determine which have a constant ﬁeld. ¾ÇÇÄ Å−−®ˆÌþÄ£ { ©²Ç®− ©Å Õ ''‡›ﬂ‚†j '‡››ﬂ‚†j '›‡›ﬂ‚†j '“””j '£¡‹‹ Ûy 524 Chapter 4 Processor Architecture Solution to Problem 4.20 (page 443) This code is similar to the code for srcA. ÑÇË® ÍË²¢ { ‰ ©²Ç®− ©Å Õ 'ﬂ–†j '‡››ﬂ‚†j '›‡›ﬂ‚† Û x Ë¢y ©²Ç®− ©Å Õ '–•·¤†j '–ﬂ–†j '£¡‹‹j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î Å−−® Ë−×©ÍÎ−Ë `y Solution to Problem 4.21 (page 444) This code is similar to the code for dstE. ÑÇË® ®ÍÎ› { ‰ ©²Ç®− ©Å Õ '›‡›ﬂ‚†j '–ﬂ–†ÛxË¡y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î ÑË©Î− þÅÔ Ë−×©ÍÎ−Ë `y Solution to Problem 4.22 (page 444) As we found in Practice Problem 4.16, we want the write via the M port to take priority over the write via the E port in order to store the value read from memory into cËÍÉ. Solution to Problem 4.23 (page 445) This code is similar to the code for aluA. ÑÇË® þÄÏ¢ { ‰ ©²Ç®− ©Å Õ '‡››ﬂ‚†j '›‡›ﬂ‚†j 'ﬂ–†j '£¡‹‹j '–•·¤†j '‡¥¶j '–ﬂ–† Û x ÌþÄ¢y ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j ''‡›ﬂ‚†Ûxny a ﬂÎ³−Ë ©ÅÍÎËÏ²Î©ÇÅÍ ®ÇÅ’Î Å−−® ¡‹• `y Solution to Problem 4.24 (page 445) Implementing conditional moves is surprisingly simple: we disable writing to the register ﬁle by setting the destination register to ‡ﬁﬂﬁ¥ when the condition does not hold. ÑÇË® ®ÍÎ¥ { ‰ ©²Ç®− ©Å Õ '‡‡›ﬂ‚† Û dd £Å® x Ë¢y ©²Ç®− ©Å Õ ''‡›ﬂ‚†j 'ﬂ–†Û x Ë¢y ©²Ç®− ©Å Õ '–•·¤†j '–ﬂ–†j '£¡‹‹j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î ÑË©Î− þÅÔ Ë−×©ÍÎ−Ë `y Solution to Problem 4.25 (page 446) This code is similar to the code for mem_addr. Solutions to Practice Problems 525 ÑÇË® À−Àˆ®þÎþ { ‰ a ‚þÄÏ− ðËÇÀ Ë−×©ÍÎ−Ë ©²Ç®− ©Å Õ '‡››ﬂ‚†j '–•·¤† Û x ÌþÄ¡y a ‡−ÎÏËÅ –£ ©²Ç®− {{ '£¡‹‹ x ÌþÄ–y a ⁄−ðþÏÄÎx ⁄ÇÅ’Î ÑË©Î− þÅÔÎ³©Å× `y Solution to Problem 4.26 (page 446) This code is similar to the code for mem_read. ¾ÇÇÄ À−ÀˆÑË©Î− { ©²Ç®− ©Å Õ '‡››ﬂ‚†j '–•·¤†j '£¡‹‹ Ûy Solution to Problem 4.27 (page 447) Computing the Stat ﬁeld requires collecting status information from several stages: aa ⁄−Î−ËÀ©Å− ©ÅÍÎËÏ²Î©ÇÅ ÍÎþÎÏÍ ÑÇË® ·ÎþÎ { ‰ ©À−Àˆ−ËËÇË ÚÚ ®À−Àˆ−ËËÇË x ·¡⁄‡y _©ÅÍÎËˆÌþÄ©®x ·'ﬁ·y ©²Ç®− {{ '¤¡‹¶ x ·¤‹¶y o x ·¡ﬂ«y `y Solution to Problem 4.28 (page 453) This problem is an interesting exercise in trying to ﬁnd the optimal balance among a set of partitions. It provides a number of opportunities to compute throughputs and latencies in pipelines. A. For a two-stage pipeline, the best partition would be to have blocks A, B, and C in the ﬁrst stage and D, E, and F in the second. The ﬁrst stage has a delay of 170 ps, giving a total cycle time of 170 + 20 = 190 ps. We therefore have a throughput of 5.26 GIPS and a latency of 380 ps. B. For a three-stage pipeline, we should have blocks A and B in the ﬁrst stage, blocks C and D in the second, and blocks E and F in the third. The ﬁrst two stages have a delay of 110 ps, giving a total cycle time of 130 ps and a throughput of 7.69 GIPS. The latency is 390 ps. C. For a four-stage pipeline, we should have block A in the ﬁrst stage, blocks B and C in the second, block D in the third, and blocks E and F in the fourth. The second stage requires 90 ps, giving a total cycle time of 110 ps and a throughput of 9.09 GIPS. The latency is 440 ps. D. The optimal design would be a ﬁve-stage pipeline, with each block in its own stage, except that the ﬁfth stage has blocks E and F. The cycle time is 80 + 20 = 100 ps, for a throughput of around 10.00 GIPS and a latency of 526 Chapter 4 Processor Architecture 500 ps. Adding more stages would not help, since we cannot run the pipeline any faster than one cycle every 100 ps. Solution to Problem 4.29 (page 454) Each stage would have combinational logic requiring 300/k ps and a pipeline register requiring 20 ps. A. The total latency would be 300 + 20k ps, while the throughput (in GIPS) would be 1,000 300 k + 20 = 1,000k 300 + 20k B. As we let k go to inﬁnity, the throughput becomes 1,000/20 = 50 GIPS. Of course, the latency would approach inﬁnity as well. This exercise quantiﬁes the diminishing returns of deep pipelining. As we try to subdivide the logic into many stages, the latency of the pipeline registers becomes a limiting factor. Solution to Problem 4.30 (page 485) This code is very similar to the corresponding code for SEQ, except that we cannot yet determine whether the data memory will generate an error signal for this instruction. a ⁄−Î−ËÀ©Å− ÍÎþÎÏÍ ²Ç®− ðÇË ð−Î²³−® ©ÅÍÎËÏ²Î©ÇÅ ÑÇË® ðˆÍÎþÎ { ‰ ©À−Àˆ−ËËÇËx ·¡⁄‡y _©ÅÍÎËˆÌþÄ©® x ·'ﬁ·y ðˆ©²Ç®− {{ '¤¡‹¶ x ·¤‹¶y o x ·¡ﬂ«y `y Solution to Problem 4.31 (page 485) This code simply involves preﬁxing the signal names in the code for SEQ with ®ˆ and ⁄ˆ. ÑÇË® ®ˆ®ÍÎ¥ { ‰ ⁄ˆ©²Ç®− ©Å Õ '‡‡›ﬂ‚†j ''‡›ﬂ‚†j 'ﬂ–†Û x ⁄ˆË¢y ⁄ˆ©²Ç®− ©Å Õ '–•·¤†j '–ﬂ–†j '£¡‹‹j '‡¥¶ Û x ‡‡·–y o x ‡ﬁﬂﬁ¥y a ⁄ÇÅ’Î ÑË©Î− þÅÔ Ë−×©ÍÎ−Ë `y Solution to Problem 4.32 (page 488) The ËËÀÇÌÊ instruction (line 5) would stall for one cycle due to a load/use hazard caused by the ÉÇÉÊ instruction (line 4). As it enters the decode stage, the ÉÇÉÊ instruction would be in the memory stage, giving both M_dstE and M_dstM equal to cËÍÉ. If the two cases were reversed, then the write back from M_valE would take priority, causing the incremented stack pointer to be passed as the argument Solutions to Practice Problems 527 to the ËËÀÇÌÊ instruction. This would not be consistent with the convention for handling ÉÇÉÊ cËÍÉ determined in Practice Problem 4.8. Solution to Problem 4.33 (page 488) This problem lets you experience one of the important tasks in processor design— devising test programs for a new processor. In general, we should have test pro- grams that will exercise all of the different hazard possibilities and will generate incorrect results if some dependency is not handled properly. For this example, we can use a slightly modiﬁed version of the program shown in Practice Problem 4.32: 1 ©ËÀÇÌÊ bsj cË®Ó 2 ©ËÀÇÌÊ bnÓonnjcËÍÉ 3 ËÀÀÇÌÊ cË®ÓjnfcËÍÉg 4 ÉÇÉÊ cËÍÉ 5 ÅÇÉ 6 ÅÇÉ 7 ËËÀÇÌÊ cËÍÉjcËþÓ The two ÅÇÉ instructions will cause the ÉÇÉÊ instruction to be in the write-back stage when the ËËÀÇÌÊ instruction is in the decode stage. If the two forwarding sources in the write-back stage are given the wrong priority, then register cËþÓ will be set to the incremented program counter rather than the value read from memory. Solution to Problem 4.34 (page 489) This logic only needs to check the ﬁve forwarding sources: ÑÇË® ®ˆÌþÄ¢ { ‰ ®ˆÍË²¢ {{ −ˆ®ÍÎ¥ x −ˆÌþÄ¥y a ƒÇËÑþË® ÌþÄ¥ ðËÇÀ −Ó−²ÏÎ− ®ˆÍË²¢ {{ ›ˆ®ÍÎ› x ÀˆÌþÄ›y a ƒÇËÑþË® ÌþÄ› ðËÇÀ À−ÀÇËÔ ®ˆÍË²¢ {{ ›ˆ®ÍÎ¥ x ›ˆÌþÄ¥y a ƒÇËÑþË® ÌþÄ¥ ðËÇÀ À−ÀÇËÔ ®ˆÍË²¢ {{ „ˆ®ÍÎ› x „ˆÌþÄ›y a ƒÇËÑþË® ÌþÄ› ðËÇÀ ÑË©Î− ¾þ²Â ®ˆÍË²¢ {{ „ˆ®ÍÎ¥ x „ˆÌþÄ¥y a ƒÇËÑþË® ÌþÄ¥ ðËÇÀ ÑË©Î− ¾þ²Â o x ®ˆËÌþÄ¢y a •Í− ÌþÄÏ− Ë−þ® ðËÇÀ Ë−×©ÍÎ−Ë ð©Ä− `y Solution to Problem 4.35 (page 490) This change would not handle the case where a conditional move fails to satisfy the condition, and therefore sets the dstE value to ‡ﬁﬂﬁ¥. The resulting value could get forwarded to the next instruction, even though the conditional transfer does not occur. 1 ©ËÀÇÌÊ bnÓopqjcËþÓ 2 ©ËÀÇÌÊ bnÓqpojcË®Ó 3 ÓÇËÊ cË²ÓjcË²Ó a ££ { onn 4 ²ÀÇÌÅ− cËþÓjcË®Ó a ﬁÇÎ ÎËþÅÍð−ËË−® 5 þ®®Ê cË®ÓjcË®Ó a ·³ÇÏÄ® ¾− nÓtrp 6 ³þÄÎ 528 Chapter 4 Processor Architecture This code initializes register cË®Ó to nÓqpo. The conditional data transfer does not take place, and so the ﬁnal þ®®Ê instruction should double the value in cË®Ó to nÓtrp. With the altered design, however, the conditional move source value nÓqpo gets forwarded into ALU input valA, while input valB correctly gets operand value nÓopq. These inputs get added to produce result nÓrrr. Solution to Problem 4.36 (page 491) This code completes the computation of the status code for this instruction. aa •É®þÎ− Î³− ÍÎþÎÏÍ ÑÇË® ÀˆÍÎþÎ { ‰ ®À−Àˆ−ËËÇË x ·¡⁄‡y o x ›ˆÍÎþÎy `y Solution to Problem 4.37 (page 497) The following test program is designed to set up control combination A (Figure 4.67) and detect whether something goes wrong: 1 a £Ç®− ÎÇ ×−Å−ËþÎ− þ ²ÇÀ¾©ÅþÎ©ÇÅ Çð ÅÇÎkÎþÂ−Å ¾ËþÅ²³ þÅ® Ë−Î 2 ©ËÀÇÌÊ ·Îþ²Âj cËÍÉ 3 ©ËÀÇÌÊ ËÎÅÉjcËþÓ 4 ÉÏÍ³Ê cËþÓ a ·−Î ÏÉ Ë−ÎÏËÅ ÉÇ©ÅÎ−Ë 5 ÓÇËÊ cËþÓjcËþÓ a ·−Î … ²ÇÅ®©Î©ÇÅ ²Ç®− 6 ÁÅ− ÎþË×−Î a ﬁÇÎ ÎþÂ−Å fƒ©ËÍÎ ÉþËÎ Çð ²ÇÀ¾©ÅþÎ©ÇÅg 7 ©ËÀÇÌÊ bojcËþÓ a ·³ÇÏÄ® −Ó−²ÏÎ− Î³©Í 8 ³þÄÎ 9 ÎþË×−Îx Ë−Î a ·−²ÇÅ® ÉþËÎ Çð ²ÇÀ¾©ÅþÎ©ÇÅ 10 ©ËÀÇÌÊ bpjcË¾Ó a ·³ÇÏÄ® ÅÇÎ −Ó−²ÏÎ− Î³©Í 11 ³þÄÎ 12 ËÎÅÉx ©ËÀÇÌÊ bqjcË®Ó a ·³ÇÏÄ® ÅÇÎ −Ó−²ÏÎ− Î³©Í 13 ³þÄÎ 14 lÉÇÍ nÓrn 15 ·Îþ²Âx This program is designed so that if something goes wrong (for example, if the Ë−Î instruction is actually executed), then the program will execute one of the extra ©ËÀÇÌÊ instructions and then halt. Thus, an error in the pipeline would cause some register to be updated incorrectly. This code illustrates the care required to implement a test program. It must set up a potential error condition and then detect whether or not an error occurs. Solution to Problem 4.38 (page 498) The following test program is designed to set up control combination B (Figure 4.67). The simulator will detect a case where the bubble and stall control signals for a pipeline register are both set to zero, and so our test program need only set up the combination for it to be detected. The biggest challenge is to make the program do something sensible when handled correctly. Solutions to Practice Problems 529 1 a ¶−ÍÎ ©ÅÍÎËÏ²Î©ÇÅ Î³þÎ ÀÇ®©ð©−Í c−ÍÉ ðÇÄÄÇÑ−® ¾Ô Ë−Î 2 ©ËÀÇÌÊ À−ÀjcË¾Ó 3 ÀËÀÇÌÊ nfcË¾ÓgjcËÍÉ a ·−ÎÍ cËÍÉ ÎÇ ÉÇ©ÅÎ ÎÇ Ë−ÎÏËÅ ÉÇ©ÅÎ 4 Ë−Î a ‡−ÎÏËÅÍ ÎÇ Ë−ÎÏËÅ ÉÇ©ÅÎ 5 ³þÄÎ a 6 ËÎÅÉÎx ©ËÀÇÌÊ bsjcËÍ© a ‡−ÎÏËÅ ÉÇ©ÅÎ 7 ³þÄÎ 8 lÉÇÍ nÓrn 9 À−Àx lÊÏþ® ÍÎþ²Â a ¤ÇÄ®Í ®−Í©Ë−® ÍÎþ²Â ÉÇ©ÅÎ−Ë 10 lÉÇÍ nÓsn 11 ÍÎþ²Âx lÊÏþ® ËÎÅÉÎ a ¶ÇÉ Çð ÍÎþ²Âx ¤ÇÄ®Í Ë−ÎÏËÅ ÉÇ©ÅÎ This program uses two initialized words in memory. The ﬁrst word (À−À) holds the address of the second (ÍÎþ²Â—the desired stack pointer). The second word holds the address of the desired return point for the Ë−Î instruction. The program loads the stack pointer into cËÍÉ and executes the Ë−Î instruction. Solution to Problem 4.39 (page 499) From Figure 4.66, we can see that pipeline register D must be stalled for a load/use hazard: ¾ÇÇÄ ⁄ˆÍÎþÄÄ { a £ÇÅ®©Î©ÇÅÍ ðÇË þ ÄÇþ®mÏÍ− ³þÖþË® ¥ˆ©²Ç®− ©Å Õ '›‡›ﬂ‚†j '–ﬂ–† Û dd ¥ˆ®ÍÎ› ©Å Õ ®ˆÍË²¡j ®ˆÍË²¢ Ûy Solution to Problem 4.40 (page 500) From Figure 4.66, we can see that pipeline register E must be set to bubble for a load/use hazard or for a mispredicted branch: ¾ÇÇÄ ¥ˆ¾Ï¾¾Ä− { a ›©ÍÉË−®©²Î−® ¾ËþÅ²³ f¥ˆ©²Ç®− {{ '“”” dd _−ˆ£Å®g ÚÚ a £ÇÅ®©Î©ÇÅÍ ðÇË þ ÄÇþ®mÏÍ− ³þÖþË® ¥ˆ©²Ç®− ©Å Õ '›‡›ﬂ‚†j '–ﬂ–† Û dd ¥ˆ®ÍÎ› ©Å Õ ®ˆÍË²¡j ®ˆÍË²¢Ûy Solution to Problem 4.41 (page 500) This control requires examining the code of the executing instruction and checking for exceptions further down the pipeline. aa ·³ÇÏÄ® Î³− ²ÇÅ®©Î©ÇÅ ²Ç®−Í ¾− ÏÉ®þÎ−®} ¾ÇÇÄ Í−Îˆ²² { ¥ˆ©²Ç®− {{ 'ﬂ–† dd a ·ÎþÎ− ²³þÅ×−Í ÇÅÄÔ ®ÏË©Å× ÅÇËÀþÄ ÇÉ−ËþÎ©ÇÅ _ÀˆÍÎþÎ ©Å Õ ·¡⁄‡j ·'ﬁ·j ·¤‹¶ Û dd _„ˆÍÎþÎ ©Å Õ ·¡⁄‡j ·'ﬁ·j ·¤‹¶ Ûy Solution to Problem 4.42 (page 500) Injecting a bubble into the memory stage on the next cycle involves checking for an exception in either the memory or the write-back stage during the current cycle. 530 Chapter 4 Processor Architecture a ·ÎþËÎ ©ÅÁ−²Î©Å× ¾Ï¾¾Ä−Í þÍ ÍÇÇÅ þÍ −Ó²−ÉÎ©ÇÅ ÉþÍÍ−Í Î³ËÇÏ×³ À−ÀÇËÔ ÍÎþ×− ¾ÇÇÄ ›ˆ¾Ï¾¾Ä− { ÀˆÍÎþÎ ©Å Õ ·¡⁄‡j ·'ﬁ·j ·¤‹¶ Û ÚÚ „ˆÍÎþÎ ©Å Õ ·¡⁄‡j ·'ﬁ·j ·¤‹¶ Ûy For stalling the write-back stage, we check only the status of the instruction in this stage. If we also stalled when an excepting instruction was in the memory stage, then this instruction would not be able to enter the write-back stage. ¾ÇÇÄ „ˆÍÎþÄÄ { „ˆÍÎþÎ ©Å Õ ·¡⁄‡j ·'ﬁ·j ·¤‹¶ Ûy Solution to Problem 4.43 (page 504) We would then have a misprediction frequency of 0.35, giving mp = 0.20 × 0.35 × 2 = 0.14, giving an overall CPI of 1.25. This seems like a fairly marginal gain, but it would be worthwhile if the cost of implementing the new branch prediction strategy were not too high. Solution to Problem 4.44 (page 504) This simpliﬁed analysis, where we focus on the inner loop, is a useful way to estimate program performance. As long as the array is sufﬁciently large, the time spent in other parts of the code will be negligible. A. The inner loop of the code using the conditional jump has 11 instructions, all of which are executed when the array element is zero or negative, and 10 of which are executed when the array element is positive. The average is 10.5. The inner loop of the code using the conditional move has 10 instructions, all of which are executed every time. B. The loop-closing jump will be predicted correctly, except when the loop terminates. For a very long array, this one misprediction will have a negligible effect on the performance. The only other source of bubbles for the jump- based code is the conditional jump, depending on whether or not the array element is positive. This will cause two bubbles, but it only occurs 50% of the time, so the average is 1.0. There are no bubbles in the conditional move code. C. Our conditional jump code requires an average of 10.5 + 1.0 = 11.5 cycles per array element (11 cycles in the best case and 12 cycles in the worst), while our conditional move code requires 10.0 cycles in all cases. Our pipeline has a branch misprediction penalty of only two cycles—far better than those for the deep pipelines of higher-performance processors. As a result, using conditional moves does not affect program performance very much. CHAPTER 5 Optimizing Program Performance 5.1 Capabilities and Limitations of Optimizing Compilers 534 5.2 Expressing Program Performance 538 5.3 Program Example 540 5.4 Eliminating Loop Inefﬁciencies 544 5.5 Reducing Procedure Calls 548 5.6 Eliminating Unneeded Memory References 550 5.7 Understanding Modern Processors 553 5.8 Loop Unrolling 567 5.9 Enhancing Parallelism 572 5.10 Summary of Results for Optimizing Combining Code 583 5.11 Some Limiting Factors 584 5.12 Understanding Memory Performance 589 5.13 Life in the Real World: Performance Improvement Techniques 597 5.14 Identifying and Eliminating Performance Bottlenecks 598 5.15 Summary 604 Bibliographic Notes 605 Homework Problems 606 Solutions to Practice Problems 609 531 532 Chapter 5 Optimizing Program Performance T he primary objective in writing a program must be to make it work correctly under all possible conditions. A program that runs fast but gives incorrect results serves no useful purpose. Programmers must write clear and concise code, not only so that they can make sense of it, but also so that others can read and understand the code during code reviews and when modiﬁcations are required later. On the other hand, there are many occasions when making a program run fast is also an important consideration. If a program must process video frames or network packets in real time, then a slow-running program will not provide the needed functionality. When a computational task is so demanding that it requires days or weeks to execute, then making it run just 20% faster can have signiﬁcant impact. In this chapter, we will explore how to make programs run faster via several different types of program optimization. Writing an efﬁcient program requires several types of activities. First, we must select an appropriate set of algorithms and data structures. Second, we must write source code that the compiler can effectively optimize to turn into efﬁcient executable code. For this second part, it is important to understand the capabilities and limitations of optimizing compilers. Seemingly minor changes in how a program is written can make large differences in how well a compiler can optimize it. Some programming languages are more easily optimized than others. Some features of C, such as the ability to perform pointer arithmetic and casting, make it challenging for a compiler to optimize. Programmers can often write their programs in ways that make it easier for compilers to generate efﬁcient code. A third technique for dealing with especially demanding computations is to divide a task into portions that can be computed in parallel, on some combination of multiple cores and multiple processors. We will defer this aspect of performance enhancement to Chapter 12. Even when exploiting parallelism, it is important that each parallel thread execute with maximum performance, and so the material of this chapter remains relevant in any case. In approaching program development and optimization, we must consider how the code will be used and what critical factors affect it. In general, program- mers must make a trade-off between how easy a program is to implement and maintain, and how fast it runs. At an algorithmic level, a simple insertion sort can be programmed in a matter of minutes, whereas a highly efﬁcient sort routine may take a day or more to implement and optimize. At the coding level, many low-level optimizations tend to reduce code readability and modularity, making the programs more susceptible to bugs and more difﬁcult to modify or extend. For code that will be executed repeatedly in a performance-critical environment, extensive optimization may be appropriate. One challenge is to maintain some degree of elegance and readability in the code despite extensive transformations. We describe a number of techniques for improving code performance. Ideally, a compiler would be able to take whatever code we write and generate the most efﬁcient possible machine-level program having the speciﬁed behavior. Modern compilers employ sophisticated forms of analysis and optimization, and they keep getting better. Even the best compilers, however, can be thwarted by optimization blockers—aspects of the program’s behavior that depend strongly on the execu- Chapter 5 Optimizing Program Performance 533 tion environment. Programmers must assist the compiler by writing code that can be optimized readily. The ﬁrst step in optimizing a program is to eliminate unnecessary work, mak- ing the code perform its intended task as efﬁciently as possible. This includes eliminating unnecessary function calls, conditional tests, and memory references. These optimizations do not depend on any speciﬁc properties of the target ma- chine. To maximize the performance of a program, both the programmer and the compiler require a model of the target machine, specifying how instructions are processed and the timing characteristics of the different operations. For example, the compiler must know timing information to be able to decide whether it should use a multiply instruction or some combination of shifts and adds. Modern com- puters use sophisticated techniques to process a machine-level program, executing many instructions in parallel and possibly in a different order than they appear in the program. Programmers must understand how these processors work to be able to tune their programs for maximum speed. We present a high-level model of such a machine based on recent designs of Intel and AMD processors. We also devise a graphical data-ﬂow notation to visualize the execution of instructions by the processor, with which we can predict program performance. With this understanding of processor operation, we can take a second step in program optimization, exploiting the capability of processors to provide instruc- tion-level parallelism, executing multiple instructions simultaneously. We cover several program transformations that reduce the data dependencies between dif- ferent parts of a computation, increasing the degree of parallelism with which they can be executed. We conclude the chapter by discussing issues related to optimizing large pro- grams. We describe the use of code proﬁlers—tools that measure the performance of different parts of a program. This analysis can help ﬁnd inefﬁciencies in the code and identify the parts of the program on which we should focus our optimization efforts. In this presentation, we make code optimization look like a simple linear process of applying a series of transformations to the code in a particular order. In fact, the task is not nearly so straightforward. A fair amount of trial-and- error experimentation is required. This is especially true as we approach the later optimization stages, where seemingly small changes can cause major changes in performance and some very promising techniques prove ineffective. As we will see in the examples that follow, it can be difﬁcult to explain exactly why a particular code sequence has a particular execution time. Performance can depend on many detailed features of the processor design for which we have relatively little documentation or understanding. This is another reason to try a number of different variations and combinations of techniques. Studying the assembly-code representation of a program is one of the most effective means for gaining an understanding of the compiler and how the gen- erated code will run. A good strategy is to start by looking carefully at the code for the inner loops, identifying performance-reducing attributes such as excessive memory references and poor use of registers. Starting with the assembly code, we 534 Chapter 5 Optimizing Program Performance can also predict what operations will be performed in parallel and how well they will use the processor resources. As we will see, we can often determine the time (or at least a lower bound on the time) required to execute a loop by identifying critical paths, chains of data dependencies that form during repeated executions of a loop. We can then go back and modify the source code to try to steer the compiler toward more efﬁcient implementations. Most major compilers, including gcc, are continually being updated and im- proved, especially in terms of their optimization abilities. One useful strategy is to do only as much rewriting of a program as is required to get it to the point where the compiler can then generate efﬁcient code. By this means, we avoid compro- mising the readability, modularity, and portability of the code as much as if we had to work with a compiler of only minimal capabilities. Again, it helps to iteratively modify the code and analyze its performance both through measurements and by examining the generated assembly code. To novice programmers, it might seem strange to keep modifying the source code in an attempt to coax the compiler into generating efﬁcient code, but this is indeed how many high-performance programs are written. Compared to the alternative of writing code in assembly language, this indirect approach has the advantage that the resulting code will still run on other machines, although per- haps not with peak performance. 5.1 Capabilities and Limitations of Optimizing Compilers Modern compilers employ sophisticated algorithms to determine what values are computed in a program and how they are used. They can then exploit opportuni- ties to simplify expressions, to use a single computation in several different places, and to reduce the number of times a given computation must be performed. Most compilers, including gcc, provide users with some control over which optimiza- tions they apply. As discussed in Chapter 3, the simplest control is to specify the optimization level. For example, invoking gcc with the command-line option kﬂ× speciﬁes that it should apply a basic set of optimizations. Invoking gcc with option kﬂo or higher (e.g., kﬂp or kﬂq) will cause it to apply more extensive optimizations. These can further improve program performance, but they may expand the program size and they may make the program more difﬁcult to debug using standard debugging tools. For our presentation, we will mostly consider code compiled with optimization level kﬂo, even though level kﬂp has become the accepted standard for most software projects that use gcc. We purposely limit the level of optimization to demonstrate how different ways of writing a function in C can affect the efﬁciency of the code generated by a compiler. We will ﬁnd that we can write C code that, when compiled just with option kﬂo, vastly outperforms a more naive version compiled with the highest possible optimization levels. Compilers must be careful to apply only safe optimizations to a program, meaning that the resulting program will have the exact same behavior as would an unoptimized version for all possible cases the program may encounter, up to the limits of the guarantees provided by the C language standards. Constraining Section 5.1 Capabilities and Limitations of Optimizing Compilers 535 the compiler to perform only safe optimizations eliminates possible sources of undesired run-time behavior, but it also means that the programmer must make more of an effort to write programs in a way that the compiler can then transform into efﬁcient machine-level code. To appreciate the challenges of deciding which program transformations are safe or not, consider the following two procedures: 1 ÌÇ©® ÎÑ©®®Ä−ofÄÇÅ× hÓÉj ÄÇÅ× hÔÉg 2 Õ 3 hÓÉ i{ hÔÉy 4 hÓÉ i{ hÔÉy 5 Û 6 7 ÌÇ©® ÎÑ©®®Ä−pfÄÇÅ× hÓÉj ÄÇÅ× hÔÉg 8 Õ 9 hÓÉ i{ ph hÔÉy 10 Û At ﬁrst glance, both procedures seem to have identical behavior. They both add twice the value stored at the location designated by pointer ÔÉ to that desig- nated by pointer ÓÉ. On the other hand, function ÎÑ©®®Ä−p is more efﬁcient. It requires only three memory references (read hÓÉ, read hÔÉ, write hÓÉ), whereas ÎÑ©®®Ä−o requires six (two reads of hÓÉ, two reads of hÔÉ, and two writes of hÓÉ). Hence, if a compiler is given procedure ÎÑ©®®Ä−o to compile, one might think it could generate more efﬁcient code based on the computations performed by ÎÑ©®®Ä−p. Consider, however, the case in which ÓÉ and ÔÉ are equal. Then function ÎÑ©®®Ä−o will perform the following computations: 3 hÓÉ i{ hÓÉy mh ⁄ÇÏ¾Ä− ÌþÄÏ− þÎ ÓÉ hm 4 hÓÉ i{ hÓÉy mh ⁄ÇÏ¾Ä− ÌþÄÏ− þÎ ÓÉ hm The result will be that the value at ÓÉ will be increased by a factor of 4. On the other hand, function ÎÑ©®®Ä−p will perform the following computation: 9 hÓÉ i{ ph hÓÉy mh ¶Ë©ÉÄ− ÌþÄÏ− þÎ ÓÉ hm The result will be that the value at ÓÉ will be increased by a factor of 3. The compiler knows nothing about how ÎÑ©®®Ä−o will be called, and so it must assume that arguments ÓÉ and ÔÉ can be equal. It therefore cannot generate code in the style of ÎÑ©®®Ä−p as an optimized version of ÎÑ©®®Ä−o. The case where two pointers may designate the same memory location is known as memory aliasing. In performing only safe optimizations, the compiler must assume that different pointers may be aliased. As another example, for a program with pointer variables É and Ê, consider the following code sequence: Ó { onnny Ô { qnnny hÊ { Ôy mh qnnn hm hÉ { Óy mh onnn hm Îo { hÊy mh onnn ÇË qnnn hm 536 Chapter 5 Optimizing Program Performance The value computed for Îo depends on whether or not pointers É and Ê are aliased—if not, it will equal 3,000, but if so it will equal 1,000. This leads to one of the major optimization blockers, aspects of programs that can severely limit the opportunities for a compiler to generate optimized code. If a compiler cannot determine whether or not two pointers may be aliased, it must assume that either case is possible, limiting the set of possible optimizations. Practice Problem 5.1 (solution page 609) The following problem illustrates the way memory aliasing can cause unexpected program behavior. Consider the following procedure to swap two values: 1 mh ·ÑþÉ ÌþÄÏ− Ó þÎ ÓÉ Ñ©Î³ ÌþÄÏ− Ô þÎ ÔÉ hm 2 ÌÇ©® ÍÑþÉfÄÇÅ× hÓÉj ÄÇÅ× hÔÉg 3 Õ 4 hÓÉ { hÓÉ i hÔÉy mh ÓiÔ hm 5 hÔÉ { hÓÉ k hÔÉy mh ÓiÔkÔ{Óhm 6 hÓÉ { hÓÉ k hÔÉy mh ÓiÔkÓ{Ôhm 7 Û If this procedure is called with ÓÉ equal to ÔÉ, what effect will it have? A second optimization blocker is due to function calls. As an example, con- sider the following two procedures: 1 ÄÇÅ× ðfgy 2 3 ÄÇÅ× ðÏÅ²ofg Õ 4 Ë−ÎÏËÅ ðfg i ðfg i ðfg i ðfgy 5 Û 6 7 ÄÇÅ× ðÏÅ²pfg Õ 8 Ë−ÎÏËÅ rhðfgy 9 Û It might seem at ﬁrst that both compute the same result, but with ðÏÅ²p calling ð only once, whereas ðÏÅ²o calls it four times. It is tempting to generate code in the style of ðÏÅ²p when given ðÏÅ²o as the source. Consider, however, the following code for ð: 1 ÄÇÅ× ²ÇÏÅÎ−Ë { ny 2 3 ÄÇÅ× ðfg Õ 4 Ë−ÎÏËÅ ²ÇÏÅÎ−Ëiiy 5 Û This function has a side effect—it modiﬁes some part of the global program state. Changing the number of times it gets called changes the program behavior. In Section 5.1 Capabilities and Limitations of Optimizing Compilers 537 Aside Optimizing function calls by inline substitution Code involving function calls can be optimized by a process known as inline substitution (or simply “inlining”), where the function call is replaced by the code for the body of the function. For example, we can expand the code for ðÏÅ²o by substituting four instantiations of function ð: 1 mh ‡−ÍÏÄÎ Çð ©ÅÄ©Å©Å× ð ©Å ðÏÅ²o hm 2 ÄÇÅ× ðÏÅ²o©Åfg Õ 3 ÄÇÅ× Î { ²ÇÏÅÎ−Ëiiy mh in hm 4 Î i{ ²ÇÏÅÎ−Ëiiy mh io hm 5 Î i{ ²ÇÏÅÎ−Ëiiy mh ip hm 6 Î i{ ²ÇÏÅÎ−Ëiiy mh iq hm 7 Ë−ÎÏËÅ Îy 8 Û This transformation both reduces the overhead of the function calls and allows further optimization of the expanded code. For example, the compiler can consolidate the updates of global variable ²ÇÏÅÎ−Ë in ðÏÅ²o©Å to generate an optimized version of the function: 1 mh ﬂÉÎ©À©ÖþÎ©ÇÅ Çð ©ÅÄ©Å−® ²Ç®− hm 2 ÄÇÅ× ðÏÅ²oÇÉÎfg Õ 3 ÄÇÅ×Î{rh ²ÇÏÅÎ−Ë i ty 4 ²ÇÏÅÎ−Ë i{ ry 5 Ë−ÎÏËÅ Îy 6 Û This code faithfully reproduces the behavior of ðÏÅ²o for this particular deﬁnition of function ð. Recent versions of gcc attempt this form of optimization, either when directed to with the command-line option kð©ÅÄ©Å− or for optimization level kﬂo and higher. Unfortunately, gcc only attempts inlining for functions deﬁned within a single ﬁle. That means it will not be applied in the common case where a set of library functions is deﬁned in one ﬁle but invoked by functions in other ﬁles. There are times when it is best to prevent a compiler from performing inline substitution. One is when the code will be evaluated using a symbolic debugger, such as gdb, as described in Section 3.10.2. If a function call has been optimized away via inline substitution, then any attempt to trace or set a breakpoint for that call will fail. The second is when evaluating the performance of a program by proﬁling, as is discussed in Section 5.14.1. Calls to functions that have been eliminated by inline substitution will not be proﬁled correctly. particular, a call to ðÏÅ²o would return 0 + 1 + 2 + 3 = 6, whereas a call to ðÏÅ²p would return 4 . 0 = 0, assuming both started with global variable ²ÇÏÅÎ−Ë set to zero. Most compilers do not try to determine whether a function is free of side effects and hence is a candidate for optimizations such as those attempted in ðÏÅ²p. Instead, the compiler assumes the worst case and leaves function calls intact. 538 Chapter 5 Optimizing Program Performance Among compilers, gcc is considered adequate, but not exceptional, in terms of its optimization capabilities. It performs basic optimizations, but it does not per- form the radical transformations on programs that more “aggressive” compilers do. As a consequence, programmers using gcc must put more effort into writing programs in a way that simpliﬁes the compiler’s task of generating efﬁcient code. 5.2 Expressing Program Performance We introduce the metric cycles per element, abbreviated CPE, to express program performance in a way that can guide us in improving the code. CPE measure- ments help us understand the loop performance of an iterative program at a detailed level. It is appropriate for programs that perform a repetitive compu- tation, such as processing the pixels in an image or computing the elements in a matrix product. The sequencing of activities by a processor is controlled by a clock providing a regular signal of some frequency, usually expressed in gigahertz (GHz), billions of cycles per second. For example, when product literature characterizes a system as a “4 GHz” processor, it means that the processor clock runs at 4.0 × 109 cycles per second. The time required for each clock cycle is given by the reciprocal of the clock frequency. These typically are expressed in nanoseconds (1 nanosecond is 10−9 seconds) or picoseconds (1 picosecond is 10−12 seconds). For example, the period of a 4 GHz clock can be expressed as either 0.25 nanoseconds or 250 picoseconds. From a programmer’s perspective, it is more instructive to express measurements in clock cycles rather than nanoseconds or picoseconds. That way, the measurements express how many instructions are being executed rather than how fast the clock runs. Many procedures contain a loop that iterates over a set of elements. For example, functions ÉÍÏÀo and ÉÍÏÀp in Figure 5.1 both compute the preﬁx sum of a vector of length n. For a vector ⃗a =⟨a0,a1,. . .,an−1⟩, the preﬁx sum ⃗p = ⟨p0,p1,...,pn−1⟩ is deﬁned as p0 = a0 pi = pi−1 + ai, 1 ≤ i< n (5.1) Function ÉÍÏÀo computes one element of the result vector per iteration. Func- tion ÉÍÏÀp uses a technique known as loop unrolling to compute two elements per iteration. We will explore the beneﬁts of loop unrolling later in this chapter. (See Problems 5.11, 5.12, and 5.19 for more about analyzing and optimizing the preﬁx- sum computation.) The time required by such a procedure can be characterized as a constant plus a factor proportional to the number of elements processed. For example, Figure 5.2 shows a plot of the number of clock cycles required by the two functions for a range of values of n. Using a least squares ﬁt, we ﬁnd that the run times (in clock cycles) for ÉÍÏÀo and ÉÍÏÀp can be approximated by the equations 368 + 9.0n and 368 + 6.0n, respectively. These equations indicate an overhead of 368 cycles due to the timing code and to initiate the procedure, set up the loop, and complete the Section 5.2 Expressing Program Performance 539 1 mh £ÇÀÉÏÎ− ÉË−ð©Ó ÍÏÀ Çð Ì−²ÎÇË þ hm 2 ÌÇ©® ÉÍÏÀofðÄÇþÎ þ‰`j ðÄÇþÎ É‰`j ÄÇÅ× Åg 3 Õ 4 ÄÇÅ× ©y 5 É‰n` { þ‰n`y 6 ðÇËf©{oy©zÅy ©iig 7 É‰©` { É‰©ko` i þ‰©`y 8 Û 9 10 ÌÇ©® ÉÍÏÀpfðÄÇþÎ þ‰`j ðÄÇþÎ É‰`j ÄÇÅ× Åg 11 Õ 12 ÄÇÅ× ©y 13 É‰n` { þ‰n`y 14 ðÇË f© { oy © z Åkoy ©i{pg Õ 15 ðÄÇþÎ À©®ˆÌþÄ { É‰©ko` i þ‰©`y 16 É‰©` { À©®ˆÌþÄy 17 É‰©io` { À©®ˆÌþÄ i þ‰©io`y 18 Û 19 mh ƒÇË −Ì−Å Åj ð©Å©Í³ Ë−Àþ©Å©Å× −Ä−À−ÅÎ hm 20 ©ð f© z Åg 21 É‰©` { É‰©ko` i þ‰©`y 22 Û Figure 5.1 Preﬁx-sum functions. These functions provide examples for how we express program performance. 2500 2000 1500 1000 500 0 06020 40 psum1 Slope = 9.0 psum2 Slope = 6.0 10080 140 160 180120 200 ElementsCycles Figure 5.2 Performance of preﬁx-sum functions. The slope of the lines indicates the number of clock cycles per element (CPE). 540 Chapter 5 Optimizing Program Performance Aside What is a least squares ﬁt? For a set of data points (x1,y1), . . . (xn,yn), we often try to draw a line that best approximates the X– Y trend represented by these data. With a least squares ﬁt, we look for a line of the form y = mx + b that minimizes the following error measure: E(m, b) = ∑ i=1,n(mxi + b − yi)2 An algorithm for computing m and b can be derived by ﬁnding the derivatives of E(m, b) with respect to m and b and setting them to 0. procedure, plus a linear factor of 6.0 or 9.0 cycles per element. For large values of n (say, greater than 200), the run times will be dominated by the linear factors. We refer to the coefﬁcients in these terms as the effective number of cycles per element. We prefer measuring the number of cycles per element rather than the number of cycles per iteration, because techniques such as loop unrolling allow us to use fewer iterations to complete the computation, but our ultimate concern is how fast the procedure will run for a given vector length. We focus our efforts on minimizing the CPE for our computations. By this measure, ÉÍÏÀp, with a CPE of 6.0, is superior to ÉÍÏÀo, with a CPE of 9.0. Practice Problem 5.2 (solution page 609) Later in this chapter we will start with a single function and generate many differ- ent variants that preserve the function’s behavior, but with different performance characteristics. For three of these variants, we found that the run times (in clock cycles) can be approximated by the following functions: Version 1: 60 + 35n Version 2: 136 + 4n Version 3: 157 + 1.25n For what values of n would each version be the fastest of the three? Remember that n will always be an integer. 5.3 Program Example To demonstrate how an abstract program can be systematically transformed into more efﬁcient code, we will use a running example based on the vector data structure shown in Figure 5.3. A vector is represented with two blocks of memory: the header and the data array. The header is a structure declared as follows: Section 5.3 Program Example 541 0 1 2len len\u00021len data . . . Figure 5.3 Vector abstract data type. A vector is represented by header information plus an array of designated length. code/opt/vec.h 1 mh £Ë−þÎ− þ¾ÍÎËþ²Î ®þÎþ ÎÔÉ− ðÇË Ì−²ÎÇË hm 2 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 3 ÄÇÅ× Ä−Åy 4 ®þÎþˆÎ h®þÎþy 5 Û Ì−²ˆË−²j hÌ−²ˆÉÎËy code/opt/vec.h The declaration uses ®þÎþˆÎ to designate the data type of the underlying elements. In our evaluation, we measured the performance of our code for integer (C ©ÅÎ and ÄÇÅ×), and ﬂoating-point (C ðÄÇþÎ and ®ÇÏ¾Ä−) data. We do this by compiling and running the program separately for different type declarations, such as the following for data type ÄÇÅ×: ÎÔÉ−®−ð ÄÇÅ× ®þÎþˆÎy We allocate the data array block to store the vector elements as an array of Ä−Å objects of type ®þÎþˆÎ. Figure 5.4 shows some basic procedures for generating vectors, accessing vec- tor elements, and determining the length of a vector. An important feature to note is that ×−ÎˆÌ−²ˆ−Ä−À−ÅÎ, the vector access routine, performs bounds checking for every vector reference. This code is similar to the array representations used in many other languages, including Java. Bounds checking reduces the chances of program error, but it can also slow down program execution. As an optimization example, consider the code shown in Figure 5.5, which combines all of the elements in a vector into a single value according to some operation. By using different deﬁnitions of compile-time constants '⁄¥ﬁ¶ and ﬂ–, the code can be recompiled to perform different operations on the data. In particular, using the declarations a®−ð©Å− '⁄¥ﬁ¶ n a®−ð©Å− ﬂ– i it sums the elements of the vector. Using the declarations a®−ð©Å− '⁄¥ﬁ¶ o a®−ð©Å− ﬂ– h it computes the product of the vector elements. In our presentation, we will proceed through a series of transformations of the code, writing different versions of the combining function. To gauge progress, 542 Chapter 5 Optimizing Program Performance code/opt/vec.c 1 mh £Ë−þÎ− Ì−²ÎÇË Çð ÍÉ−²©ð©−® Ä−Å×Î³ hm 2 Ì−²ˆÉÎË Å−ÑˆÌ−²fÄÇÅ× Ä−Åg 3 Õ 4 mh ¡ÄÄÇ²þÎ− ³−þ®−Ë ÍÎËÏ²ÎÏË− hm 5 Ì−²ˆÉÎË Ë−ÍÏÄÎ { fÌ−²ˆÉÎËg ÀþÄÄÇ²fÍ©Ö−ÇðfÌ−²ˆË−²ggy 6 ®þÎþˆÎ h®þÎþ { ﬁ•‹‹y 7 ©ð f_Ë−ÍÏÄÎg 8 Ë−ÎÏËÅ ﬁ•‹‹y mh £ÇÏÄ®Å’Î þÄÄÇ²þÎ− ÍÎÇËþ×− hm 9 Ë−ÍÏÄÎk|Ä−Å { Ä−Åy 10 mh ¡ÄÄÇ²þÎ− þËËþÔ hm 11 ©ð fÄ−Å | ng Õ 12 ®þÎþ { f®þÎþˆÎ hg²þÄÄÇ²fÄ−Åj Í©Ö−Çðf®þÎþˆÎggy 13 ©ð f_®þÎþg Õ 14 ðË−−ffÌÇ©® hg Ë−ÍÏÄÎgy 15 Ë−ÎÏËÅ ﬁ•‹‹y mh £ÇÏÄ®Å’Î þÄÄÇ²þÎ− ÍÎÇËþ×− hm 16 Û 17 Û 18 mh ⁄þÎþ Ñ©ÄÄ −©Î³−Ë ¾− ﬁ•‹‹ ÇË þÄÄÇ²þÎ−® þËËþÔ hm 19 Ë−ÍÏÄÎk|®þÎþ { ®þÎþy 20 Ë−ÎÏËÅ Ë−ÍÏÄÎy 21 Û 22 23 mh 24 h ‡−ÎË©−Ì− Ì−²ÎÇË −Ä−À−ÅÎ þÅ® ÍÎÇË− þÎ ®−ÍÎl 25 h ‡−ÎÏËÅ n fÇÏÎ Çð ¾ÇÏÅ®Íg ÇË o fÍÏ²²−ÍÍðÏÄg 26 hm 27 ©ÅÎ ×−ÎˆÌ−²ˆ−Ä−À−ÅÎfÌ−²ˆÉÎË Ìj ÄÇÅ× ©Å®−Ój ®þÎþˆÎ h®−ÍÎg 28 Õ 29 ©ð f©Å®−ÓznÚÚ ©Å®−Ó |{ Ìk|Ä−Åg 30 Ë−ÎÏËÅ ny 31 h®−ÍÎ { Ìk|®þÎþ‰©Å®−Ó`y 32 Ë−ÎÏËÅ oy 33 Û 34 35 mh ‡−ÎÏËÅ Ä−Å×Î³ Çð Ì−²ÎÇË hm 36 ÄÇÅ× Ì−²ˆÄ−Å×Î³fÌ−²ˆÉÎË Ìg 37 Õ 38 Ë−ÎÏËÅ Ìk|Ä−Åy 39 Û code/opt/vec.c Figure 5.4 Implementation of vector abstract data type. In the actual program, data type ®þÎþˆÎ is declared to be ©ÅÎ, ÄÇÅ×, ðÄÇþÎ,or ®ÇÏ¾Ä−. Section 5.3 Program Example 543 1 mh 'ÀÉÄ−À−ÅÎþÎ©ÇÅ Ñ©Î³ ÀþÓ©ÀÏÀ ÏÍ− Çð ®þÎþ þ¾ÍÎËþ²Î©ÇÅ hm 2 ÌÇ©® ²ÇÀ¾©Å−ofÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 6 h®−ÍÎ { '⁄¥ﬁ¶y 7 ðÇË f© { ny © z Ì−²ˆÄ−Å×Î³fÌgy ©iig Õ 8 ®þÎþˆÎ ÌþÄy 9 ×−ÎˆÌ−²ˆ−Ä−À−ÅÎfÌj ©j dÌþÄgy 10 h®−ÍÎ { h®−ÍÎ ﬂ– ÌþÄy 11 Û 12 Û Figure 5.5 Initial implementation of combining operation. Using different decla- rations of identity element '⁄¥ﬁ¶ and combining operation ﬂ–, we can measure the routine for different operations. we measured the CPE performance of the functions on a machine with an Intel Core i7 Haswell processor, which we refer to as our reference machine. Some characteristics of this processor were given in Section 3.1. These measurements characterize performance in terms of how the programs run on just one particular machine, and so there is no guarantee of comparable performance on other combinations of machine and compiler. However, we have compared the results with those for a number of different compiler/processor combinations, and we have found them generally consistent with those presented here. As we proceed through a set of transformations, we will ﬁnd that many lead to only minimal performance gains, while others have more dramatic ef- fects. Determining which combinations of transformations to apply is indeed part of the “black art” of writing fast code. Some combinations that do not pro- vide measurable beneﬁts are indeed ineffective, while others are important as ways to enable further optimizations by the compiler. In our experience, the best approach involves a combination of experimentation and analysis: repeat- edly attempting different approaches, performing measurements, and examining the assembly-code representations to identify underlying performance bottle- necks. As a starting point, the following table shows CPE measurements for ²ÇÀ¾©Å−o running on our reference machine, with different combinations of operation (addition or multiplication) and data type (long integer and double- precision ﬂoating point). Our experiments with many different programs showed that operations on 32-bit and 64-bit integers have identical performance, with the exception of code involving division operations. Similarly, we found identical performance for programs operating on single- or double-precision ﬂoating-point data. In our tables, we will therefore show only separate results for integer data and for ﬂoating-point data. 544 Chapter 5 Optimizing Program Performance Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−o 543 Abstract unoptimized 22.68 20.02 19.98 20.18 ²ÇÀ¾©Å−o 543 Abstract kﬂo 10.12 10.12 10.17 11.14 We can see that our measurements are somewhat imprecise. The more likely CPE number for integer sum is 23.00, rather than 22.68, while the number for integer product is likely 20.0 instead of 20.02. Rather than “fudging” our numbers to make them look good, we will present the measurements we actually obtained. There are many factors that complicate the task of reliably measuring the precise number of clock cycles required by some code sequence. It helps when examining these numbers to mentally round the results up or down by a few hundredths of a clock cycle. The unoptimized code provides a direct translation of the C code into machine code, often with obvious inefﬁciencies. By simply giving the command-line option kﬂo, we enable a basic set of optimizations. As can be seen, this signiﬁcantly improves the program performance—more than a factor of 2—with no effort on behalf of the programmer. In general, it is good to get into the habit of enabling some level of optimization. (Similar performance results were obtained with optimization level kﬂ×.) For the remainder of our measurements, we use optimization levels kﬂo and kﬂp when generating and measuring our programs. 5.4 Eliminating Loop Inefﬁciencies Observe that procedure ²ÇÀ¾©Å−o, as shown in Figure 5.5, calls function Ì−²ˆ Ä−Å×Î³ as the test condition of the ðÇË loop. Recall from our discussion of how to translate code containing loops into machine-level programs (Section 3.6.7) that the test condition must be evaluated on every iteration of the loop. On the other hand, the length of the vector does not change as the loop proceeds. We could therefore compute the vector length only once and use this value in our test condition. Figure 5.6 shows a modiﬁed version called ²ÇÀ¾©Å−p. It calls Ì−²ˆÄ−Å×Î³ at the beginning and assigns the result to a local variable Ä−Å×Î³. This transformation has noticeable effect on the overall performance for some data types and oper- ations, and minimal or even none for others. In any case, this transformation is required to eliminate inefﬁciencies that would become bottlenecks as we attempt further optimizations. Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−o 543 Abstract kﬂo 10.12 10.12 10.17 11.14 ²ÇÀ¾©Å−p 545 Move Ì−²ˆÄ−Å×Î³ 7.02 9.03 9.02 11.03 This optimization is an instance of a general class of optimizations known as code motion. They involve identifying a computation that is performed multiple Section 5.4 Eliminating Loop Inefﬁciencies 545 1 mh ›ÇÌ− ²þÄÄ ÎÇ Ì−²ˆÄ−Å×Î³ ÇÏÎ Çð ÄÇÇÉ hm 2 ÌÇ©® ²ÇÀ¾©Å−pfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 7 h®−ÍÎ { '⁄¥ﬁ¶y 8 ðÇË f© { ny © z Ä−Å×Î³y ©iig Õ 9 ®þÎþˆÎ ÌþÄy 10 ×−ÎˆÌ−²ˆ−Ä−À−ÅÎfÌj ©j dÌþÄgy 11 h®−ÍÎ { h®−ÍÎ ﬂ– ÌþÄy 12 Û 13 Û Figure 5.6 Improving the efﬁciency of the loop test. By moving the call to Ì−²ˆ Ä−Å×Î³ out of the loop test, we eliminate the need to execute it on every iteration. times, (e.g., within a loop), but such that the result of the computation will not change. We can therefore move the computation to an earlier section of the code that does not get evaluated as often. In this case, we moved the call to Ì−²ˆÄ−Å×Î³ from within the loop to just before the loop. Optimizing compilers attempt to perform code motion. Unfortunately, as dis- cussed previously, they are typically very cautious about making transformations that change where or how many times a procedure is called. They cannot reliably detect whether or not a function will have side effects, and so they assume that it might. For example, if Ì−²ˆÄ−Å×Î³ had some side effect, then ²ÇÀ¾©Å−o and ²ÇÀ¾©Å−p could have different behaviors. To improve the code, the programmer must often help the compiler by explicitly performing code motion. As an extreme example of the loop inefﬁciency seen in ²ÇÀ¾©Å−o, consider the procedure ÄÇÑ−Ëo shown in Figure 5.7. This procedure is styled after routines sub- mitted by several students as part of a network programming project. Its purpose is to convert all of the uppercase letters in a string to lowercase. The procedure steps through the string, converting each uppercase character to lowercase. The case conversion involves shifting characters in the range ‘¡’to‘…’ to the range ‘þ’ to ‘Ö’. The library function ÍÎËÄ−Å is called as part of the loop test of ÄÇÑ−Ëo. Al- though ÍÎËÄ−Å is typically implemented with special x86 string-processing instruc- tions, its overall execution is similar to the simple version that is also shown in Figure 5.7. Since strings in C are null-terminated character sequences, ÍÎËÄ−Å can only determine the length of a string by stepping through the sequence until it hits a null character. For a string of length n, ÍÎËÄ−Å takes time proportional to n. Since ÍÎËÄ−Å is called in each of the n iterations of ÄÇÑ−Ëo, the overall run time of ÄÇÑ−Ëo is quadratic in the string length, proportional to n2. 546 Chapter 5 Optimizing Program Performance 1 mh £ÇÅÌ−ËÎ ÍÎË©Å× ÎÇ ÄÇÑ−Ë²þÍ−x ÍÄÇÑ hm 2 ÌÇ©® ÄÇÑ−Ëof²³þË hÍg 3 Õ 4 ÄÇÅ× ©y 5 6 ðÇË f© { ny © z ÍÎËÄ−ÅfÍgy ©iig 7 ©ð fÍ‰©` |{ ’¡’ dd Í‰©` z{ ’…’g 8 Í‰©` k{ f’¡’ k ’þ’gy 9 Û 10 11 mh £ÇÅÌ−ËÎ ÍÎË©Å× ÎÇ ÄÇÑ−Ë²þÍ−x ðþÍÎ−Ë hm 12 ÌÇ©® ÄÇÑ−Ëpf²³þË hÍg 13 Õ 14 ÄÇÅ× ©y 15 ÄÇÅ× Ä−Å { ÍÎËÄ−ÅfÍgy 16 17 ðÇË f© { ny © z Ä−Åy ©iig 18 ©ð fÍ‰©` |{ ’¡’ dd Í‰©` z{ ’…’g 19 Í‰©` k{ f’¡’ k ’þ’gy 20 Û 21 22 mh ·þÀÉÄ− ©ÀÉÄ−À−ÅÎþÎ©ÇÅ Çð Ä©¾ËþËÔ ðÏÅ²Î©ÇÅ ÍÎËÄ−Å hm 23 mh £ÇÀÉÏÎ− Ä−Å×Î³ Çð ÍÎË©Å× hm 24 Í©Ö−ˆÎ ÍÎËÄ−Åf²ÇÅÍÎ ²³þË hÍg 25 Õ 26 ÄÇÅ× Ä−Å×Î³ { ny 27 Ñ³©Ä− fhÍ _{ ’¿n’g Õ 28 Íiiy 29 Ä−Å×Î³iiy 30 Û 31 Ë−ÎÏËÅ Ä−Å×Î³y 32 Û Figure 5.7 Lowercase conversion routines. The two procedures have radically different performance. This analysis is conﬁrmed by actual measurements of the functions for differ- ent length strings, as shown in Figure 5.8 (and using the library version of ÍÎËÄ−Å). The graph of the run time for ÄÇÑ−Ëo rises steeply as the string length increases (Figure 5.8(a)). Figure 5.8(b) shows the run times for seven different lengths (not the same as shown in the graph), each of which is a power of 2. Observe that for ÄÇÑ−Ëo each doubling of the string length causes a quadrupling of the run time. This is a clear indicator of a quadratic run time. For a string of length 1,048,576, ÄÇÑ−Ëo requires over 17 minutes of CPU time. Section 5.4 Eliminating Loop Inefﬁciencies 547 250 200 150 100 50 0 0 100,000 200,000 300,000 400,000 500,000 String lengthCPU seconds lower1 lower2 (a) String length Function 16,384 32,768 65,536 131,072 262,144 524,288 1,048,576 ÄÇÑ−Ëo 0.26 1.03 4.10 16.41 65.62 262.48 1,049.89 ÄÇÑ−Ëp 0.0000 0.0001 0.0001 0.0003 0.0005 0.0010 0.0020 (b) Figure 5.8 Comparative performance of lowercase conversion routines. The original code ÄÇÑ−Ëo has a quadratic run time due to an inefﬁcient loop structure. The modiﬁed code ÄÇÑ−Ëp has a linear run time. Function ÄÇÑ−Ëp shown in Figure 5.7 is identical to that of ÄÇÑ−Ëo, except that we have moved the call to ÍÎËÄ−Å out of the loop. The performance im- proves dramatically. For a string length of 1,048,576, the function requires just 2.0 milliseconds—over 500,000 times faster than ÄÇÑ−Ëo. Each doubling of the string length causes a doubling of the run time—a clear indicator of linear run time. For longer strings, the run-time improvement will be even greater. In an ideal world, a compiler would recognize that each call to ÍÎËÄ−Å in the loop test will return the same result, and thus the call could be moved out of the loop. This would require a very sophisticated analysis, since ÍÎËÄ−Å checks the elements of the string and these values are changing as ÄÇÑ−Ëo proceeds. The compiler would need to detect that even though the characters within the string are changing, none are being set from nonzero to zero, or vice versa. Such an analysis is well beyond the ability of even the most sophisticated compilers, even if they employ inlining, and so programmers must do such transformations themselves. This example illustrates a common problem in writing programs, in which a seemingly trivial piece of code has a hidden asymptotic inefﬁciency. One would not expect a lowercase conversion routine to be a limiting factor in a program’s performance. Typically, programs are tested and analyzed on small data sets, for which the performance of ÄÇÑ−Ëo is adequate. When the program is ultimately 548 Chapter 5 Optimizing Program Performance deployed, however, it is entirely possible that the procedure could be applied to strings of over one million characters. All of a sudden this benign piece of code has become a major performance bottleneck. By contrast, the performance of ÄÇÑ−Ëp will be adequate for strings of arbitrary length. Stories abound of major programming projects in which problems of this sort occur. Part of the job of a competent programmer is to avoid ever introducing such asymptotic inefﬁciency. Practice Problem 5.3 (solution page 609) Consider the following functions: ÄÇÅ× À©ÅfÄÇÅ× Ój ÄÇÅ× Ôg Õ Ë−ÎÏËÅÓzÔ}ÓxÔyÛ ÄÇÅ× ÀþÓfÄÇÅ× Ój ÄÇÅ× Ôg Õ Ë−ÎÏËÅÓzÔ}ÔxÓyÛ ÌÇ©® ©Å²ËfÄÇÅ× hÓÉj ÄÇÅ× Ìg Õ hÓÉ i{ Ìy Û ÄÇÅ× ÍÊÏþË−fÄÇÅ× Óg Õ Ë−ÎÏËÅ ÓhÓy Û The following three code fragments call these functions: A. ðÇË f© { À©ÅfÓj Ôgy © z ÀþÓfÓj Ôgy ©Å²Ëfd©j ogg Î i{ ÍÊÏþË−f©gy B. ðÇË f© { ÀþÓfÓj Ôg k oy © |{ À©ÅfÓj Ôgy ©Å²Ëfd©j kogg Î i{ ÍÊÏþË−f©gy C. ÄÇÅ× ÄÇÑ { À©ÅfÓj Ôgy ÄÇÅ× ³©×³ { ÀþÓfÓj Ôgy ðÇË f© { ÄÇÑy © z ³©×³y ©Å²Ëfd©j ogg Î i{ ÍÊÏþË−f©gy Assume Ó equals 10 and Ô equals 100. Fill in the following table indicating the number of times each of the four functions is called in code fragments A–C: Code À©Å ÀþÓ ©Å²Ë ÍÊÏþË− A. B. C. 5.5 Reducing Procedure Calls As we have seen, procedure calls can incur overhead and also block most forms of program optimization. We can see in the code for ²ÇÀ¾©Å−p (Figure 5.6) that ×−Îˆ Ì−²ˆ−Ä−À−ÅÎ is called on every loop iteration to retrieve the next vector element. This function checks the vector index © against the loop bounds with every vector reference, a clear source of inefﬁciency. Bounds checking might be a useful feature when dealing with arbitrary array accesses, but a simple analysis of the code for ²ÇÀ¾©Å−p shows that all references will be valid. Section 5.5 Reducing Procedure Calls 549 code/opt/vec.c 1 ®þÎþˆÎ h×−ÎˆÌ−²ˆÍÎþËÎfÌ−²ˆÉÎË Ìg 2 Õ 3 Ë−ÎÏËÅ Ìk|®þÎþy 4 Û code/opt/vec.c 1 mh ⁄©Ë−²Î þ²²−ÍÍ ÎÇ Ì−²ÎÇË ®þÎþ hm 2 ÌÇ©® ²ÇÀ¾©Å−qfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 7 8 h®−ÍÎ { '⁄¥ﬁ¶y 9 ðÇË f© { ny © z Ä−Å×Î³y ©iig Õ 10 h®−ÍÎ { h®−ÍÎ ﬂ– ®þÎþ‰©`y 11 Û 12 Û Figure 5.9 Eliminating function calls within the loop. The resulting code does not show a performance gain, but it enables additional optimizations. Suppose instead that we add a function ×−ÎˆÌ−²ˆÍÎþËÎ to our abstract data type. This function returns the starting address of the data array, as shown in Figure 5.9. We could then write the procedure shown as ²ÇÀ¾©Å−q in this ﬁgure, having no function calls in the inner loop. Rather than making a function call to retrieve each vector element, it accesses the array directly. A purist might say that this transformation seriously impairs the program modularity. In principle, the user of the vector abstract data type should not even need to know that the vector contents are stored as an array, rather than as some other data structure such as a linked list. A more pragmatic programmer would argue that this transformation is a necessary step toward achieving high-performance results. Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−p 545 Move Ì−²ˆÄ−Å×Î³ 7.02 9.03 9.02 11.03 ²ÇÀ¾©Å−q 549 Direct data access 7.17 9.02 9.02 11.03 Surprisingly, there is no apparent performance improvement. Indeed, the performance for integer sum has gotten slightly worse. Evidently, other operations in the inner loop are forming a bottleneck that limits the performance more than the call to ×−ÎˆÌ−²ˆ−Ä−À−ÅÎ. We will return to this function later (Section 5.11.2) and see why the repeated bounds checking by ²ÇÀ¾©Å−p does not incur a performance penalty. For now, we can view this transformation as one of a series of steps that will ultimately lead to greatly improved performance. 550 Chapter 5 Optimizing Program Performance 5.6 Eliminating Unneeded Memory References The code for ²ÇÀ¾©Å−q accumulates the value being computed by the combining operation at the location designated by the pointer ®−ÍÎ. This attribute can be seen by examining the assembly code generated for the inner loop of the compiled code. We show here the x86-64 code generated for data type ®ÇÏ¾Ä− and with multiplication as the combining operation: Inner loop of combine3. data_t = double, OP = * dest in %rbx, data+i in %rdx, data+length in %rax 1 l‹oux loop: 2 ÌÀÇÌÍ® fcË¾Ógj cÓÀÀn Read product from dest 3 ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn Multiply product by data[i] 4 ÌÀÇÌÍ® cÓÀÀnj fcË¾Óg Store product at dest 5 þ®®Ê bvj cË®Ó Increment data+i 6 ²ÀÉÊ cËþÓj cË®Ó Compare to data+length 7 ÁÅ− l‹ou If !=, goto loop We see in this loop code that the address corresponding to pointer ®−ÍÎ is held in register cË¾Ó. It has also transformed the code to maintain a pointer to the ith data element in register cË®Ó, shown in the annotations as ®þÎþi©. This pointer is in- cremented by 8 on every iteration. The loop termination is detected by comparing this pointer to one stored in register cËþÓ. We can see that the accumulated value is read from and written to memory on each iteration. This reading and writing is wasteful, since the value read from ®−ÍÎ at the beginning of each iteration should simply be the value written at the end of the previous iteration. We can eliminate this needless reading and writing of memory by rewriting the code in the style of ²ÇÀ¾©Å−r in Figure 5.10. We introduce a temporary variable þ²² that is used in the loop to accumulate the computed value. The result is stored at ®−ÍÎ only after the loop has been completed. As the assembly code that follows shows, the compiler can now use register cÓÀÀn to hold the accumulated value. Compared to the loop in ²ÇÀ¾©Å−q, we have reduced the memory operations per iteration from two reads and one write to just a single read. Inner loop of combine4. data_t = double, OP = * acc in %xmm0, data+i in %rdx, data+length in %rax 1 l‹psx loop: 2 ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn Multiply acc by data[i] 3 þ®®Ê bvj cË®Ó Increment data+i 4 ²ÀÉÊ cËþÓj cË®Ó Compare to data+length 5 ÁÅ− l‹ps If !=, goto loop We see a signiﬁcant improvement in program performance, as shown in the following table: Section 5.6 Eliminating Unneeded Memory References 551 1 mh ¡²²ÏÀÏÄþÎ− Ë−ÍÏÄÎ ©Å ÄÇ²þÄ ÌþË©þ¾Ä− hm 2 ÌÇ©® ²ÇÀ¾©Å−rfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 7 ®þÎþˆÎ þ²² { '⁄¥ﬁ¶y 8 9 ðÇË f© { ny © z Ä−Å×Î³y ©iig Õ 10 þ²² { þ²² ﬂ– ®þÎþ‰©`y 11 Û 12 h®−ÍÎ { þ²²y 13 Û Figure 5.10 Accumulating result in temporary. Holding the accumulated value in local variable þ²² (short for “accumulator”) eliminates the need to retrieve it from memory and write back the updated value on every loop iteration. Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−q 549 Direct data access 7.17 9.02 9.02 11.03 ²ÇÀ¾©Å−r 551 Accumulate in temporary 1.27 3.01 3.01 5.01 All of our times improve by factors ranging from 2.2× to 5.7×, with the integer addition case dropping to just 1.27 clock cycles per element. Again, one might think that a compiler should be able to automatically trans- form the ²ÇÀ¾©Å−q code shown in Figure 5.9 to accumulate the value in a register, as it does with the code for ²ÇÀ¾©Å−r shown in Figure 5.10. In fact, however, the two functions can have different behaviors due to memory aliasing. Consider, for example, the case of integer data with multiplication as the operation and 1 as the identity element. Let Ì = [2, 3, 5] be a vector of three elements and consider the following two function calls: ²ÇÀ¾©Å−qfÌj ×−ÎˆÌ−²ˆÍÎþËÎfÌg i pgy ²ÇÀ¾©Å−rfÌj ×−ÎˆÌ−²ˆÍÎþËÎfÌg i pgy That is, we create an alias between the last element of the vector and the destina- tion for storing the result. The two functions would then execute as follows: Function Initial Before loop © =0 © =1 © = 2 Final ²ÇÀ¾©Å−q [2, 3, 5] [2, 3, 1] [2, 3, 2] [2, 3, 6] [2, 3, 36] [2, 3, 36] ²ÇÀ¾©Å−r [2, 3, 5] [2, 3, 5] [2, 3, 5] [2, 3, 5] [2, 3, 5] [2, 3, 30] 552 Chapter 5 Optimizing Program Performance As shown previously, ²ÇÀ¾©Å−q accumulates its result at the destination, which in this case is the ﬁnal vector element. This value is therefore set ﬁrst to 1, then to 2 . 1 = 2, and then to 3 . 2 = 6. On the last iteration, this value is then multiplied by itself to yield a ﬁnal value of 36. For the case of ²ÇÀ¾©Å−r, the vector remains unchanged until the end, when the ﬁnal element is set to the computed result 1 . 2 . 3 . 5 = 30. Of course, our example showing the distinction between ²ÇÀ¾©Å−q and ²ÇÀ¾©Å−r is highly contrived. One could argue that the behavior of ²ÇÀ¾©Å−r more closely matches the intention of the function description. Unfortunately, a compiler cannot make a judgment about the conditions under which a function might be used and what the programmer’s intentions might be. Instead, when given ²ÇÀ¾©Å−q to compile, the conservative approach is to keep reading and writing memory, even though this is less efﬁcient. Practice Problem 5.4 (solution page 610) When we use gcc to compile ²ÇÀ¾©Å−q with command-line option kﬂp, we get code with substantially better CPE performance than with kﬂo: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−q 549 Compiled kﬂo 7.17 9.02 9.02 11.03 ²ÇÀ¾©Å−q 549 Compiled kﬂp 1.60 3.01 3.01 5.01 ²ÇÀ¾©Å−r 551 Accumulate in temporary 1.27 3.01 3.01 5.01 We achieve performance comparable to that for ²ÇÀ¾©Å−r, except for the case of integer sum, but even it improves signiﬁcantly. On examining the assembly code generated by the compiler, we ﬁnd an interesting variant for the inner loop: Inner loop of combine3. data_t = double, OP = *. Compiled -O2 dest in %rbx, data+i in %rdx, data+length in %rax Accumulated product in %xmm0 1 l‹ppx loop: 2 ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn Multiply product by data[i] 3 þ®®Ê bvj cË®Ó Increment data+i 4 ²ÀÉÊ cËþÓj cË®Ó Compare to data+length 5 ÌÀÇÌÍ® cÓÀÀnj fcË¾Óg Store product at dest 6 ÁÅ− l‹pp If !=, goto loop We can compare this to the version created with optimization level 1: Inner loop of combine3. data_t = double, OP = *. Compiled -O1 dest in %rbx, data+i in %rdx, data+length in %rax 1 l‹oux loop: 2 ÌÀÇÌÍ® fcË¾Ógj cÓÀÀn Read product from dest 3 ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn Multiply product by data[i] 4 ÌÀÇÌÍ® cÓÀÀnj fcË¾Óg Store product at dest Section 5.7 Understanding Modern Processors 553 5 þ®®Ê bvj cË®Ó Increment data+i 6 ²ÀÉÊ cËþÓj cË®Ó Compare to data+length 7 ÁÅ− l‹ou If !=, goto loop We see that, besides some reordering of instructions, the only difference is that the more optimized version does not contain the ÌÀÇÌÍ® implementing the read from the location designated by ®−ÍÎ (line 2). A. How does the role of register cÓÀÀn differ in these two loops? B. Will the more optimized version faithfully implement the C code of ²ÇÀk ¾©Å−q, including when there is memory aliasing between ®−ÍÎ and the vec- tor data? C. Either explain why this optimization preserves the desired behavior, or give an example where it would produce different results than the less optimized code. With this ﬁnal transformation, we reached a point where we require just 1.25–5 clock cycles for each element to be computed. This is a considerable improvement over the original 9–11 cycles when we ﬁrst enabled optimization. We would now like to see just what factors are constraining the performance of our code and how we can improve things even further. 5.7 Understanding Modern Processors Up to this point, we have applied optimizations that did not rely on any features of the target machine. They simply reduced the overhead of procedure calls and eliminated some of the critical “optimization blockers” that cause difﬁculties for optimizing compilers. As we seek to push the performance further, we must consider optimizations that exploit the microarchitecture of the processor—that is, the underlying system design by which a processor executes instructions. Getting every last bit of performance requires a detailed analysis of the program as well as code generation tuned for the target processor. Nonetheless, we can apply some basic optimizations that will yield an overall performance improvement on a large class of processors. The detailed performance results we report here may not hold for other machines, but the general principles of operation and optimization apply to a wide variety of machines. To understand ways to improve performance, we require a basic understand- ing of the microarchitectures of modern processors. Due to the large number of transistors that can be integrated onto a single chip, modern microprocessors em- ploy complex hardware that attempts to maximize program performance. One result is that their actual operation is far different from the view that is perceived by looking at machine-level programs. At the code level, it appears as if instruc- tions are executed one at a time, where each instruction involves fetching values from registers or memory, performing an operation, and storing results back to a register or memory location. In the actual processor, a number of instructions 554 Chapter 5 Optimizing Program Performance are evaluated simultaneously, a phenomenon referred to as instruction-level paral- lelism. In some designs, there can be 100 or more instructions “in ﬂight.” Elaborate mechanisms are employed to make sure the behavior of this parallel execution exactly captures the sequential semantic model required by the machine-level program. This is one of the remarkable feats of modern microprocessors: they employ complex and exotic microarchitectures, in which multiple instructions can be executed in parallel, while presenting an operational view of simple sequential instruction execution. Although the detailed design of a modern microprocessor is well beyond the scope of this book, having a general idea of the principles by which they operate sufﬁces to understand how they achieve instruction-level parallelism. We will ﬁnd that two different lower bounds characterize the maximum performance of a program. The latency bound is encountered when a series of operations must be performed in strict sequence, because the result of one operation is required before the next one can begin. This bound can limit program performance when the data dependencies in the code limit the ability of the processor to exploit instruction-level parallelism. The throughput bound characterizes the raw computing capacity of the processor’s functional units. This bound becomes the ultimate limit on program performance. 5.7.1 Overall Operation Figure 5.11 shows a very simpliﬁed view of a modern microprocessor. Our hy- pothetical processor design is based loosely on the structure of recent Intel pro- cessors. These processors are described in the industry as being superscalar, which means they can perform multiple operations on every clock cycle and out of order, meaning that the order in which instructions execute need not correspond to their ordering in the machine-level program. The overall design has two main parts: the instruction control unit (ICU), which is responsible for reading a sequence of instructions from memory and generating from these a set of primitive operations to perform on program data, and the execution unit (EU), which then executes these operations. Compared to the simple in-order pipeline we studied in Chap- ter 4, out-of-order processors require far greater and more complex hardware, but they are better at achieving higher degrees of instruction-level parallelism. The ICU reads the instructions from an instruction cache—a special high- speed memory containing the most recently accessed instructions. In general, the ICU fetches well ahead of the currently executing instructions, so that it has enough time to decode these and send operations down to the EU. One problem, however, is that when a program hits a branch,1 there are two possible directions the program might go. The branch can be taken, with control passing to the branch target. Alternatively, the branch can be not taken, with control passing to the next 1. We use the term “branch” speciﬁcally to refer to conditional jump instructions. Other instructions that can transfer control to multiple destinations, such as procedure return and indirect jumps, provide similar challenges for the processor. Section 5.7 Understanding Modern Processors 555 Instruction control unit Address Instructions Retirement unit Fetch control Instruction decode Operations Instruction cache Prediction OK? Register updates Operation results Addr. Addr. Data Data Data cache Execution unit Functional units StoreLoad Arithmetic operations Arithmetic operationsBranch Register file Figure 5.11 Block diagram of an out-of-order processor. The instruction control unit is responsible for reading instructions from memory and generating a sequence of primitive operations. The execution unit then performs the operations and indicates whether the branches were correctly predicted. instruction in the instruction sequence. Modern processors employ a technique known as branch prediction, in which they guess whether or not a branch will be taken and also predict the target address for the branch. Using a technique known as speculative execution, the processor begins fetching and decoding instructions at where it predicts the branch will go, and even begins executing these operations before it has been determined whether or not the branch prediction was correct. If it later determines that the branch was predicted incorrectly, it resets the state to that at the branch point and begins fetching and executing instructions in the other direction. The block labeled “Fetch control” incorporates branch prediction to perform the task of determining which instructions to fetch. The instruction decoding logic takes the actual program instructions and con- verts them into a set of primitive operations (sometimes referred to as micro- operations). Each of these operations performs some simple computational task such as adding two numbers, reading data from memory, or writing data to mem- ory. For machines with complex instructions, such as x86 processors, an instruction 556 Chapter 5 Optimizing Program Performance can be decoded into multiple operations. The details of how instructions are de- coded into sequences of operations varies between machines, and this information is considered highly proprietary. Fortunately, we can optimize our programs with- out knowing the low-level details of a particular machine implementation. In a typical x86 implementation, an instruction that only operates on registers, such as þ®®Ê cËþÓjcË®Ó is converted into a single operation. On the other hand, an instruction involving one or more memory references, such as þ®®Ê cËþÓjvfcË®Óg yields multiple operations, separating the memory references from the arithmetic operations. This particular instruction would be decoded as three operations: one to load a value from memory into the processor, one to add the loaded value to the value in register c−þÓ, and one to store the result back to memory. The decoding splits instructions to allow a division of labor among a set of dedicated hardware units. These units can then execute the different parts of multiple instructions in parallel. The EU receives operations from the instruction fetch unit. Typically, it can receive a number of them on each clock cycle. These operations are dispatched to a set of functional units that perform the actual operations. These functional units are specialized to handle different types of operations. Reading and writing memory is implemented by the load and store units. The load unit handles operations that read data from the memory into the processor. This unit has an adder to perform address computations. Similarly, the store unit handles operations that write data from the processor to the memory. It also has an adder to perform address computations. As shown in the ﬁgure, the load and store units access memory via a data cache, a high-speed memory containing the most recently accessed data values. With speculative execution, the operations are evaluated, but the ﬁnal results are not stored in the program registers or data memory until the processor can be certain that these instructions should actually have been executed. Branch operations are sent to the EU, not to determine where the branch should go, but rather to determine whether or not they were predicted correctly. If the prediction was incorrect, the EU will discard the results that have been computed beyond the branch point. It will also signal the branch unit that the prediction was incorrect and indicate the correct branch destination. In this case, the branch unit begins fetching at the new location. As we saw in Section 3.6.6, such a misprediction incurs a signiﬁcant cost in performance. It takes a while before the new instructions can be fetched, decoded, and sent to the functional units. Figure 5.11 indicates that the different functional units are designed to per- form different operations. Those labeled as performing “arithmetic operations” are typically specialized to perform different combinations of integer and ﬂoating- point operations. As the number of transistors that can be integrated onto a single Section 5.7 Understanding Modern Processors 557 microprocessor chip has grown over time, successive models of microprocessors have increased the total number of functional units, the combinations of opera- tions each unit can perform, and the performance of each of these units. The arith- metic units are intentionally designed to be able to perform a variety of different operations, since the required operations vary widely across different programs. For example, some programs might involve many integer operations, while others require many ﬂoating-point operations. If one functional unit were specialized to perform integer operations while another could only perform ﬂoating-point oper- ations, then none of these programs would get the full beneﬁt of having multiple functional units. For example, our Intel Core i7 Haswell reference machine has eight functional units, numbered 0–7. Here is a partial list of each one’s capabilities: 0. Integer arithmetic, ﬂoating-point multiplication, integer and ﬂoating-point division, branches 1. Integer arithmetic, ﬂoating-point addition, integer multiplication, ﬂoating- point multiplication 2. Load, address computation 3. Load, address computation 4. Store 5. Integer arithmetic 6. Integer arithmetic, branches 7. Store address computation In the above list, “integer arithmetic” refers to basic operations, such as addition, bitwise operations, and shifting. Multiplication and division require more special- ized resources. We see that a store operation requires two functional units—one to compute the store address and one to actually store the data. We will discuss the mechanics of store (and load) operations in Section 5.12. We can see that this combination of functional units has the potential to perform multiple operations of the same type simultaneously. It has four units capable of performing integer operations, two that can perform load operations, and two that can perform ﬂoating-point multiplication. We will later see the impact these resources have on the maximum performance our programs can achieve. Within the ICU, the retirement unit keeps track of the ongoing processing and makes sure that it obeys the sequential semantics of the machine-level program. Our ﬁgure shows a register ﬁle containing the integer, ﬂoating-point, and, more recently, SSE and AVX registers as part of the retirement unit, because this unit controls the updating of these registers. As an instruction is decoded, information about it is placed into a ﬁrst-in, ﬁrst-out queue. This information remains in the queue until one of two outcomes occurs. First, once the operations for the instruction have completed and any branch points leading to this instruction are conﬁrmed as having been correctly predicted, the instruction can be retired, with any updates to the program registers being made. If some branch point leading to this instruction was mispredicted, on the other hand, the instruction will be 558 Chapter 5 Optimizing Program Performance Aside The history of out-of-order processing Out-of-order processing was ﬁrst implemented in the Control Data Corporation 6600 processor in 1964. Instructions were processed by 10 different functional units, each of which could be operated independently. In its day, this machine, with a clock rate of 10 MHz, was considered the premium machine for scientiﬁc computing. IBM ﬁrst implemented out-of-order processing with the IBM 360/91 processor in 1966, but just to execute the ﬂoating-point instructions. For around 25 years, out-of-order processing was considered an exotic technology, found only in machines striving for the highest possible performance, until IBM reintroduced it in the RS/6000 line of workstations in 1990. This design became the basis for the IBM/Motorola PowerPC line, with the model 601, introduced in 1993, becoming the ﬁrst single- chip microprocessor to use out-of-order processing. Intel introduced out-of-order processing with its PentiumPro model in 1995, with an underlying microarchitecture similar to that of our reference machine. ﬂushed, discarding any results that may have been computed. By this means, mispredictions will not alter the program state. As we have described, any updates to the program registers occur only as instructions are being retired, and this takes place only after the processor can be certain that any branches leading to this instruction have been correctly predicted. To expedite the communication of results from one instruction to another, much of this information is exchanged among the execution units, shown in the ﬁgure as “Operation results.” As the arrows in the ﬁgure show, the execution units can send results directly to each other. This is a more elaborate form of the data-forwarding techniques we incorporated into our simple processor design in Section 4.5.5. The most common mechanism for controlling the communication of operands among the execution units is called register renaming. When an instruction that updates register r is decoded, a tag t is generated giving a unique identiﬁer to the result of the operation. An entry (r, t) is added to a table maintaining the association between program register r and tag t for an operation that will update this register. When a subsequent instruction using register r as an operand is decoded, the operation sent to the execution unit will contain t as the source for the operand value. When some execution unit completes the ﬁrst operation, it generates a result (v, t), indicating that the operation with tag t produced value v. Any operation waiting for t as a source will then use v as the source value, a form of data forwarding. By this mechanism, values can be forwarded directly from one operation to another, rather than being written to and read from the register ﬁle, enabling the second operation to begin as soon as the ﬁrst has completed. The renaming table only contains entries for registers having pending write operations. When a decoded instruction requires a register r, and there is no tag associated with this register, the operand is retrieved directly from the register ﬁle. With register renaming, an entire sequence of operations can be performed speculatively, even though the registers are updated only after the processor is certain of the branch outcomes. Section 5.7 Understanding Modern Processors 559 Integer Floating point Operation Latency Issue Capacity Latency Issue Capacity Addition 1 1 4 3 1 1 Multiplication 3 1 1 5 1 2 Division 3–30 3–30 1 3–15 3–15 1 Figure 5.12 Latency, issue time, and capacity characteristics of reference machine operations. Latency indicates the total number of clock cycles required to perform the actual operations, while issue time indicates the minimum number of cycles between two independent operations. The capacity indicates how many of these operations can be issued simultaneously. The times for division depend on the data values. 5.7.2 Functional Unit Performance Figure 5.12 documents the performance of some of the arithmetic operations for our Intel Core i7 Haswell reference machine, determined by both measurements and by reference to Intel literature [49]. These timings are typical for other proces- sors as well. Each operation is characterized by its latency, meaning the total time required to perform the operation, the issue time, meaning the minimum num- ber of clock cycles between two independent operations of the same type, and the capacity, indicating the number of functional units capable of performing that operation. We see that the latencies increase in going from integer to ﬂoating-point operations. We see also that the addition and multiplication operations all have issue times of 1, meaning that on each clock cycle, the processor can start a new one of these operations. This short issue time is achieved through the use of pipelining. A pipelined function unit is implemented as a series of stages, each of which performs part of the operation. For example, a typical ﬂoating- point adder contains three stages (and hence the three-cycle latency): one to process the exponent values, one to add the fractions, and one to round the result. The arithmetic operations can proceed through the stages in close succession rather than waiting for one operation to complete before the next begins. This capability can be exploited only if there are successive, logically independent operations to be performed. Functional units with issue times of 1 cycle are said to be fully pipelined: they can start a new operation every clock cycle. Operations with capacity greater than 1 arise due to the capabilities of the multiple functional units, as was described earlier for the reference machine. We see also that the divider (used for integer and ﬂoating-point division, as well as ﬂoating-point square root) is not pipelined—its issue time equals its latency. What this means is that the divider must perform a complete division before it can begin a new one. We also see that the latencies and issue times for division are given as ranges, because some combinations of dividend and divisor require more steps than others. The long latency and issue times of division make it a comparatively costly operation. 560 Chapter 5 Optimizing Program Performance A more common way of expressing issue time is to specify the maximum throughput of the unit, deﬁned as the reciprocal of the issue time. A fully pipelined functional unit has a maximum throughput of 1 operation per clock cycle, while units with higher issue times have lower maximum throughput. Having multiple functional units can increase throughput even further. For an operation with capacity C and issue time I , the processor can potentially achieve a throughput of C/I operations per clock cycle. For example, our reference machine is capable of performing ﬂoating-point multiplication operations at a rate of 2 per clock cycle. We will see how this capability can be exploited to increase program performance. Circuit designers can create functional units with wide ranges of performance characteristics. Creating a unit with short latency or with pipelining requires more hardware, especially for more complex functions such as multiplication and ﬂoating-point operations. Since there is only a limited amount of space for these units on the microprocessor chip, CPU designers must carefully balance the number of functional units and their individual performance to achieve op- timal overall performance. They evaluate many different benchmark programs and dedicate the most resources to the most critical operations. As Figure 5.12 indicates, integer multiplication and ﬂoating-point multiplication and addition were considered important operations in the design of the Core i7 Haswell pro- cessor, even though a signiﬁcant amount of hardware is required to achieve the low latencies and high degree of pipelining shown. On the other hand, division is relatively infrequent and difﬁcult to implement with either short latency or full pipelining. The latencies, issue times, and capacities of these arithmetic operations can affect the performance of our combining functions. We can express these effects in terms of two fundamental bounds on the CPE values: Integer Floating point Bound ihi h Latency 1.00 3.00 3.00 5.00 Throughput 0.50 1.00 1.00 0.50 The latency bound gives a minimum value for the CPE for any function that must perform the combining operation in a strict sequence. The throughput bound gives a minimum bound for the CPE based on the maximum rate at which the functional units can produce results. For example, since there is only one integer multiplier, and it has an issue time of 1 clock cycle, the processor cannot possibly sustain a rate of more than 1 multiplication per clock cycle. On the other hand, with four functional units capable of performing integer addition, the processor can potentially sustain a rate of 4 operations per cycle. Unfortunately, the need to read elements from memory creates an additional throughput bound. The two load units limit the processor to reading at most 2 data values per clock cycle, yielding a throughput bound of 0.50. We will demonstrate the effect of both the latency and throughput bounds with different versions of the combining functions. Section 5.7 Understanding Modern Processors 561 5.7.3 An Abstract Model of Processor Operation As a tool for analyzing the performance of a machine-level program executing on a modern processor, we will use a data-ﬂow representation of programs, a graphical notation showing how the data dependencies between the different operations constrain the order in which they are executed. These constraints then lead to critical paths in the graph, putting a lower bound on the number of clock cycles required to execute a set of machine instructions. Before proceeding with the technical details, it is instructive to examine the CPE measurements obtained for function ²ÇÀ¾©Å−r, our fastest code up to this point: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−r 551 Accumulate in temporary 1.27 3.01 3.01 5.01 Latency bound 1.00 3.00 3.00 5.00 Throughput bound 0.50 1.00 1.00 0.50 We can see that these measurements match the latency bound for the proces- sor, except for the case of integer addition. This is not a coincidence—it indicates that the performance of these functions is dictated by the latency of the sum or product computation being performed. Computing the product or sum of n elements requires around L . n + K clock cycles, where L is the latency of the combining operation and K represents the overhead of calling the function and initiating and terminating the loop. The CPE is therefore equal to the latency bound L. From Machine-Level Code to Data-Flow Graphs Our data-ﬂow representation of programs is informal. We use it as a way to visualize how the data dependencies in a program dictate its performance. We present the data-ﬂow notation by working with ²ÇÀ¾©Å−r (Figure 5.10) as an example. We focus just on the computation performed by the loop, since this is the dominating factor in performance for large vectors. We consider the case of data type ®ÇÏ¾Ä− with multiplication as the combining operation. Other combinations of data type and operation yield similar code. The compiled code for this loop consists of four instructions, with registers cË®Ó holding a pointer to the ith element of array ®þÎþ, cËþÓ holding a pointer to the end of the array, and cÓÀÀn holding the accumulated value þ²². Inner loop of combine4. data_t = double, OP = * acc in %xmm0, data+i in %rdx, data+length in %rax 1 l‹psx loop: 2 ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn Multiply acc by data[i] 3 þ®®Ê bvj cË®Ó Increment data+i 4 ²ÀÉÊ cËþÓj cË®Ó Compare to data+length 5 ÁÅ− l‹ps If !=, goto loop 562 Chapter 5 Optimizing Program Performance %rax %rdx %xmm0 vmulsd (%rdx), %xmm0, %xmm0 addq $8,%rdx cmpq %rax,%rdx jne loop %rax %rdx %xmm0 load mul add cmp jne Figure 5.13 Graphical representation of inner-loop code for ²ÇÀ¾©Å−r. Instructions are dynamically translated into one or two operations, each of which receives values from other operations or from registers and produces values for other operations and for registers. We show the target of the ﬁnal instruction as the label ÄÇÇÉ. It jumps to the ﬁrst instruction shown. As Figure 5.13 indicates, with our hypothetical processor design, the four in- structions are expanded by the instruction decoder into a series of ﬁve operations, with the initial multiplication instruction being expanded into a load operation to read the source operand from memory, and a mul operation to perform the multiplication. As a step toward generating a data-ﬂow graph representation of the program, the boxes and lines along the left-hand side of Figure 5.13 show how the registers are used and updated by the different operations, with the boxes along the top representing the register values at the beginning of the loop, and those along the bottom representing the values at the end. For example, register cËþÓ is only used as a source value by the cmp operation, and so the register has the same value at the end of the loop as at the beginning. Register cË®Ó, on the other hand, is both used and updated within the loop. Its initial value is used by the load and add operations; its new value is generated by the add operation, which is then used by the cmp operation. Register cÓÀÀn is also updated within the loop by the mul operation, which ﬁrst uses the initial value as a source value. Some of the operations in Figure 5.13 produce values that do not correspond to registers. We show these as arcs between operations on the right-hand side. The load operation reads a value from memory and passes it directly to the mul operation. Since these two operations arise from decoding a single ÌÀÏÄÍ® instruction, there is no register associated with the intermediate value passing between them. The cmp operation updates the condition codes, and these are then tested by the jne operation. For a code segment forming a loop, we can classify the registers that are accessed into four categories: Section 5.7 Understanding Modern Processors 563 Figure 5.14 Abstracting ²ÇÀ¾©Å−r operations as a data-ﬂow graph. We rearrange the operators of Figure 5.13 to more clearly show the data dependencies (a), and then further show only those operations that use values from one iteration to produce new values for the next (b). %rax %rdx%xmm0 %rdx%xmm0 data[i ] load (a) (b) mul add cmp jne %rdx%xmm0 %rdx%xmm0 load mul add Read-only. These are used as source values, either as data or to compute mem- ory addresses, but they are not modiﬁed within the loop. The only read- only register for the loop in ²ÇÀ¾©Å−r is cËþÓ. Write-only. These are used as the destinations of data-movement operations. There are no such registers in this loop. Local. These are updated and used within the loop, but there is no dependency from one iteration to another. The condition code registers are examples for this loop: they are updated by the cmp operation and used by the jne operation, but this dependency is contained within individual iterations. Loop. These are used both as source values and as destinations for the loop, with the value generated in one iteration being used in another. We can see that cË®Ó and cÓÀÀn are loop registers for ²ÇÀ¾©Å−r, corresponding to program values ®þÎþi© and þ²². As we will see, the chains of operations between loop registers determine the performance-limiting data dependencies. Figure 5.14 shows further reﬁnements of the graphical representation of Fig- ure 5.13, with a goal of showing only those operations and data dependencies that affect the program execution time. We see in Figure 5.14(a) that we rearranged the operators to show more clearly the ﬂow of data from the source registers at the top (both read-only and loop registers) and to the destination registers at the bottom (both write-only and loop registers). In Figure 5.14(a), we also color operators white if they are not part of some chain of dependencies between loop registers. For this example, the comparison (cmp) and branch (jne) operations do not directly affect the ﬂow of data in the program. We assume that the instruction control unit predicts that branch will be taken, and hence the program will continue looping. The purpose of the compare and branch operations is to test the branch condition and notify the ICU if it is 564 Chapter 5 Optimizing Program Performance not taken. We assume this checking can be done quickly enough that it does not slow down the processor. In Figure 5.14(b), we have eliminated the operators that were colored white on the left, and we have retained only the loop registers. What we have left is an abstract template showing the data dependencies that form among loop registers due to one iteration of the loop. We can see in this diagram that there are two data dependencies from one iteration to the next. Along one side, we see the dependencies between successive values of program value þ²², stored in register cÓÀÀn. The loop computes a new value for þ²² by multiplying the old value by a data element, generated by the load operation. Along the other side, we see the dependencies between successive values of the pointer to the ith data element. On each iteration, the old value is used as the address for the load operation, and it is also incremented by the add operation to compute its new value. Figure 5.15 shows the data-ﬂow representation of n iterations by the inner loop of function ²ÇÀ¾©Å−r. This graph was obtained by simply replicating the template shown in Figure 5.14(b) n times. We can see that the program has two chains of data Figure 5.15 Data-ﬂow representation of computation by n iterations of the inner loop of ²ÇÀ¾©Å−r. The sequence of multiplication operations forms a critical path that limits program performance. data[0] load Critical path mul add data[1] load mul add data[n-2] load mul add data[n-1] load mul add Section 5.7 Understanding Modern Processors 565 dependencies, corresponding to the updating of program values þ²² and ®þÎþi© with operations mul and add, respectively. Given that ﬂoating-point multiplication has a latency of 5 cycles, while integer addition has a latency of 1 cycle, we can see that the chain on the left will form a critical path, requiring 5n cycles to execute. The chain on the right would require only n cycles to execute, and so it does not limit the program performance. Figure 5.15 demonstrates why we achieved a CPE equal to the latency bound of 5 cycles for ²ÇÀ¾©Å−r, when performing ﬂoating-point multiplication. When ex- ecuting the function, the ﬂoating-point multiplier becomes the limiting resource. The other operations required during the loop—manipulating and testing pointer value ®þÎþi© and reading data from memory—proceed in parallel with the mul- tiplication. As each successive value of þ²² is computed, it is fed back around to compute the next value, but this will not occur until 5 cycles later. The ﬂow for other combinations of data type and operation are identical to those shown in Figure 5.15, but with a different data operation forming the chain of data dependencies shown on the left. For all of the cases where the operation has a latency L greater than 1, we see that the measured CPE is simply L, indicating that this chain forms the performance-limiting critical path. Other Performance Factors For the case of integer addition, on the other hand, our measurements of ²ÇÀ¾©Å−r show a CPE of 1.27, slower than the CPE of 1.00 we would predict based on the chains of dependencies formed along either the left- or the right-hand side of the graph of Figure 5.15. This illustrates the principle that the critical paths in a data- ﬂow representation provide only a lower bound on how many cycles a program will require. Other factors can also limit performance, including the total number of functional units available and the number of data values that can be passed among the functional units on any given step. For the case of integer addition as the combining operation, the data operation is sufﬁciently fast that the rest of the operations cannot supply data fast enough. Determining exactly why the program requires 1.27 cycles per element would require a much more detailed knowledge of the hardware design than is publicly available. To summarize our performance analysis of ²ÇÀ¾©Å−r: our abstract data-ﬂow representation of program operation showed that ²ÇÀ¾©Å−r has a critical path of length L . n caused by the successive updating of program value þ²², and this path limits the CPE to at least L. This is indeed the CPE we measure for all cases except integer addition, which has a measured CPE of 1.27 rather than the CPE of 1.00 we would expect from the critical path length. It may seem that the latency bound forms a fundamental limit on how fast our combining operations can be performed. Our next task will be to restructure the operations to enhance instruction-level parallelism. We want to transform the program in such a way that our only limitation becomes the throughput bound, yielding CPEs below or close to 1.00. 566 Chapter 5 Optimizing Program Performance Practice Problem 5.5 (solution page 611) Suppose we wish to write a function to evaluate a polynomial, where a polynomial of degree n is deﬁned to have a set of coefﬁcients a0,a1,a2,. . .,an. For a value x, we evaluate the polynomial by computing a0 + a1x + a2x2 + ... + anxn (5.2) This evaluation can be implemented by the following function, having as argu- ments an array of coefﬁcients þ, a value Ó, and the polynomial degree ®−×Ë−− (the value n in Equation 5.2). In this function, we compute both the successive terms of the equation and the successive powers of x within a single loop: 1 ®ÇÏ¾Ä− ÉÇÄÔf®ÇÏ¾Ä− þ‰`j ®ÇÏ¾Ä− Ój ÄÇÅ× ®−×Ë−−g 2 Õ 3 ÄÇÅ× ©y 4 ®ÇÏ¾Ä− Ë−ÍÏÄÎ { þ‰n`y 5 ®ÇÏ¾Ä− ÓÉÑË { Óy mh ¥ÊÏþÄÍ Ó´© þÎ ÍÎþËÎ Çð ÄÇÇÉ hm 6 ðÇË f© { oy © z{ ®−×Ë−−y ©iig Õ 7 Ë−ÍÏÄÎ i{ þ‰©` h ÓÉÑËy 8 ÓÉÑË{Óh ÓÉÑËy 9 Û 10 Ë−ÎÏËÅ Ë−ÍÏÄÎy 11 Û A. For degree n, how many additions and how many multiplications does this code perform? B. On our reference machine, with arithmetic operations having the latencies shown in Figure 5.12, we measure the CPE for this function to be 5.00. Ex- plain how this CPE arises based on the data dependencies formed between iterations due to the operations implementing lines 7–8 of the function. Practice Problem 5.6 (solution page 611) Let us continue exploring ways to evaluate polynomials, as described in Practice Problem 5.5. We can reduce the number of multiplications in evaluating a polyno- mial by applying Horner’s method, named after British mathematician William G. Horner (1786–1837). The idea is to repeatedly factor out the powers of x to get the following evaluation: a0 + x(a1 + x(a2 + ... + x(an−1 + xan) ...)) (5.3) Using Horner’s method, we can implement polynomial evaluation using the following code: 1 mh ¡ÉÉÄÔ ¤ÇËÅ−Ë’Í À−Î³Ç® hm 2 ®ÇÏ¾Ä− ÉÇÄÔ³f®ÇÏ¾Ä− þ‰`j ®ÇÏ¾Ä− Ój ÄÇÅ× ®−×Ë−−g 3 Õ Section 5.8 Loop Unrolling 567 4 ÄÇÅ× ©y 5 ®ÇÏ¾Ä− Ë−ÍÏÄÎ { þ‰®−×Ë−−`y 6 ðÇË f© { ®−×Ë−−koy © |{ ny ©kkg 7 Ë−ÍÏÄÎ { þ‰©` i ÓhË−ÍÏÄÎy 8 Ë−ÎÏËÅ Ë−ÍÏÄÎy 9 Û A. For degree n, how many additions and how many multiplications does this code perform? B. On our reference machine, with the arithmetic operations having the laten- cies shown in Figure 5.12, we measure the CPE for this function to be 8.00. Explain how this CPE arises based on the data dependencies formed be- tween iterations due to the operations implementing line 7 of the function. C. Explain how the function shown in Practice Problem 5.5 can run faster, even though it requires more operations. 5.8 Loop Unrolling Loop unrolling is a program transformation that reduces the number of iterations for a loop by increasing the number of elements computed on each iteration. We saw an example of this with the function ÉÍÏÀp (Figure 5.1), where each iteration computes two elements of the preﬁx sum, thereby halving the total number of iterations required. Loop unrolling can improve performance in two ways. First, it reduces the number of operations that do not contribute directly to the program result, such as loop indexing and conditional branching. Second, it exposes ways in which we can further transform the code to reduce the number of operations in the critical paths of the overall computation. In this section, we will examine simple loop unrolling, without any further transformations. Figure 5.16 shows a version of our combining code using what we will refer to as “2 × 1 loop unrolling.” The ﬁrst loop steps through the array two elements at a time. That is, the loop index © is incremented by 2 on each iteration, and the combining operation is applied to array elements i and i + 1 in a single iteration. In general, the vector length will not be a multiple of 2. We want our code to work correctly for arbitrary vector lengths. We account for this requirement in two ways. First, we make sure the ﬁrst loop does not overrun the array bounds. For a vector of length n, we set the loop limit to be n − 1. We are then assured that the loop will only be executed when the loop index i satisﬁes i< n − 1, and hence the maximum array index i + 1 will satisfy i + 1 <(n − 1) + 1 = n. We can generalize this idea to unroll a loop by any factor k, yielding k × 1 loop unrolling. To do so, we set the upper limit to be n − k + 1 and within the loop apply the combining operation to elements i through i + k − 1. Loop index © is incremented by k in each iteration. The maximum array index i + k − 1 will then be less than n. We include the second loop to step through the ﬁnal few elements of the vector one at a time. The body of this loop will be executed between 0 and k − 1 times. For k = 2, we could use a simple conditional statement 568 Chapter 5 Optimizing Program Performance 1 mhpÓo ÄÇÇÉ ÏÅËÇÄÄ©Å× hm 2 ÌÇ©® ²ÇÀ¾©Å−sfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ÄÇÅ× Ä©À©Î { Ä−Å×Î³koy 7 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 8 ®þÎþˆÎ þ²² { '⁄¥ﬁ¶y 9 10 mh £ÇÀ¾©Å− p −Ä−À−ÅÎÍ þÎ þ Î©À− hm 11 ðÇË f© { ny © z Ä©À©Îy ©i{pg Õ 12 þ²² { fþ²² ﬂ– ®þÎþ‰©`g ﬂ– ®þÎþ‰©io`y 13 Û 14 15 mh ƒ©Å©Í³ þÅÔ Ë−Àþ©Å©Å× −Ä−À−ÅÎÍ hm 16 ðÇË fy © z Ä−Å×Î³y ©iig Õ 17 þ²² { þ²² ﬂ– ®þÎþ‰©`y 18 Û 19 h®−ÍÎ { þ²²y 20 Û Figure 5.16 Applying 2 × 1 loop unrolling. This transformation can reduce the effect of loop overhead. to optionally add a ﬁnal iteration, as we did with the function ÉÍÏÀp (Figure 5.1). For k> 2, the ﬁnishing cases are better expressed with a loop, and so we adopt this programming convention for k = 2 as well. We refer to this transformation as “k × 1 loop unrolling,” since we unroll by a factor of k but accumulate values in a single variable þ²². Practice Problem 5.7 (solution page 611) Modify the code for ²ÇÀ¾©Å−s to unroll the loop by a factor k = 5. When we measure the performance of unrolled code for unrolling factors k = 2(²ÇÀ¾©Å−s) and k = 3, we get the following results: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−r 551 No unrolling 1.27 3.01 3.01 5.01 ²ÇÀ¾©Å−s 568 2 × 1 unrolling 1.01 3.01 3.01 5.01 3 × 1 unrolling 1.01 3.01 3.01 5.01 Latency bound 1.00 3.00 3.00 5.00 Throughput bound 0.50 1.00 1.00 0.50 Section 5.8 Loop Unrolling 569 Figure 5.17 CPE performance for different degrees of k × 1 loop unrolling. Only integer addition improves with this transformation. 6 5 4 3 2 1 0 1 2 3 4 Unrolling factor kCPE 5 6 double * double + long * long + We see that the CPE for integer addition improves, achieving the latency bound of 1.00. This result can be attributed to the beneﬁts of reducing loop overhead operations. By reducing the number of overhead operations relative to the number of additions required to compute the vector sum, we can reach the point where the 1-cycle latency of integer addition becomes the performance- limiting factor. On the other hand, none of the other cases improve—they are already at their latency bounds. Figure 5.17 shows CPE measurements when unrolling the loop by up to a factor of 10. We see that the trends we observed for unrolling by 2 and 3 continue—none go below their latency bounds. To understand why k × 1 unrolling cannot improve performance beyond the latency bound, let us examine the machine-level code for the inner loop of ²ÇÀ¾©Å−s, having k = 2. The following code gets generated when type ®þÎþˆÎ is ®ÇÏ¾Ä−, and the operation is multiplication: Inner loop of combine5. data_t = double, OP = * i in %rdx, data %rax, limit in %rbx, acc in %xmm0 1 l‹qsx loop: 2 ÌÀÏÄÍ® fcËþÓjcË®Ójvgj cÓÀÀnj cÓÀÀn Multiply acc by data[i] 3 ÌÀÏÄÍ® vfcËþÓjcË®Ójvgj cÓÀÀnj cÓÀÀn Multiply acc by data[i+1] 4 þ®®Ê bpj cË®Ó Increment i by 2 5 ²ÀÉÊ cË®Ój cË¾É Compare to limit:i 6 Á× l‹qs If >, goto loop We can see that gcc uses a more direct translation of the array referencing seen in the C code, compared to the pointer-based code generated for ²ÇÀ¾©Å−r.2 Loop index © is held in register cË®Ó, and the address of ®þÎþ is held in register cËþÓ. As before, the accumulated value þ²² is held in vector register cÓÀÀn.The loop unrolling leads to two ÌÀÏÄÍ® instructions—one to add ®þÎþ‰©` to þ²², and 2. The gcc optimizer operates by generating multiple variants of a function and then choosing one that it predicts will yield the best performance and smallest code size. As a consequence, small changes in the source code can yield widely varying forms of machine code. We have found that the choice of pointer-based or array-based code has no impact on the performance of programs running on our reference machine. 570 Chapter 5 Optimizing Program Performance %rax %rbp %rdx %xmm0 vmulsd (%rax,%rdx,8), %xmm0, %xmm0 vmulsd 8(%rax,%rdx,8), %xmm0, %xmm0 addq $2,%rdx cmpq %rdx,%rbp jg loop %rax %rbp %rdx %xmm0 load mul load mul add cmp jg Figure 5.18 Graphical representation of inner-loop code for ²ÇÀ¾©Å−s. Each iteration has two ÌÀÏÄÍ® instructions, each of which is translated into a load and a mul operation. Figure 5.19 Abstracting ²ÇÀ¾©Å−s operations as a data- ﬂow graph. We rearrange, simplify, and abstract the representation of Figure 5.18 to show the data dependencies between successive iterations (a). We see that each iteration must perform two multiplications in sequence (b). %rax %rbp %rdx%xmm0 %rdx%xmm0 data[i ] data[i +1] load load mul mul add cmp (a) (b) jg %rdx%xmm0 %rdx%xmm0 load mul add load mul the second to add ®þÎþ‰©io` to þ²². Figure 5.18 shows a graphical representation of this code. The ÌÀÏÄÍ® instructions each get translated into two operations: one to load an array element from memory and one to multiply this value by the accumulated value. We see here that register cÓÀÀn gets read and written twice in each execution of the loop. We can rearrange, simplify, and abstract this graph, following the process shown in Figure 5.19(a), to obtain the template shown in Figure 5.19(b). We then replicate this template n/2 times to show the computation for a vector of length n, obtaining the data-ﬂow representation Section 5.8 Loop Unrolling 571 Figure 5.20 Data-ﬂow representation of ²ÇÀ¾©Å−s operating on a vector of length n. Even though the loop has been unrolled by a factor of 2, there are still n mul operations along the critical path. data[0] load mul data[1] load mul add data[2] load mul data[3] load mul add data[n-2] load mul data[n-1] load mul add Critical path shown in Figure 5.20. We see here that there is still a critical path of n mul operations in this graph—there are half as many iterations, but each iteration has two multiplication operations in sequence. Since the critical path was the limiting factor for the performance of the code without loop unrolling, it remains so with k × 1 loop unrolling. Aside Getting the compiler to unroll loops Loop unrolling can easily be performed by a compiler. Many compilers do this as part of their collection of optimizations. gcc will perform some forms of loop unrolling when invoked with optimization level 3 or higher. 572 Chapter 5 Optimizing Program Performance 5.9 Enhancing Parallelism At this point, our functions have hit the bounds imposed by the latencies of the arithmetic units. As we have noted, however, the functional units performing ad- dition and multiplication are all fully pipelined, meaning that they can start new operations every clock cycle, and some of the operations can be performed by multiple functional units. The hardware has the potential to perform multiplica- tions and additions at a much higher rate, but our code cannot take advantage of this capability, even with loop unrolling, since we are accumulating the value as a single variable þ²². We cannot compute a new value for þ²² until the preceding computation has completed. Even though the functional unit computing a new value for þ²² can start a new operation every clock cycle, it will only start one every L cycles, where L is the latency of the combining operation. We will now investigate ways to break this sequential dependency and get performance better than the latency bound. 5.9.1 Multiple Accumulators For a combining operation that is associative and commutative, such as integer addition or multiplication, we can improve performance by splitting the set of combining operations into two or more parts and combining the results at the end. For example, let Pn denote the product of elements a0,a1,...,an−1: Pn = n−1∏ i=0 ai Assuming n is even, we can also write this as Pn = PEn × POn, where PEn is the product of the elements with even indices, and POn is the product of the elements with odd indices: PEn = n/2−1∏ i=0 a2i POn = n/2−1∏ i=0 a2i+1 Figure 5.21 shows code that uses this method. It uses both two-way loop unrolling, to combine more elements per iteration, and two-way parallelism, accumulating elements with even indices in variable þ²²n and elements with odd indices in variable þ²²o. We therefore refer to this as “2 × 2 loop unrolling.” As before, we include a second loop to accumulate any remaining array elements for the case where the vector length is not a multiple of 2. We then apply the combining operation to þ²²n and þ²²o to compute the ﬁnal result. Comparing loop unrolling alone to loop unrolling with two-way parallelism, we obtain the following performance: Section 5.9 Enhancing Parallelism 573 1 mhpÓp ÄÇÇÉ ÏÅËÇÄÄ©Å× hm 2 ÌÇ©® ²ÇÀ¾©Å−tfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ÄÇÅ× Ä©À©Î { Ä−Å×Î³koy 7 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 8 ®þÎþˆÎ þ²²n { '⁄¥ﬁ¶y 9 ®þÎþˆÎ þ²²o { '⁄¥ﬁ¶y 10 11 mh £ÇÀ¾©Å− p −Ä−À−ÅÎÍ þÎ þ Î©À− hm 12 ðÇË f© { ny © z Ä©À©Îy ©i{pg Õ 13 þ²²n { þ²²n ﬂ– ®þÎþ‰©`y 14 þ²²o { þ²²o ﬂ– ®þÎþ‰©io`y 15 Û 16 17 mh ƒ©Å©Í³ þÅÔ Ë−Àþ©Å©Å× −Ä−À−ÅÎÍ hm 18 ðÇË fy © z Ä−Å×Î³y ©iig Õ 19 þ²²n { þ²²n ﬂ– ®þÎþ‰©`y 20 Û 21 h®−ÍÎ { þ²²n ﬂ– þ²²oy 22 Û Figure 5.21 Applying 2 × 2 loop unrolling. By maintaining multiple accumulators, this approach can make better use of the multiple functional units and their pipelining capabilities. Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−r 551 Accumulate in temporary 1.27 3.01 3.01 5.01 ²ÇÀ¾©Å−s 568 2 × 1 unrolling 1.01 3.01 3.01 5.01 ²ÇÀ¾©Å−t 573 2 × 2 unrolling 0.81 1.51 1.51 2.51 Latency bound 1.00 3.00 3.00 5.00 Throughput bound 0.50 1.00 1.00 0.50 We see that we have improved the performance for all cases, with integer product, ﬂoating-point addition, and ﬂoating-point multiplication improving by a factor of around 2, and integer addition improving somewhat as well. Most signiﬁcantly, we have broken through the barrier imposed by the latency bound. The processor no longer needs to delay the start of one sum or product operation until the previous one has completed. To understand the performance of ²ÇÀ¾©Å−t, we start with the code and operation sequence shown in Figure 5.22. We can derive a template showing the 574 Chapter 5 Optimizing Program Performance %rax %rbp %rdx %xmm0 vmulsd (%rax,%rdx,8), %xmm0, %xmm0 vmulsd 8(%rax,%rdx,8), %xmm1, %xmm1 addq $2,%rdx cmpq %rdx,%rbp jg loop %rax %rbp %rdx %xmm0 %xmm1 %xmm1 load mul load mul add cmp jg Figure 5.22 Graphical representation of inner-loop code for ²ÇÀ¾©Å−t. Each iteration has two ÌÀÏÄÍ® instructions, each of which is translated into a load and a mul operation. %rax %rbp %rdx%xmm0 %xmm1 %xmm1 %rdx%xmm0 data[i ] data[i +1] load load (a) (b) add cmp jg %rdx%xmm0 %rdx%xmm0 load mul %xmm1 %xmm1 load mul add mul mul Figure 5.23 Abstracting ²ÇÀ¾©Å−t operations as a data-ﬂow graph. We rearrange, simplify, and abstract the representation of Figure 5.22 to show the data dependencies between successive iterations (a). We see that there is no dependency between the two mul operations (b). data dependencies between iterations through the process shown in Figure 5.23. As with ²ÇÀ¾©Å−s, the inner loop contains two ÌÀÏÄÍ® operations, but these instructions translate into mul operations that read and write separate registers, with no data dependency between them (Figure 5.23(b)). We then replicate this template n/2 times (Figure 5.24), modeling the execution of the function on a vector of length n. We see that we now have two critical paths, one corresponding to computing the product of even-numbered elements (program value þ²²n) and Section 5.9 Enhancing Parallelism 575 Figure 5.24 Data-ﬂow representation of ²ÇÀ¾©Å−t operating on a vector of length n. We now have two critical paths, each containing n/2 operations. data[0] data[1] load mul load mul add data[2] data[3] load mul load mul add data[n-2] data[n-1] load mul load mul add Critical paths one for the odd-numbered elements (program value þ²²o). Each of these critical paths contains only n/2 operations, thus leading to a CPE of around 5.00/2 = 2.50. A similar analysis explains our observed CPE of around L/2 for operations with latency L for the different combinations of data type and combining operation. Operationally, the programs are exploiting the capabilities of the functional units to increase their utilization by a factor of 2. The only exception is for integer addition. We have reduced the CPE to below 1.0, but there is still too much loop overhead to achieve the theoretical limit of 0.50. We can generalize the multiple accumulator transformation to unroll the loop by a factor of k and accumulate k values in parallel, yielding k × k loop unrolling. Figure 5.25 demonstrates the effect of applying this transformation for values up to k = 10. We can see that, for sufﬁciently large values of k, the program can 576 Chapter 5 Optimizing Program Performance 6 5 4 3 2 1 0 1 2 3 4 Unrolling factor kCPE 5 6 double * double + long * long + 7 8 9 10 Figure 5.25 CPE performance of k × k loop unrolling. All of the CPEs improve with this transformation, achieving near or at their throughput bounds. achieve nearly the throughput bounds for all cases. Integer addition achieves a CPE of 0.54 with k = 7, close to the throughput bound of 0.50 caused by the two load units. Integer multiplication and ﬂoating-point addition achieve CPEs of 1.01 when k ≥ 3, approaching the throughput bound of 1.00 set by their functional units. Floating-point multiplication achieves a CPE of 0.51 for k ≥ 10, approaching the throughput bound of 0.50 set by the two ﬂoating-point multipliers and the two load units. It is worth noting that our code is able to achieve nearly twice the throughput with ﬂoating-point multiplication as it can with ﬂoating-point addition, even though multiplication is a more complex operation. In general, a program can achieve the throughput bound for an operation only when it can keep the pipelines ﬁlled for all of the functional units capable of performing that operation. For an operation with latency L and capacity C, this requires an unrolling factor k ≥ C . L. For example, ﬂoating-point multiplication has C = 2 and L = 5, necessitating an unrolling factor of k ≥ 10. Floating-point addition has C = 1 and L = 3, achieving maximum throughput with k ≥ 3. In performing the k × k unrolling transformation, we must consider whether it preserves the functionality of the original function. We have seen in Chapter 2 that two’s-complement arithmetic is commutative and associative, even when overﬂow occurs. Hence, for an integer data type, the result computed by ²ÇÀ¾©Å−t will be identical to that computed by ²ÇÀ¾©Å−s under all possible conditions. Thus, an optimizing compiler could potentially convert the code shown in ²ÇÀ¾©Å−r ﬁrst to a two-way unrolled variant of ²ÇÀ¾©Å−s by loop unrolling, and then to that of ²ÇÀ¾©Å−t by introducing parallelism. Some compilers do either this or similar transformations to improve performance for integer data. On the other hand, ﬂoating-point multiplication and addition are not as- sociative. Thus, ²ÇÀ¾©Å−s and ²ÇÀ¾©Å−t could produce different results due to rounding or overﬂow. Imagine, for example, a product computation in which all of the elements with even indices are numbers with very large absolute values, while those with odd indices are very close to 0.0. In such a case, product PEn might overﬂow, or POn might underﬂow, even though computing product Pn pro- Section 5.9 Enhancing Parallelism 577 ceeds normally. In most real-life applications, however, such patterns are unlikely. Since most physical phenomena are continuous, numerical data tend to be reason- ably smooth and well behaved. Even when there are discontinuities, they do not generally cause periodic patterns that lead to a condition such as that sketched ear- lier. It is unlikely that multiplying the elements in strict order gives fundamentally better accuracy than does multiplying two groups independently and then mul- tiplying those products together. For most applications, achieving a performance gain of 2× outweighs the risk of generating different results for strange data pat- terns. Nevertheless, a program developer should check with potential users to see if there are particular conditions that may cause the revised algorithm to be unac- ceptable. Most compilers do not attempt such transformations with ﬂoating-point code, since they have no way to judge the risks of introducing transformations that can change the program behavior, no matter how small. 5.9.2 Reassociation Transformation We now explore another way to break the sequential dependencies and thereby improve performance beyond the latency bound. We saw that the k × 1 loop un- rolling of ²ÇÀ¾©Å−s did not change the set of operations performed in combining the vector elements to form their sum or product. By a very small change in the code, however, we can fundamentally change the way the combining is performed, and also greatly increase the program performance. Figure 5.26 shows a function ²ÇÀ¾©Å−u that differs from the unrolled code of ²ÇÀ¾©Å−s (Figure 5.16) only in the way the elements are combined in the inner loop. In ²ÇÀ¾©Å−s, the combining is performed by the statement 12 þ²² { fþ²² ﬂ– ®þÎþ‰©`g ﬂ– ®þÎþ‰©io`y while in ²ÇÀ¾©Å−u it is performed by the statement 12 þ²² { þ²² ﬂ– f®þÎþ‰©` ﬂ– ®þÎþ‰©io`gy differing only in how two parentheses are placed. We call this a reassociation trans- formation, because the parentheses shift the order in which the vector elements are combined with the accumulated value þ²², yielding a form of loop unrolling we refer to as “2 × 1a.” To an untrained eye, the two statements may seem essentially the same, but when we measure the CPE, we get a surprising result: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−r 551 Accumulate in temporary 1.27 3.01 3.01 5.01 ²ÇÀ¾©Å−s 568 2 × 1 unrolling 1.01 3.01 3.01 5.01 ²ÇÀ¾©Å−t 573 2 × 2 unrolling 0.81 1.51 1.51 2.51 ²ÇÀ¾©Å−u 578 2 × 1a unrolling 1.01 1.51 1.51 2.51 Latency bound 1.00 3.00 3.00 5.00 Throughput bound 0.50 1.00 1.00 0.50 578 Chapter 5 Optimizing Program Performance 1 mhpÓoþ ÄÇÇÉ ÏÅËÇÄÄ©Å× hm 2 ÌÇ©® ²ÇÀ¾©Å−ufÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ÄÇÅ× Ä©À©Î { Ä−Å×Î³koy 7 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 8 ®þÎþˆÎ þ²² { '⁄¥ﬁ¶y 9 10 mh £ÇÀ¾©Å− p −Ä−À−ÅÎÍ þÎ þ Î©À− hm 11 ðÇË f© { ny © z Ä©À©Îy ©i{pg Õ 12 þ²² { þ²² ﬂ– f®þÎþ‰©` ﬂ– ®þÎþ‰©io`gy 13 Û 14 15 mh ƒ©Å©Í³ þÅÔ Ë−Àþ©Å©Å× −Ä−À−ÅÎÍ hm 16 ðÇË fy © z Ä−Å×Î³y ©iig Õ 17 þ²² { þ²² ﬂ– ®þÎþ‰©`y 18 Û 19 h®−ÍÎ { þ²²y 20 Û Figure 5.26 Applying 2 × 1a unrolling. By reassociating the arithmetic, this approach increases the number of operations that can be performed in parallel. The integer addition case matches the performance of k × 1 unrolling (²ÇÀ¾©Å−s), while the other three cases match the performance of the versions with parallel accumulators (²ÇÀ¾©Å−t), doubling the performance relative to k × 1 unrolling. These cases have broken through the barrier imposed by the latency bound. Figure 5.27 illustrates how the code for the inner loop of ²ÇÀ¾©Å−u (for the case of multiplication as the combining operation and ®ÇÏ¾Ä− as data type) gets decoded into operations and the resulting data dependencies. We see that the load operations resulting from the ÌÀÇÌÍ® and the ﬁrst ÌÀÏÄÍ® instructions load vector elements i and i + 1 from memory, and the ﬁrst mul operation multiplies them together. The second mul operation then multiples this result by the accumulated value þ²². Figure 5.28(a) shows how we rearrange, reﬁne, and abstract the op- erations of Figure 5.27 to get a template representing the data dependencies for one iteration (Figure 5.28(b)). As with the templates for ²ÇÀ¾©Å−s and ²ÇÀ¾©Å−u, we have two load and two mul operations, but only one of the mul operations forms a data-dependency chain between loop registers. When we then replicate this template n/2 times to show the computations performed in multiplying n vec- tor elements (Figure 5.29), we see that we only have n/2 operations along the critical path. The ﬁrst multiplication within each iteration can be performed with- out waiting for the accumulated value from the previous iteration. Thus, we reduce the minimum possible CPE by a factor of around 2. Section 5.9 Enhancing Parallelism 579 %rax %rbp %rdx %xmm0 vmulsd 8(%rax,%rdx,8), %xmm0, %xmm0 vmulsd %xmm0, %xmm1, %xmm1 vmovsd (%rax,%rdx,8), %xmm0 addq $2,%rdx cmpq %rdx,%rbp jg loop %rax %rbp %rdx %xmm0 %xmm1 %xmm1 load load mul mul add cmp jg Figure 5.27 Graphical representation of inner-loop code for ²ÇÀ¾©Å−u. Each iteration gets decoded into similar operations as for ²ÇÀ¾©Å−s or ²ÇÀ¾©Å−t, but with different data dependencies. %rax %rbp %rdx%xmm1 %xmm1 %rdx data[i ] data[i +1] load load mul mul add cmp (a) (b) jg %rdx%xmm1 %xmm1 %rdx load load mul mul add Figure 5.28 Abstracting ²ÇÀ¾©Å−u operations as a data-ﬂow graph. We rearrange, simplify, and abstract the representation of Figure 5.27 to show the data dependencies between successive iterations. The upper mul operation multiplies two 2-vector elements with each other, while the lower one multiplies the result by loop variable þ²². 580 Chapter 5 Optimizing Program Performance Figure 5.29 Data-ﬂow representation of ²ÇÀ¾©Å−u operating on a vector of length n. We have a single critical path, but it contains only n/2 operations. data[0] data[1] load load mul mul add data[2] data[3] load load mul mul add data[n-2] data[n-1] load load mul mul add Critical path Figure 5.30 demonstrates the effect of applying the reassociation transforma- tion to achieve what we refer to as k × 1a loop unrolling for values up to k = 10. We can see that this transformation yields performance results similar to what is achieved by maintaining k separate accumulators with k × k unrolling. In all cases, we come close to the throughput bounds imposed by the functional units. In performing the reassociation transformation, we once again change the order in which the vector elements will be combined together. For integer addition and multiplication, the fact that these operations are associative implies that this reordering will have no effect on the result. For the ﬂoating-point cases, we must once again assess whether this reassociation is likely to signiﬁcantly affect Section 5.9 Enhancing Parallelism 581 6 5 4 3 2 1 0 1 2 3 4 Unrolling factor kCPE 5 6 double * double + long * long + 7 8 9 10 Figure 5.30 CPE performance for k × 1a loop unrolling. All of the CPEs improve with this transformation, nearly approaching their throughput bounds. the outcome. We would argue that the difference would be immaterial for most applications. In summary, a reassociation transformation can reduce the number of opera- tions along the critical path in a computation, resulting in better performance by better utilizing the multiple functional units and their pipelining capabilities. Most compilers will not attempt any reassociations of ﬂoating-point operations, since these operations are not guaranteed to be associative. Current versions of gcc do perform reassociations of integer operations, but not always with good effects. In general, we have found that unrolling a loop and accumulating multiple values in parallel is a more reliable way to achieve improved program performance. Practice Problem 5.8 (solution page 612) Consider the following function for computing the product of an array of n double- precision numbers. We have unrolled the loop by a factor of 3. ®ÇÏ¾Ä− þÉËÇ®f®ÇÏ¾Ä− þ‰`j ÄÇÅ× Åg Õ ÄÇÅ× ©y ®ÇÏ¾Ä− Ój Ôj Öy ®ÇÏ¾Ä−Ë{oy ðÇË f© { ny © z Åkpy ©i{ qg Õ Ó { þ‰©`y Ô { þ‰©io`y Ö { þ‰©ip`y Ë{ËhÓhÔhÖymh –ËÇ®Ï²Î ²ÇÀÉÏÎþÎ©ÇÅ hm Û ðÇËfy©zÅy ©iig Ë h{ þ‰©`y Ë−ÎÏËÅ Ëy Û 582 Chapter 5 Optimizing Program Performance For the line labeled “–ËÇ®Ï²Î ²ÇÀÉÏÎþÎ©ÇÅ,” we can use parentheses to cre- ate ﬁve different associations of the computation, as follows: Ë{ffËhÓghÔghÖymh¡ohm Ë{fËhfÓhÔgghÖymh¡phm Ë{ËhffÓhÔghÖgymh¡qhm Ë{ËhfÓhfÔh Öggy mh ¡r hm Ë{fËhÓghfÔhÖgymh¡shm Assume we run these functions on a machine where ﬂoating-point multiplication has a latency of 5 clock cycles. Determine the lower bound on the CPE set by the data dependencies of the multiplication. (Hint: It helps to draw a data-ﬂow representation of how Ë is computed on every iteration.) Web Aside OPT:SIMD Achieving greater parallelism with vector instructions As described in Section 3.1, Intel introduced the SSE instructions in 1999, where SSE is the acronym for “streaming SIMD extensions” and, in turn, SIMD (pronounced “sim-dee”) is the acronym for “single instruction, multiple data.” The SSE capability has gone through multiple generations, with more recent versions being named advanced vector extensions, or AVX. The SIMD execution model involves operating on entire vectors of data within single instructions. These vectors are held in a special set of vector registers, named cÔÀÀn–cÔÀÀos. Current AVX vector registers are 32 bytes long, and therefore each can hold eight 32-bit numbers or four 64-bit numbers, where the numbers can be either integer or ﬂoating-point values. AVX instructions can then perform vector operations on these registers, such as adding or multiplying eight or four sets of values in parallel. For example, if YMM register cÔÀÀn contains eight single-precision ﬂoating-point numbers, which we denote a0,...,a7, and cË²Ó contains the memory address of a sequence of eight single-precision ﬂoating-point numbers, which we denote b0,...,b7, then the instruction ÌÀÏÄÉÍ fcË²Ígj cÔÀÀnj cÔÀÀo will read the eight values from memory and perform eight multiplications in parallel, computing ai ← ai . bi, for 0 ≤ i ≤ 7 and storing the resulting eight products in vector register cÔÀÀo. We see that a single instruction is able to generate a computation over multiple data values, hence the term “SIMD.” gcc supports extensions to the C language that let programmers express a program in terms of vector operations that can be compiled into the vector instructions of AVX (as well as code based on the earlier SSE instructions). This coding style is preferable to writing code directly in assembly language, since gcc can also generate code for the vector instructions found on other processors. Using a combination of gcc instructions, loop unrolling, and multiple accumulators, we are able to achieve the following performance for our combining functions: Section 5.10 Summary of Results for Optimizing Combining Code 583 Web Aside OPT:SIMD Achieving greater parallelism with vector instructions (continued) Integer Floating point ©ÅÎ ÄÇÅ× ©ÅÎ ÄÇÅ× Method ihihihih Scalar 10 × 10 0.54 1.01 0.55 1.00 1.01 0.51 1.01 0.52 Scalar throughput bound 0.50 0.50 1.00 1.00 1.00 1.00 0.50 0.50 Vector 8 × 8 0.05 0.24 0.13 1.51 0.12 0.08 0.25 0.16 Vector throughput bound 0.06 0.12 0.12 — 0.12 0.06 0.25 0.12 In this chart, the ﬁrst set of numbers is for conventional, scalar code written in the style of ²ÇÀ¾©Å−t, unrolling by a factor of 10 and maintaining 10 accumulators. The second set of numbers is for code written in a form that gcc can compile into AVX vector code. In addition to using vector operations, this version unrolls the main loop by a factor of 8 and maintains eight separate vector accumulators. We show results for both 32-bit and 64-bit numbers, since the vector instructions achieve 8-way parallelism in the ﬁrst case, but only 4-way parallelism in the second. We can see that the vector code achieves almost an eightfold improvement on the four 32-bit cases, and a fourfold improvement on three of the four 64-bit cases. Only the long integer multiplication code does not perform well when we attempt to express it in vector code. The AVX instruction set does not include one to do parallel multiplication of 64-bit integers, and so gcc cannot generate vector code for this case. Using vector instructions creates a new throughput bound for the combining operations. These are eight times lower for 32-bit operations and four times lower for 64-bit operations than the scalar limits. Our code comes close to achieving these bounds for several combinations of data type and operation. 5.10 Summary of Results for Optimizing Combining Code Our efforts at maximizing the performance of a routine that adds or multiplies the elements of a vector have clearly paid off. The following summarizes the results we obtain with scalar code, not making use of the vector parallelism provided by AVX vector instructions: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−o 543 Abstract kﬂo 10.12 10.12 10.17 11.14 ²ÇÀ¾©Å−t 573 2 × 2 unrolling 0.81 1.51 1.51 2.51 10 × 10 unrolling 0.55 1.00 1.01 0.52 Latency bound 1.00 3.00 3.00 5.00 Throughput bound 0.50 1.00 1.00 0.50 584 Chapter 5 Optimizing Program Performance By using multiple optimizations, we have been able to achieve CPEs close to the throughput bounds of 0.50 and 1.00, limited only by the capacities of the func- tional units. These represent 10–20× improvements on the original code. This has all been done using ordinary C code and a standard compiler. Rewriting the code to take advantage of the newer SIMD instructions yields additional performance gains of nearly 4× or 8×. For example, for single-precision multiplication, the CPE drops from the original value of 11.14 down to 0.06, an overall performance gain of over 180×. This example demonstrates that modern processors have consid- erable amounts of computing power, but we may need to coax this power out of them by writing our programs in very stylized ways. 5.11 Some Limiting Factors We have seen that the critical path in a data-ﬂow graph representation of a program indicates a fundamental lower bound on the time required to execute a program. That is, if there is some chain of data dependencies in a program where the sum of all of the latencies along that chain equals T , then the program will require at least T cycles to execute. We have also seen that the throughput bounds of the functional units also impose a lower bound on the execution time for a program. That is, assume that a program requires a total of N computations of some operation, that the microprocessor has C functional units capable of performing that operation, and that these units have an issue time of I . Then the program will require at least N . I/C cycles to execute. In this section, we will consider some other factors that limit the performance of programs on actual machines. 5.11.1 Register Spilling The beneﬁts of loop parallelism are limited by the ability to express the compu- tation in assembly code. If a program has a degree of parallelism P that exceeds the number of available registers, then the compiler will resort to spilling, stor- ing some of the temporary values in memory, typically by allocating space on the run-time stack. As an example, the following measurements compare the result of extending the multiple accumulator scheme of ²ÇÀ¾©Å−t to the cases of k = 10 and k = 20: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−t 573 10 × 10 unrolling 0.55 1.00 1.01 0.52 20 × 20 unrolling 0.83 1.03 1.02 0.68 Throughput bound 0.50 1.00 1.00 0.50 Section 5.11 Some Limiting Factors 585 We can see that none of the CPEs improve with this increased unrolling, and some even get worse. Modern x86-64 processors have 16 integer registers and can make use of the 16 YMM registers to store ﬂoating-point data. Once the number of loop variables exceeds the number of available registers, the program must allocate some on the stack. As an example, the following snippet of code shows how accumulator þ²²n is updated in the inner loop of the code with 10 × 10 unrolling: Updating of accumulator acc0 in 10 x 10 urolling ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn acc0 *= data[i] We can see that the accumulator is kept in register cÓÀÀn, and so the program can simply read ®þÎþ‰©` from memory and multiply it by this register. The comparable part of the code for 20 × 20 unrolling has a much different form: Updating of accumulator acc0 in 20 x 20 unrolling ÌÀÇÌÍ® rnfcËÍÉgj cÓÀÀn ÌÀÏÄÍ® fcË®Ógj cÓÀÀnj cÓÀÀn ÌÀÇÌÍ® cÓÀÀnj rnfcËÍÉg The accumulator is kept as a local variable on the stack, at offset 40 from the stack pointer. The program must read both its value and the value of ®þÎþ‰©` from memory, multiply them, and store the result back to memory. Once a compiler must resort to register spilling, any advantage of maintaining multiple accumulators will most likely be lost. Fortunately, x86-64 has enough registers that most loops will become throughput limited before this occurs. 5.11.2 Branch Prediction and Misprediction Penalties We demonstrated via experiments in Section 3.6.6 that a conditional branch can incur a signiﬁcant misprediction penalty when the branch prediction logic does not correctly anticipate whether or not a branch will be taken. Now that we have learned something about how processors operate, we can understand where this penalty arises. Modern processors work well ahead of the currently executing instructions, reading new instructions from memory and decoding them to determine what operations to perform on what operands. This instruction pipelining works well as long as the instructions follow in a simple sequence. When a branch is encountered, the processor must guess which way the branch will go. For the case of a conditional jump, this means predicting whether or not the branch will be taken. For an instruction such as an indirect jump (as we saw in the code to jump to an address speciﬁed by a jump table entry) or a procedure return, this means predicting the target address. In this discussion, we focus on conditional branches. In a processor that employs speculative execution, the processor begins exe- cuting the instructions at the predicted branch target. It does this in a way that avoids modifying any actual register or memory locations until the actual out- come has been determined. If the prediction is correct, the processor can then 586 Chapter 5 Optimizing Program Performance “commit” the results of the speculatively executed instructions by storing them in registers or memory. If the prediction is incorrect, the processor must discard all of the speculatively executed results and restart the instruction fetch process at the correct location. The misprediction penalty is incurred in doing this, because the instruction pipeline must be reﬁlled before useful results are generated. We saw in Section 3.6.6 that recent versions of x86 processors, including all processors capable of executing x86-64 programs, have conditional move instruc- tions. gcc can generate code that uses these instructions when compiling condi- tional statements and expressions, rather than the more traditional realizations based on conditional transfers of control. The basic idea for translating into con- ditional moves is to compute the values along both branches of a conditional expression or statement and then use conditional moves to select the desired value. We saw in Section 4.5.7 that conditional move instructions can be implemented as part of the pipelined processing of ordinary instructions. There is no need to guess whether or not the condition will hold, and hence no penalty for guessing incorrectly. How, then, can a C programmer make sure that branch misprediction penal- ties do not hamper a program’s efﬁciency? Given the 19-cycle misprediction penalty we measured for the reference machine, the stakes are very high. There is no simple answer to this question, but the following general principles apply. Do Not Be Overly Concerned about Predictable Branches We have seen that the effect of a mispredicted branch can be very high, but that does not mean that all program branches will slow a program down. In fact, the branch prediction logic found in modern processors is very good at discerning regular patterns and long-term trends for the different branch instructions. For example, the loop-closing branches in our combining routines would typically be predicted as being taken, and hence would only incur a misprediction penalty on the last time around. As another example, consider the results we observed when shifting from ²ÇÀ¾©Å−p to ²ÇÀ¾©Å−q, when we took the function ×−ÎˆÌ−²ˆ−Ä−À−ÅÎ out of the inner loop of the function, as is reproduced below: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−p 545 Move Ì−²ˆÄ−Å×Î³ 7.02 9.03 9.02 11.03 ²ÇÀ¾©Å−q 549 Direct data access 7.17 9.02 9.02 11.03 The CPE did not improve, even though the transformation eliminated two condi- tionals on each iteration that check whether the vector index is within bounds. For this function, the checks always succeed, and hence they are highly predictable. As a way to measure the performance impact of bounds checking, consider the following combining code, where we have modiﬁed the inner loop of ²ÇÀ¾©Å−r by replacing the access to the data element with the result of performing an inline substitution of the code for ×−ÎˆÌ−²ˆ−Ä−À−ÅÎ. We will call this new version Section 5.11 Some Limiting Factors 587 ²ÇÀ¾©Å−r¾. This code performs bounds checking and also references the vector elements through the vector data structure. 1 mh 'Å²ÄÏ®− ¾ÇÏÅ®Í ²³−²Â ©Å ÄÇÇÉ hm 2 ÌÇ©® ²ÇÀ¾©Å−r¾fÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ®þÎþˆÎ þ²² { '⁄¥ﬁ¶y 7 8 ðÇË f© { ny © z Ä−Å×Î³y ©iig Õ 9 ©ð f© |{ n dd © z Ìk|Ä−Åg Õ 10 þ²² { þ²² ﬂ– Ìk|®þÎþ‰©`y 11 Û 12 Û 13 h®−ÍÎ { þ²²y 14 Û We can then directly compare the CPE for the functions with and without bounds checking: Integer Floating point Function Page Method ihih ²ÇÀ¾©Å−r 551 No bounds checking 1.27 3.01 3.01 5.01 ²ÇÀ¾©Å−r¾ 551 Bounds checking 2.02 3.01 3.01 5.01 The version with bounds checking is slightly slower for the case of integer addition, but it achieves the same performance for the other three cases. The performance of these cases is limited by the latencies of their respective combining operations. The additional computation required to perform bounds checking can take place in parallel with the combining operations. The processor is able to predict the outcomes of these branches, and so none of this evaluation has much effect on the fetching and processing of the instructions that form the critical path in the program execution. Write Code Suitable for Implementation with Conditional Moves Branch prediction is only reliable for regular patterns. Many tests in a program are completely unpredictable, dependent on arbitrary features of the data, such as whether a number is negative or positive. For these, the branch prediction logic will do very poorly. For inherently unpredictable cases, program performance can be greatly enhanced if the compiler is able to generate code using conditional data transfers rather than conditional control transfers. This cannot be controlled directly by the C programmer, but some ways of expressing conditional behavior can be more directly translated into conditional moves than others. We have found that gcc is able to generate conditional moves for code written in a more “functional” style, where we use conditional operations to compute 588 Chapter 5 Optimizing Program Performance values and then update the program state with these values, as opposed to a more “imperative” style, where we use conditionals to selectively update program state. There are no strict rules for these two styles, and so we illustrate with an example. Suppose we are given two arrays of integers þ and ¾, and at each position i, we want to set þ‰i` to the minimum of þ‰i` and ¾‰i`, and ¾‰i` to the maximum. An imperative style of implementing this function is to check at each position i and swap the two elements if they are out of order: 1 mh ‡−þËËþÅ×− ÎÑÇ Ì−²ÎÇËÍ ÍÇ Î³þÎ ðÇË −þ²³ ©j ¾‰©` |{ þ‰©` hm 2 ÌÇ©® À©ÅÀþÓofÄÇÅ× þ‰`j ÄÇÅ× ¾‰`j ÄÇÅ× Åg Õ 3 ÄÇÅ× ©y 4 ðÇËf©{ny©zÅy ©iig Õ 5 ©ð fþ‰©` | ¾‰©`g Õ 6 ÄÇÅ× Î { þ‰©`y 7 þ‰©` { ¾‰©`y 8 ¾‰©` { Îy 9 Û 10 Û 11 Û Our measurements for this function show a CPE of around 13.5 for random data and 2.5–3.5 for predictable data, an indication of a misprediction penalty of around 20 cycles. A functional style of implementing this function is to compute the minimum and maximum values at each position i and then assign these values to þ‰i` and ¾‰i`, respectively: 1 mh ‡−þËËþÅ×− ÎÑÇ Ì−²ÎÇËÍ ÍÇ Î³þÎ ðÇË −þ²³ ©j ¾‰©` |{ þ‰©` hm 2 ÌÇ©® À©ÅÀþÓpfÄÇÅ× þ‰`j ÄÇÅ× ¾‰`j ÄÇÅ× Åg Õ 3 ÄÇÅ× ©y 4 ðÇËf©{ny©zÅy ©iig Õ 5 ÄÇÅ× À©Å { þ‰©` z ¾‰©` } þ‰©` x ¾‰©`y 6 ÄÇÅ× ÀþÓ { þ‰©` z ¾‰©` } ¾‰©` x þ‰©`y 7 þ‰©` { À©Åy 8 ¾‰©` { ÀþÓy 9 Û 10 Û Our measurements for this function show a CPE of around 4.0 regardless of whether the data are arbitrary or predictable. (We also examined the generated assembly code to make sure that it indeed uses conditional moves.) As discussed in Section 3.6.6, not all conditional behavior can be implemented with conditional data transfers, and so there are inevitably cases where program- mers cannot avoid writing code that will lead to conditional branches for which the processor will do poorly with its branch prediction. But, as we have shown, a little cleverness on the part of the programmer can sometimes make code more amenable to translation into conditional data transfers. This requires some amount Section 5.12 Understanding Memory Performance 589 of experimentation, writing different versions of the function and then examining the generated assembly code and measuring performance. Practice Problem 5.9 (solution page 612) The traditional implementation of the merge step of mergesort requires three loops [98]: 1 ÌÇ©® À−Ë×−fÄÇÅ× ÍË²o‰`j ÄÇÅ× ÍË²p‰`j ÄÇÅ× ®−ÍÎ‰`j ÄÇÅ× Åg Õ 2 ÄÇÅ× ©o { ny 3 ÄÇÅ× ©p { ny 4 ÄÇÅ× ©® { ny 5 Ñ³©Ä− f©ozÅdd©pzÅgÕ 6 ©ð fÍË²o‰©o` z ÍË²p‰©p`g 7 ®−ÍÎ‰©®ii` { ÍË²o‰©oii`y 8 −ÄÍ− 9 ®−ÍÎ‰©®ii` { ÍË²p‰©pii`y 10 Û 11 Ñ³©Ä− f©o z Åg 12 ®−ÍÎ‰©®ii` { ÍË²o‰©oii`y 13 Ñ³©Ä− f©p z Åg 14 ®−ÍÎ‰©®ii` { ÍË²p‰©pii`y 15 Û The branches caused by comparing variables ©o and ©p to Å have good prediction performance—the only mispredictions occur when they ﬁrst become false. The comparison between values ÍË²o‰©o` and ÍË²p‰©p` (line 6), on the other hand, is highly unpredictable for typical data. This comparison controls a conditional branch, yielding a CPE (where the number of elements is 2n) of around 15.0 when run on random data. Rewrite the code so that the effect of the conditional statement in the ﬁrst loop (lines 6–9) can be implemented with a conditional move. 5.12 Understanding Memory Performance All of the code we have written thus far, and all the tests we have run, access relatively small amounts of memory. For example, the combining routines were measured over vectors of length less than 1,000 elements, requiring no more than 8,000 bytes of data. All modern processors contain one or more cache memories to provide fast access to such small amounts of memory. In this section, we will further investigate the performance of programs that involve load (reading from memory into registers) and store (writing from registers to memory) operations, considering only the cases where all data are held in cache. In Chapter 6, we go into much more detail about how caches work, their performance characteristics, and how to write code that makes best use of caches. 590 Chapter 5 Optimizing Program Performance As Figure 5.11 shows, modern processors have dedicated functional units to perform load and store operations, and these units have internal buffers to hold sets of outstanding requests for memory operations. For example, our reference machine has two load units, each of which can hold up to 72 pending read requests. It has a single store unit with a store buffer containing up to 42 write requests. Each of these units can initiate 1 operation every clock cycle. 5.12.1 Load Performance The performance of a program containing load operations depends on both the pipelining capability and the latency of the load unit. In our experiments with combining operations using our reference machine, we saw that the CPE never got below 0.50 for any combination of data type and combining operation, except when using SIMD operations. One factor limiting the CPE for our examples is that they all require reading one value from memory for each element computed. With two load units, each able to initiate at most 1 load operation every clock cycle, the CPE cannot be less than 0.50. For applications where we must load k values for every element computed, we can never achieve a CPE lower than k/2 (see, for example, Problem 5.15). In our examples so far, we have not seen any performance effects due to the latency of load operations. The addresses for our load operations depended only on the loop index i, and so the load operations did not form part of a performance- limiting critical path. To determine the latency of the load operation on a machine, we can set up a computation with a sequence of load operations, where the outcome of one determines the address for the next. As an example, consider the function Ä©ÍÎˆ Ä−Å in Figure 5.31, which computes the length of a linked list. In the loop of this function, each successive value of variable ÄÍ depends on the value read by the pointer reference ÄÍk|Å−ÓÎ. Our measurements show that function Ä©ÍÎˆÄ−Å has 1 ÎÔÉ−®−ð ÍÎËÏ²Î ¥‹¥ Õ 2 ÍÎËÏ²Î ¥‹¥ hÅ−ÓÎy 3 ÄÇÅ× ®þÎþy 4 Û Ä©ÍÎˆ−Ä−j hÄ©ÍÎˆÉÎËy 5 6 ÄÇÅ× Ä©ÍÎˆÄ−ÅfÄ©ÍÎˆÉÎË ÄÍg Õ 7 ÄÇÅ× Ä−Å { ny 8 Ñ³©Ä− fÄÍg Õ 9 Ä−Åiiy 10 ÄÍ { ÄÍk|Å−ÓÎy 11 Û 12 Ë−ÎÏËÅ Ä−Åy 13 Û Figure 5.31 Linked list function. Its performance is limited by the latency of the load operation. Section 5.12 Understanding Memory Performance 591 a CPE of 4.00, which we claim is a direct indication of the latency of the load operation. To see this, consider the assembly code for the loop: Inner loop of list_len ls in %rdi, len in %rax 1 l‹qx loop: 2 þ®®Ê boj cËþÓ Increment len 3 ÀÇÌÊ fcË®©gj cË®© ls = ls->next 4 Î−ÍÎÊ cË®©j cË®© Test ls 5 ÁÅ− l‹q If nonnull, goto loop The ÀÇÌÊ instruction on line 3 forms the critical bottleneck in this loop. Each successive value of register cË®© depends on the result of a load operation having the value in cË®© as its address. Thus, the load operation for one iteration cannot begin until the one for the previous iteration has completed. The CPE of 4.00 for this function is determined by the latency of the load operation. Indeed, this measurement matches the documented access time of 4 cycles for the reference machine’s L1 cache, as is discussed in Section 6.4. 5.12.2 Store Performance In all of our examples thus far, we analyzed only functions that reference mem- ory mostly with load operations, reading from a memory location into a register. Its counterpart, the store operation, writes a register value to memory. The per- formance of this operation, particularly in relation to its interactions with load operations, involves several subtle issues. As with the load operation, in most cases, the store operation can operate in a fully pipelined mode, beginning a new store on every cycle. For example, consider the function shown in Figure 5.32 that sets the elements of an array ®−ÍÎ of length Å to zero. Our measurements show a CPE of 1.0. This is the best we can achieve on a machine with a single store functional unit. Unlike the other operations we have considered so far, the store operation does not affect any register values. Thus, by their very nature, a series of store operations cannot create a data dependency. Only a load operation is affected by the result of a store operation, since only a load can read back the memory value that has been written by the store. The function ÑË©Î−ˆË−þ® shown in Figure 5.33 1 mh ·−Î −Ä−À−ÅÎÍ Çð þËËþÔ ÎÇ n hm 2 ÌÇ©® ²Ä−þËˆþËËþÔfÄÇÅ× h®−ÍÎj ÄÇÅ× Åg Õ 3 ÄÇÅ× ©y 4 ðÇËf©{ny©zÅy ©iig 5 ®−ÍÎ‰©` { ny 6 Û Figure 5.32 Function to set array elements to 0. This code achieves a CPE of 1.0. 592 Chapter 5 Optimizing Program Performance 1 mh „Ë©Î− ÎÇ ®−ÍÎj Ë−þ® ðËÇÀ ÍË² hm 2 ÌÇ©® ÑË©Î−ˆË−þ®fÄÇÅ× hÍË²j ÄÇÅ× h®ÍÎj ÄÇÅ× Åg 3 Õ 4 ÄÇÅ× ²ÅÎ { Åy 5 ÄÇÅ× ÌþÄ { ny 6 7 Ñ³©Ä− f²ÅÎg Õ 8 h®ÍÎ { ÌþÄy 9 ÌþÄ { fhÍË²gioy 10 ²ÅÎkky 11 Û 12 Û Initial Example A: write_read(&a[0],&a[1],3) 3cnt a val 0 \u000210 17 Iter. 1 2 \u00029 \u000210 0 Iter. 2 1 \u00029 \u000210 \u00029 Iter. 3 0 \u00029 \u000210 \u00029 Initial Example B: write_read(&a[0],&a[0],3) 3cnt a val 0 \u000210 17 Iter. 1 2 1 017 Iter. 2 1 2 117 Iter. 3 0 3 217 Figure 5.33 Code to write and read memory locations, along with illustrative executions. This function highlights the interactions between stores and loads when arguments ÍË² and ®−ÍÎ are equal. illustrates the potential interactions between loads and stores. This ﬁgure also shows two example executions of this function, when it is called for a two-element array þ, with initial contents −10 and 17, and with argument ²ÅÎ equal to 3. These executions illustrate some subtleties of the load and store operations. In Example A of Figure 5.33, argument ÍË² is a pointer to array element þ‰n`, while ®−ÍÎ is a pointer to array element þ‰o`. In this case, each load by the pointer reference hÍË² will yield the value −10. Hence, after two iterations, the array elements will remain ﬁxed at −10 and −9, respectively. The result of the read from ÍË² is not affected by the write to ®−ÍÎ. Measuring this example over a larger number of iterations gives a CPE of 1.3. In Example B of Figure 5.33, both arguments ÍË² and ®−ÍÎ are pointers to array element þ‰n`. In this case, each load by the pointer reference hÍË² will yield the value stored by the previous execution of the pointer reference h®−ÍÎ. Section 5.12 Understanding Memory Performance 593 Figure 5.34 Detail of load and store units. The store unit maintains a buffer of pending writes. The load unit must check its address with those in the store unit to detect a write/read dependency. Load unit Store unit Store buffer Address Address Address Data Data Data Matching addresses Address Data Data cache As a consequence, a series of ascending values will be stored in this location. In general, if function ÑË©Î−ˆË−þ® is called with arguments ÍË² and ®−ÍÎ pointing to the same memory location, and with argument ²ÅÎ having some value n> 0, the net effect is to set the location to n − 1. This example illustrates a phenomenon we will call a write/read dependency—the outcome of a memory read depends on a recent memory write. Our performance measurements show that Example B has a CPE of 7.3. The write/read dependency causes a slowdown in the processing of around 6 clock cycles. To see how the processor can distinguish between these two cases and why one runs slower than the other, we must take a more detailed look at the load and store execution units, as shown in Figure 5.34. The store unit includes a store buffer containing the addresses and data of the store operations that have been issued to the store unit, but have not yet been completed, where completion involves updating the data cache. This buffer is provided so that a series of store operations can be executed without having to wait for each one to update the cache. When a load operation occurs, it must check the entries in the store buffer for matching addresses. If it ﬁnds a match (meaning that any of the bytes being written have the same address as any of the bytes being read), it retrieves the corresponding data entry as the result of the load operation. gcc generates the following code for the inner loop of ÑË©Î−ˆË−þ®: Inner loop of write_read src in %rdi, dst in %rsi, val in %rax l‹qx loop: ÀÇÌÊ cËþÓj fcËÍ©g Write val to dst ÀÇÌÊ fcË®©gj cËþÓ t = *src þ®®Ê boj cËþÓ val = t+1 ÍÏ¾Ê boj cË®Ó cnt-- ÁÅ− l‹q If != 0, goto loop 594 Chapter 5 Optimizing Program Performance Figure 5.35 Graphical representation of inner-loop code for ÑË©Î−ˆË−þ®. The ﬁrst ÀÇÌÄ instruction is decoded into separate operations to compute the store address and to store the data to memory. %rax %rdi %rsi %rdx movq %rax,(%rsi) movq (%rdi),%rax addq $1,%rax subq $1,%rdx jne loop %rax %rdi %rsi %rdx s_addr s_data load add sub jne Figure 5.35 shows a data-ﬂow representation of this loop code. The instruction ÀÇÌÊ cËþÓjfcËÍ©g is translated into two operations: The s_addr instruction com- putes the address for the store operation, creates an entry in the store buffer, and sets the address ﬁeld for that entry. The s_data operation sets the data ﬁeld for the entry. As we will see, the fact that these two computations are performed inde- pendently can be important to program performance. This motivates the separate functional units for these operations in the reference machine. In addition to the data dependencies between the operations caused by the writing and reading of registers, the arcs on the right of the operators denote a set of implicit dependencies for these operations. In particular, the address computation of the s_addr operation must clearly precede the s_data operation. In addition, the load operation generated by decoding the instruction ÀÇÌÊ fcË®©gj cËþÓ must check the addresses of any pending store operations, creating a data dependency between it and the s_addr operation. The ﬁgure shows a dashed arc between the s_data and load operations. This dependency is conditional: if the two addresses match, the load operation must wait until the s_data has deposited its result into the store buffer, but if the two addresses differ, the two operations can proceed independently. Figure 5.36 illustrates the data dependencies between the operations for the inner loop of ÑË©Î−ˆË−þ®. In Figure 5.36(a), we have rearranged the operations to allow the dependencies to be seen more clearly. We have labeled the three dependencies involving the load and store operations for special attention. The arc labeled “1” represents the requirement that the store address must be computed before the data can be stored. The arc labeled “2” represents the need for the load operation to compare its address with that for any pending store operations. Finally, the dashed arc labeled “3” represents the conditional data dependency that arises when the load and store addresses match. Figure 5.36(b) illustrates what happens when we take away those operations that do not directly affect the ﬂow of data from one iteration to the next. The data-ﬂow graph shows just two chains of dependencies: the one on the left, with data values being stored, loaded, and incremented (only for the case of matching addresses); and the one on the right, decrementing variable ²ÅÎ. Section 5.12 Understanding Memory Performance 595 Figure 5.36 Abstracting the operations for ÑË©Î−ˆ Ë−þ®. We ﬁrst rearrange the operators of Figure 5.35(a) and then show only those operations that use values from one iteration to produce new values for the next (b). %rax %rdi %rsi %rdx %rax %rdx s_addr 1 2 3 s_data load (a) (b) add sub jne %rax %rdx %rax %rdx s_data load add sub We can now understand the performance characteristics of function ÑË©Î−ˆ Ë−þ®. Figure 5.37 illustrates the data dependencies formed by multiple iterations of its inner loop. For the case of Example A in Figure 5.33, with differing source and destination addresses, the load and store operations can proceed independently, and hence the only critical path is formed by the decrementing of variable ²ÅÎ, resulting in a CPE bound of 1.0. For the case of Example B with matching source and destination addresses, the data dependency between the s_data and load instructions causes a critical path to form involving data being stored, loaded, and incremented. We found that these three operations in sequence require a total of around 7 clock cycles. As these two examples show, the implementation of memory operations in- volves many subtleties. With operations on registers, the processor can determine which instructions will affect which others as they are being decoded into opera- tions. With memory operations, on the other hand, the processor cannot predict which will affect which others until the load and store addresses have been com- puted. Efﬁcient handling of memory operations is critical to the performance of many programs. The memory subsystem makes use of many optimizations, such as the potential parallelism when operations can proceed independently. Practice Problem 5.10 (solution page 613) As another example of code with potential load-store interactions, consider the following function to copy the contents of one array to another: 1 ÌÇ©® ²ÇÉÔˆþËËþÔfÄÇÅ× hÍË²j ÄÇÅ× h®−ÍÎj ÄÇÅ× Åg 2 Õ 3 ÄÇÅ× ©y 4 ðÇËf©{ny©zÅy ©iig 5 ®−ÍÎ‰©` { ÍË²‰©`y 6 Û 596 Chapter 5 Optimizing Program Performance Figure 5.37 Data-ﬂow representation of function ÑË©Î−ˆË−þ®l When the two addresses do not match, the only critical path is formed by the decrementing of ²ÅÎ (Example A). When they do match, the chain of data being stored, loaded, and incremented forms the critical path (Example B). s_data load add s_data load s_data load add sub s_data load add sub s_data load add sub s_data load add sub sub subadd Example A Example B Critical path Critical path Suppose þ is an array of length 1,000 initialized so that each element þ‰i` equals i. A. What would be the effect of the call ²ÇÉÔˆþËËþÔfþiojþjwwwg? B. What would be the effect of the call ²ÇÉÔˆþËËþÔfþjþiojwwwg? C. Our performance measurements indicate that the call of part A has a CPE of 1.2 (which drops to 1.0 when the loop is unrolled by a factor of 4), while the call of part B has a CPE of 5.0. To what factor do you attribute this performance difference? D. What performance would you expect for the call ²ÇÉÔˆþËËþÔfþjþjwwwg? Section 5.13 Life in the Real World: Performance Improvement Techniques 597 Practice Problem 5.11 (solution page 613) We saw that our measurements of the preﬁx-sum function ÉÍÏÀo (Figure 5.1) yield a CPE of 9.00 on a machine where the basic operation to be performed, ﬂoating- point addition, has a latency of just 3 clock cycles. Let us try to understand why our function performs so poorly. The following is the assembly code for the inner loop of the function: Inner loop of psum1 a in %rdi, i in %rax, cnt in %rdx 1 l‹sx loop: 2 ÌÀÇÌÍÍ krfcËÍ©jcËþÓjrgj cÓÀÀn Get p[i-1] 3 Ìþ®®ÍÍ fcË®©jcËþÓjrgj cÓÀÀnj cÓÀÀn Add a[i] 4 ÌÀÇÌÍÍ cÓÀÀnj fcËÍ©jcËþÓjrg Store at p[i] 5 þ®®Ê boj cËþÓ Increment i 6 ²ÀÉÊ cË®Ój cËþÓ Compare i:cnt 7 ÁÅ− l‹s If !=, goto loop Perform an analysis similar to those shown for ²ÇÀ¾©Å−q (Figure 5.14) and for ÑË©Î−ˆË−þ® (Figure 5.36) to diagram the data dependencies created by this loop, and hence the critical path that forms as the computation proceeds. Explain why the CPE is so high. Practice Problem 5.12 (solution page 613) Rewrite the code for ÉÍÏÀo (Figure 5.1) so that it does not need to repeatedly retrieve the value of É‰©` from memory. You do not need to use loop unrolling. We measured the resulting code to have a CPE of 3.00, limited by the latency of ﬂoating-point addition. 5.13 Life in the Real World: Performance Improvement Techniques Although we have only considered a limited set of applications, we can draw important lessons on how to write efﬁcient code. We have described a number of basic strategies for optimizing program performance: High-level design. Choose appropriate algorithms and data structures for the problem at hand. Be especially vigilant to avoid algorithms or coding techniques that yield asymptotically poor performance. Basic coding principles. Avoid optimization blockers so that a compiler can generate efﬁcient code. Eliminate excessive function calls. Move computations out of loops when possible. Consider selective compromises of program modularity to gain greater efﬁciency. 598 Chapter 5 Optimizing Program Performance Eliminate unnecessary memory references. Introduce temporary vari- ables to hold intermediate results. Store a result in an array or global variable only when the ﬁnal value has been computed. Low-level optimizations. Structure code to take advantage of the hardware capabilities. Unroll loops to reduce overhead and to enable further optimizations. Find ways to increase instruction-level parallelism by techniques such as multiple accumulators and reassociation. Rewrite conditional operations in a functional style to enable compi- lation via conditional data transfers. A ﬁnal word of advice to the reader is to be vigilant to avoid introducing errors as you rewrite programs in the interest of efﬁciency. It is very easy to make mistakes when introducing new variables, changing loop bounds, and making the code more complex overall. One useful technique is to use checking code to test each version of a function as it is being optimized, to ensure no bugs are introduced during this process. Checking code applies a series of tests to the new versions of a function and makes sure they yield the same results as the original. The set of test cases must become more extensive with highly optimized code, since there are more cases to consider. For example, checking code that uses loop unrolling requires testing for many different loop bounds to make sure it handles all of the different possible numbers of single-step iterations required at the end. 5.14 Identifying and Eliminating Performance Bottlenecks Up to this point, we have only considered optimizing small programs, where there is some clear place in the program that limits its performance and therefore should be the focus of our optimization efforts. When working with large programs, even knowing where to focus our optimization efforts can be difﬁcult. In this section, we describe how to use code proﬁlers, analysis tools that collect performance data about a program as it executes. We also discuss some general principles of code optimization, including the implications of Amdahl’s law, introduced in Section 1.9.1. 5.14.1 Program Proﬁling Program proﬁling involves running a version of a program in which instrumenta- tion code has been incorporated to determine how much time the different parts of the program require. It can be very useful for identifying the parts of a program we should focus on in our optimization efforts. One strength of proﬁling is that it can be performed while running the actual program on realistic benchmark data. Unix systems provide the proﬁling program gprof. This program generates two forms of information. First, it determines how much CPU time was spent for each of the functions in the program. Second, it computes a count of how many times each function gets called, categorized by which function performs the call. Both forms of information can be quite useful. The timings give a sense of Section 5.14 Identifying and Eliminating Performance Bottlenecks 599 the relative importance of the different functions in determining the overall run time. The calling information allows us to understand the dynamic behavior of the program. Proﬁling with gprof requires three steps, as shown for a C program ÉËÇ×l², which runs with command-line argument ð©Ä−lÎÓÎ: 1. The program must be compiled and linked for proﬁling. With gcc (and other C compilers), this involves simply including the run-time ﬂag kÉ× on the command line. It is important to ensure that the compiler does not attempt to perform any optimizations via inline substitution, or else the calls to functions may not be tabulated accurately. We use optimization ﬂag kﬂ×, guaranteeing that function calls will be tracked properly. Ä©ÅÏÓ| gcc -Og -pg prog.c -o prog 2. The program is then executed as usual: Ä©ÅÏÓ| ./prog file.txt It runs slightly (around a factor of 2) slower than normal, but otherwise the only difference is that it generates a ﬁle ×ÀÇÅlÇÏÎ. 3. gprof is invoked to analyze the data in ×ÀÇÅlÇÏÎ: Ä©ÅÏÓ| gprof prog The ﬁrst part of the proﬁle report lists the times spent executing the different functions, sorted in descending order. As an example, the following listing shows this part of the report for the three most time-consuming functions in a program: c ²ÏÀÏÄþÎ©Ì− Í−Äð Í−Äð ÎÇÎþÄ Î©À− Í−²ÇÅ®Í Í−²ÇÅ®Í ²þÄÄÍ Ím²þÄÄ Ím²þÄÄ ÅþÀ− wulsv pnqltt pnqltt o pnqltt pnqltt ÍÇËÎˆÑÇË®Í plqp pnvlsn rlvs wtsnpu nlnn nlnn ð©Å®ˆ−Ä−ˆË−² nlor pnvlvo nlqn opsoonqo nlnn nlnn ·ÎËÄ−Å Each row represents the time spent for all calls to some function. The ﬁrst column indicates the percentage of the overall time spent on the function. The second shows the cumulative time spent by the functions up to and including the one on this row. The third shows the time spent on this particular function, and the fourth shows how many times it was called (not counting recursive calls). In our example, the function ÍÇËÎˆÑÇË®Í was called only once, but this single call required 203.66 seconds, while the function ð©Å®ˆ−Ä−ˆË−² was called 965,027 times (not including recursive calls), requiring a total of 4.85 seconds. Function ·ÎËÄ−Å computes the length of a string by calling the library function ÍÎËÄ−Å. Library function calls are normally not shown in the results by gprof. Their times are usually reported as part of the function calling them. By creating the “wrapper function” ·ÎËÄ−Å, we can reliably track the calls to ÍÎËÄ−Å, showing that it was called 12,511,031 times but only requiring a total of 0.30 seconds. 600 Chapter 5 Optimizing Program Performance The second part of the proﬁle report shows the calling history of the functions. The following is the history for a recursive function ð©Å®ˆ−Ä−ˆË−²: osvtssups ð©Å®ˆ−Ä−ˆË−² ‰s` rlvs nlon wtsnpumwtsnpu ©ÅÍ−ËÎˆÍÎË©Å× ‰r` ‰s` plr rlvs nlon wtsnpuiosvtssups ð©Å®ˆ−Ä−ˆË−² ‰s` nlnv nlno qtqnqwmqtqnqw ÍþÌ−ˆÍÎË©Å× ‰v` nlnn nlno qtqnqwmqtqnqw Å−Ñˆ−Ä− ‰op` osvtssups ð©Å®ˆ−Ä−ˆË−² ‰s` This history shows both the functions that called ð©Å®ˆ−Ä−ˆË−², as well as the functions that it called. The ﬁrst two lines show the calls to the function: 158,655,725 calls by itself recursively, and 965,027 calls by function ©ÅÍ−ËÎˆÍÎË©Å× (which is itself called 965,027 times). Function ð©Å®ˆ−Ä−ˆË−², in turn, called two other functions, ÍþÌ−ˆÍÎË©Å× and Å−Ñˆ−Ä−, each a total of 363,039 times. From these call data, we can often infer useful information about the program behavior. For example, the function ð©Å®ˆ−Ä−ˆË−² is a recursive procedure that scans the linked list for a hash bucket looking for a particular string. For this function, comparing the number of recursive calls with the number of top-level calls provides statistical information about the lengths of the traversals through these lists. Given that their ratio is 164.4:1, we can infer that the program scanned an average of around 164 elements each time. Some properties of gprof are worth noting: . The timing is not very precise. It is based on a simple interval counting scheme in which the compiled program maintains a counter for each function record- ing the time spent executing that function. The operating system causes the program to be interrupted at some regular time interval δ. Typical values of δ range between 1.0 and 10.0 milliseconds. It then determines what function the program was executing when the interrupt occurred and increments the counter for that function by δ. Of course, it may happen that this function just started executing and will shortly be completed, but it is assigned the full cost of the execution since the previous interrupt. Some other function may run between two interrupts and therefore not be charged any time at all. Over a long duration, this scheme works reasonably well. Statistically, ev- ery function should be charged according to the relative time spent executing it. For programs that run for less than around 1 second, however, the numbers should be viewed as only rough estimates. . The calling information is quite reliable, assuming no inline substitutions have been performed. The compiled program maintains a counter for each combination of caller and callee. The appropriate counter is incremented every time a procedure is called. . By default, the timings for library functions are not shown. Instead, these times are incorporated into the times for the calling functions. Section 5.14 Identifying and Eliminating Performance Bottlenecks 601 5.14.2 Using a Proﬁler to Guide Optimization As an example of using a proﬁler to guide program optimization, we created an ap- plication that involves several different tasks and data structures. This application analyzes the n-gram statistics of a text document, where an n-gram is a sequence of n words occurring in a document. For n = 1, we collect statistics on individual words, for n = 2 on pairs of words, and so on. For a given value of n, our program reads a text ﬁle, creates a table of unique n-grams and how many times each one occurs, then sorts the n-grams in descending order of occurrence. As a benchmark, we ran it on a ﬁle consisting of the complete works of William Shakespeare, totaling 965,028 words, of which 23,706 are unique. We found that for n = 1, even a poorly written analysis program can readily process the entire ﬁle in under 1 second, and so we set n = 2 to make things more challenging. For the case of n = 2, n-grams are referred to as bigrams (pronounced “bye-grams”). We determined that Shakespeare’s works contain 363,039 unique bigrams. The most common is “I am,” occurring 1,892 times. Perhaps his most famous bigram, “to be,” occurs 1,020 times. Fully 266,018 of the bigrams occur only once. Our program consists of the following parts. We created multiple versions, starting with simple algorithms for the different parts and then replacing them with more sophisticated ones: 1. Each word is read from the ﬁle and converted to lowercase. Our initial version used the function ÄÇÑ−Ëo (Figure 5.7), which we know to have quadratic run time due to repeated calls to ÍÎËÄ−Å. 2. A hash function is applied to the string to create a number between 0 and s − 1, for a hash table with s buckets. Our initial function simply summed the ASCII codes for the characters modulo s. 3. Each hash bucket is organized as a linked list. The program scans down this list looking for a matching entry. If one is found, the frequency for this n-gram is incremented. Otherwise, a new list element is created. Our initial version performed this operation recursively, inserting new elements at the end of the list. 4. Once the table has been generated, we sort all of the elements according to the frequencies. Our initial version used insertion sort. Figure 5.38 shows the proﬁle results for six different versions of our n-gram- frequency analysis program. For each version, we divide the time into the follow- ing categories: Sort. Sorting n-grams by frequency List. Scanning the linked list for a matching n-gram, inserting a new element if necessary Lower. Converting strings to lowercase Strlen. Computing string lengths 602 Chapter 5 Optimizing Program Performance Initial Quicksort Iter first Iter last (a) All versions (b) All but the slowest version Big table Better hash Linear lower 250 200 150 100 50 0 6 5 4 3 2 1 0CPU secondsSort List Lower Strlen Hash Rest Better hashQuicksort Iter first Iter last Big table Linear lowerCPU secondsSort List Lower Strlen Hash Rest Figure 5.38 Proﬁle results for different versions of bigram-frequency counting program. Time is divided according to the different major operations in the program. Hash. Computing the hash function Rest. The sum of all other functions As part (a) of the ﬁgure shows, our initial version required 3.5 minutes, with most of the time spent sorting. This is not surprising, since insertion sort has quadratic run time and the program sorted 363,039 values. In our next version, we performed sorting using the library function ÊÍÇËÎ, which is based on the quicksort algorithm [98]. It has an expected run time of O(n log n). This version is labeled “Quicksort” in the ﬁgure. The more efﬁcient sorting algorithm reduces the time spent sorting to become negligible, and the overall run time to around 5.4 seconds. Part (b) of the ﬁgure shows the times for the remaining version on a scale where we can see them more clearly. Section 5.14 Identifying and Eliminating Performance Bottlenecks 603 With improved sorting, we now ﬁnd that list scanning becomes the bottleneck. Thinking that the inefﬁciency is due to the recursive structure of the function, we replaced it by an iterative one, shown as “Iter ﬁrst.” Surprisingly, the run time increases to around 7.5 seconds. On closer study, we ﬁnd a subtle difference between the two list functions. The recursive version inserted new elements at the end of the list, while the iterative one inserted them at the front. To maximize performance, we want the most frequent n-grams to occur near the beginning of the lists. That way, the function will quickly locate the common cases. Assuming that n-grams are spread uniformly throughout the document, we would expect the ﬁrst occurrence of a frequent one to come before that of a less frequent one. By inserting new n-grams at the end, the ﬁrst function tended to order n- grams in descending order of frequency, while the second function tended to do just the opposite. We therefore created a third list-scanning function that uses iteration but inserts new elements at the end of this list. With this version, shown as “Iter last,” the time dropped to around 5.3 seconds, slightly better than with the recursive version. These measurements demonstrate the importance of running experiments on a program as part of an optimization effort. We initially assumed that converting recursive code to iterative code would improve its performance and did not consider the distinction between adding to the end or to the beginning of a list. Next, we consider the hash table structure. The initial version had only 1,021 buckets (typically, the number of buckets is chosen to be a prime number to enhance the ability of the hash function to distribute keys uniformly among the buckets). For a table with 363,039 entries, this would imply an average load of 363,039/1,021 = 355.6. That explains why so much of the time is spent performing list operations—the searches involve testing a signiﬁcant number of candidate n- grams. It also explains why the performance is so sensitive to the list ordering. We then increased the number of buckets to 199,999, reducing the average load to 1.8. Oddly enough, however, our overall run time only drops to 5.1 seconds, a difference of only 0.2 seconds. On further inspection, we can see that the minimal performance gain with a larger table was due to a poor choice of hash function. Simply summing the character codes for a string does not produce a very wide range of values. In particular, the maximum code value for a letter is 122, and so a string of n char- acters will generate a sum of at most 122n. The longest bigram in our document, “honoriﬁcabilitudinitatibus thou” sums to just 3,371, and so most of the buck- ets in our hash table will go unused. In addition, a commutative hash function, such as addition, does not differentiate among the different possible orderings of characters with a string. For example, the words “rat” and “tar” will generate the same sums. We switched to a hash function that uses shift and exclusive-or operations. With this version, shown as “Better hash,” the time drops to 0.6 seconds. A more systematic approach would be to study the distribution of keys among the buckets more carefully, making sure that it comes close to what one would expect if the hash function had a uniform output distribution. 604 Chapter 5 Optimizing Program Performance Finally, we have reduced the run time to the point where most of the time is spent in ÍÎËÄ−Å, and most of the calls to ÍÎËÄ−Å occur as part of the lowercase con- version. We have already seen that function ÄÇÑ−Ëo has quadratic performance, especially for long strings. The words in this document are short enough to avoid the disastrous consequences of quadratic performance; the longest bigram is just 32 characters. Still, switching to ÄÇÑ−Ëp, shown as “Linear lower,” yields a signif- icant improvement, with the overall time dropping to around 0.2 seconds. With this exercise, we have shown that code proﬁling can help drop the time required for a simple application from 3.5 minutes down to 0.2 seconds, yielding a performance gain of around 1,000×. The proﬁler helps us focus our attention on the most time-consuming parts of the program and also provides useful information about the procedure call structure. Some of the bottlenecks in our code, such as using a quadratic sort routine, are easy to anticipate, while others, such as whether to append to the beginning or end of a list, emerge only through a careful analysis. We can see that proﬁling is a useful tool to have in the toolbox, but it should not be the only one. The timing measurements are imperfect, especially for shorter (less than 1 second) run times. More signiﬁcantly, the results apply only to the particular data tested. For example, if we had run the original function on data consisting of a smaller number of longer strings, we would have found that the lowercase conversion routine was the major performance bottleneck. Even worse, if it only proﬁled documents with short words, we might never detect hidden bottlenecks such as the quadratic performance of ÄÇÑ−Ëo. In general, proﬁling can help us optimize for typical cases, assuming we run the program on representative data, but we should also make sure the program will have respectable performance for all possible cases. This mainly involves avoiding algorithms (such as insertion sort) and bad programming practices (such as ÄÇÑ−Ëo) that yield poor asymptotic performance. Amdahl’s law, described in Section 1.9.1, provides some additional insights into the performance gains that can be obtained by targeted optimizations. For our n-gram code, we saw the total execution time drop from 209.0 to 5.4 seconds when we replaced insertion sort by quicksort. The initial version spent 203.7 of its 209.0 seconds performing insertion sort, giving α = 0.974, the fraction of time subject to speedup. With quicksort, the time spent sorting becomes negligible, giving a predicted speedup of 209/α = 39.0, close to the measured speedup of 38.5. We were able to gain a large speedup because sorting constituted a very large fraction of the overall execution time. However, when one bottleneck is eliminated, a new one arises, and so gaining additional speedup required focusing on other parts of the program. 5.15 Summary Although most presentations on code optimization describe how compilers can generate efﬁcient code, much can be done by an application programmer to assist the compiler in this task. No compiler can replace an inefﬁcient algorithm or data Bibliographic Notes 605 structure by a good one, and so these aspects of program design should remain a primary concern for programmers. We also have seen that optimization block- ers, such as memory aliasing and procedure calls, seriously restrict the ability of compilers to perform extensive optimizations. Again, the programmer must take primary responsibility for eliminating these. These should simply be considered parts of good programming practice, since they serve to eliminate unneeded work. Tuning performance beyond a basic level requires some understanding of the processor’s microarchitecture, describing the underlying mechanisms by which the processor implements its instruction set architecture. For the case of out- of-order processors, just knowing something about the operations, capabilities, latencies, and issue times of the functional units establishes a baseline for predict- ing program performance. We have studied a series of techniques—including loop unrolling, creating multiple accumulators, and reassociation—that can exploit the instruction-level parallelism provided by modern processors. As we get deeper into the optimiza- tion, it becomes important to study the generated assembly code and to try to understand how the computation is being performed by the machine. Much can be gained by identifying the critical paths determined by the data dependencies in the program, especially between the different iterations of a loop. We can also compute a throughput bound for a computation, based on the number of oper- ations that must be computed and the number and issue times of the units that perform those operations. Programs that involve conditional branches or complex interactions with the memory system are more difﬁcult to analyze and optimize than the simple loop programs we ﬁrst considered. The basic strategy is to try to make branches more predictable or make them amenable to implementation using conditional data transfers. We must also watch out for the interactions between store and load operations. Keeping values in local variables, allowing them to be stored in registers, can often be helpful. When working with large programs, it becomes important to focus our op- timization efforts on the parts that consume the most time. Code proﬁlers and related tools can help us systematically evaluate and improve program perfor- mance. We described gprof, a standard Unix proﬁling tool. More sophisticated proﬁlers are available, such as the vtune program development system from In- tel, and valgrind, commonly available on Linux systems. These tools can break down the execution time below the procedure level to estimate the performance of each basic block of the program. (A basic block is a sequence of instructions that has no transfers of control out of its middle, and so the block is always executed in its entirety.) Bibliographic Notes Our focus has been to describe code optimization from the programmer’s perspec- tive, demonstrating how to write code that will make it easier for compilers to gen- erate efﬁcient code. An extended paper by Chellappa, Franchetti, and P ¨uschel [19] 606 Chapter 5 Optimizing Program Performance takes a similar approach but goes into more detail with respect to the processor’s characteristics. Many publications describe code optimization from a compiler’s perspective, formulating ways that compilers can generate more efﬁcient code. Muchnick’s book is considered the most comprehensive [80]. Wadleigh and Crawford’s book on software optimization [115] covers some of the material we have presented, but it also describes the process of getting high performance on parallel machines. An early paper by Mahlke et al. [75] describes how several techniques developed for compilers that map programs onto parallel machines can be adapted to exploit the instruction-level parallelism of modern processors. This paper covers the code transformations we presented, including loop unrolling, multiple accumulators (which they refer to as accumulator variable expansion), and reassociation (which they refer to as tree height reduction). Our presentation of the operation of an out-of-order processor is fairly brief and abstract. More complete descriptions of the general principles can be found in advanced computer architecture textbooks, such as the one by Hennessy and Pat- terson [46, Ch. 2–3]. Shen and Lipasti’s book [100] provides an in-depth treatment of modern processor design. Homework Problems 5.13 ◆◆ Suppose we wish to write a procedure that computes the inner product of two vectors Ï and Ì. An abstract version of the function has a CPE of 14–18 with x86- 64 for different types of integer and ﬂoating-point data. By doing the same sort of transformations we did to transform the abstract program ²ÇÀ¾©Å−o into the more efﬁcient ²ÇÀ¾©Å−r, we get the following code: 1 mh 'ÅÅ−Ë ÉËÇ®Ï²Îl ¡²²ÏÀÏÄþÎ− ©Å Î−ÀÉÇËþËÔ hm 2 ÌÇ©® ©ÅÅ−ËrfÌ−²ˆÉÎË Ïj Ì−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÏgy 6 ®þÎþˆÎ hÏ®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÏgy 7 ®þÎþˆÎ hÌ®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 8 ®þÎþˆÎ ÍÏÀ { f®þÎþˆÎg ny 9 10 ðÇË f© { ny © z Ä−Å×Î³y ©iig Õ 11 ÍÏÀ { ÍÏÀ i Ï®þÎþ‰©` h Ì®þÎþ‰©`y 12 Û 13 h®−ÍÎ { ÍÏÀy 14 Û Our measurements show that this function has CPEs of 1.50 for integer data and 3.00 for ﬂoating-point data. For data type ®ÇÏ¾Ä−, the x86-64 assembly code for the inner loop is as follows: Homework Problems 607 Inner loop of inner4. data_t = double, OP = * udata in %rbp, vdata in %rax, sum in %xmm0 i in %rcx, limit in %rbx 1 l‹osx loop: 2 ÌÀÇÌÍ® nfcË¾ÉjcË²Ójvgj cÓÀÀo Get udata[i] 3 ÌÀÏÄÍ® fcËþÓjcË²Ójvgj cÓÀÀoj cÓÀÀo Multiply by vdata[i] 4 Ìþ®®Í® cÓÀÀoj cÓÀÀnj cÓÀÀn Add to sum 5 þ®®Ê boj cË²Ó Increment i 6 ²ÀÉÊ cË¾Ój cË²Ó Compare i:limit 7 ÁÅ− l‹os If !=, goto loop Assume that the functional units have the characteristics listed in Figure 5.12. A. Diagram how this instruction sequence would be decoded into operations and show how the data dependencies between them would create a critical path of operations, in the style of Figures 5.13 and 5.14. B. For data type ®ÇÏ¾Ä−, what lower bound on the CPE is determined by the critical path? C. Assuming similar instruction sequences for the integer code as well, what lower bound on the CPE is determined by the critical path for integer data? D. Explain how the ﬂoating-point versions can have CPEs of 3.00, even though the multiplication operation requires 5 clock cycles. 5.14 ◆ Write a version of the inner product procedure described in Problem 5.13 that uses 6 × 1 loop unrolling. For x86-64, our measurements of the unrolled version give a CPE of 1.07 for integer data but still 3.01 for both ﬂoating-point data. A. Explain why any (scalar) version of an inner product procedure running on an Intel Core i7 Haswell processor cannot achieve a CPE less than 1.00. B. Explain why the performance for ﬂoating-point data did not improve with loop unrolling. 5.15 ◆ Write a version of the inner product procedure described in Problem 5.13 that uses 6 × 6 loop unrolling. Our measurements for this function with x86-64 give a CPE of 1.06 for integer data and 1.01 for ﬂoating-point data. What factor limits the performance to a CPE of 1.00? 5.16 ◆ Write a version of the inner product procedure described in Problem 5.13 that uses 6 × 1a loop unrolling to enable greater parallelism. Our measurements for this function give a CPE of 1.10 for integer data and 1.05 for ﬂoating-point data. 5.17 ◆◆ The library function À−ÀÍ−Î has the following prototype: ÌÇ©® hÀ−ÀÍ−ÎfÌÇ©® hÍj ©ÅÎ ²j Í©Ö−ˆÎ Ågy 608 Chapter 5 Optimizing Program Performance This function ﬁlls Å bytes of the memory area starting at Í with copies of the low- order byte of ². For example, it can be used to zero out a region of memory by giving argument 0 for ², but other values are possible. The following is a straightforward implementation of À−ÀÍ−Î: 1 mh ¢þÍ©² ©ÀÉÄ−À−ÅÎþÎ©ÇÅ Çð À−ÀÍ−Î hm 2 ÌÇ©® h¾þÍ©²ˆÀ−ÀÍ−ÎfÌÇ©® hÍj ©ÅÎ ²j Í©Ö−ˆÎ Åg 3 Õ 4 Í©Ö−ˆÎ ²ÅÎ { ny 5 ÏÅÍ©×Å−® ²³þË hÍ²³þË { Íy 6 Ñ³©Ä− f²ÅÎ z Åg Õ 7 hÍ²³þËii { fÏÅÍ©×Å−® ²³þËg ²y 8 ²ÅÎiiy 9 Û 10 Ë−ÎÏËÅ Íy 11 Û Implement a more efﬁcient version of the function by using a word of data type ÏÅÍ©×Å−® ÄÇÅ× to pack eight copies of ², and then step through the region using word-level writes. You might ﬁnd it helpful to do additional loop unrolling as well. On our reference machine, we were able to reduce the CPE from 1.00 for the straightforward implementation to 0.127. That is, the program is able to write 8 bytes every clock cycle. Here are some additional guidelines. To ensure portability, let K denote the value of Í©Ö−ÇðfÏÅÍ©×Å−® ÄÇÅ×g for the machine on which you run your program. . You may not call any library functions. . Your code should work for arbitrary values of Å, including when it is not a multiple of K. You can do this in a manner similar to the way we ﬁnish the last few iterations with loop unrolling. . You should write your code so that it will compile and run correctly on any machine regardless of the value of K. Make use of the operation Í©Ö−Çð to do this. . On some machines, unaligned writes can be much slower than aligned ones. (On some non-x86 machines, they can even cause segmentation faults.) Write your code so that it starts with byte-level writes until the destination address is a multiple of K, then do word-level writes, and then (if necessary) ﬁnish with byte-level writes. . Beware of the case where ²ÅÎ is small enough that the upper bounds on some of the loops become negative. With expressions involving the Í©Ö−Çð operator, the testing may be performed with unsigned arithmetic. (See Sec- tion 2.2.8 and Problem 2.72.) 5.18 ◆◆◆ We considered the task of polynomial evaluation in Practice Problems 5.5 and 5.6, with both a direct evaluation and an evaluation by Horner’s method. Try to write Solutions to Practice Problems 609 faster versions of the function using the optimization techniques we have explored, including loop unrolling, parallel accumulation, and reassociation. You will ﬁnd many different ways of mixing together Horner’s scheme and direct evaluation with these optimization techniques. Ideally, you should be able to reach a CPE close to the throughput limit of your machine. Our best version achieves a CPE of 1.07 on our reference machine. 5.19 ◆◆◆ In Problem 5.12, we were able to reduce the CPE for the preﬁx-sum computation to 3.00, limited by the latency of ﬂoating-point addition on this machine. Simple loop unrolling does not improve things. Using a combination of loop unrolling and reassociation, write code for a preﬁx sum that achieves a CPE less than the latency of ﬂoating-point addition on your machine. Doing this requires actually increasing the number of additions performed. For example, our version with two-way unrolling requires three ad- ditions per iteration, while our version with four-way unrolling requires ﬁve. Our best implementation achieves a CPE of 1.67 on our reference machine. Determine how the throughput and latency limits of your machine limit the minimum CPE you can achieve for the preﬁx-sum operation. Solutions to Practice Problems Solution to Problem 5.1 (page 536) This problem illustrates some of the subtle effects of memory aliasing. As the following commented code shows, the effect will be to set the value at ÓÉ to zero: 4 hÓÉ { hÓÉ i hÓÉy mh pÓ hm 5 hÓÉ { hÓÉ k hÓÉy mh pÓkpÓ{nhm 6 hÓÉ { hÓÉ k hÓÉy mh nkn{nhm This example illustrates that our intuition about program behavior can often be wrong. We naturally think of the case where ÓÉ and ÔÉ are distinct but overlook the possibility that they might be equal. Bugs often arise due to conditions the programmer does not anticipate. Solution to Problem 5.2 (page 540) This problem illustrates the relationship between CPE and absolute performance. It can be solved using elementary algebra. We ﬁnd that for n ≤ 2, version 1 is the fastest. Version 2 is fastest for 3 ≤ n ≤ 7, and version 3 is fastest for n ≥ 8. Solution to Problem 5.3 (page 548) This is a simple exercise, but it is important to recognize that the four statements of a ðÇË loop—initial, test, update, and body—get executed different numbers of times. 610 Chapter 5 Optimizing Program Performance Code À©Å ÀþÓ ©Å²Ë ÍÊÏþË− A. 1 91 90 90 B. 91 1 90 90 C. 1 1 90 90 Solution to Problem 5.4 (page 552) This assembly code demonstrates a clever optimization opportunity detected by gcc. It is worth studying this code carefully to better understand the subtleties of code optimization. A. In the less optimized code, register cÓÀÀn is simply used as a temporary value, both set and used on each loop iteration. In the more optimized code, it is used more in the manner of variable þ²² in ²ÇÀ¾©Å−r, accumulating the product of the vector elements. The difference with ²ÇÀ¾©Å−r, however, is that location ®−ÍÎ is updated on each iteration by the second ÌÀÇÌÍ® instruction. We can see that this optimized version operates much like the following C code: 1 mh ›þÂ− ÍÏË− ®−ÍÎ ÏÉ®þÎ−® ÇÅ −þ²³ ©Î−ËþÎ©ÇÅ hm 2 ÌÇ©® ²ÇÀ¾©Å−qÑfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 3 Õ 4 ÄÇÅ× ©y 5 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 6 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 7 ®þÎþˆÎ þ²² { '⁄¥ﬁ¶y 8 9 mh 'Å©Î©þÄ©Ö− ©Å −Ì−ÅÎ Ä−Å×Î³ z{ n hm 10 h®−ÍÎ { þ²²y 11 12 ðÇË f© { ny © z Ä−Å×Î³y ©iig Õ 13 þ²² { þ²² ﬂ– ®þÎþ‰©`y 14 h®−ÍÎ { þ²²y 15 Û 16 Û B. The two versions of ²ÇÀ¾©Å−q will have identical functionality, even with memory aliasing. C. This transformation can be made without changing the program behavior, because, with the exception of the ﬁrst iteration, the value read from ®−ÍÎ at the beginning of each iteration will be the same value written to this register Solutions to Practice Problems 611 at the end of the previous iteration. Therefore, the combining instruction can simply use the value already in cÓÀÀn at the beginning of the loop. Solution to Problem 5.5 (page 566) Polynomial evaluation is a core technique for solving many problems. For example, polynomial functions are commonly used to approximate trigonometric functions in math libraries. A. The function performs 2n multiplications and n additions. B. We can see that the performance-limiting computation here is the repeated computation of the expression ÓÉÑË{Óh ÓÉÑË. This requires a ﬂoating- point multiplication (5 clock cycles), and the computation for one iteration cannot begin until the one for the previous iteration has completed. The updating of Ë−ÍÏÄÎ only requires a ﬂoating-point addition (3 clock cycles) between successive iterations. Solution to Problem 5.6 (page 566) This problem demonstrates that minimizing the number of operations in a com- putation may not improve its performance. A. The function performs n multiplications and n additions, half the number of multiplications as the original function ÉÇÄÔ. B. We can see that the performance-limiting computation here is the repeated computation of the expression Ë−ÍÏÄÎ { þ‰©` i ÓhË−ÍÏÄÎ. Starting from the value of Ë−ÍÏÄÎ from the previous iteration, we must ﬁrst multiply it by Ó (5 clock cycles) and then add it to þ‰©` (3 cycles) before we have the value for this iteration. Thus, each iteration imposes a minimum latency of 8 cycles, exactly our measured CPE. C. Although each iteration in function ÉÇÄÔ requires two multiplications rather than one, only a single multiplication occurs along the critical path per iteration. Solution to Problem 5.7 (page 568) The following code directly follows the rules we have stated for unrolling a loop by some factor k: 1 ÌÇ©® ÏÅËÇÄÄsfÌ−²ˆÉÎË Ìj ®þÎþˆÎ h®−ÍÎg 2 Õ 3 ÄÇÅ× ©y 4 ÄÇÅ× Ä−Å×Î³ { Ì−²ˆÄ−Å×Î³fÌgy 5 ÄÇÅ× Ä©À©Î { Ä−Å×Î³kry 6 ®þÎþˆÎ h®þÎþ { ×−ÎˆÌ−²ˆÍÎþËÎfÌgy 7 ®þÎþˆÎ þ²² { '⁄¥ﬁ¶y 8 612 Chapter 5 Optimizing Program Performance 9 mh £ÇÀ¾©Å− s −Ä−À−ÅÎÍ þÎ þ Î©À− hm 10 ðÇË f© { ny © z Ä©À©Îy ©i{sg Õ 11 þ²² { þ²² ﬂ– ®þÎþ‰©` ﬂ– ®þÎþ‰©io`y 12 þ²² { þ²² ﬂ– ®þÎþ‰©ip` ﬂ– ®þÎþ‰©iq`y 13 þ²² { þ²² ﬂ– ®þÎþ‰©ir`y 14 Û 15 16 mh ƒ©Å©Í³ þÅÔ Ë−Àþ©Å©Å× −Ä−À−ÅÎÍ hm 17 ðÇË fy © z Ä−Å×Î³y ©iig Õ 18 þ²² { þ²² ﬂ– ®þÎþ‰©`y 19 Û 20 h®−ÍÎ { þ²²y 21 Û Solution to Problem 5.8 (page 581) This problem demonstrates how small changes in a program can yield dramatic performance differences, especially on a machine with out-of-order execution. Figure 5.39 diagrams the three multiplication operations for a single iteration of the function. In this ﬁgure, the operations shown as blue boxes are along the critical path—they need to be computed in sequence to compute a new value for loop variable Ë. The operations shown as light boxes can be computed in parallel with the critical path operations. For a loop with P operations along the critical path, each iteration will require a minimum of 5P clock cycles and will compute the product for three elements, giving a lower bound on the CPE of 5P/3. This implies lower bounds of 5.00 for A1, 3.33 for A2 and A5, and 1.67 for A3 and A4. We ran these functions on an Intel Core i7 Haswell processor and found that it could achieve these CPE values. Solution to Problem 5.9 (page 589) This is another demonstration that a slight change in coding style can make it much easier for the compiler to detect opportunities to use conditional moves: Ñ³©Ä− f©ozÅdd©pzÅgÕ ÄÇÅ× Ìo { ÍË²o‰©o`y r A1: ((r*x)*y)*z r x y z * * * * r A2: (r*(x*y))*z r x y z * * * r A3: r*((x*y)*z) r x y z * * * r A4: r*(x*(y*z)) r x y z * * * r A5: (r*x)*(y*z) r x y z * * Figure 5.39 Data dependencies among multiplication operations for cases in Problem 5.8. The operations shown as blue boxes form the critical paths for the iterations. Solutions to Practice Problems 613 ÄÇÅ× Ìp { ÍË²p‰©p`y ÄÇÅ× ÎþÂ−o { Ìo z Ìpy ®−ÍÎ‰©®ii` { ÎþÂ−o } Ìo x Ìpy ©o i{ ÎþÂ−oy ©p i{ fokÎþÂ−ogy Û We measured a CPE of around 12.0 for this version of the code, a modest improve- ment over the original CPE of 15.0. Solution to Problem 5.10 (page 595) This problem requires you to analyze the potential load-store interactions in a program. A. It will set each element þ‰i` to i + 1, for 0 ≤ i ≤ 998. B. It will set each element þ‰i` to 0, for 1 ≤ i ≤ 999. C. In the second case, the load of one iteration depends on the result of the store from the previous iteration. Thus, there is a write/read dependency between successive iterations. D. It will give a CPE of 1.2, the same as for Example A, since there are no dependencies between stores and subsequent loads. Solution to Problem 5.11 (page 597) We can see that this function has a write/read dependency between successive iterations—the destination value É‰©` on one iteration matches the source value É‰©ko` on the next. A critical path is therefore formed for each iteration consisting of a store (from the previous iteration), a load, and a ﬂoating-point addition. The CPE measurement of 9.0 is consistent with our measurement of 7.3 for the CPE of ÑË©Î−ˆË−þ® when there is a data dependency, since ÑË©Î−ˆË−þ® involves an integer addition (1 clock-cycle latency), while ÉÍÏÀo involves a ﬂoating-point addition (3 clock-cycle latency). Solution to Problem 5.12 (page 597) Here is a revised version of the function: 1 ÌÇ©® ÉÍÏÀoþfðÄÇþÎ þ‰`j ðÄÇþÎ É‰`j ÄÇÅ× Åg 2 Õ 3 ÄÇÅ× ©y 4 mh ÄþÍÎˆÌþÄ ³ÇÄ®Í É‰©ko`y ÌþÄ ³ÇÄ®Í É‰©` hm 5 ðÄÇþÎ ÄþÍÎˆÌþÄj ÌþÄy 6 ÄþÍÎˆÌþÄ { É‰n` { þ‰n`y 7 ðÇËf©{oy©zÅy ©iig Õ 8 ÌþÄ { ÄþÍÎˆÌþÄ i þ‰©`y 9 É‰©` { ÌþÄy 10 ÄþÍÎˆÌþÄ { ÌþÄy 11 Û 12 Û 614 Chapter 5 Optimizing Program Performance We introduce a local variable ÄþÍÎˆÌþÄ. At the start of iteration ©, it holds the value of É‰©ko`. We then compute ÌþÄ to be the value of É‰©` and to be the new value for ÄþÍÎˆÌþÄ. This version compiles to the following assembly code: Inner loop of psum1a a in %rdi, i in %rax, cnt in %rdx, last_val in %xmm0 1 l‹otx loop: 2 Ìþ®®ÍÍ fcË®©jcËþÓjrgj cÓÀÀnj cÓÀÀn last_val = val = last_val + a[i] 3 ÌÀÇÌÍÍ cÓÀÀnj fcËÍ©jcËþÓjrg Store val in p[i] 4 þ®®Ê boj cËþÓ Increment i 5 ²ÀÉÊ cË®Ój cËþÓ Compare i:cnt 6 ÁÅ− l‹ot If !=, goto loop This code holds ÄþÍÎˆÌþÄ in cÓÀÀn, avoiding the need to read É‰©ko` from memory and thus eliminating the write/read dependency seen in ÉÍÏÀo. CHAPTER 6 The Memory Hierarchy 6.1 Storage Technologies 617 6.2 Locality 640 6.3 The Memory Hierarchy 645 6.4 Cache Memories 650 6.5 Writing Cache-Friendly Code 669 6.6 Putting It Together: The Impact of Caches on Program Performance 675 6.7 Summary 684 Bibliographic Notes 684 Homework Problems 685 Solutions to Practice Problems 696 615 616 Chapter 6 The Memory Hierarchy T o this point in our study of systems, we have relied on a simple model of a computer system as a CPU that executes instructions and a memory system that holds instructions and data for the CPU. In our simple model, the memory system is a linear array of bytes, and the CPU can access each memory location in a constant amount of time. While this is an effective model up to a point, it does not reﬂect the way that modern systems really work. In practice, a memory system is a hierarchy of storage devices with different capacities, costs, and access times. CPU registers hold the most frequently used data. Small, fast cache memories nearby the CPU act as staging areas for a subset of the data and instructions stored in the relatively slow main memory. The main memory stages data stored on large, slow disks, which in turn often serve as staging areas for data stored on the disks or tapes of other machines connected by networks. Memory hierarchies work because well-written programs tend to access the storage at any particular level more frequently than they access the storage at the next lower level. So the storage at the next level can be slower, and thus larger and cheaper per bit. The overall effect is a large pool of memory that costs as much as the cheap storage near the bottom of the hierarchy but that serves data to programs at the rate of the fast storage near the top of the hierarchy. As a programmer, you need to understand the memory hierarchy because it has a big impact on the performance of your applications. If the data your program needs are stored in a CPU register, then they can be accessed in 0 cycles during the execution of the instruction. If stored in a cache, 4 to 75 cycles. If stored in main memory, hundreds of cycles. And if stored in disk, tens of millions of cycles! Here, then, is a fundamental and enduring idea in computer systems: if you understand how the system moves data up and down the memory hierarchy, then you can write your application programs so that their data items are stored higher in the hierarchy, where the CPU can access them more quickly. This idea centers around a fundamental property of computer programs known as locality. Programs with good locality tend to access the same set of data items over and over again, or they tend to access sets of nearby data items. Programs with good locality tend to access more data items from the upper levels of the memory hierarchy than programs with poor locality, and thus run faster. For example, on our Core i7 system, the running times of different matrix mul- tiplication kernels that perform the same number of arithmetic operations, but have different degrees of locality, can vary by a factor of almost 40! In this chapter, we will look at the basic storage technologies—SRAM mem- ory, DRAM memory, ROM memory, and rotating and solid state disks—and describe how they are organized into hierarchies. In particular, we focus on the cache memories that act as staging areas between the CPU and main memory, be- cause they have the most impact on application program performance. We show you how to analyze your C programs for locality, and we introduce techniques for improving the locality in your programs. You will also learn an interesting way to characterize the performance of the memory hierarchy on a particular machine as a “memory mountain” that shows read access times as a function of locality. Section 6.1 Storage Technologies 617 6.1 Storage Technologies Much of the success of computer technology stems from the tremendous progress in storage technology. Early computers had a few kilobytes of random access memory. The earliest IBM PCs didn’t even have a hard disk. That changed with the introduction of the IBM PC-XT in 1982, with its 10-megabyte disk. By the year 2015, typical machines had 300,000 times as much disk storage, and the amount of storage was increasing by a factor of 2 every couple of years. 6.1.1 Random Access Memory Random access memory (RAM) comes in two varieties—static and dynamic. Static RAM (SRAM) is faster and signiﬁcantly more expensive than dynamic RAM (DRAM). SRAM is used for cache memories, both on and off the CPU chip. DRAM is used for the main memory plus the frame buffer of a graphics system. Typically, a desktop system will have no more than a few tens of megabytes of SRAM, but hundreds or thousands of megabytes of DRAM. Static RAM SRAM stores each bit in a bistable memory cell. Each cell is implemented with a six-transistor circuit. This circuit has the property that it can stay indeﬁnitely in either of two different voltage conﬁgurations, or states. Any other state will be unstable—starting from there, the circuit will quickly move toward one of the stable states. Such a memory cell is analogous to the inverted pendulum illustrated in Figure 6.1. The pendulum is stable when it is tilted either all the way to the left or all the way to the right. From any other position, the pendulum will fall to one side or the other. In principle, the pendulum could also remain balanced in a vertical position indeﬁnitely, but this state is metastable—the smallest disturbance would make it start to fall, and once it fell it would never return to the vertical position. Due to its bistable nature, an SRAM memory cell will retain its value indef- initely, as long as it is kept powered. Even when a disturbance, such as electrical noise, perturbs the voltages, the circuit will return to the stable value when the disturbance is removed. Figure 6.1 Inverted pendulum. Like an SRAM cell, the pendulum has only two stable conﬁgurations, or states. Stable left Stable rightUnstable 618 Chapter 6 The Memory Hierarchy Transistors Relative Relative per bit access time Persistent? Sensitive? cost Applications SRAM 6 1× Yes No 1,000× Cache memory DRAM 1 10× No Yes 1× Main memory, frame buffers Figure 6.2 Characteristics of DRAM and SRAM memory. Dynamic RAM DRAM stores each bit as charge on a capacitor. This capacitor is very small— typically around 30 femtofarads—that is, 30 × 10−15 farads. Recall, however, that a farad is a very large unit of measure. DRAM storage can be made very dense— each cell consists of a capacitor and a single access transistor. Unlike SRAM, however, a DRAM memory cell is very sensitive to any disturbance. When the capacitor voltage is disturbed, it will never recover. Exposure to light rays will cause the capacitor voltages to change. In fact, the sensors in digital cameras and camcorders are essentially arrays of DRAM cells. Various sources of leakage current cause a DRAM cell to lose its charge within a time period of around 10 to 100 milliseconds. Fortunately, for computers operating with clock cycle times measured in nanoseconds, this retention time is quite long. The memory system must periodically refresh every bit of memory by reading it out and then rewriting it. Some systems also use error-correcting codes, where the computer words are encoded using a few more bits (e.g., a 64-bit word might be encoded using 72 bits), such that circuitry can detect and correct any single erroneous bit within a word. Figure 6.2 summarizes the characteristics of SRAM and DRAM memory. SRAM is persistent as long as power is applied. Unlike DRAM, no refresh is necessary. SRAM can be accessed faster than DRAM. SRAM is not sensitive to disturbances such as light and electrical noise. The trade-off is that SRAM cells use more transistors than DRAM cells and thus have lower densities, are more expensive, and consume more power. Conventional DRAMs The cells (bits) in a DRAM chip are partitioned into d supercells, each consisting of w DRAM cells. A d × w DRAM stores a total of dw bits of information. The supercells are organized as a rectangular array with r rows and c columns, where rc = d. Each supercell has an address of the form (i, j ), where i denotes the row and j denotes the column. For example, Figure 6.3 shows the organization of a 16 × 8 DRAM chip with d = 16 supercells, w = 8 bits per supercell, r = 4 rows, and c = 4 columns. The shaded box denotes the supercell at address (2, 1). Information ﬂows in and out of the chip via external connectors called pins. Each pin carries a 1-bit signal. Figure 6.3 shows two of these sets of pins: eight ®þÎþ pins that can transfer 1 byte Section 6.1 Storage Technologies 619 Aside A note on terminology The storage community has never settled on a standard name for a DRAM array element. Computer architects tend to refer to it as a “cell,” overloading the term with the DRAM storage cell. Circuit designers tend to refer to it as a “word,” overloading the term with a word of main memory. To avoid confusion, we have adopted the unambiguous term “supercell.” Figure 6.3 High-level view of a 128-bit 16 × 8 DRAM chip. Memory controller 2 addr 8 data (to CPU) DRAM chip Cols 0 0 1 2 3 123 Supercell (2,1) Internal row buffer Rows in or out of the chip, and two þ®®Ë pins that carry two-bit row and column supercell addresses. Other pins that carry control information are not shown. Each DRAM chip is connected to some circuitry, known as the memory controller, that can transfer w bits at a time to and from each DRAM chip. To read the contents of supercell (i, j ), the memory controller sends the row address i to the DRAM, followed by the column address j . The DRAM responds by sending the contents of supercell (i, j ) back to the controller. The row address i is called a RAS (row access strobe) request. The column address j is called a CAS (column access strobe) request. Notice that the RAS and CAS requests share the same DRAM address pins. For example, to read supercell (2, 1) from the 16 × 8 DRAM in Figure 6.3, the memory controller sends row address 2, as shown in Figure 6.4(a). The DRAM responds by copying the entire contents of row 2 into an internal row buffer. Next, the memory controller sends column address 1, as shown in Figure 6.4(b). The DRAM responds by copying the 8 bits in supercell (2, 1) from the row buffer and sending them to the memory controller. One reason circuit designers organize DRAMs as two-dimensional arrays instead of linear arrays is to reduce the number of address pins on the chip. For example, if our example 128-bit DRAM were organized as a linear array of 16 supercells with addresses 0 to 15, then the chip would need four address pins instead of two. The disadvantage of the two-dimensional array organization is that addresses must be sent in two distinct steps, which increases the access time. 620 Chapter 6 The Memory Hierarchy Memory controller RASRAS == 22 8 data DRAM chip Cols 0 0 1 2 3 12 3 Internal row buffer (a) Select row 2 (RAS request). Rows Row 2 2 addr (b) Select column 1 (CAS request). Memory controller 2 CASCAS == 11 addr 8 data Supercell (2,1) DRAM chip Cols 0 0 1 2 3 12 3 Internal row buffer Rows Figure 6.4 Reading the contents of a DRAM supercell. Memory Modules DRAM chips are packaged in memory modules that plug into expansion slots on the main system board (motherboard). Core i7 systems use the 240-pin dual inline memory module (DIMM), which transfers data to and from the memory controller in 64-bit chunks. Figure 6.5 shows the basic idea of a memory module. The example module stores a total of 64 MB (megabytes) using eight 64-Mbit 8M × 8 DRAM chips, numbered 0 to 7. Each supercell stores 1 byte of main memory, and each 64-bit word at byte address A in main memory is represented by the eight supercells whose corresponding supercell address is (i, j ). In the example in Figure 6.5, DRAM 0 stores the ﬁrst (lower-order) byte, DRAM 1 stores the next byte, and so on. To retrieve the word at memory address A, the memory controller converts A to a supercell address (i, j ) and sends it to the memory module, which then broadcasts i and j to each DRAM. In response, each DRAM outputs the 8-bit contents of its (i, j ) supercell. Circuitry in the module collects these outputs and forms them into a 64-bit word, which it returns to the memory controller. Main memory can be aggregated by connecting multiple memory modules to the memory controller. In this case, when the controller receives an address A, the controller selects the module k that contains A, converts A to its (i, j ) form, and sends (i, j ) to module k. Practice Problem 6.1 (solution page 696) In the following, let r be the number of rows in a DRAM array, c the number of columns, br the number of bits needed to address the rows, and bc the number of bits needed to address the columns. For each of the following DRAMs, determine the power-of-2 array dimensions that minimize max(br,bc), the maximum number of bits needed to address the rows or columns of the array. Section 6.1 Storage Technologies 621 Figure 6.5 Reading the contents of a memory module. addr (row = i, col = j) DRAM 7 DRAM 0 data : Supercell (i, j) 64 MB memory module consisting of eight 8M × 8 DRAMs Memory controller 64-bit word to CPU chip 64-bit word at main memory address A Bits 0-7 Bits 8-15 Bits 16-23 Bits 24-31 Bits 32-39 Bits 40-47 Bits 48-55 Bits 56-63 63 56 55 48 47 40 39 32 3124 23 16 15 8 7 0 Organization rc br bc max(br,bc) 16 × 1 16 × 4 128 × 8 512 × 4 1,024 × 4 Enhanced DRAMs There are many kinds of DRAM memories, and new kinds appear on the market with regularity as manufacturers attempt to keep up with rapidly increasing pro- cessor speeds. Each is based on the conventional DRAM cell, with optimizations that improve the speed with which the basic DRAM cells can be accessed. Fast page mode DRAM (FPM DRAM). A conventional DRAM copies an entire row of supercells into its internal row buffer, uses one, and then discards the rest. FPM DRAM improves on this by allowing consecutive accesses to the same row to be served directly from the row buffer. For example, to read four supercells from row i of a conventional DRAM, the memory controller must send four RAS/CAS requests, even though the row address i is identical in each case. To read supercells from the same row of an FPM DRAM, the memory controller sends an initial RAS/CAS request, followed by three CAS requests. The initial RAS/CAS request copies row i into the row buffer and returns the supercell addressed by the 622 Chapter 6 The Memory Hierarchy CAS. The next three supercells are served directly from the row buffer, and thus are returned more quickly than the initial supercell. Extended data out DRAM (EDO DRAM). An enhanced form of FPM DRAM that allows the individual CAS signals to be spaced closer to- gether in time. Synchronous DRAM (SDRAM). Conventional, FPM, and EDO DRAMs are asynchronous in the sense that they communicate with the memory con- troller using a set of explicit control signals. SDRAM replaces many of these control signals with the rising edges of the same external clock sig- nal that drives the memory controller. Without going into detail, the net effect is that an SDRAM can output the contents of its supercells at a faster rate than its asynchronous counterparts. Double Data-Rate Synchronous DRAM (DDR SDRAM). DDR SDRAM is an enhancement of SDRAM that doubles the speed of the DRAM by using both clock edges as control signals. Different types of DDR SDRAMs are characterized by the size of a small prefetch buffer that increases the effective bandwidth: DDR (2 bits), DDR2 (4 bits), and DDR3 (8 bits). Video RAM (VRAM). Used in the frame buffers of graphics systems. VRAM is similar in spirit to FPM DRAM. Two major differences are that (1) VRAM output is produced by shifting the entire contents of the internal buffer in sequence and (2) VRAM allows concurrent reads and writes to the memory. Thus, the system can be painting the screen with the pixels in the frame buffer (reads) while concurrently writing new values for the next update (writes). Nonvolatile Memory DRAMs and SRAMs are volatile in the sense that they lose their information if the supply voltage is turned off. Nonvolatile memories, on the other hand, retain their information even when they are powered off. There are a variety of nonvolatile memories. For historical reasons, they are referred to collectively as read-only memories (ROMs), even though some types of ROMs can be written to as well as read. ROMs are distinguished by the number of times they can be reprogrammed (written to) and by the mechanism for reprogramming them. Aside Historical popularity of DRAM technologies Until 1995, most PCs were built with FPM DRAMs. From 1996 to 1999, EDO DRAMs dominated the market, while FPM DRAMs all but disappeared. SDRAMs ﬁrst appeared in 1995 in high-end systems, and by 2002 most PCs were built with SDRAMs and DDR SDRAMs. By 2010, most server and desktop systems were built with DDR3 SDRAMs. In fact, the Intel Core i7 supports only DDR3 SDRAM. Section 6.1 Storage Technologies 623 A programmable ROM (PROM) can be programmed exactly once. PROMs include a sort of fuse with each memory cell that can be blown once by zapping it with a high current. An erasable programmable ROM (EPROM) has a transparent quartz window that permits light to reach the storage cells. The EPROM cells are cleared to zeros by shining ultraviolet light through the window. Programming an EPROM is done by using a special device to write ones into the EPROM. An EPROM can be erased and reprogrammed on the order of 1,000 times. An electrically erasable PROM (EEPROM) is akin to an EPROM, but it does not require a physically separate programming device, and thus can be reprogrammed in-place on printed circuit cards. An EEPROM can be reprogrammed on the order of 105 times before it wears out. Flash memory is a type of nonvolatile memory, based on EEPROMs, that has become an important storage technology. Flash memories are everywhere, providing fast and durable nonvolatile storage for a slew of electronic devices, including digital cameras, cell phones, and music players, as well as laptop, desktop, and server computer systems. In Section 6.1.3, we will look in detail at a new form of ﬂash-based disk drive, known as a solid state disk (SSD), that provides a faster, sturdier, and less power-hungry alternative to conventional rotating disks. Programs stored in ROM devices are often referred to as ﬁrmware. When a computer system is powered up, it runs ﬁrmware stored in a ROM. Some systems provide a small set of primitive input and output functions in ﬁrmware—for example, a PC’s BIOS (basic input/output system) routines. Complicated devices such as graphics cards and disk drive controllers also rely on ﬁrmware to translate I/O (input/output) requests from the CPU. Accessing Main Memory Data ﬂows back and forth between the processor and the DRAM main memory over shared electrical conduits called buses. Each transfer of data between the CPU and memory is accomplished with a series of steps called a bus transaction. A read transaction transfers data from the main memory to the CPU. A write transaction transfers data from the CPU to the main memory. A bus is a collection of parallel wires that carry address, data, and control signals. Depending on the particular bus design, data and address signals can share the same set of wires or can use different sets. Also, more than two devices can share the same bus. The control wires carry signals that synchronize the transaction and identify what kind of transaction is currently being performed. For example, is this transaction of interest to the main memory, or to some other I/O device such as a disk controller? Is the transaction a read or a write? Is the information on the bus an address or a data item? Figure 6.6 shows the conﬁguration of an example computer system. The main components are the CPU chip, a chipset that we will call an I/O bridge (which includes the memory controller), and the DRAM memory modules that make up main memory. These components are connected by a pair of buses: a system bus that connects the CPU to the I/O bridge, and a memory bus that connects the I/O 624 Chapter 6 The Memory Hierarchy Aside A note on bus designs Bus design is a complex and rapidly changing aspect of computer systems. Different vendors develop different bus architectures as a way to differentiate their products. For example, some Intel systems use chipsets known as the northbridge and the southbridge to connect the CPU to memory and I/O devices, respectively. In older Pentium and Core 2 systems, a front side bus (FSB) connects the CPU to the northbridge. Systems from AMD replace the FSB with the HyperTransport interconnect, while newer Intel Core i7 systems use the QuickPath interconnect. The details of these different bus architectures are beyond the scope of this text. Instead, we will use the high-level bus architecture from Figure 6.6 as a running example throughout. It is a simple but useful abstraction that allows us to be concrete. It captures the main ideas without being tied too closely to the detail of any proprietary designs. Figure 6.6 Example bus structure that connects the CPU and main memory. CPU chip Register file System bus Memory bus Main memoryBus interface I/O bridge ALU bridge to the main memory. The I/O bridge translates the electrical signals of the system bus into the electrical signals of the memory bus. As we will see, the I/O bridge also connects the system bus and memory bus to an I/O bus that is shared by I/O devices such as disks and graphics cards. For now, though, we will focus on the memory bus. Consider what happens when the CPU performs a load operation such as ÀÇÌÊ ¡jcËþÓ where the contents of address A are loaded into register cËþÓ. Circuitry on the CPU chip called the bus interface initiates a read transaction on the bus. The read transaction consists of three steps. First, the CPU places the address A on the system bus. The I/O bridge passes the signal along to the memory bus (Figure 6.7(a)). Next, the main memory senses the address signal on the memory bus, reads the address from the memory bus, fetches the data from the DRAM, and writes the data to the memory bus. The I/O bridge translates the memory bus signal into a system bus signal and passes it along to the system bus (Figure 6.7(b)). Finally, the CPU senses the data on the system bus, reads the data from the bus, and copies the data to register cËþÓ (Figure 6.7(c)). Conversely, when the CPU performs a store operation such as ÀÇÌÊ cËþÓj¡ Section 6.1 Storage Technologies 625 Figure 6.7 Memory read transaction for a load operation: ÀÇÌÊ ¡jcËþÓ. (a) CPU places address A on the memory bus. Main memory Bus interface Register file I/O bridge ALU A X 0 A %rax (b) Main memory reads A from the bus, retrieves word x, and places it on the bus. Register file Main memory Bus interface I/O bridge ALU x X 0 A %rax (c) CPU reads word x from the bus, and copies it into register %rax. Register file Main memory Bus interface I/O bridge ALU X X 0 A %rax where the contents of register cËþÓ are written to address A, the CPU initiates a write transaction. Again, there are three basic steps. First, the CPU places the address on the system bus. The memory reads the address from the memory bus and waits for the data to arrive (Figure 6.8(a)). Next, the CPU copies the data in cËþÓ to the system bus (Figure 6.8(b)). Finally, the main memory reads the data from the memory bus and stores the bits in the DRAM (Figure 6.8(c)). 6.1.2 Disk Storage Disks are workhorse storage devices that hold enormous amounts of data, on the order of hundreds to thousands of gigabytes, as opposed to the hundreds or thousands of megabytes in a RAM-based memory. However, it takes on the order of milliseconds to read information from a disk, a hundred thousand times longer than from DRAM and a million times longer than from SRAM. 626 Chapter 6 The Memory Hierarchy (a) CPU places address A on the memory bus. Main memory reads it and waits for the data word. Register file Main memory Bus interface I/O bridge ALU A y 0 A %rax (b) CPU places data word y on the bus. Register file Main memory Bus interface I/O bridge ALU y y 0 A %rax (c) Main memory reads data word y from the bus and stores it at address A. Register file Main memory Bus interface I/O bridge ALU y 0 Ay %rax Figure 6.8 Memory write transaction for a store operation: ÀÇÌÊ cËþÓj¡. Disk Geometry Disks are constructed from platters. Each platter consists of two sides, or surfaces, that are coated with magnetic recording material. A rotating spindle in the center of the platter spins the platter at a ﬁxed rotational rate, typically between 5,400 and 15,000 revolutions per minute (RPM). A disk will typically contain one or more of these platters encased in a sealed container. Figure 6.9(a) shows the geometry of a typical disk surface. Each surface consists of a collection of concentric rings called tracks. Each track is partitioned into a collection of sectors. Each sector contains an equal number of data bits (typically 512 bytes) encoded in the magnetic material on the sector. Sectors are separated by gaps where no data bits are stored. Gaps store formatting bits that identify sectors. Section 6.1 Storage Technologies 627 Tracks (a) Single-platter view Track k Gaps Surface Spindle Sectors (b) Multiple-platter view Cylinder k Platter 0 Surface 0 Surface 1 Surface 2 Platter 1 Platter 2 Spindle Surface 3 Surface 4 Surface 5 Figure 6.9 Disk geometry. A disk consists of one or more platters stacked on top of each other and encased in a sealed package, as shown in Figure 6.9(b). The entire assembly is often referred to as a disk drive, although we will usually refer to it as simply a disk. We will sometimes refer to disks as rotating disks to distinguish them from ﬂash-based solid state disks (SSDs), which have no moving parts. Disk manufacturers describe the geometry of multiple-platter drives in terms of cylinders, where a cylinder is the collection of tracks on all the surfaces that are equidistant from the center of the spindle. For example, if a drive has three platters and six surfaces, and the tracks on each surface are numbered consistently, then cylinder k is the collection of the six instances of track k. Disk Capacity The maximum number of bits that can be recorded by a disk is known as its max- imum capacity, or simply capacity. Disk capacity is determined by the following technology factors: Recording density (bits/in). The number of bits that can be squeezed into a 1- inch segment of a track. Track density (tracks/in). The number of tracks that can be squeezed into a 1-inch segment of the radius extending from the center of the platter. Areal density (bits/in2). The product of the recording density and the track density. Disk manufacturers work tirelessly to increase areal density (and thus capac- ity), and this is doubling every couple of years. The original disks, designed in an age of low areal density, partitioned every track into the same number of sec- tors, which was determined by the number of sectors that could be recorded on the innermost track. To maintain a ﬁxed number of sectors per track, the sectors were spaced farther apart on the outer tracks. This was a reasonable approach 628 Chapter 6 The Memory Hierarchy Aside How much is a gigabyte? Unfortunately, the meanings of preﬁxes such as kilo (K), mega (M), giga (G), and tera (T) depend on the context. For measures that relate to the capacity of DRAMs and SRAMs, typically K = 210, M = 220,G = 230, and T = 240. For measures related to the capacity of I/O devices such as disks and networks, typically K = 103,M = 106,G = 109, and T = 1012. Rates and throughputs usually use these preﬁx values as well. Fortunately, for the back-of-the-envelope estimates that we typically rely on, either assumption works ﬁne in practice. For example, the relative difference between 230 and 109 is not that large: (230 − 109)/109 ≈ 7%. Similarly, (240 − 1012)/1012 ≈ 10%. when areal densities were relatively low. However, as areal densities increased, the gaps between sectors (where no data bits were stored) became unacceptably large. Thus, modern high-capacity disks use a technique known as multiple zone recording, where the set of cylinders is partitioned into disjoint subsets known as recording zones. Each zone consists of a contiguous collection of cylinders. Each track in each cylinder in a zone has the same number of sectors, which is deter- mined by the number of sectors that can be packed into the innermost track of the zone. The capacity of a disk is given by the following formula: Capacity = # bytes sector × average # sectors track × # tracks surface × # surfaces platter × # platters disk For example, suppose we have a disk with ﬁve platters, 512 bytes per sector, 20,000 tracks per surface, and an average of 300 sectors per track. Then the capacity of the disk is Capacity = 512 bytes sector × 300 sectors track × 20,000 tracks surface × 2 surfaces platter × 5 platters disk = 30,720,000,000 bytes = 30.72 GB Notice that manufacturers express disk capacity in units of gigabytes (GB) or terabytes (TB), where 1 GB = 109 bytes and 1 TB = 1012 bytes. Practice Problem 6.2 (solution page 697) What is the capacity of a disk with 3 platters, 15,000 cylinders, an average of 500 sectors per track, and 1,024 bytes per sector? Disk Operation Disks read and write bits stored on the magnetic surface using a read/write head connected to the end of an actuator arm, as shown in Figure 6.10(a). By moving Section 6.1 Storage Technologies 629 Spindle The disk surface spins at a fixed rotational rate. The read/write head is attached to the end of the arm and flies over the disk surface on a thin cushion of air. By moving radially, the arm can position the read/write head over any track. (a) Single-platter view Read/write heads Arm Spindle (b) Multiple-platter view Figure 6.10 Disk dynamics. the arm back and forth along its radial axis, the drive can position the head over any track on the surface. This mechanical motion is known as a seek. Once the head is positioned over the desired track, then, as each bit on the track passes underneath, the head can either sense the value of the bit (read the bit) or alter the value of the bit (write the bit). Disks with multiple platters have a separate read/write head for each surface, as shown in Figure 6.10(b). The heads are lined up vertically and move in unison. At any point in time, all heads are positioned on the same cylinder. The read/write head at the end of the arm ﬂies (literally) on a thin cushion of air over the disk surface at a height of about 0.1 microns and a speed of about 80 km/h. This is analogous to placing a skyscraper on its side and ﬂying it around the world at a height of 2.5 cm (1 inch) above the ground, with each orbit of the earth taking only 8 seconds! At these tolerances, a tiny piece of dust on the surface is like a huge boulder. If the head were to strike one of these boulders, the head would cease ﬂying and crash into the surface (a so-called head crash). For this reason, disks are always sealed in airtight packages. Disks read and write data in sector-size blocks. The access time for a sector has three main components: seek time, rotational latency, and transfer time: Seek time. To read the contents of some target sector, the arm ﬁrst positions the head over the track that contains the target sector. The time required to move the arm is called the seek time. The seek time, Tseek, depends on the previous position of the head and the speed that the arm moves across the surface. The average seek time in modern drives, Tavg seek, measured by taking the mean of several thousand seeks to random sectors, is typically on the order of 3 to 9 ms. The maximum time for a single seek, Tmax seek, can be as high as 20 ms. 630 Chapter 6 The Memory Hierarchy Rotational latency. Once the head is in position over the track, the drive waits for the ﬁrst bit of the target sector to pass under the head. The perfor- mance of this step depends on both the position of the surface when the head arrives at the target track and the rotational speed of the disk. In the worst case, the head just misses the target sector and waits for the disk to make a full rotation. Thus, the maximum rotational latency, in seconds, is given by Tmax rotation = 1 RPM × 60 secs 1 min The average rotational latency, Tavg rotation, is simply half of Tmax rotation. Transfer time. When the ﬁrst bit of the target sector is under the head, the drive can begin to read or write the contents of the sector. The transfer time for one sector depends on the rotational speed and the number of sectors per track. Thus, we can roughly estimate the average transfer time for one sector in seconds as Tavg transfer = 1 RPM × 1 (average # sectors/track) × 60 secs 1 min We can estimate the average time to access the contents of a disk sector as the sum of the average seek time, the average rotational latency, and the average transfer time. For example, consider a disk with the following parameters: Parameter Value Rotational rate 7,200 RPM Tavg seek 9ms Average number of sectors/track 400 For this disk, the average rotational latency (in ms) is Tavg rotation = 1/2 × Tmax rotation = 1/2 × (60 secs/7,200 RPM) × 1,000 ms/sec ≈ 4ms The average transfer time is Tavg transfer = 60/7,200 RPM × 1/400 sectors/track × 1,000 ms/sec ≈ 0.02 ms Putting it all together, the total estimated access time is Taccess = Tavg seek + Tavg rotation + Tavg transfer = 9ms + 4ms + 0.02 ms = 13.02 ms Section 6.1 Storage Technologies 631 This example illustrates some important points: . The time to access the 512 bytes in a disk sector is dominated by the seek time and the rotational latency. Accessing the ﬁrst byte in the sector takes a long time, but the remaining bytes are essentially free. . Since the seek time and rotational latency are roughly the same, twice the seek time is a simple and reasonable rule for estimating disk access time. . The access time for a 64-bit word stored in SRAM is roughly 4 ns, and 60 ns for DRAM. Thus, the time to read a 512-byte sector-size block from memory is roughly 256 ns for SRAM and 4,000 ns for DRAM. The disk access time, roughly 10 ms, is about 40,000 times greater than SRAM, and about 2,500 times greater than DRAM. Practice Problem 6.3 (solution page 697) Estimate the average time (in ms) to access a sector on the following disk: Parameter Value Rotational rate 12,000 RPM Tavg seek 5ms Average number of sectors/track 300 Logical Disk Blocks As we have seen, modern disks have complex geometries, with multiple surfaces and different recording zones on those surfaces. To hide this complexity from the operating system, modern disks present a simpler view of their geometry as a sequence of B sector-size logical blocks, numbered 0, 1,. . .,B − 1. A small hardware/ﬁrmware device in the disk package, called the disk controller, maintains the mapping between logical block numbers and actual (physical) disk sectors. When the operating system wants to perform an I/O operation such as reading a disk sector into main memory, it sends a command to the disk controller asking it to read a particular logical block number. Firmware on the controller performs a fast table lookup that translates the logical block number into a (surface, track, sector) triple that uniquely identiﬁes the corresponding physical sector. Hardware on the controller interprets this triple to move the heads to the appropriate cylinder, waits for the sector to pass under the head, gathers up the bits sensed by the head into a small memory buffer on the controller, and copies them into main memory. Practice Problem 6.4 (solution page 697) Suppose thata1MBﬁle consisting of 512-byte logical blocks is stored on a disk drive with the following characteristics: 632 Chapter 6 The Memory Hierarchy Aside Formatted disk capacity Before a disk can be used to store data, it must be formatted by the disk controller. This involves ﬁlling in the gaps between sectors with information that identiﬁes the sectors, identifying any cylinders with surface defects and taking them out of action, and setting aside a set of cylinders in each zone as spares that can be called into action if one or more cylinders in the zone goes bad during the lifetime of the disk. The formatted capacity quoted by disk manufacturers is less than the maximum capacity because of the existence of these spare cylinders. Parameter Value Rotational rate 13,000 RPM Tavg seek 6ms Average number of sectors/track 5,000 Surfaces 4 Sector size 512 bytes For each case below, suppose that a program reads the logical blocks of the ﬁle sequentially, one after the other, and that the time to position the head over the ﬁrst block is Tavg seek + Tavg rotation. A. Best case: Estimate the optimal time (in ms) required to read the ﬁle given the best possible mapping of logical blocks to disk sectors (i.e., sequential). B. Random case: Estimate the time (in ms) required to read the ﬁle if blocks are mapped randomly to disk sectors. Connecting I/O Devices Input/output (I/O) devices such as graphics cards, monitors, mice, keyboards, and disks are connected to the CPU and main memory using an I/O bus. Unlike the system bus and memory buses, which are CPU-speciﬁc, I/O buses are designed to be independent of the underlying CPU. Figure 6.11 shows a representative I/O bus structure that connects the CPU, main memory, and I/O devices. Although the I/O bus is slower than the system and memory buses, it can accommodate a wide variety of third-party I/O devices. For example, the bus in Figure 6.11 has three different types of devices attached to it. . A Universal Serial Bus (USB) controller is a conduit for devices attached to a USB bus, which is a wildly popular standard for connecting a variety of peripheral I/O devices, including keyboards, mice, modems, digital cameras, game controllers, printers, external disk drives, and solid state disks. USB 3.0 buses have a maximum bandwidth of 625 MB/s. USB 3.1 buses have a maximum bandwidth of 1,250 MB/s. Section 6.1 Storage Technologies 633 Figure 6.11 Example bus structure that connects the CPU, main memory, and I/O devices. CPU Register file System bus Memory bus I/O bus MonitorKey- board Mouse Disk drive Main memory Expansion slots for other devices such as network adapters Bus interface I/O bridge USB controller Graphics adapter Disk controller Host bus adapter (SCSI/SATA) ALU Solid state disk . A graphics card (or adapter) contains hardware and software logic that is re- sponsible for painting the pixels on the display monitor on behalf of the CPU. . A host bus adapter that connects one or more disks to the I/O bus using a communication protocol deﬁned by a particular host bus interface.The two most popular such interfaces for disks are SCSI (pronounced “scuzzy”) and SATA (pronounced “sat-uh”). SCSI disks are typically faster and more expensive than SATA drives. A SCSI host bus adapter (often called a SCSI controller) can support multiple disk drives, as opposed to SATA adapters, which can only support one drive. Additional devices such as network adapters can be attached to the I/O bus by plugging the adapter into empty expansion slots on the motherboard that provide a direct electrical connection to the bus. Accessing Disks While a detailed description of how I/O devices work and how they are pro- grammed is outside our scope here, we can give you a general idea. For example, Figure 6.12 summarizes the steps that take place when a CPU reads data from a disk. 634 Chapter 6 The Memory Hierarchy Aside Advances in I/O bus designs The I/O bus in Figure 6.11 is a simple abstraction that allows us to be concrete, without being tied too closely to the details of any speciﬁc system. It is based on the peripheral component interconnect (PCI) bus, which was popular until around 2010. In the PCI model, each device in the system shares the bus, and only one device at a time can access these wires. In modern systems, the shared PCI bus has been replaced by a PCI express (PCIe) bus, which is a set of high-speed serial, point-to-point links connected by switches, akin to the switched Ethernets that you will learn about in Chapter 11. A PCIe bus, with a maximum throughput of 16 GB/s, is an order of magnitude faster than a PCI bus, which has a maximum throughput of 533 MB/s. Except for measured I/O performance, the differences between the different bus designs are not visible to application programs, so we will use the simple shared bus abstraction throughout the text. The CPU issues commands to I/O devices using a technique called memory- mapped I/O (Figure 6.12(a)). In a system with memory-mapped I/O, a block of addresses in the address space is reserved for communicating with I/O devices. Each of these addresses is known as an I/O port. Each device is associated with (or mapped to) one or more ports when it is attached to the bus. As a simple example, suppose that the disk controller is mapped to port nÓþn. Then the CPU might initiate a disk read by executing three store instructions to address nÓþn: The ﬁrst of these instructions sends a command word that tells the disk to initiate a read, along with other parameters such as whether to interrupt the CPU when the read is ﬁnished. (We will discuss interrupts in Section 8.1.) The second instruction indicates the logical block number that should be read. The third instruction indicates the main memory address where the contents of the disk sector should be stored. After it issues the request, the CPU will typically do other work while the disk is performing the read. Recall that a 1 GHz processor witha1ns clock cycle can potentially execute 16 million instructions in the 16 ms it takes to read the disk. Simply waiting and doing nothing while the transfer is taking place would be enormously wasteful. After the disk controller receives the read command from the CPU, it trans- lates the logical block number to a sector address, reads the contents of the sector, and transfers the contents directly to main memory, without any intervention from the CPU (Figure 6.12(b)). This process, whereby a device performs a read or write bus transaction on its own, without any involvement of the CPU, is known as direct memory access (DMA). The transfer of data is known as a DMA transfer. After the DMA transfer is complete and the contents of the disk sector are safely stored in main memory, the disk controller notiﬁes the CPU by sending an interrupt signal to the CPU (Figure 6.12(c)). The basic idea is that an interrupt signals an external pin on the CPU chip. This causes the CPU to stop what it is currently working on and jump to an operating system routine. The routine records the fact that the I/O has ﬁnished and then returns control to the point where the CPU was interrupted. Figure 6.12 Reading a disk sector. KeyboardMouse USB controller CPU chip (a) The CPU initiates a disk read by writing a command, logical block number, and destination memory address to the memory-mapped address associated with the disk. Register file I/O bus Monitor Disk Main memory Bus interface Graphics adapter Disk controller ALU KeyboardMouse USB controller CPU chip Register file I/O bus Monitor Disk Main memory Bus interface Graphics adapter Disk controller ALU (b) The disk controller reads the sector and performs a DMA transfer into main memory. KeyboardMouse USB controller CPU chip Register file Interrupt I/O bus Monitor Disk Main memory Bus interface Graphics adapter Disk controller ALU (c) When the DMA transfer is complete, the disk controller notiﬁes the CPU with an interrupt. 636 Chapter 6 The Memory Hierarchy Aside Characteristics of a commercial disk drive Disk manufacturers publish a lot of useful high-level technical information on their Web sites. For example, the Seagate Web site contains the following information (and much more!) about one of their popular drives, the Barracuda 7400. (Seagate.com) Geometry characteristic Value Geometry characteristic Value Surface diameter 3.5 in Rotational rate 7,200 RPM Formatted capacity 3 TB Average rotational latency 4.16 ms Platters 3 Average seek time 8.5 ms Surfaces 6 Track-to-track seek time 1.0 ms Logical blocks 5,860,533,168 Average transfer rate 156 MB/s Logical block size 512 bytes Maximum sustained transfer rate 210 MB/s Figure 6.13 Solid state disk (SSD). Page 0 Page 1 . . . . . .Page P-1 Block 0 Page 0 Page 1 . . . Page P-1 Block B-1 Flash memory Solid state disk (SSD) I/O bus Flash translation layer Requests to read and write logical disk blocks 6.1.3 Solid State Disks A solid state disk (SSD) is a storage technology, based on ﬂash memory (Sec- tion 6.1.1), that in some situations is an attractive alternative to the conventional rotating disk. Figure 6.13 shows the basic idea. An SSD package plugs into a stan- dard disk slot on the I/O bus (typically USB or SATA) and behaves like any other disk, processing requests from the CPU to read and write logical disk blocks. An SSD package consists of one or more ﬂash memory chips, which replace the me- chanical drive in a conventional rotating disk, and a ﬂash translation layer, which is a hardware/ﬁrmware device that plays the same role as a disk controller, trans- lating requests for logical blocks into accesses of the underlying physical device. Figure 6.14 shows the performance characteristics of a typical SSD. Notice that reading from SSDs is faster than writing. The difference between random reading and writing performance is caused by a fundamental property of the underlying ﬂash memory. As shown in Figure 6.13, a ﬂash memory consists of a sequence of B blocks, where each block consists of P pages. Typically, pages are 512 bytes to 4 KB in size, and a block consists of 32–128 pages, with total block sizes ranging from 16 Section 6.1 Storage Technologies 637 Reads Writes Sequential read throughput 550 MB/s Sequential write throughput 470 MB/s Random read throughput (IOPS) 89,000 IOPS Random write throughput (IOPS) 74,000 IOPS Random read throughput (MB/s) 365 MB/s Random write throughput (MB/s) 303 MB/s Avg. sequential read access time 50 μs Avg. sequential write access time 60 μs Figure 6.14 Performance characteristics of a commercial solid state disk. Source: Intel SSD 730 product speciﬁcation [53]. IOPS is I/O operations per second. Throughput numbers are based on reads and writes of 4 KB blocks. (Intel SSD 730 product speciﬁcation. Intel Corporation. 52.) KB to 512 KB. Data are read and written in units of pages. A page can be written only after the entire block to which it belongs has been erased (typically, this means that all bits in the block are set to 1). However, once a block is erased, each page in the block can be written once with no further erasing. A block wears out after roughly 100,000 repeated writes. Once a block wears out, it can no longer be used. Random writes are slower for two reasons. First, erasing a block takes a relatively long time, on the order of 1 ms, which is more than an order of magnitude longer than it takes to access a page. Second, if a write operation attempts to modify a page p that contains existing data (i.e., not all ones), then any pages in the same block with useful data must be copied to a new (erased) block before the write to page p can occur. Manufacturers have developed sophisticated logic in the ﬂash translation layer that attempts to amortize the high cost of erasing blocks and to minimize the number of internal copies on writes, but it is unlikely that random writing will ever perform as well as reading. SSDs have a number of advantages over rotating disks. They are built of semiconductor memory, with no moving parts, and thus have much faster random access times than rotating disks, use less power, and are more rugged. However, there are some disadvantages. First, because ﬂash blocks wear out after repeated writes, SSDs have the potential to wear out as well. Wear-leveling logic in the ﬂash translation layer attempts to maximize the lifetime of each block by spreading erasures evenly across all blocks. In practice, the wear-leveling logic is so good that it takes many years for SSDs to wear out (see Practice Problem 6.5). Second, SSDs are about 30 times more expensive per byte than rotating disks, and thus the typical storage capacities are signiﬁcantly less than rotating disks. However, SSD prices are decreasing rapidly as they become more popular, and the gap between the two is decreasing. SSDs have completely replaced rotating disks in portable music devices, are popular as disk replacements in laptops, and have even begun to appear in desk- tops and servers. While rotating disks are here to stay, it is clear that SSDs are an important alternative. Practice Problem 6.5 (solution page 698) As we have seen, a potential drawback of SSDs is that the underlying ﬂash memory can wear out. For example, for the SSD in Figure 6.14, Intel guarantees about 638 Chapter 6 The Memory Hierarchy 128 petabytes (128 × 1015 bytes) of writes before the drive wears out. Given this assumption, estimate the lifetime (in years) of this SSD for the following workloads: A. Worst case for sequential writes: The SSD is written to continuously at a rate of 470 MB/s (the average sequential write throughput of the device). B. Worst case for random writes: The SSD is written to continuously at a rate of 303 MB/s (the average random write throughput of the device). C. Average case: The SSD is written to at a rate of 20 GB/day (the average daily write rate assumed by some computer manufacturers in their mobile computer workload simulations). 6.1.4 Storage Technology Trends There are several important concepts to take away from our discussion of storage technologies. Different storage technologies have different price and performance trade-offs. SRAM is somewhat faster than DRAM, and DRAM is much faster than disk. On the other hand, fast storage is always more expensive than slower storage. SRAM costs more per byte than DRAM. DRAM costs much more than disk. SSDs split the difference between DRAM and rotating disk. The price and performance properties of different storage technologies are changing at dramatically different rates. Figure 6.15 summarizes the price and per- formance properties of storage technologies since 1985, shortly after the ﬁrst PCs were introduced. The numbers were culled from back issues of trade magazines and the Web. Although they were collected in an informal survey, the numbers reveal some interesting trends. Since 1985, both the cost and performance of SRAM technology have im- proved at roughly the same rate. Access times and cost per megabyte have de- creased by a factor of about 100 (Figure 6.15(a)). However, the trends for DRAM and disk are much more dramatic and divergent. While the cost per megabyte of DRAM has decreased by a factor of 44,000 (more than four orders of magnitude!), DRAM access times have decreased by only a factor of 10 (Figure 6.15(b)). Disk technology has followed the same trend as DRAM and in even more dramatic fashion. While the cost of a megabyte of disk storage has plummeted by a factor of more than 3,000,000 (more than six orders of magnitude!) since 1980, access times have improved much more slowly, by only a factor of 25 (Figure 6.15(c)). These startling long-term trends highlight a basic truth of memory and disk tech- nology: it is much easier to increase density (and thereby reduce cost) than to decrease access time. DRAM and disk performance are lagging behind CPU performance.As we see in Figure 6.15(d), CPU cycle times improved by a factor of 500 between 1985 and 2010. If we look at the effective cycle time —which we deﬁne to be the cycle time of an individual CPU (processor) divided by the number of its processor cores— then the improvement between 1985 and 2010 is even greater, a factor of 2,000. Section 6.1 Storage Technologies 639 Metric 1985 1990 1995 2000 2005 2010 2015 2015:1985 $/MB 2,900 320 256 100 75 60 25 116 Access (ns) 150 35 15 3 2 1.5 1.3 115 (a) SRAM trends Metric 1985 1990 1995 2000 2005 2010 2015 2015:1985 $/MB 880 100 30 1 0.1 0.06 0.02 44,000 Access (ns) 200 100 70 60 50 40 20 10 Typical size (MB) 0.256 4 16 64 2,000 8,000 16,000 62,500 (b) DRAM trends Metric 1985 1990 1995 2000 2005 2010 2015 2015:1985 $/GB 100,000 8,000 300 10 5 0.3 0.03 3,333,333 Min. seek time (ms) 75 28 10 8 5 3 3 25 Typical size (GB) 0.01 0.16 1 20 160 1,500 3,000 300,000 (c) Rotating disk trends Metric 1985 1990 1995 2000 2003 2005 2010 2015 2015:1985 Intel CPU 80286 80386 Pent. P-III Pent. 4 Core 2 Core i7 (n) Core i7 (h) — Clock rate (MHz) 6 20 150 600 3,300 2,000 2,500 3,000 500 Cycle time (ns) 166 50 6 1.6 0.3 0.5 0.4 0.33 500 Cores 1 1 1 1 1 2 4 4 4 Effective cycle 166 50 6 1.6 0.30 0.25 0.10 0.08 2,075 time (ns) (d) CPU trends Figure 6.15 Storage and processing technology trends. The Core i7 circa 2010 uses the Nehalem processor core. The Core i7 circa 2015 uses the Haswell core. The split in the CPU performance curve around 2003 reﬂects the introduction of multi-core processors (see aside on page 641). After this split, cycle times of individual cores actually increased a bit before starting to decrease again, albeit at a slower rate than before. Note that while SRAM performance lags, it is roughly keeping up. However, the gap between DRAM and disk performance and CPU performance is actually widening. Until the advent of multi-core processors around 2003, this performance gap was a function of latency, with DRAM and disk access times decreasing more slowly than the cycle time of an individual processor. However, with the introduction of multiple cores, this performance gap is increasingly a function of 640 Chapter 6 The Memory Hierarchy 100,000,000.0 10,000,000.0 1,000,000.0 100,000.0 10,000.0 1,000.0 100.0 10.0 1.0 0.1 0.0 1985 1990 1995 2000 2003 2005 2010 2015 YearTime (ns) Disk seek time SSD access time DRAM access time SRAM access time CPU cycle time Effective CPU cycle time Figure 6.16 The gap between disk, DRAM, and CPU speeds. throughput, with multiple processor cores issuing requests to the DRAM and disk in parallel. The various trends are shown quite clearly in Figure 6.16, which plots the access and cycle times from Figure 6.15 on a semi-log scale. As we will see in Section 6.4, modern computers make heavy use of SRAM- based caches to try to bridge the processor–memory gap. This approach works because of a fundamental property of application programs known as locality, which we discuss next. Practice Problem 6.6 (solution page 698) Using the data from the years 2005 to 2015 in Figure 6.15(c), estimate the year when you will be able to buy a petabyte (1015 bytes) of rotating disk storage for $200. Assume actual dollars (no inﬂation). 6.2 Locality Well-written computer programs tend to exhibit good locality. That is, they tend to reference data items that are near other recently referenced data items or that were recently referenced themselves. This tendency, known as the principle of locality, is an enduring concept that has enormous impact on the design and performance of hardware and software systems. Locality is typically described as having two distinct forms: temporal locality and spatial locality. In a program with good temporal locality, a memory location that is referenced once is likely to be referenced again multiple times in the near future. In a program with good spatial locality, if a memory location is referenced Section 6.2 Locality 641 Aside When cycle time stood still: The advent of multi-core processors The history of computers is marked by some singular events that caused profound changes in the industry and the world. Interestingly, these inﬂection points tend to occur about once per decade: the development of Fortran in the 1950s, the introduction of the IBM 360 in the early 1960s, the dawn of the Internet (then called ARPANET) in the early 1970s, the introduction of the IBM PC in the early 1980s, and the creation of the World Wide Web in the early 1990s. The most recent such event occurred early in the 21st century, when computer manufacturers ran headlong into the so-called power wall, discovering that they could no longer increase CPU clock frequencies as quickly because the chips would then consume too much power. The solution was to improve performance by replacing a single large processor with multiple smaller processor cores, each a complete processor capable of executing programs independently and in parallel with the other cores. This multi-core approach works in part because the power consumed by a processor is proportional to P = fCV 2, where f is the clock frequency, C is the capacitance, and V is the voltage. The capacitance C is roughly proportional to the area, so the power drawn by multiple cores can be held constant as long as the total area of the cores is constant. As long as feature sizes continue to shrink at the exponential Moore’s Law rate, the number of cores in each processor, and thus its effective performance, will continue to increase. From this point forward, computers will get faster not because the clock frequency increases but because the number of cores in each processor increases, and because architectural innovations increase the efﬁciency of programs running on those cores. We can see this trend clearly in Figure 6.16. CPU cycle time reached its lowest point in 2003 and then actually started to rise before leveling off and starting to decline again at a slower rate than before. However, because of the advent of multi-core processors (dual-core in 2004 and quad-core in 2007), the effective cycle time continues to decrease at close to its previous rate. once, then the program is likely to reference a nearby memory location in the near future. Programmers should understand the principle of locality because, in general, programs with good locality run faster than programs with poor locality. All levels of modern computer systems, from the hardware, to the operating system, to application programs, are designed to exploit locality. At the hardware level, the principle of locality allows computer designers to speed up main memory accesses by introducing small fast memories known as cache memories that hold blocks of the most recently referenced instructions and data items. At the operating system level, the principle of locality allows the system to use the main memory as a cache of the most recently referenced chunks of the virtual address space. Similarly, the operating system uses main memory to cache the most recently used disk blocks in the disk ﬁle system. The principle of locality also plays a crucial role in the design of application programs. For example, Web browsers exploit temporal locality by caching recently referenced documents on a local disk. High-volume Web servers hold recently requested documents in front-end disk caches that satisfy requests for these documents without requiring any intervention from the server. 642 Chapter 6 The Memory Hierarchy 1 ©ÅÎ ÍÏÀÌ−²f©ÅÎ Ì‰ﬁ`g 2 Õ 3 ©ÅÎ ©j ÍÏÀ { ny 4 5 ðÇËf©{ny©zﬁy ©iig 6 ÍÏÀ i{ Ì‰©`y 7 Ë−ÎÏËÅ ÍÏÀy 8 Û (a) Address 0 4 8 12 16 20 24 28 Contents v0 v1 v2 v3 v4 v5 v6 v7 Access order 1 2 3 45678 (b) Figure 6.17 (a) A function with good locality. (b) Reference pattern for vector Ì (N = 8). Notice how the vector elements are accessed in the same order that they are stored in memory. 6.2.1 Locality of References to Program Data Consider the simple function in Figure 6.17(a) that sums the elements of a vector. Does this function have good locality? To answer this question, we look at the reference pattern for each variable. In this example, the ÍÏÀ variable is referenced once in each loop iteration, and thus there is good temporal locality with respect to ÍÏÀ. On the other hand, since ÍÏÀ is a scalar, there is no spatial locality with respect to ÍÏÀ. As we see in Figure 6.17(b), the elements of vector Ì are read sequentially, one after the other, in the order they are stored in memory (we assume for convenience that the array starts at address 0). Thus, with respect to variable Ì, the function has good spatial locality but poor temporal locality since each vector element is accessed exactly once. Since the function has either good spatial or temporal locality with respect to each variable in the loop body, we can conclude that the ÍÏÀÌ−² function enjoys good locality. A function such as ÍÏÀÌ−² that visits each element of a vector sequentially is said to have a stride-1 reference pattern (with respect to the element size). We will sometimes refer to stride-1 reference patterns as sequential reference patterns. Visiting every kth element of a contiguous vector is called a stride-k reference pattern. Stride-1 reference patterns are a common and important source of spatial locality in programs. In general, as the stride increases, the spatial locality decreases. Stride is also an important issue for programs that reference multidimensional arrays. For example, consider the ÍÏÀþËËþÔËÇÑÍ function in Figure 6.18(a) that sums the elements of a two-dimensional array. The doubly nested loop reads the elements of the array in row-major order. That is, the inner loop reads the elements of the ﬁrst row, then the second row, and so on. The ÍÏÀþËËþÔËÇÑÍ function enjoys good spatial locality because it references the array in the same row-major order that the array is stored (Fig- ure 6.18(b)). The result is a nice stride-1 reference pattern with excellent spatial locality. Section 6.2 Locality 643 1 ©ÅÎ ÍÏÀþËËþÔËÇÑÍf©ÅÎ þ‰›`‰ﬁ`g 2 Õ 3 ©ÅÎ ©j Áj ÍÏÀ { ny 4 5 ðÇËf©{ny©z›y ©iig 6 ðÇËfÁ{nyÁzﬁy Áiig 7 ÍÏÀ i{ þ‰©`‰Á`y 8 Ë−ÎÏËÅ ÍÏÀy 9 Û (a) Address 0 4 8 12 16 20 Contents a00 a01 a02 a10 a11 a12 Access order 1 2 3 4 5 6 (b) Figure 6.18 (a) Another function with good locality. (b) Reference pattern for array þ (M = 2, N = 3). There is good spatial locality because the array is accessed in the same row-major order in which it is stored in memory. 1 ©ÅÎ ÍÏÀþËËþÔ²ÇÄÍf©ÅÎ þ‰›`‰ﬁ`g 2 Õ 3 ©ÅÎ ©j Áj ÍÏÀ { ny 4 5 ðÇËfÁ{nyÁzﬁy Áiig 6 ðÇËf©{ny©z›y ©iig 7 ÍÏÀ i{ þ‰©`‰Á`y 8 Ë−ÎÏËÅ ÍÏÀy 9 Û (a) Address 0 4 8 12 16 20 Contents a00 a01 a02 a10 a11 a12 Access order 1 3 5 2 4 6 (b) Figure 6.19 (a) A function with poor spatial locality. (b) Reference pattern for array þ (M = 2, N = 3). The function has poor spatial locality because it scans memory with a stride-N reference pattern. Seemingly trivial changes to a program can have a big impact on its locality. For example, the ÍÏÀþËËþÔ²ÇÄÍ function in Figure 6.19(a) computes the same result as the ÍÏÀþËËþÔËÇÑÍ function in Figure 6.18(a). The only difference is that we have interchanged the i and j loops. What impact does interchanging the loops have on its locality? The ÍÏÀþËËþÔ²ÇÄÍ function suffers from poor spatial locality because it scans the array column-wise instead of row-wise. Since C arrays are laid out in memory row-wise, the result is a stride-N reference pattern, as shown in Figure 6.19(b). 6.2.2 Locality of Instruction Fetches Since program instructions are stored in memory and must be fetched (read) by the CPU, we can also evaluate the locality of a program with respect to its instruction fetches. For example, in Figure 6.17 the instructions in the body of the 644 Chapter 6 The Memory Hierarchy ðÇË loop are executed in sequential memory order, and thus the loop enjoys good spatial locality. Since the loop body is executed multiple times, it also enjoys good temporal locality. An important property of code that distinguishes it from program data is that it is rarely modiﬁed at run time. While a program is executing, the CPU reads its instructions from memory. The CPU rarely overwrites or modiﬁes these instructions. 6.2.3 Summary of Locality In this section, we have introduced the fundamental idea of locality and have identiﬁed some simple rules for qualitatively evaluating the locality in a program: . Programs that repeatedly reference the same variables enjoy good temporal locality. . For programs with stride-k reference patterns, the smaller the stride, the better the spatial locality. Programs with stride-1 reference patterns have good spatial locality. Programs that hop around memory with large strides have poor spatial locality. . Loops have good temporal and spatial locality with respect to instruction fetches. The smaller the loop body and the greater the number of loop it- erations, the better the locality. Later in this chapter, after we have learned about cache memories and how they work, we will show you how to quantify the idea of locality in terms of cache hits and misses. It will also become clear to you why programs with good locality typically run faster than programs with poor locality. Nonetheless, knowing how to glance at a source code and getting a high-level feel for the locality in the program is a useful and important skill for a programmer to master. Practice Problem 6.7 (solution page 698) Permute the loops in the following function so that it scans the three-dimensional array a with a stride-1 reference pattern. 1 ©ÅÎ ÉËÇ®Ï²ÎþËËþÔq®f©ÅÎ þ‰ﬁ`‰ﬁ`‰ﬁ`g 2 Õ 3 ©ÅÎ ©j Áj Âj ÉËÇ®Ï²Î { oy 4 5 ðÇË f© { ﬁkoy © |{ ny ©kkg Õ 6 ðÇË fÁ { ﬁkoy Á |{ ny Ákkg Õ 7 ðÇË fÂ { ﬁkoy Â |{ ny Âkkg Õ 8 ÉËÇ®Ï²Î h{ þ‰Á`‰Â`‰©`y 9 Û 10 Û 11 Û 12 Ë−ÎÏËÅ ÉËÇ®Ï²Îy 13 Û Section 6.3 The Memory Hierarchy 645 (a) An array of ÍÎËÏ²ÎÍ 1 a®−ð©Å− ﬁ onnn 2 3 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 4 ©ÅÎ Ì−Ä‰q`y 5 ©ÅÎ þ²²‰q`y 6 Û ÉÇ©ÅÎy 7 8 ÉÇ©ÅÎ É‰ﬁ`y (b) The ²Ä−þËo function 1 ÌÇ©® ²Ä−þËofÉÇ©ÅÎ hÉj ©ÅÎ Åg 2 Õ 3 ©ÅÎ ©j Áy 4 5 ðÇËf©{ny©zÅy ©iig Õ 6 ðÇËfÁ{nyÁzqy Áiig 7 É‰©`lÌ−Ä‰Á` { ny 8 ðÇËfÁ{nyÁzqy Áiig 9 É‰©`lþ²²‰Á` { ny 10 Û 11 Û (c) The ²Ä−þËp function 1 ÌÇ©® ²Ä−þËpfÉÇ©ÅÎ hÉj ©ÅÎ Åg 2 Õ 3 ©ÅÎ ©j Áy 4 5 ðÇËf©{ny©zÅy ©iig Õ 6 ðÇËfÁ{nyÁzqy Áiig Õ 7 É‰©`lÌ−Ä‰Á` { ny 8 É‰©`lþ²²‰Á` { ny 9 Û 10 Û 11 Û (d) The ²Ä−þËq function 1 ÌÇ©® ²Ä−þËqfÉÇ©ÅÎ hÉj ©ÅÎ Åg 2 Õ 3 ©ÅÎ ©j Áy 4 5 ðÇËfÁ{nyÁzqy Áiig Õ 6 ðÇËf©{ny©zÅy ©iig 7 É‰©`lÌ−Ä‰Á` { ny 8 ðÇËf©{ny©zÅy ©iig 9 É‰©`lþ²²‰Á` { ny 10 Û 11 Û Figure 6.20 Code examples for Practice Problem 6.8. Practice Problem 6.8 (solution page 699) The three functions in Figure 6.20 perform the same operation with varying de- grees of spatial locality. Rank-order the functions with respect to the spatial local- ity enjoyed by each. Explain how you arrived at your ranking. 6.3 The Memory Hierarchy Sections 6.1 and 6.2 described some fundamental and enduring properties of storage technology and computer software: Storage technology. Different storage technologies have widely different access times. Faster technologies cost more per byte than slower ones and have less capacity. The gap between CPU and main memory speed is widening. Computer software. Well-written programs tend to exhibit good locality. 646 Chapter 6 The Memory Hierarchy CPU registers hold words retrieved from cache memory. L1 cache holds cache lines retrieved from L2 cache. L2 cache holds cache lines retrieved from L3 cache. Main memory holds disk blocks retrieved from local disks. Local disks hold files retrieved from disks on remote network servers. Regs L3 cache (SRAM) L2 cache (SRAM) L1 cache (SRAM) Main memory (DRAM) Local secondary storage (local disks) Remote secondary storage (distributed file systems, Web servers) Smaller, faster, and costlier (per byte) storage devices Larger, slower, and cheaper (per byte) storage devices L0: L1: L2: L3: L4: L5: L6: L3 cache holds cache lines retrieved from memory. Figure 6.21 The memory hierarchy. In one of the happier coincidences of computing, these fundamental properties of hardware and software complement each other beautifully. Their complementary nature suggests an approach for organizing memory systems, known as the mem- ory hierarchy, that is used in all modern computer systems. Figure 6.21 shows a typical memory hierarchy. In general, the storage devices get slower, cheaper, and larger as we move from higher to lower levels. At the highest level (L0) are a small number of fast CPU registers that the CPU can access in a single clock cycle. Next are one or more small to moderate-size SRAM-based cache memories that can be accessed in a few CPU clock cycles. These are followed by a large DRAM-based main memory that can be accessed in tens to hundreds of clock cycles. Next are slow but enormous local disks. Finally, some systems even include an additional level of disks on remote servers that can be accessed over a network. For example, distributed ﬁle systems such as the Andrew File System (AFS) or the Network File System (NFS) allow a program to access ﬁles that are stored on remote network-connected servers. Similarly, the World Wide Web allows programs to access remote ﬁles stored on Web servers anywhere in the world. 6.3.1 Caching in the Memory Hierarchy In general, a cache (pronounced “cash”) is a small, fast storage device that acts as a staging area for the data objects stored in a larger, slower device. The process of using a cache is known as caching (pronounced “cashing”). The central idea of a memory hierarchy is that for each k, the faster and smaller storage device at level k serves as a cache for the larger and slower storage device Section 6.3 The Memory Hierarchy 647 Aside Other memory hierarchies We have shown you one example of a memory hierarchy, but other combinations are possible, and indeed common. For example, many sites, including Google datacenters, back up local disks onto archival magnetic tapes. At some of these sites, human operators manually mount the tapes onto tape drives as needed. At other sites, tape robots handle this task automatically. In either case, the collection of tapes represents a level in the memory hierarchy, below the local disk level, and the same general principles apply. Tapes are cheaper per byte than disks, which allows sites to archive multiple snapshots of their local disks. The trade-off is that tapes take longer to access than disks. As another example, solid state disks are playing an increasingly important role in the memory hierarchy, bridging the gulf between DRAM and rotating disk. 4 9 14 3 0123 4567 89 10 11 12 13 14 15 Level k: Level k + 1: Smaller, faster, more expensive device at level k caches a subset of the blocks from level k + 1. Larger, slower, cheaper storage device at level k + 1 is partitioned into blocks. Data are copied between levels in block-size transfer units. Figure 6.22 The basic principle of caching in a memory hierarchy. at level k + 1. In other words, each level in the hierarchy caches data objects from the next lower level. For example, the local disk serves as a cache for ﬁles (such as Web pages) retrieved from remote disks over the network, the main memory serves as a cache for data on the local disks, and so on, until we get to the smallest cache of all, the set of CPU registers. Figure 6.22 shows the general concept of caching in a memory hierarchy. The storage at level k + 1 is partitioned into contiguous chunks of data objects called blocks. Each block has a unique address or name that distinguishes it from other blocks. Blocks can be either ﬁxed size (the usual case) or variable size (e.g., the remote HTML ﬁles stored on Web servers). For example, the level k + 1 storage in Figure 6.22 is partitioned into 16 ﬁxed-size blocks, numbered 0 to 15. Similarly, the storage at level k is partitioned into a smaller set of blocks that are the same size as the blocks at level k + 1. At any point in time, the cache at level k contains copies of a subset of the blocks from level k + 1. For example, in 648 Chapter 6 The Memory Hierarchy Figure 6.22, the cache at level k has room for four blocks and currently contains copies of blocks 4, 9, 14, and 3. Data are always copied back and forth between level k and level k + 1in block-size transfer units. It is important to realize that while the block size is ﬁxed between any particular pair of adjacent levels in the hierarchy, other pairs of levels can have different block sizes. For example, in Figure 6.21, transfers between L1 and L0 typically use word-size blocks. Transfers between L2 and L1 (and L3 and L2, and L4 and L3) typically use blocks of tens of bytes. And transfers between L5 and L4 use blocks with hundreds or thousands of bytes. In general, devices lower in the hierarchy (further from the CPU) have longer access times, and thus tend to use larger block sizes in order to amortize these longer access times. Cache Hits When a program needs a particular data object d from level k + 1, it ﬁrst looks for d in one of the blocks currently stored at level k.If d happens to be cached at level k, then we have what is called a cache hit. The program reads d directly from level k, which by the nature of the memory hierarchy is faster than reading d from level k + 1. For example, a program with good temporal locality might read a data object from block 14, resulting in a cache hit from level k. Cache Misses If, on the other hand, the data object d is not cached at level k, then we have what is called a cache miss. When there is a miss, the cache at level k fetches the block containing d from the cache at level k + 1, possibly overwriting an existing block if the level k cache is already full. This process of overwriting an existing block is known as replacing or evicting the block. The block that is evicted is sometimes referred to as a victim block. The decision about which block to replace is governed by the cache’s replacement policy. For example, a cache with a random replacement policy would choose a random victim block. A cache with a least recently used (LRU) replacement policy would choose the block that was last accessed the furthest in the past. After the cache at level k has fetched the block from level k + 1, the program can read d from level k as before. For example, in Figure 6.22, reading a data object from block 12 in the level k cache would result in a cache miss because block 12 is not currently stored in the level k cache. Once it has been copied from level k + 1 to level k, block 12 will remain there in expectation of later accesses. Kinds of Cache Misses It is sometimes helpful to distinguish between different kinds of cache misses. If the cache at level k is empty, then any access of any data object will miss. An empty cache is sometimes referred to as a cold cache, and misses of this kind are called compulsory misses or cold misses. Cold misses are important because they are often transient events that might not occur in steady state, after the cache has been warmed up by repeated memory accesses. Section 6.3 The Memory Hierarchy 649 Whenever there is a miss, the cache at level k must implement some placement policy that determines where to place the block it has retrieved from level k + 1. The most ﬂexible placement policy is to allow any block from level k + 1to be stored in any block at level k. For caches high in the memory hierarchy (close to the CPU) that are implemented in hardware and where speed is at a premium, this policy is usually too expensive to implement because randomly placed blocks are expensive to locate. Thus, hardware caches typically implement a simpler placement policy that restricts a particular block at level k + 1 to a small subset (sometimes a singleton) of the blocks at level k. For example, in Figure 6.22, we might decide that a block i at level k + 1 must be placed in block (i mod 4) at level k. For example, blocks 0, 4, 8, and 12 at level k + 1 would map to block 0 at level k; blocks 1, 5, 9, and 13 would map to block 1; and so on. Notice that our example cache in Figure 6.22 uses this policy. Restrictive placement policies of this kind lead to a type of miss known as a conﬂict miss, in which the cache is large enough to hold the referenced data objects, but because they map to the same cache block, the cache keeps missing. For example, in Figure 6.22, if the program requests block 0, then block 8, then block 0, then block 8, and so on, each of the references to these two blocks would miss in the cache at level k, even though this cache can hold a total of four blocks. Programs often run as a sequence of phases (e.g., loops) where each phase accesses some reasonably constant set of cache blocks. For example, a nested loop might access the elements of the same array over and over again. This set of blocks is called the working set of the phase. When the size of the working set exceeds the size of the cache, the cache will experience what are known as capacity misses. In other words, the cache is just too small to handle this particular working set. Cache Management As we have noted, the essence of the memory hierarchy is that the storage device at each level is a cache for the next lower level. At each level, some form of logic must manage the cache. By this we mean that something has to partition the cache storage into blocks, transfer blocks between different levels, decide when there are hits and misses, and then deal with them. The logic that manages the cache can be hardware, software, or a combination of the two. For example, the compiler manages the register ﬁle, the highest level of the cache hierarchy. It decides when to issue loads when there are misses, and determines which register to store the data in. The caches at levels L1, L2, and L3 are managed entirely by hardware logic built into the caches. In a system with virtual memory, the DRAM main memory serves as a cache for data blocks stored on disk, and is managed by a combination of operating system software and address translation hardware on the CPU. For a machine with a distributed ﬁle system such as AFS, the local disk serves as a cache that is managed by the AFS client process running on the local machine. In most cases, caches operate automatically and do not require any speciﬁc or explicit actions from the program. 650 Chapter 6 The Memory Hierarchy Type What cached Where cached Latency (cycles) Managed by CPU registers 4-byte or 8-byte words On-chip CPU registers 0 Compiler TLB Address translations On-chip TLB 0 Hardware MMU L1 cache 64-byte blocks On-chip L1 cache 4 Hardware L2 cache 64-byte blocks On-chip L2 cache 10 Hardware L3 cache 64-byte blocks On-chip L3 cache 50 Hardware Virtual memory 4-KB pages Main memory 200 Hardware + OS Buffer cache Parts of ﬁles Main memory 200 OS Disk cache Disk sectors Disk controller 100,000 Controller ﬁrmware Network cache Parts of ﬁles Local disk 10,000,000 NFS client Browser cache Web pages Local disk 10,000,000 Web browser Web cache Web pages Remote server disks 1,000,000,000 Web proxy server Figure 6.23 The ubiquity of caching in modern computer systems. Acronyms: TLB: translation lookaside buffer; MMU: memory management unit; OS: operating system; NFS: network ﬁle system. 6.3.2 Summary of Memory Hierarchy Concepts To summarize, memory hierarchies based on caching work because slower storage is cheaper than faster storage and because programs tend to exhibit locality: Exploiting temporal locality. Because of temporal locality, the same data objects are likely to be reused multiple times. Once a data object has been copied into the cache on the ﬁrst miss, we can expect a number of subsequent hits on that object. Since the cache is faster than the storage at the next lower level, these subsequent hits can be served much faster than the original miss. Exploiting spatial locality. Blocks usually contain multiple data objects. Because of spatial locality, we can expect that the cost of copying a block after a miss will be amortized by subsequent references to other objects within that block. Caches are used everywhere in modern systems. As you can see from Fig- ure 6.23, caches are used in CPU chips, operating systems, distributed ﬁle systems, and on the World Wide Web. They are built from and managed by various com- binations of hardware and software. Note that there are a number of terms and acronyms in Figure 6.23 that we haven’t covered yet. We include them here to demonstrate how common caches are. 6.4 Cache Memories The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main memory, and disk storage. However, because of the increasing gap between CPU and main memory, system designers were compelled to insert Section 6.4 Cache Memories 651 Figure 6.24 Typical bus structure for cache memories. I/O bridge CPU chip Cache memories Register file System bus Memory bus Bus interface Main memory ALU a small SRAM cache memory, called an L1 cache (level 1 cache) between the CPU register ﬁle and main memory, as shown in Figure 6.24. The L1 cache can be accessed nearly as fast as the registers, typically in about 4 clock cycles. As the performance gap between the CPU and main memory continued to increase, system designers responded by inserting an additional larger cache, called an L2 cache, between the L1 cache and main memory, that can be accessed in about 10 clock cycles. Many modern systems include an even larger cache, called an L3 cache, which sits between the L2 cache and main memory in the memory hierarchy and can be accessed in about 50 cycles. While there is considerable variety in the arrangements, the general principles are the same. For our discussion in the next section, we will assume a simple memory hierarchy with a single L1 cache between the CPU and main memory. 6.4.1 Generic Cache Memory Organization Consider a computer system where each memory address has m bits that form M = 2m unique addresses. As illustrated in Figure 6.25(a), a cache for such a machine is organized as an array of S = 2s cache sets. Each set consists of E cache lines. Each line consists of a data block of B = 2b bytes, a valid bit that indicates whether or not the line contains meaningful information, and t = m − (b + s) tag bits (a subset of the bits from the current block’s memory address) that uniquely identify the block stored in the cache line. In general, a cache’s organization can be characterized by the tuple (S, E, B, m). The size (or capacity) of a cache, C, is stated in terms of the aggregate size of all the blocks. The tag bits and valid bit are not included. Thus, C = S × E × B. When the CPU is instructed by a load instruction to read a word from address A of main memory, it sends address A to the cache. If the cache is holding a copy of the word at address A, it sends the word immediately back to the CPU. So how does the cache know whether it contains a copy of the word at address A?The cache is organized so that it can ﬁnd the requested word by simply inspecting the bits of the address, similar to a hash table with an extremely simple hash function. Here is how it works: The parameters S and B induce a partitioning of the m address bits into the three ﬁelds shown in Figure 6.25(b). The s set index bits in A form an index into 652 Chapter 6 The Memory Hierarchy Figure 6.25 General organization of cache (S,E,B,m). (a) A cache is an array of sets. Each set contains one or more lines. Each line contains a valid bit, some tag bits, and a block of data. (b) The cache organization induces a partition of the m address bits into t tag bits, s set index bits, and b block offset bits. Valid Tag 0 1 B–1. . .. . .Valid Tag 0 1 B–1. . . Set 0: Valid Tag 0 1 B–1. . .. . .Valid Tag 0 1 B–1. . . Set 1: Valid Tag Cache size: C = B × E × S data bytes 01 B–1. . .. . .. . . Valid Tag 0 1 B–1. . . Set S–1: 1 valid bit per line t tag bits per line B = 2 b bytes per cache block S = 2s sets E lines per set (a) m–1 0 t bits Address: Tag Set index Block offset s bits (b) b bits the array of S sets. The ﬁrst set is set 0, the second set is set 1, and so on. When interpreted as an unsigned integer, the set index bits tell us which set the word must be stored in. Once we know which set the word must be contained in, the t tag bits in A tell us which line (if any) in the set contains the word. A line in the set contains the word if and only if the valid bit is set and the tag bits in the line match the tag bits in the address A. Once we have located the line identiﬁed by the tag in the set identiﬁed by the set index, then the b block offset bits give us the offset of the word in the B-byte data block. As you may have noticed, descriptions of caches use a lot of symbols. Fig- ure 6.26 summarizes these symbols for your reference. Practice Problem 6.9 (solution page 699) The following table gives the parameters for a number of different caches. For each cache, determine the number of cache sets (S), tag bits (t), set index bits (s), and block offset bits (b). Cache mC B E S t s b 1. 32 1,024 4 1 2. 32 1,024 8 4 3. 32 1,024 32 32 Section 6.4 Cache Memories 653 Parameter Description Fundamental parameters S = 2s Number of sets E Number of lines per set B = 2b Block size (bytes) m = log2(M) Number of physical (main memory) address bits Derived quantities M = 2m Maximum number of unique memory addresses s = log2(S) Number of set index bits b = log2(B) Number of block offset bits t = m − (s + b) Number of tag bits C = B × E × S Cache size (bytes), not including overhead such as the valid and tag bits Figure 6.26 Summary of cache parameters. Figure 6.27 Direct-mapped cache (E = 1). There is exactly one line per set. Valid Tag Cache blockSet 0: Valid Tag Cache blockSet 1: Valid Tag Cache blockSet S–1:. . . E = 1 line per set 6.4.2 Direct-Mapped Caches Caches are grouped into different classes based on E, the number of cache lines per set. A cache with exactly one line per set (E = 1) is known as a direct-mapped cache (see Figure 6.27). Direct-mapped caches are the simplest both to implement and to understand, so we will use them to illustrate some general concepts about how caches work. Suppose we have a system with a CPU, a register ﬁle, an L1 cache, and a main memory. When the CPU executes an instruction that reads a memory word w, it requests the word from the L1 cache. If the L1 cache has a cached copy of w, then we have an L1 cache hit, and the cache quickly extracts w and returns it to the CPU. Otherwise, we have a cache miss, and the CPU must wait while the L1 cache requests a copy of the block containing w from the main memory. When the requested block ﬁnally arrives from memory, the L1 cache stores the block in one of its cache lines, extracts word w from the stored block, and returns it to the CPU. The process that a cache goes through of determining whether a request is a hit or a miss and then extracting the requested word consists of three steps: (1) set selection, (2) line matching, and (3) word extraction. 654 Chapter 6 The Memory Hierarchy Figure 6.28 Set selection in a direct- mapped cache. m–1 0 t bits Tag Set index Block offset s bits b bits Selected set 0 0 0 0 1 Valid Tag Cache blockSet 0: Valid Tag Cache blockSet 1: Valid Tag Cache blockSet S–1:. . . Figure 6.29 Line matching and word selection in a direct- mapped cache. Within the cache block, w0 denotes the low-order byte of the word w, w1 the next byte, and so on. 01 m–1 0 234567 10110 t bits Tag Set index Block offset s bits b bits = ? w0 w1 w2 w3 0110 i 100 = 1? (1) The valid bit must be set. Selected set (i ): The tag bits in the cache line must match the tag bits in the address. (3) If (1) and (2), then cache hit, and block offset selects starting byte. (2) Set Selection in Direct-Mapped Caches In this step, the cache extracts the s set index bits from the middle of the address for w. These bits are interpreted as an unsigned integer that corresponds to a set number. In other words, if we think of the cache as a one-dimensional array of sets, then the set index bits form an index into this array. Figure 6.28 shows how set selection works for a direct-mapped cache. In this example, the set index bits 000012 are interpreted as an integer index that selects set 1. Line Matching in Direct-Mapped Caches Now that we have selected some set i in the previous step, the next step is to determine if a copy of the word w is stored in one of the cache lines contained in set i. In a direct-mapped cache, this is easy and fast because there is exactly one line per set. A copy of w is contained in the line if and only if the valid bit is set and the tag in the cache line matches the tag in the address of w. Figure 6.29 shows how line matching works in a direct-mapped cache. In this example, there is exactly one cache line in the selected set. The valid bit for this line is set, so we know that the bits in the tag and block are meaningful. Since the tag bits in the cache line match the tag bits in the address, we know that a copy of the word we want is indeed stored in the line. In other words, we have a cache hit. On the other hand, if either the valid bit were not set or the tags did not match, then we would have had a cache miss. Section 6.4 Cache Memories 655 Word Selection in Direct-Mapped Caches Once we have a hit, we know that w is somewhere in the block. This last step determines where the desired word starts in the block. As shown in Figure 6.29, the block offset bits provide us with the offset of the ﬁrst byte in the desired word. Similar to our view of a cache as an array of lines, we can think of a block as an array of bytes, and the byte offset as an index into that array. In the example, the block offset bits of 1002 indicate that the copy of w starts at byte 4 in the block. (We are assuming that words are 4 bytes long.) Line Replacement on Misses in Direct-Mapped Caches If the cache misses, then it needs to retrieve the requested block from the next level in the memory hierarchy and store the new block in one of the cache lines of the set indicated by the set index bits. In general, if the set is full of valid cache lines, then one of the existing lines must be evicted. For a direct-mapped cache, where each set contains exactly one line, the replacement policy is trivial: the current line is replaced by the newly fetched line. Putting It Together: A Direct-Mapped Cache in Action The mechanisms that a cache uses to select sets and identify lines are extremely simple. They have to be, because the hardware must perform them in a few nanoseconds. However, manipulating bits in this way can be confusing to us humans. A concrete example will help clarify the process. Suppose we have a direct-mapped cache described by (S,E,B,m) = (4, 1, 2, 4) In other words, the cache has four sets, one line per set, 2 bytes per block, and 4- bit addresses. We will also assume that each word is a single byte. Of course, these assumptions are totally unrealistic, but they will help us keep the example simple. When you are ﬁrst learning about caches, it can be very instructive to enumer- ate the entire address space and partition the bits, as we’ve done in Figure 6.30 for our 4-bit example. There are some interesting things to notice about this enumer- ated space: . The concatenation of the tag and index bits uniquely identiﬁes each block in memory. For example, block 0 consists of addresses 0 and 1, block 1 consists of addresses 2 and 3, block 2 consists of addresses 4 and 5, and so on. . Since there are eight memory blocks but only four cache sets, multiple blocks map to the same cache set (i.e., they have the same set index). For example, blocks 0 and 4 both map to set 0, blocks 1 and 5 both map to set 1, and so on. . Blocks that map to the same cache set are uniquely identiﬁed by the tag. For example, block 0 has a tag bit of 0 while block 4 has a tag bit of 1, block 1 has a tag bit of 0 while block 5 has a tag bit of 1, and so on. 656 Chapter 6 The Memory Hierarchy Address bits Address Tag bits Index bits Offset bits Block number (decimal) (t = 1) (s = 2) (b = 1) (decimal) 00 00 0 0 10 00 1 0 20 01 0 1 30 01 1 1 40 10 0 2 50 10 1 2 60 11 0 3 70 11 1 3 81 00 0 4 91 00 1 4 10 1 01 0 5 11 1 01 1 5 12 1 10 0 6 13 1 10 1 6 14 1 11 0 7 15 1 11 1 7 Figure 6.30 4-bit address space for example direct-mapped cache. Let us simulate the cache in action as the CPU performs a sequence of reads. Remember that for this example we are assuming that the CPU reads 1-byte words. While this kind of manual simulation is tedious and you may be tempted to skip it, in our experience students do not really understand how caches work until they work their way through a few of them. Initially, the cache is empty (i.e., each valid bit is 0): Set Valid Tag block[0] block[1] 00 10 20 30 Each row in the table represents a cache line. The ﬁrst column indicates the set that the line belongs to, but keep in mind that this is provided for convenience and is not really part of the cache. The next four columns represent the actual bits in each cache line. Now, let’s see what happens when the CPU performs a sequence of reads: 1. Read word at address 0. Since the valid bit for set 0 is 0, this is a cache miss. The cache fetches block 0 from memory (or a lower-level cache) and stores the Section 6.4 Cache Memories 657 block in set 0. Then the cache returns m[0] (the contents of memory location 0) from block[0] of the newly fetched cache line. Set Valid Tag block[0] block[1] 0 1 0 m[0] m[1] 10 20 30 2. Read word at address 1. This is a cache hit. The cache immediately returns m[1] from block[1] of the cache line. The state of the cache does not change. 3. Read word at address 13. Since the cache line in set 2 is not valid, this is a cache miss. The cache loads block 6 into set 2 and returns m[13] from block[1] of the new cache line. Set Valid Tag block[0] block[1] 0 1 0 m[0] m[1] 10 2 1 1 m[12] m[13] 30 4. Read word at address 8. This is a miss. The cache line in set 0 is indeed valid, but the tags do not match. The cache loads block 4 into set 0 (replacing the line that was there from the read of address 0) and returns m[8] from block[0] of the new cache line. Set Valid Tag block[0] block[1] 0 1 1 m[8] m[9] 10 2 1 1 m[12] m[13] 30 5. Read word at address 0. This is another miss, due to the unfortunate fact that we just replaced block 0 during the previous reference to address 8. This kind of miss, where we have plenty of room in the cache but keep alternating references to blocks that map to the same set, is an example of a conﬂict miss. Set Valid Tag block[0] block[1] 0 1 0 m[0] m[1] 10 2 1 1 m[12] m[13] 30 658 Chapter 6 The Memory Hierarchy Conﬂict Misses in Direct-Mapped Caches Conﬂict misses are common in real programs and can cause bafﬂing performance problems. Conﬂict misses in direct-mapped caches typically occur when programs access arrays whose sizes are a power of 2. For example, consider a function that computes the dot product of two vectors: 1 ðÄÇþÎ ®ÇÎÉËÇ®fðÄÇþÎ Ó‰v`j ðÄÇþÎ Ô‰v`g 2 Õ 3 ðÄÇþÎ ÍÏÀ { nlny 4 ©ÅÎ ©y 5 6 ðÇËf©{ny©zvy ©iig 7 ÍÏÀ i{ Ó‰©` h Ô‰©`y 8 Ë−ÎÏËÅ ÍÏÀy 9 Û This function has good spatial locality with respect to Ó and Ô, and so we might ex- pect it to enjoy a good number of cache hits. Unfortunately, this is not always true. Suppose that ﬂoats are 4 bytes, that Ó is loaded into the 32 bytes of contiguous memory starting at address 0, and that Ô starts immediately after Ó at address 32. For simplicity, suppose that a block is 16 bytes (big enough to hold four ﬂoats) and that the cache consists of two sets, for a total cache size of 32 bytes. We will assume that the variable ÍÏÀ is actually stored in a CPU register and thus does not require a memory reference. Given these assumptions, each Ó‰©` and Ô‰©` will map to the identical cache set: Element Address Set index Element Address Set index Ó‰n` 00 Ô‰n` 32 0 Ó‰o` 40 Ô‰o` 36 0 Ó‰p` 80 Ô‰p` 40 0 Ó‰q` 12 0 Ô‰q` 44 0 Ó‰r` 16 1 Ô‰r` 48 1 Ó‰s` 20 1 Ô‰s` 52 1 Ó‰t` 24 1 Ô‰t` 56 1 Ó‰u` 28 1 Ô‰u` 60 1 At run time, the ﬁrst iteration of the loop references Ó‰n`, a miss that causes the block containing Ó‰n`–Ó‰q` to be loaded into set 0. The next reference is to Ô‰n`, another miss that causes the block containing Ô‰n`–Ô‰q` to be copied into set 0, overwriting the values of Ó that were copied in by the previous reference. During the next iteration, the reference to Ó‰o` misses, which causes the Ó‰n`– Ó‰q` block to be loaded back into set 0, overwriting the Ô‰n`–Ô‰q` block. So now we have a conﬂict miss, and in fact each subsequent reference to Ó and Ô will result in a conﬂict miss as we thrash back and forth between blocks of Ó and Ô. The term thrashing describes any situation where a cache is repeatedly loading and evicting the same sets of cache blocks. Section 6.4 Cache Memories 659 Aside Why index with the middle bits? You may be wondering why caches use the middle bits for the set index instead of the high-order bits. There is a good reason why the middle bits are better. Figure 6.31 shows why. If the high-order bits are used as an index, then some contiguous memory blocks will map to the same cache set. For example, in the ﬁgure, the ﬁrst four blocks map to the ﬁrst cache set, the second four blocks map to the second set, and so on. If a program has good spatial locality and scans the elements of an array sequentially, then the cache can only hold a block-size chunk of the array at any point in time. This is an inefﬁcient use of the cache. Contrast this with middle-bit indexing, where adjacent blocks always map to different cache sets. In this case, the cache can hold an entire C-size chunk of the array, where C is the cache size. Set index bits Four-set cache High-order bit indexing Middle-order bit indexing 00 01 10 11 0000 1100 1101 1110 1111 0101 0110 0111 1000 1001 1010 1011 0001 0010 0011 0100 0000 1100 1101 1110 1111 0101 0110 0111 1000 1001 1010 1011 0001 0010 0011 0100 Figure 6.31 Why caches index with the middle bits. The bottom line is that even though the program has good spatial locality and we have room in the cache to hold the blocks for both Ó‰©` and Ô‰©`, each reference results in a conﬂict miss because the blocks map to the same cache set. It is not unusual for this kind of thrashing to result in a slowdown by a factor of 2 or 3. Also, be aware that even though our example is extremely simple, the problem is real for larger and more realistic direct-mapped caches. Luckily, thrashing is easy for programmers to ﬁx once they recognize what is going on. One easy solution is to put B bytes of padding at the end of each array. 660 Chapter 6 The Memory Hierarchy For example, instead of deﬁning Ó to be ðÄÇþÎ Ó‰v`, we deﬁne it to be ðÄÇþÎ Ó‰op`. Assuming Ô starts immediately after Ó in memory, we have the following mapping of array elements to sets: Element Address Set index Element Address Set index Ó‰n` 00 Ô‰n` 48 1 Ó‰o` 40 Ô‰o` 52 1 Ó‰p` 80 Ô‰p` 56 1 Ó‰q` 12 0 Ô‰q` 60 1 Ó‰r` 16 1 Ô‰r` 64 0 Ó‰s` 20 1 Ô‰s` 68 0 Ó‰t` 24 1 Ô‰t` 72 0 Ó‰u` 28 1 Ô‰u` 76 0 With the padding at the end of Ó, Ó‰©` and Ô‰©` now map to different sets, which eliminates the thrashing conﬂict misses. Practice Problem 6.10 (solution page 699) In the previous ®ÇÎÉËÇ® example, what fraction of the total references to Ó and Ô will be hits once we have padded array Ó? Practice Problem 6.11 (solution page 699) Imagine a hypothetical cache that uses the high-order s bits of an address as the set index. For such a cache, contiguous chunks of memory blocks are mapped to the same cache set. A. How many blocks are in each of these contiguous array chunks? B. Consider the following code that runs on a system with a cache of the form (S,E,B,m) = (512, 1, 32, 32): ©ÅÎ þËËþÔ‰rnwt`y ðÇË f© { ny © z rnwty ©iig ÍÏÀ i{ þËËþÔ‰©`y What is the maximum number of array blocks that are stored in the cache at any point in time? 6.4.3 Set Associative Caches The problem with conﬂict misses in direct-mapped caches stems from the con- straint that each set has exactly one line (or in our terminology, E = 1). A set associative cache relaxes this constraint so that each set holds more than one cache line. A cache with 1 <E <C/B is often called an E-way set associative cache. We Section 6.4 Cache Memories 661 Figure 6.32 Set associative cache (1 <E <C/B). In a set associative cache, each set contains more than one line. This particular example shows a two-way set associative cache. Valid Tag Cache block Set 0: Valid Tag Cache block Set S – 1:. . . E = 2 lines per set Valid Tag Cache block Valid Tag Cache block Valid Tag Cache block Valid Tag Cache block Set 1: Figure 6.33 Set selection in a set associative cache. Valid Tag Cache block Set 0: Valid Tag Cache block Set S–1:. . . Valid Tag Cache block Valid Tag Cache block Valid Tag Cache block Valid Tag Cache block Set 1: m–1 0 t bits Tag Set index Block offset s bits b bits Selected set 0 0 0 0 1 will discuss the special case, where E = C/B, in the next section. Figure 6.32 shows the organization of a two-way set associative cache. Set Selection in Set Associative Caches Set selection is identical to a direct-mapped cache, with the set index bits identi- fying the set. Figure 6.33 summarizes this principle. Line Matching and Word Selection in Set Associative Caches Line matching is more involved in a set associative cache than in a direct-mapped cache because it must check the tags and valid bits of multiple lines in order to determine if the requested word is in the set. A conventional memory is an array of values that takes an address as input and returns the value stored at that address. An associative memory, on the other hand, is an array of (key, value) pairs that takes as input the key and returns a value from one of the (key, value) pairs that matches the input key. Thus, we can think of each set in a set associative cache as a small associative memory where the keys are the concatenation of the tag and valid bits, and the values are the contents of a block. 662 Chapter 6 The Memory Hierarchy Figure 6.34 Line matching and word selection in a set associative cache. 01 m–1 0 234567 1 1 1001 0110 t bits Tag Set index Block offset s bits b bits = ? w0 w1 w2 w3 0110 i 100 = 1? (1) The valid bit must be set. Selected set (i ): (2) The tag bits in one of the cache lines must match the tag bits in the address. (3) If (1) and (2), then cache hit, and block offset selects starting byte. Figure 6.34 shows the basic idea of line matching in an associative cache. An important idea here is that any line in the set can contain any of the memory blocks that map to that set. So the cache must search each line in the set for a valid line whose tag matches the tag in the address. If the cache ﬁnds such a line, then we have a hit and the block offset selects a word from the block, as before. Line Replacement on Misses in Set Associative Caches If the word requested by the CPU is not stored in any of the lines in the set, then we have a cache miss, and the cache must fetch the block that contains the word from memory. However, once the cache has retrieved the block, which line should it replace? Of course, if there is an empty line, then it would be a good candidate. But if there are no empty lines in the set, then we must choose one of the nonempty lines and hope that the CPU does not reference the replaced line anytime soon. It is very difﬁcult for programmers to exploit knowledge of the cache replace- ment policy in their codes, so we will not go into much detail about it here. The simplest replacement policy is to choose the line to replace at random. Other more sophisticated policies draw on the principle of locality to try to minimize the prob- ability that the replaced line will be referenced in the near future. For example, a least frequently used (LFU) policy will replace the line that has been referenced the fewest times over some past time window. A least recently used (LRU) policy will replace the line that was last accessed the furthest in the past. All of these policies require additional time and hardware. But as we move further down the memory hierarchy, away from the CPU, the cost of a miss becomes more expen- sive and it becomes more worthwhile to minimize misses with good replacement policies. 6.4.4 Fully Associative Caches A fully associative cache consists of a single set (i.e., E = C/B) that contains all of the cache lines. Figure 6.35 shows the basic organization. Section 6.4 Cache Memories 663 Figure 6.35 Fully associative cache (E = C/B). In a fully associative cache, a single set contains all of the lines. Valid Tag Cache block Set 0: Valid Tag Cache block. . .E = C/B lines in the one and only set Valid Tag Cache block Figure 6.36 Set selection in a fully associative cache. Notice that there are no set index bits. Valid Tag Cache block Set 0: Valid Tag Cache block. . . Valid Tag Cache block m–1 0 t bits Tag Block offset b bits The entire cache is one set, so by default set 0 is always selected. Figure 6.37 Line matching and word selection in a fully associative cache. m–1 0 1 0 0110 1110 t bits Tag Block offset b bits = ? w0 w1 w2 w3 0110 100 01234567 1 0 1001 0110 = 1? (1) The valid bit must be set. Entire cache (2) The tag bits in one of the cache lines must match the tag bits in the address. (3) If (1) and (2), then cache hit, and block offset selects starting byte. Set Selection in Fully Associative Caches Set selection in a fully associative cache is trivial because there is only one set, summarized in Figure 6.36. Notice that there are no set index bits in the address, which is partitioned into only a tag and a block offset. Line Matching and Word Selection in Fully Associative Caches Line matching and word selection in a fully associative cache work the same as with a set associative cache, as we show in Figure 6.37. The difference is mainly a question of scale. Because the cache circuitry must search for many matching tags in parallel, it is difﬁcult and expensive to build an associative cache that is both large and fast. As a result, fully associative caches are only appropriate for small caches, such 664 Chapter 6 The Memory Hierarchy as the translation lookaside buffers (TLBs) in virtual memory systems that cache page table entries (Section 9.6.2). Practice Problem 6.12 (solution page 699) The problems that follow will help reinforce your understanding of how caches work. Assume the following: . The memory is byte addressable. . Memory accesses are to 1-byte words (not to 4-byte words). . Addresses are 13 bits wide. . The cache is two-way set associative (E = 2), with a 4-byte block size (B = 4) and eight sets (S = 8). The contents of the cache are as follows, with all numbers given in hexadecimal notation. 2-way set associative cache Line 0 Line 1 Set index Tag Valid Byte 0 Byte 1 Byte 2 Byte 3 Tag Valid Byte 0 Byte 1 Byte 2 Byte 3 0 09 1 8630 3F 10 00 0 ———— 1 45 1 60 4F E0 23 38 1 00 BC 0B 37 2 EB 0 ———— 0B 0 ———— 3 06 0 ———— 32 1 1208 7B AD 4 C7 1 06 78 07 C5 05 1 40 67 C2 3B 5 71 1 0B DE 18 4B 6E 0 ———— 6 91 1 A0 B7 26 2D F0 0 ———— 7 46 0 ———— DE 1 12 C0 8837 The following ﬁgure shows the format of an address (1 bit per box). Indicate (by labeling the diagram) the ﬁelds that would be used to determine the following: CO. The cache block offset CI. The cache set index CT. The cache tag 12 11 10 9 8 7 6 5 4 3 2 1 0 Practice Problem 6.13 (solution page 700) Suppose a program running on the machine in Problem 6.12 references the 1-byte word at address nÓn⁄sq. Indicate the cache entry accessed and the cache byte Section 6.4 Cache Memories 665 value returned in hexadecimal notation. Indicate whether a cache miss occurs. If there is a cache miss, enter “—” for “Cache byte returned.” A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 B. Memory reference: Parameter Value Cache block offset (CO) nÓ Cache set index (CI) nÓ Cache tag (CT) nÓ Cache hit? (Y/N) Cache byte returned nÓ Practice Problem 6.14 (solution page 700) Repeat Problem 6.13 for memory address nÓn£¢r. A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 B. Memory reference: Parameter Value Cache block offset (CO) nÓ Cache set index (CI) nÓ Cache tag (CT) nÓ Cache hit? (Y/N) Cache byte returned nÓ Practice Problem 6.15 (solution page 700) Repeat Problem 6.13 for memory address nÓn¡qo. A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 666 Chapter 6 The Memory Hierarchy B. Memory reference: Parameter Value Cache block offset (CO) nÓ Cache set index (CI) nÓ Cache tag (CT) nÓ Cache hit? (Y/N) Cache byte returned nÓ Practice Problem 6.16 (solution page 701) For the cache in Problem 6.12, list all of the hexadecimal memory addresses that will hit in set 3. 6.4.5 Issues with Writes As we have seen, the operation of a cache with respect to reads is straightforward. First, look for a copy of the desired word w in the cache. If there is a hit, return w immediately. If there is a miss, fetch the block that contains w from the next lower level of the memory hierarchy, store the block in some cache line (possibly evicting a valid line), and then return w. The situation for writes is a little more complicated. Suppose we write a word w that is already cached (a write hit). After the cache updates its copy of w, what does it do about updating the copy of w in the next lower level of the hierarchy? The simplest approach, known as write-through, is to immediately write w’s cache block to the next lower level. While simple, write-through has the disadvantage of causing bus trafﬁc with every write. Another approach, known as write-back, defers the update as long as possible by writing the updated block to the next lower level only when it is evicted from the cache by the replacement algorithm. Because of locality, write-back can signiﬁcantly reduce the amount of bus trafﬁc, but it has the disadvantage of additional complexity. The cache must maintain an additional dirty bit for each cache line that indicates whether or not the cache block has been modiﬁed. Another issue is how to deal with write misses. One approach, known as write- allocate, loads the corresponding block from the next lower level into the cache and then updates the cache block. Write-allocate tries to exploit spatial locality of writes, but it has the disadvantage that every miss results in a block transfer from the next lower level to the cache. The alternative, known as no-write-allocate, bypasses the cache and writes the word directly to the next lower level. Write- through caches are typically no-write-allocate. Write-back caches are typically write-allocate. Optimizing caches for writes is a subtle and difﬁcult issue, and we are only scratching the surface here. The details vary from system to system and are often proprietary and poorly documented. To the programmer trying to write reason- Section 6.4 Cache Memories 667 ably cache-friendly programs, we suggest adopting a mental model that assumes write-back, write-allocate caches. There are several reasons for this suggestion: As a rule, caches at lower levels of the memory hierarchy are more likely to use write- back instead of write-through because of the larger transfer times. For example, virtual memory systems (which use main memory as a cache for the blocks stored on disk) use write-back exclusively. But as logic densities increase, the increased complexity of write-back is becoming less of an impediment and we are seeing write-back caches at all levels of modern systems. So this assumption matches cur- rent trends. Another reason for assuming a write-back, write-allocate approach is that it is symmetric to the way reads are handled, in that write-back write-allocate tries to exploit locality. Thus, we can develop our programs at a high level to exhibit good spatial and temporal locality rather than trying to optimize for a particular memory system. 6.4.6 Anatomy of a Real Cache Hierarchy So far, we have assumed that caches hold only program data. But, in fact, caches can hold instructions as well as data. A cache that holds instructions only is called an i-cache. A cache that holds program data only is called a d-cache. A cache that holds both instructions and data is known as a uniﬁed cache. Modern processors include separate i-caches and d-caches. There are a number of reasons for this. With two separate caches, the processor can read an instruction word and a data word at the same time. I-caches are typically read-only, and thus simpler. The two caches are often optimized to different access patterns and can have different block sizes, associativities, and capacities. Also, having separate caches ensures that data accesses do not create conﬂict misses with instruction accesses, and vice versa, at the cost of a potential increase in capacity misses. Figure 6.38 shows the cache hierarchy for the Intel Core i7 processor. Each CPU chip has four cores. Each core has its own private L1 i-cache, L1 d-cache, and L2 uniﬁed cache. All of the cores share an on-chip L3 uniﬁed cache. An interesting feature of this hierarchy is that all of the SRAM cache memories are contained in the CPU chip. Figure 6.39 summarizes the basic characteristics of the Core i7 caches. 6.4.7 Performance Impact of Cache Parameters Cache performance is evaluated with a number of metrics: Miss rate. The fraction of memory references during the execution of a pro- gram, or a part of a program, that miss. It is computed as # misses/ # references. Hit rate. The fraction of memory references that hit. It is computed as 1 − miss rate. Hit time. The time to deliver a word in the cache to the CPU, including the time for set selection, line identiﬁcation, and word selection. Hit time is on the order of several clock cycles for L1 caches. 668 Chapter 6 The Memory Hierarchy Figure 6.38 Intel Core i7 cache hierarchy. Processor package Core 0 Core 3 . . . Regs L1 d-cache L2 unified cache L3 unified cache (shared by all cores) Main memory L1 i-cache Regs L1 d-cache L2 unified cache L1 i-cache Cache type Access time (cycles) Cache size (C) Assoc. (E) Block size (B) Sets (S) L1 i-cache 4 32 KB 8 64 B 64 L1 d-cache 4 32 KB 8 64 B 64 L2 uniﬁed cache 10 256 KB 8 64 B 512 L3 uniﬁed cache 40–75 8 MB 16 64 B 8,192 Figure 6.39 Characteristics of the Intel Core i7 cache hierarchy. Miss penalty. Any additional time required because of a miss. The penalty for L1 misses served from L2 is on the order of 10 cycles; from L3, 50 cycles; and from main memory, 200 cycles. Optimizing the cost and performance trade-offs of cache memories is a subtle exercise that requires extensive simulation on realistic benchmark codes and thus is beyond our scope. However, it is possible to identify some of the qualitative trade-offs. Impact of Cache Size On the one hand, a larger cache will tend to increase the hit rate. On the other hand, it is always harder to make large memories run faster. As a result, larger caches tend to increase the hit time. This explains why an L1 cache is smaller than an L2 cache, and an L2 cache is smaller than an L3 cache. Section 6.5 Writing Cache-Friendly Code 669 Impact of Block Size Large blocks are a mixed blessing. On the one hand, larger blocks can help increase the hit rate by exploiting any spatial locality that might exist in a program. However, for a given cache size, larger blocks imply a smaller number of cache lines, which can hurt the hit rate in programs with more temporal locality than spatial locality. Larger blocks also have a negative impact on the miss penalty, since larger blocks cause larger transfer times. Modern systems such as the Core i7 compromise with cache blocks that contain 64 bytes. Impact of Associativity The issue here is the impact of the choice of the parameter E, the number of cache lines per set. The advantage of higher associativity (i.e., larger values of E) is that it decreases the vulnerability of the cache to thrashing due to conﬂict misses. However, higher associativity comes at a signiﬁcant cost. Higher associativity is expensive to implement and hard to make fast. It requires more tag bits per line, additional LRU state bits per line, and additional control logic. Higher associativity can increase hit time, because of the increased complexity, and it can also increase the miss penalty because of the increased complexity of choosing a victim line. The choice of associativity ultimately boils down to a trade-off between the hit time and the miss penalty. Traditionally, high-performance systems that pushed the clock rates would opt for smaller associativity for L1 caches (where the miss penalty is only a few cycles) and a higher degree of associativity for the lower levels, where the miss penalty is higher. For example, in Intel Core i7 systems, the L1 and L2 caches are 8-way associative, and the L3 cache is 16-way. Impact of Write Strategy Write-through caches are simpler to implement and can use a write buffer that works independently of the cache to update memory. Furthermore, read misses are less expensive because they do not trigger a memory write. On the other hand, write-back caches result in fewer transfers, which allows more bandwidth to memory for I/O devices that perform DMA. Further, reducing the number of transfers becomes increasingly important as we move down the hierarchy and the transfer times increase. In general, caches further down the hierarchy are more likely to use write-back than write-through. 6.5 Writing Cache-Friendly Code In Section 6.2, we introduced the idea of locality and talked in qualitative terms about what constitutes good locality. Now that we understand how cache memo- ries work, we can be more precise. Programs with better locality will tend to have lower miss rates, and programs with lower miss rates will tend to run faster than programs with higher miss rates. Thus, good programmers should always try to 670 Chapter 6 The Memory Hierarchy Aside Cache lines, sets, and blocks: What’s the difference? It is easy to confuse the distinction between cache lines, sets, and blocks. Let’s review these ideas and make sure they are clear: . A block is a ﬁxed-size packet of information that moves back and forth between a cache and main memory (or a lower-level cache). . A line is a container in a cache that stores a block, as well as other information such as the valid bit and the tag bits. . A set is a collection of one or more lines. Sets in direct-mapped caches consist of a single line. Sets in set associative and fully associative caches consist of multiple lines. In direct-mapped caches, sets and lines are indeed equivalent. However, in associative caches, sets and lines are very different things and the terms cannot be used interchangeably. Since a line always stores a single block, the terms “line” and “block” are often used interchange- ably. For example, systems professionals usually refer to the “line size” of a cache, when what they really mean is the block size. This usage is very common and shouldn’t cause any confusion as long as you understand the distinction between blocks and lines. write code that is cache friendly, in the sense that it has good locality. Here is the basic approach we use to try to ensure that our code is cache friendly. 1. Make the common case go fast. Programs often spend most of their time in a few core functions. These functions often spend most of their time in a few loops. So focus on the inner loops of the core functions and ignore the rest. 2. Minimize the number of cache misses in each inner loop. All other things being equal, such as the total number of loads and stores, loops with better miss rates will run faster. To see how this works in practice, consider the ÍÏÀÌ−² function from Sec- tion 6.2: 1 ©ÅÎ ÍÏÀÌ−²f©ÅÎ Ì‰ﬁ`g 2 Õ 3 ©ÅÎ ©j ÍÏÀ { ny 4 5 ðÇËf©{ny©zﬁy ©iig 6 ÍÏÀ i{ Ì‰©`y 7 Ë−ÎÏËÅ ÍÏÀy 8 Û Is this function cache friendly? First, notice that there is good temporal locality in the loop body with respect to the local variables © and ÍÏÀ. In fact, because these are local variables, any reasonable optimizing compiler will cache them in the register ﬁle, the highest level of the memory hierarchy. Now consider the stride- 1 references to vector Ì. In general, if a cache has a block size of B bytes, then a Section 6.5 Writing Cache-Friendly Code 671 stride-k reference pattern (where k is expressed in words) results in an average of min (1,(word size × k)/B) misses per loop iteration. This is minimized for k = 1, so the stride-1 references to Ì are indeed cache friendly. For example, suppose that Ì is block aligned, words are 4 bytes, cache blocks are 4 words, and the cache is initially empty (a cold cache). Then, regardless of the cache organization, the references to Ì will result in the following pattern of hits and misses: Ì‰©` i = 0 i = 1 i = 2 i = 3 i = 4 i = 5 i = 6 i = 7 Access order, [h]it or [m]iss 1 [m] 2 [h] 3 [h] 4 [h] 5 [m] 6 [h] 7 [h] 8 [h] In this example, the reference to Ì‰n` misses and the corresponding block, which contains Ì‰n`–Ì‰q`, is loaded into the cache from memory. Thus, the next three references are all hits. The reference to Ì‰r` causes another miss as a new block is loaded into the cache, the next three references are hits, and so on. In general, three out of four references will hit, which is the best we can do in this case with a cold cache. To summarize, our simple ÍÏÀÌ−² example illustrates two important points about writing cache-friendly code: . Repeated references to local variables are good because the compiler can cache them in the register ﬁle (temporal locality). . Stride-1 reference patterns are good because caches at all levels of the memory hierarchy store data as contiguous blocks (spatial locality). Spatial locality is especially important in programs that operate on multi- dimensional arrays. For example, consider the ÍÏÀþËËþÔËÇÑÍ function from Sec- tion 6.2, which sums the elements of a two-dimensional array in row-major order: 1 ©ÅÎ ÍÏÀþËËþÔËÇÑÍf©ÅÎ þ‰›`‰ﬁ`g 2 Õ 3 ©ÅÎ ©j Áj ÍÏÀ { ny 4 5 ðÇËf©{ny©z›y ©iig 6 ðÇËfÁ{nyÁzﬁy Áiig 7 ÍÏÀ i{ þ‰©`‰Á`y 8 Ë−ÎÏËÅ ÍÏÀy 9 Û Since C stores arrays in row-major order, the inner loop of this function has the same desirable stride-1 access pattern as ÍÏÀÌ−². For example, suppose we make the same assumptions about the cache as for ÍÏÀÌ−². Then the references to the array þ will result in the following pattern of hits and misses: þ‰©`‰Á` j = 0 j = 1 j = 2 j = 3 j = 4 j = 5 j = 6 j = 7 i = 01 [m] 2 [h] 3 [h] 4 [h] 5 [m] 6 [h] 7 [h] 8 [h] i = 19 [m] 10 [h] 11 [h] 12 [h] 13 [m] 14 [h] 15 [h] 16 [h] i = 217 [m] 18 [h] 19 [h] 20 [h] 21 [m] 22 [h] 23 [h] 24 [h] i = 325 [m] 26 [h] 27 [h] 28 [h] 29 [m] 30 [h] 31 [h] 32 [h] 672 Chapter 6 The Memory Hierarchy But consider what happens if we make the seemingly innocuous change of permuting the loops: 1 ©ÅÎ ÍÏÀþËËþÔ²ÇÄÍf©ÅÎ þ‰›`‰ﬁ`g 2 Õ 3 ©ÅÎ ©j Áj ÍÏÀ { ny 4 5 ðÇËfÁ{nyÁzﬁy Áiig 6 ðÇËf©{ny©z›y ©iig 7 ÍÏÀ i{ þ‰©`‰Á`y 8 Ë−ÎÏËÅ ÍÏÀy 9 Û In this case, we are scanning the array column by column instead of row by row. If we are lucky and the entire array ﬁts in the cache, then we will enjoy the same miss rate of 1/4. However, if the array is larger than the cache (the more likely case), then each and every access of þ‰©`‰Á` will miss! þ‰©`‰Á` j = 0 j = 1 j = 2 j = 3 j = 4 j = 5 j = 6 j = 7 i = 01 [m] 5 [m] 9 [m] 13 [m] 17 [m] 21 [m] 25 [m] 29 [m] i = 12 [m] 6 [m] 10 [m] 14 [m] 18 [m] 22 [m] 26 [m] 30 [m] i = 23 [m] 7 [m] 11 [m] 15 [m] 19 [m] 23 [m] 27 [m] 31 [m] i = 34 [m] 8 [m] 12 [m] 16 [m] 20 [m] 24 [m] 28 [m] 32 [m] Higher miss rates can have a signiﬁcant impact on running time. For example, on our desktop machine, ÍÏÀþËËþÔËÇÑÍ runs 25 times faster than ÍÏÀþËËþÔ²ÇÄÍ for large array sizes. To summarize, programmers should be aware of locality in their programs and try to write programs that exploit it. Practice Problem 6.17 (solution page 701) Transposing the rows and columns of a matrix is an important problem in signal processing and scientiﬁc computing applications. It is also interesting from a local- ity point of view because its reference pattern is both row-wise and column-wise. For example, consider the following transpose routine: 1 ÎÔÉ−®−ð ©ÅÎ þËËþÔ‰p`‰p`y 2 3 ÌÇ©® ÎËþÅÍÉÇÍ−ofþËËþÔ ®ÍÎj þËËþÔ ÍË²g 4 Õ 5 ©ÅÎ ©j Áy 6 7 ðÇËf©{ny©zpy ©iig Õ 8 ðÇËfÁ{nyÁzpy Áiig Õ 9 ®ÍÎ‰Á`‰©` { ÍË²‰©`‰Á`y 10 Û 11 Û 12 Û Section 6.5 Writing Cache-Friendly Code 673 Assume this code runs on a machine with the following properties: . Í©Ö−Çð(©ÅÎ) = 4. . The ÍË² array starts at address 0 and the ®ÍÎ array starts at address 16 (decimal). . There is a single L1 data cache that is direct-mapped, write-through, and write- allocate, with a block size of 8 bytes. . The cache has a total size of 16 data bytes and the cache is initially empty. . Accesses to the ÍË² and ®ÍÎ arrays are the only sources of read and write misses, respectively. A. For each ËÇÑ and ²ÇÄ, indicate whether the access to ÍË²‰ËÇÑ`‰²ÇÄ` and ®ÍÎ‰ËÇÑ`‰²ÇÄ` is a hit (h) or a miss (m). For example, reading ÍË²‰n`‰n` is a miss and writing ®ÍÎ‰n`‰n` is also a miss. ®ÍÎ array ÍË² array Col. 0 Col. 1 Col. 0 Col. 1 Row 0 m Row0 m Row 1 Row 1 B. Repeat the problem for a cache with 32 data bytes. Practice Problem 6.18 (solution page 702) The heart of the recent hit game SimAquarium is a tight loop that calculates the average position of 512 algae. You are evaluating its cache performance on a machine with a 2,048-byte direct-mapped data cache with 32-byte blocks (B = 32). You are given the following deﬁnitions: 1 ÍÎËÏ²Î þÄ×þ−ˆÉÇÍ©Î©ÇÅ Õ 2 ©ÅÎ Óy 3 ©ÅÎ Ôy 4 Ûy 5 6 ÍÎËÏ²Î þÄ×þ−ˆÉÇÍ©Î©ÇÅ ×Ë©®‰qp`‰qp`y 7 ©ÅÎ ÎÇÎþÄˆÓ { nj ÎÇÎþÄˆÔ { ny 8 ©ÅÎ ©j Áy You should also assume the following: . Í©Ö−Çð(©ÅÎ) = 4. . ×Ë©® begins at memory address 0. . The cache is initially empty. . The only memory accesses are to the entries of the array ×Ë©®. Variables ©, Á, ÎÇÎþÄˆÓ, and ÎÇÎþÄˆÔ are stored in registers. 674 Chapter 6 The Memory Hierarchy Determine the cache performance for the following code: 1 ðÇË f© { qoy © |{ ny ©kkg Õ 2 ðÇË fÁ { qoy Á |{ ny Ákkg Õ 3 ÎÇÎþÄˆÓ i{ ×Ë©®‰©`‰Á`lÓy 4 Û 5 Û 6 7 ðÇË f© { qoy © |{ ny ©kkg Õ 8 ðÇË fÁ { qoy Á |{ ny Ákkg Õ 9 ÎÇÎþÄˆÔ i{ ×Ë©®‰©`‰Á`lÔy 10 Û 11 Û A. What is the total number of reads? B. What is the total number of reads that miss in the cache? C. What is the miss rate? Practice Problem 6.19 (solution page 702) Given the assumptions of Practice Problem 6.18, determine the cache perfor- mance of the following code: 1 ðÇË f© { qoy © |{ ny ©kkgÕ 2 ðÇË fÁ { qoy Á |{ ny Ákkg Õ 3 ÎÇÎþÄˆÓ i{ ×Ë©®‰Á`‰©`lÓy 4 ÎÇÎþÄˆÔ i{ ×Ë©®‰Á`‰©`lÔy 5 Û 6 Û A. What is the total number of reads? B. What is the total number of reads that hit in the cache? C. What is the hit rate? D. What would the miss hit be if the cache were twice as big? Practice Problem 6.20 (solution page 702) Given the assumptions of Practice Problem 6.18, determine the cache perfor- mance of the following code: 1 ðÇË f© { qoy © |{ ny ©kkgÕ 2 ðÇË fÁ { qoy Á |{ ny Ákkg Õ 3 ÎÇÎþÄˆÓ i{ ×Ë©®‰©`‰Á`lÓy 4 ÎÇÎþÄˆÔ i{ ×Ë©®‰©`‰Á`lÔy 5 Û 6 Û Section 6.6 Putting It Together: The Impact of Caches on Program Performance 675 A. What is the total number of reads? B. What is the total number of reads that hit in the cache? C. What is the hit rate? D. What would the hit rate be if the cache were twice as big? 6.6 Putting It Together: The Impact of Caches on Program Performance This section wraps up our discussion of the memory hierarchy by studying the im- pact that caches have on the performance of programs running on real machines. 6.6.1 The Memory Mountain The rate that a program reads data from the memory system is called the read throughput, or sometimes the read bandwidth. If a program reads n bytes over a period of s seconds, then the read throughput over that period is n/s, typically expressed in units of megabytes per second (MB/s). If we were to write a program that issued a sequence of read requests from a tight program loop, then the measured read throughput would give us some insight into the performance of the memory system for that particular sequence of reads. Figure 6.40 shows a pair of functions that measure the read throughput for a particular read sequence. The Î−ÍÎ function generates the read sequence by scanning the ﬁrst −Ä−ÀÍ elements of an array with a stride of ÍÎË©®−. To increase the available parallelism in the inner loop, it uses 4 × 4 unrolling (Section 5.9). The ËÏÅ function is a wrapper that calls the Î−ÍÎ function and returns the measured read throughput. The call to the Î−ÍÎ function in line 37 warms the cache. The ð²Ô²p function in line 38 calls the Î−ÍÎ function with arguments −Ä−ÀÍ and estimates the running time of the Î−ÍÎ function in CPU cycles. Notice that the Í©Ö− argument to the ËÏÅ function is in units of bytes, while the corresponding −Ä−ÀÍ argument to the Î−ÍÎ function is in units of array elements. Also, notice that line 39 computes MB/s as 106 bytes/s, as opposed to 220 bytes/s. The Í©Ö− and ÍÎË©®− arguments to the ËÏÅ function allow us to control the degree of temporal and spatial locality in the resulting read sequence. Smaller values of Í©Ö− result in a smaller working set size, and thus better temporal locality. Smaller values of ÍÎË©®− result in better spatial locality. If we call the ËÏÅ function repeatedly with different values of Í©Ö− and ÍÎË©®−, then we can recover a fascinating two-dimensional function of read throughput versus temporal and spatial locality. This function is called a memory mountain [112]. Every computer has a unique memory mountain that characterizes the ca- pabilities of its memory system. For example, Figure 6.41 shows the memory mountain for an Intel Core i7 Haswell system. In this example, the Í©Ö− varies from 16 KB to 128 MB, and the ÍÎË©®− varies from 1 to 12 elements, where each element is an 8-byte ÄÇÅ× ©ÅÎ. 676 Chapter 6 The Memory Hierarchy code/mem/mountain/mountain.c 1 ÄÇÅ× ®þÎþ‰›¡”¥‹¥›·`y mh ¶³− ×ÄÇ¾þÄ þËËþÔ Ñ−’ÄÄ ¾− ÎËþÌ−ËÍ©Å× hm 2 3 mh Î−ÍÎ k 'Î−ËþÎ− ÇÌ−Ë ð©ËÍÎ ‘−Ä−ÀÍ‘ −Ä−À−ÅÎÍ Çð þËËþÔ ‘®þÎþ‘ Ñ©Î³ 4 h ÍÎË©®− Çð ‘ÍÎË©®−‘j ÏÍ©Å×rÓr ÄÇÇÉ ÏÅËÇÄÄ©Å×l 5 hm 6 ©ÅÎ Î−ÍÎf©ÅÎ −Ä−ÀÍj ©ÅÎ ÍÎË©®−g 7 Õ 8 ÄÇÅ× ©j ÍÓp { ÍÎË©®−hpj ÍÓq { ÍÎË©®−hqj ÍÓr { ÍÎË©®−hry 9 ÄÇÅ× þ²²n { nj þ²²o { nj þ²²p { nj þ²²q { ny 10 ÄÇÅ× Ä−Å×Î³ { −Ä−ÀÍy 11 ÄÇÅ× Ä©À©Î { Ä−Å×Î³ k ÍÓry 12 13 mh £ÇÀ¾©Å− r −Ä−À−ÅÎÍ þÎ þ Î©À− hm 14 ðÇË f© { ny © z Ä©À©Îy © i{ ÍÓrg Õ 15 þ²²n { þ²²n i ®þÎþ‰©`y 16 þ²²o { þ²²o i ®þÎþ‰©iÍÎË©®−`y 17 þ²²p { þ²²p i ®þÎþ‰©iÍÓp`y 18 þ²²q { þ²²q i ®þÎþ‰©iÍÓq`y 19 Û 20 21 mh ƒ©Å©Í³ þÅÔ Ë−Àþ©Å©Å× −Ä−À−ÅÎÍ hm 22 ðÇË fy © z Ä−Å×Î³y ©iig Õ 23 þ²²n { þ²²n i ®þÎþ‰©`y 24 Û 25 Ë−ÎÏËÅ ffþ²²n i þ²²og i fþ²²p i þ²²qggy 26 Û 27 28 mh ËÏÅ k ‡ÏÅ Î−ÍÎf−Ä−ÀÍj ÍÎË©®−g þÅ® Ë−ÎÏËÅ Ë−þ® Î³ËÇÏ×³ÉÏÎ f›¢mÍgl 29 h ‘Í©Ö−‘ ©Í ©Å ¾ÔÎ−Íj ‘ÍÎË©®−‘ ©Í ©Å þËËþÔ −Ä−À−ÅÎÍj þÅ® ›³Ö ©Í 30 h £–• ²ÄÇ²Â ðË−ÊÏ−Å²Ô ©Å ›³Öl 31 hm 32 ®ÇÏ¾Ä− ËÏÅf©ÅÎ Í©Ö−j ©ÅÎ ÍÎË©®−j ®ÇÏ¾Ä− ›³Ög 33 Õ 34 ®ÇÏ¾Ä− ²Ô²Ä−Íy 35 ©ÅÎ −Ä−ÀÍ { Í©Ö− m Í©Ö−Çðf®ÇÏ¾Ä−gy 36 37 Î−ÍÎf−Ä−ÀÍj ÍÎË©®−gy mh „þËÀ ÏÉ Î³− ²þ²³− hm 38 ²Ô²Ä−Í { ð²Ô²pfÎ−ÍÎj −Ä−ÀÍj ÍÎË©®−j ngy mh £þÄÄ Î−ÍÎf−Ä−ÀÍjÍÎË©®−g hm 39 Ë−ÎÏËÅ fÍ©Ö− m ÍÎË©®−g m f²Ô²Ä−Í m ›³Ögy mh £ÇÅÌ−ËÎ ²Ô²Ä−Í ÎÇ ›¢mÍ hm 40 Û code/mem/mountain/mountain.c Figure 6.40 Functions that measure and compute read throughput. We can generate a memory mountain for a particular computer by calling the ËÏÅ function with different values of Í©Ö− (which corresponds to temporal locality) and ÍÎË©®− (which corresponds to spatial locality). Section 6.6 Putting It Together: The Impact of Caches on Program Performance 677 Core i7 Haswell 2.1 GHz 32 KB L1 d-cache 256 KB L2 cache 8 MB L3 cache 64 B block size Ridges of temporal locality 128 M 32 M 8 M 2 M 512 K 128 K 32 K 0 2,000 4,000 6,000 8,000 10,000 12,000 14,000 16,000 s1 s3 s5 s7 s9 s11 Size (bytes) Stride (x8 bytes) 8 M 2 M 512 K 128 K 32 K 0 0 s1 s3 s5 s7 Slopes of spatial localityRead throughput (MB/s) Mem L1 L2 L3 Figure 6.41 A memory mountain. Shows read throughput as a function of temporal and spatial locality. The geography of the Core i7 mountain reveals a rich structure. Perpendicular to the Í©Ö− axis are four ridges that correspond to the regions of temporal locality where the working set ﬁts entirely in the L1 cache, L2 cache, L3 cache, and main memory, respectively. Notice that there is more than an order of magnitude difference between the highest peak of the L1 ridge, where the CPU reads at a rate of over 14 GB/s, and the lowest point of the main memory ridge, where the CPU reads at a rate of 900 MB/s. On each of the L2, L3, and main memory ridges, there is a slope of spatial locality that falls downhill as the stride increases and spatial locality decreases. Notice that even when the working set is too large to ﬁt in any of the caches, the highest point on the main memory ridge is a factor of 8 higher than its lowest point. So even when a program has poor temporal locality, spatial locality can still come to the rescue and make a signiﬁcant difference. There is a particularly interesting ﬂat ridge line that extends perpendicular to the stride axis for a stride of 1, where the read throughput is a relatively ﬂat 12 GB/s, even though the working set exceeds the capacities of L1 and L2. This is apparently due to a hardware prefetching mechanism in the Core i7 memory system that automatically identiﬁes sequential stride-1 reference patterns and attempts to fetch those blocks into the cache before they are accessed. While the 678 Chapter 6 The Memory Hierarchy 14,000 12,000 10,000 8,000 6,000 4,000 2,000 064 M128 M32 M16 M8 M4 M2 M1,024 K512 K256 K128 K64 K32 K16 K Working set size (bytes)Read throughput (MB/s) L1 cache region L2 cache region L3 cache region Main memory region Figure 6.42 Ridges of temporal locality in the memory mountain. The graph shows a slice through Figure 6.41 with ÍÎË©®− = 8. details of the particular prefetching algorithm are not documented, it is clear from the memory mountain that the algorithm works best for small strides—yet another reason to favor sequential stride-1 accesses in your code. If we take a slice through the mountain, holding the stride constant as in Fig- ure 6.42, we can see the impact of cache size and temporal locality on performance. For sizes up to 32 KB, the working set ﬁts entirely in the L1 d-cache, and thus reads are served from L1 at throughput of about 12 GB/s. For sizes up to 256 KB, the working set ﬁts entirely in the uniﬁed L2 cache, and for sizes up to 8 MB, the working set ﬁts entirely in the uniﬁed L3 cache. Larger working set sizes are served primarily from main memory. The dips in read throughputs at the leftmost edges of the L2 and L3 cache regions—where the working set sizes of 256 KB and 8 MB are equal to their respective cache sizes—are interesting. It is not entirely clear why these dips occur. The only way to be sure is to perform a detailed cache simulation, but it is likely that the drops are caused by conﬂicts with other code and data lines. Slicing through the memory mountain in the opposite direction, holding the working set size constant, gives us some insight into the impact of spatial locality on the read throughput. For example, Figure 6.43 shows the slice for a ﬁxed working set size of 4 MB. This slice cuts along the L3 ridge in Figure 6.41, where the working set ﬁts entirely in the L3 cache but is too large for the L2 cache. Notice how the read throughput decreases steadily as the stride increases from one to eight words. In this region of the mountain, a read miss in L2 causes a block to be transferred from L3 to L2. This is followed by some number of hits Section 6.6 Putting It Together: The Impact of Caches on Program Performance 679 12,000 10,000 8,000 6,000 4,000 2,000 0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11Read throughput (MB/s) Stride (x8 bytes) One access per cache line Figure 6.43 A slope of spatial locality. The graph shows a slice through Figure 6.41 with Í©Ö− = 4MB. on the block in L2, depending on the stride. As the stride increases, the ratio of L2 misses to L2 hits increases. Since misses are served more slowly than hits, the read throughput decreases. Once the stride reaches eight 8-byte words, which on this system equals the block size of 64 bytes, every read request misses in L2 and must be served from L3. Thus, the read throughput for strides of at least eight is a constant rate determined by the rate that cache blocks can be transferred from L3 into L2. To summarize our discussion of the memory mountain, the performance of the memory system is not characterized by a single number. Instead, it is a mountain of temporal and spatial locality whose elevations can vary by over an order of magnitude. Wise programmers try to structure their programs so that they run in the peaks instead of the valleys. The aim is to exploit temporal locality so that heavily used words are fetched from the L1 cache, and to exploit spatial locality so that as many words as possible are accessed from a single L1 cache line. Practice Problem 6.21 (solution page 702) Use the memory mountain in Figure 6.41 to estimate the time, in CPU cycles, to read a 16-byte word from the L1 d-cache. 6.6.2 Rearranging Loops to Increase Spatial Locality Consider the problem of multiplying a pair of n × n matrices: C = AB. For exam- ple, if n = 2, then [ c11 c12 c21 c22 ] = [ a11 a12 a21 a22 ][ b11 b12 b21 b22 ] 680 Chapter 6 The Memory Hierarchy where c11 = a11b11 + a12b21 c12 = a11b12 + a12b22 c21 = a21b11 + a22b21 c22 = a21b12 + a22b22 A matrix multiply function is usually implemented using three nested loops, which are identiﬁed by their indices i, j , and k. If we permute the loops and make some other minor code changes, we can create the six functionally equivalent versions of matrix multiply shown in Figure 6.44. Each version is uniquely identiﬁed by the ordering of its loops. At a high level, the six versions are quite similar. If addition is associative, then each version computes an identical result.1 Each version performs O(n3) total operations and an identical number of adds and multiplies. Each of the n2 elements of A and B is read n times. Each of the n2 elements of C is computed by summing n values. However, if we analyze the behavior of the innermost loop iterations, we ﬁnd that there are differences in the number of accesses and the locality. For the purposes of this analysis, we make the following assumptions: . Each array is an n × n array of ®ÇÏ¾Ä−, with Í©Ö−Çð(®ÇÏ¾Ä−) = 8. . There is a single cache with a 32-byte block size (B = 32). . The array size n is so large that a single matrix row does not ﬁt in the L1 cache. . The compiler stores local variables in registers, and thus references to local variables inside loops do not require any load or store instructions. Figure 6.45 summarizes the results of our inner-loop analysis. Notice that the six versions pair up into three equivalence classes, which we denote by the pair of matrices that are accessed in the inner loop. For example, versions ij k and jik are members of class AB because they reference arrays A and B (but not C) in their innermost loop. For each class, we have counted the number of loads (reads) and stores (writes) in each inner-loop iteration, the number of references to A, B, and C that will miss in the cache in each loop iteration, and the total number of cache misses per iteration. The inner loops of the class AB routines (Figure 6.44(a) and (b)) scan a row of array A with a stride of 1. Since each cache block holds four 8-byte words, the miss rate for A is 0.25 misses per iteration. On the other hand, the inner loop scans a column of B with a stride of n. Since n is large, each access of array B results in a miss, for a total of 1.25 misses per iteration. The inner loops in the class AC routines (Figure 6.44(c) and (d)) have some problems. Each iteration performs two loads and a store (as opposed to the 1. As we learned in Chapter 2, ﬂoating-point addition is commutative, but in general not associative. In practice, if the matrices do not mix extremely large values with extremely small ones, as often is true when the matrices store physical properties, then the assumption of associativity is reasonable. Section 6.6 Putting It Together: The Impact of Caches on Program Performance 681 (a) Version ij k code/mem/matmult/mm.c 1 ðÇËf©{ny©zÅy ©iig 2 ðÇËfÁ{nyÁzÅy Áiig Õ 3 ÍÏÀ { nlny 4 ðÇËfÂ{nyÂzÅy Âiig 5 ÍÏÀ i{ ¡‰©`‰Â`h¢‰Â`‰Á`y 6 £‰©`‰Á` i{ ÍÏÀy 7 Û code/mem/matmult/mm.c (c) Version jki code/mem/matmult/mm.c 1 ðÇËfÁ{nyÁzÅy Áiig 2 ðÇËfÂ{nyÂzÅy Âiig Õ 3 Ë { ¢‰Â`‰Á`y 4 ðÇËf©{ny©zÅy ©iig 5 £‰©`‰Á` i{ ¡‰©`‰Â`hËy 6 Û code/mem/matmult/mm.c (e) Version kij code/mem/matmult/mm.c 1 ðÇËfÂ{nyÂzÅy Âiig 2 ðÇËf©{ny©zÅy ©iig Õ 3 Ë { ¡‰©`‰Â`y 4 ðÇËfÁ{nyÁzÅy Áiig 5 £‰©`‰Á` i{ Ëh¢‰Â`‰Á`y 6 Û code/mem/matmult/mm.c (b) Version jik code/mem/matmult/mm.c 1 ðÇËfÁ{nyÁzÅy Áiig 2 ðÇËf©{ny©zÅy ©iig Õ 3 ÍÏÀ { nlny 4 ðÇËfÂ{nyÂzÅy Âiig 5 ÍÏÀ i{ ¡‰©`‰Â`h¢‰Â`‰Á`y 6 £‰©`‰Á` i{ ÍÏÀy 7 Û code/mem/matmult/mm.c (d) Version kj i code/mem/matmult/mm.c 1 ðÇËfÂ{nyÂzÅy Âiig 2 ðÇËfÁ{nyÁzÅy Áiig Õ 3 Ë { ¢‰Â`‰Á`y 4 ðÇËf©{ny©zÅy ©iig 5 £‰©`‰Á` i{ ¡‰©`‰Â`hËy 6 Û code/mem/matmult/mm.c (f) Version ikj code/mem/matmult/mm.c 1 ðÇËf©{ny©zÅy ©iig 2 ðÇËfÂ{nyÂzÅy Âiig Õ 3 Ë { ¡‰©`‰Â`y 4 ðÇËfÁ{nyÁzÅy Áiig 5 £‰©`‰Á` i{ Ëh¢‰Â`‰Á`y 6 Û code/mem/matmult/mm.c Figure 6.44 Six versions of matrix multiply. Each version is uniquely identiﬁed by the ordering of its loops. Per iterationMatrix multiply version (class) Loads Stores A misses B misses C misses Total misses ij k & jik (AB) 2 0 0.25 1.00 0.00 1.25 jki & kj i (AC) 2 1 1.00 0.00 1.00 2.00 kij & ikj (BC) 2 1 0.00 0.25 0.25 0.50 Figure 6.45 Analysis of matrix multiply inner loops. The six versions partition into three equivalence classes, denoted by the pair of arrays that are accessed in the inner loop. 682 Chapter 6 The Memory Hierarchy 100 10 1 50 100 150 200 250 300 350 400 450 500 550 600 650 700 Array size (n)Cycles per inner-loop iteration jki kji ijk jik kij ikj Figure 6.46 Core i7 matrix multiply performance. class AB routines, which perform two loads and no stores). Second, the inner loop scans the columns of A and C with a stride of n. The result is a miss on each load, for a total of two misses per iteration. Notice that interchanging the loops has decreased the amount of spatial locality compared to the class AB routines. The BC routines (Figure 6.44(e) and (f)) present an interesting trade-off: With two loads and a store, they require one more memory operation than the AB routines. On the other hand, since the inner loop scans both B and C row-wise with a stride-1 access pattern, the miss rate on each array is only 0.25 misses per iteration, for a total of 0.50 misses per iteration. Figure 6.46 summarizes the performance of different versions of matrix mul- tiply on a Core i7 system. The graph plots the measured number of CPU cycles per inner-loop iteration as a function of array size (n). There are a number of interesting points to notice about this graph: . For large values of n, the fastest version runs almost 40 times faster than the slowest version, even though each performs the same number of ﬂoating-point arithmetic operations. . Pairs of versions with the same number of memory references and misses per iteration have almost identical measured performance. . The two versions with the worst memory behavior, in terms of the number of accesses and misses per iteration, run signiﬁcantly slower than the other four versions, which have fewer misses or fewer accesses, or both. . Miss rate, in this case, is a better predictor of performance than the total number of memory accesses. For example, the class BC routines, with 0.5 misses per iteration, perform much better than the class AB routines, with 1.25 misses per iteration, even though the class BC routines perform more Section 6.6 Putting It Together: The Impact of Caches on Program Performance 683 Web Aside MEM:BLOCKING Using blocking to increase temporal locality There is an interesting technique called blocking that can improve the temporal locality of inner loops. The general idea of blocking is to organize the data structures in a program into large chunks called blocks. (In this context, “block” refers to an application-level chunk of data, not to a cache block.) The program is structured so that it loads a chunk into the L1 cache, does all the reads and writes that it needs to on that chunk, then discards the chunk, loads in the next chunk, and so on. Unlike the simple loop transformations for improving spatial locality, blocking makes the code harder to read and understand. For this reason, it is best suited for optimizing compilers or frequently executed library routines. Blocking does not improve the performance of matrix multiply on the Core i7, because of its sophisticated prefetching hardware. Still, the technique is interesting to study and understand because it is a general concept that can produce big performance gains on systems that don’t prefetch. memory references in the inner loop (two loads and one store) than the class AB routines (two loads). . For large values of n, the performance of the fastest pair of versions (kij and ikj ) is constant. Even though the array is much larger than any of the SRAM cache memories, the prefetching hardware is smart enough to recognize the stride-1 access pattern, and fast enough to keep up with memory accesses in the tight inner loop. This is a stunning accomplishment by the Intel engi- neers who designed this memory system, providing even more incentive for programmers to develop programs with good spatial locality. 6.6.3 Exploiting Locality in Your Programs As we have seen, the memory system is organized as a hierarchy of storage devices, with smaller, faster devices toward the top and larger, slower devices toward the bottom. Because of this hierarchy, the effective rate that a program can access memory locations is not characterized by a single number. Rather, it is a wildly varying function of program locality (what we have dubbed the memory mountain) that can vary by orders of magnitude. Programs with good locality access most of their data from fast cache memories. Programs with poor locality access most of their data from the relatively slow DRAM main memory. Programmers who understand the nature of the memory hierarchy can ex- ploit this understanding to write more efﬁcient programs, regardless of the speciﬁc memory system organization. In particular, we recommend the following tech- niques: . Focus your attention on the inner loops, where the bulk of the computations and memory accesses occur. . Try to maximize the spatial locality in your programs by reading data objects sequentially, with stride 1, in the order they are stored in memory. . Try to maximize the temporal locality in your programs by using a data object as often as possible once it has been read from memory. 684 Chapter 6 The Memory Hierarchy 6.7 Summary The basic storage technologies are random access memories (RAMs), nonvolatile memories (ROMs), and disks. RAM comes in two basic forms. Static RAM (SRAM) is faster and more expensive and is used for cache memories. Dynamic RAM (DRAM) is slower and less expensive and is used for the main memory and graphics frame buffers. ROMs retain their information even if the supply voltage is turned off. They are used to store ﬁrmware. Rotating disks are mechanical non- volatile storage devices that hold enormous amounts of data at a low cost per bit, but with much longer access times than DRAM. Solid state disks (SSDs) based on nonvolatile ﬂash memory are becoming increasingly attractive alternatives to rotating disks for some applications. In general, faster storage technologies are more expensive per bit and have smaller capacities. The price and performance properties of these technologies are changing at dramatically different rates. In particular, DRAM and disk access times are much larger than CPU cycle times. Systems bridge these gaps by orga- nizing memory as a hierarchy of storage devices, with smaller, faster devices at the top and larger, slower devices at the bottom. Because well-written programs have good locality, most data are served from the higher levels, and the effect is a memory system that runs at the rate of the higher levels, but at the cost and capacity of the lower levels. Programmers can dramatically improve the running times of their programs by writing programs with good spatial and temporal locality. Exploiting SRAM- based cache memories is especially important. Programs that fetch data primarily from cache memories can run much faster than programs that fetch data primarily from memory. Bibliographic Notes Memory and disk technologies change rapidly. In our experience, the best sources of technical information are the Web pages maintained by the manufacturers. Companies such as Micron, Toshiba, and Samsung provide a wealth of current technical information on memory devices. The pages for Seagate and Western Digital provide similarly useful information about disks. Textbooks on circuit and logic design provide detailed information about memory technology [58, 89]. IEEE Spectrum published a series of survey arti- cles on DRAM [55]. The International Symposiums on Computer Architecture (ISCA) and High Performance Computer Architecture (HPCA) are common fo- rums for characterizations of DRAM memory performance [28, 29, 18]. Wilkes wrote the ﬁrst paper on cache memories [117]. Smith wrote a clas- sic survey [104]. Przybylski wrote an authoritative book on cache design [86]. Hennessy and Patterson provide a comprehensive discussion of cache design is- sues [46]. Levinthal wrote a comprehensive performance guide for the Intel Core i7 [70]. Stricker introduced the idea of the memory mountain as a comprehensive characterization of the memory system in [112] and suggested the term “memory mountain” informally in later presentations of the work. Compiler researchers Homework Problems 685 work to increase locality by automatically performing the kinds of manual code transformations we discussed in Section 6.6 [22, 32, 66, 72, 79, 87, 119]. Carter and colleagues have proposed a cache-aware memory controller [17]. Other re- searchers have developed cache-oblivious algorithms that are designed to run well without any explicit knowledge of the structure of the underlying cache mem- ory [30, 38, 39, 9]. There is a large body of literature on building and using disk storage. Many storage researchers look for ways to aggregate individual disks into larger, more robust, and more secure storage pools [20, 40, 41, 83, 121]. Others look for ways to use caches and locality to improve the performance of disk accesses [12, 21]. Systems such as Exokernel provide increased user-level control of disk and mem- ory resources [57]. Systems such as the Andrew File System [78] and Coda [94] extend the memory hierarchy across computer networks and mobile notebook computers. Schindler and Ganger developed an interesting tool that automatically characterizes the geometry and performance of SCSI disk drives [95]. Researchers have investigated techniques for building and using ﬂash-based SSDs [8, 81]. Homework Problems 6.22 ◆◆ Suppose you are asked to design a rotating disk where the number of bits per track is constant. You know that the number of bits per track is determined by the circumference of the innermost track, which you can assume is also the circumference of the hole. Thus, if you make the hole in the center of the disk larger, the number of bits per track increases, but the total number of tracks decreases. If you let r denote the radius of the platter, and x . r the radius of the hole, what value of x maximizes the capacity of the disk? 6.23 ◆ Estimate the average time (in ms) to access a sector on the following disk: Parameter Value Rotational rate 12,000 RPM Tavg seek 3ms Average number of sectors/track 500 6.24 ◆◆ Suppose thata2MBﬁle consisting of 512-byte logical blocks is stored on a disk drive with the following characteristics: Parameter Value Rotational rate 18,000 RPM Tavg seek 8ms Average number of sectors/track 2,000 Surfaces 4 Sector size 512 bytes 686 Chapter 6 The Memory Hierarchy For each case below, suppose that a program reads the logical blocks of the ﬁle sequentially, one after the other, and that the time to position the head over the ﬁrst block is Tavg seek + Tavg rotation. A. Best case: Estimate the optimal time (in ms) required to read the ﬁle given the best possible mapping of logical blocks to disk sectors (i.e., sequential). B. Random case: Estimate the time (in ms) required to read the ﬁle if blocks are mapped randomly to disk sectors. 6.25 ◆ The following table gives the parameters for a number of different caches. For each cache, ﬁll in the missing ﬁelds in the table. Recall that m is the number of physical address bits, C is the cache size (number of data bytes), B is the block size in bytes, E is the associativity, S is the number of cache sets, t is the number of tag bits, s is the number of set index bits, and b is the number of block offset bits. Cache mC B E S t s b 1. 32 1,024 4 4 2. 32 1,024 4 256 3. 32 1,024 8 1 4. 32 1,024 8 128 5. 32 1,024 32 1 6. 32 1,024 32 4 6.26 ◆ The following table gives the parameters for a number of different caches. Your task is to ﬁll in the missing ﬁelds in the table. Recall that m is the number of physical address bits, C is the cache size (number of data bytes), B is the block size in bytes, E is the associativity, S is the number of cache sets, t is the number of tag bits, s is the number of set index bits, and b is the number of block offset bits. Cache m CBE S t s b 1. 32 81 21 8 3 2. 32 2,048 128 23 7 2 3. 32 1,024 2 8 64 1 4. 32 1,024 216 23 4 6.27 ◆ This problem concerns the cache in Practice Problem 6.12. A. List all of the hex memory addresses that will hit in set 1. B. List all of the hex memory addresses that will hit in set 6. 6.28 ◆◆ This problem concerns the cache in Practice Problem 6.12. A. List all of the hex memory addresses that will hit in set 2. Homework Problems 687 B. List all of the hex memory addresses that will hit in set 4. C. List all of the hex memory addresses that will hit in set 5. D. List all of the hex memory addresses that will hit in set 7. 6.29 ◆◆ Suppose we have a system with the following properties: . The memory is byte addressable. . Memory accesses are to 1-byte words (not to 4-byte words). . Addresses are 12 bits wide. . The cache is two-way set associative (E = 2), with a 4-byte block size (B = 4) and four sets (S = 4). The contents of the cache are as follows, with all addresses, tags, and values given in hexadecimal notation: Set index Tag Valid Byte 0 Byte 1 Byte 2 Byte 3 0 00 1 40 41 42 43 83 1 FE 97 CC D0 1 00 1 44 45 46 47 83 0 ———— 2 00 1 48 49 4A 4B 40 0 ———— 3 FF 1 9A C0 03 FF 00 0 ———— A. The following diagram shows the format of an address (1 bit per box). Indicate (by labeling the diagram) the ﬁelds that would be used to determine the following: CO. The cache block offset CI. The cache set index CT. The cache tag 12 11 10 9 8 7 6 5 4 3 2 1 0 B. For each of the following memory accesses, indicate if it will be a cache hit or miss when carried out in sequence as listed. Also give the value of a read if it can be inferred from the information in the cache. Operation Address Hit? Read value (or unknown) Read 0x834 Write 0x836 Read 0xFFD 688 Chapter 6 The Memory Hierarchy 6.30 ◆ Suppose we have a system with the following properties: . The memory is byte addressable. . Memory accesses are to 1-byte words (not to 4-byte words). . Addresses are 13 bits wide. . The cache is 4-way set associative (E = 4), with a 4-byte block size (B = 4) and eight sets (S = 8). Consider the following cache state. All addresses, tags, and values are given in hexadecimal format. The Index column contains the set index for each set of four lines. The Tag columns contain the tag value for each line. The V columns contain the valid bit for each line. The Bytes 0–3 columns contain the data for each line, numbered left to right starting with byte 0 on the left. 4-way set associative cache Index Tag V Bytes 0–3 Tag V Bytes 0–3 Tag V Bytes 0–3 Tag V Bytes 0–3 0 F0 1 ED 32 0A A2 8A 1 BF 80 1D FC 14 1 EF 09 86 2A BC 0 25 44 6F 1A 1 BC 0 03 3E CD 38 A0 0 16 7B ED 5A BC 1 8E 4C DF 18 E4 1 FB B7 12 02 2 BC 1 54 9E 1E FA B6 1 DC 81 B2 14 00 0 B6 1F 7B 44 74 0 10 F5 B8 2E 3 BE 0 2F 7E 3D A8 C0 1 27 95 A4 74 C4 0 07 11 6B D8 BC 0 C7 B7 AF C2 4 7E 1 32 21 1C 2C 8A 1 22 C2 DC 34 BC 1 BA DD 37 D8 DC 0 E7 A2 39 BA 5 98 0 A9 76 2B EE 54 0 BC 91 D5 92 98 1 80 BA 9B F6 BC 1 48 16 81 0A 6 38 0 5D 4D F7 DA BC 1 69 C2 8C 74 8A 1 A8 CE 7F DA 38 1 FA 93 EB 48 7 8A 1 04 2A 32 6A 9E 0 B1 86 56 0E CC 1 96 30 47 F2 BC 1 F8 1D 42 30 A. What is the size (C) of this cache in bytes? B. The box that follows shows the format of an address (1 bit per box). Indicate (by labeling the diagram) the ﬁelds that would be used to determine the following: CO. The cache block offset CI. The cache set index CT. The cache tag 12 11 10 9 8 7 6 5 4 3 2 1 0 6.31 ◆◆ Suppose that a program using the cache in Problem 6.30 references the 1-byte word at address nÓnuo¡. Indicate the cache entry accessed and the cache byte value returned in hex. Indicate whether a cache miss occurs. If there is a cache miss, enter “—” for “Cache byte returned.” Hint: Pay attention to those valid bits! Homework Problems 689 A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 B. Memory reference: Parameter Value Block offset (CO) 0x Index (CI) 0x Cache tag (CT) 0x Cache hit? (Y/N) Cache byte returned 0x 6.32 ◆◆ Repeat Problem 6.31 for memory address nÓot¥v. A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 B. Memory reference: Parameter Value Cache offset (CO) 0x Cache index (CI) 0x Cache tag (CT) 0x Cache hit? (Y/N) Cache byte returned 0x 6.33 ◆◆ For the cache in Problem 6.30, list the eight memory addresses (in hex) that will hit in set 2. 6.34 ◆◆ Consider the following matrix transpose routine: 1 ÎÔÉ−®−ð ©ÅÎ þËËþÔ‰r`‰r`y 2 3 ÌÇ©® ÎËþÅÍÉÇÍ−pfþËËþÔ ®ÍÎj þËËþÔ ÍË²g 4 Õ 5 ©ÅÎ ©j Áy 6 690 Chapter 6 The Memory Hierarchy 7 ðÇËf©{ny©zry ©iig Õ 8 ðÇËfÁ{nyÁzry Áiig Õ 9 ®ÍÎ‰Á`‰©` { ÍË²‰©`‰Á`y 10 Û 11 Û 12 Û Assume this code runs on a machine with the following properties: . Í©Ö−Çð(©ÅÎ) = 4. . The ÍË² array starts at address 0 and the ®ÍÎ array starts at address 64 (decimal). . There is a single L1 data cache that is direct-mapped, write-through, write- allocate, with a block size of 16 bytes. . The cache has a total size of 32 data bytes, and the cache is initially empty. . Accesses to the ÍË² and ®ÍÎ arrays are the only sources of read and write misses, respectively. A. For each ËÇÑ and ²ÇÄ, indicate whether the access to ÍË²‰ËÇÑ`‰²ÇÄ` and ®ÍÎ‰ËÇÑ`‰²ÇÄ` is a hit (h) or a miss (m). For example, reading ÍË²‰n`‰n` is a miss and writing ®ÍÎ‰n`‰n` is also a miss. ®ÍÎ array ÍË² array Col. 0 Col. 1 Col. 2 Col. 3 Col. 0 Col. 1 Col. 2 Col. 3 Row 0 m Row 0 m Row 1 Row 1 Row 2 Row 2 Row 3 Row 3 6.35 ◆◆ Repeat Problem 6.34 for a cache with a total size of 128 data bytes. ®ÍÎ array ÍË² array Col. 0 Col. 1 Col. 2 Col. 3 Col. 0 Col. 1 Col. 2 Col. 3 Row 0 Row 0 Row 1 Row 1 Row 2 Row 2 Row 3 Row 3 6.36 ◆◆ This problem tests your ability to predict the cache behavior of C code. You are given the following code to analyze: 1 ©ÅÎ Ó‰p`‰opv`y 2 ©ÅÎ ©y Homework Problems 691 3 ©ÅÎ ÍÏÀ { ny 4 5 ðÇË f© { ny © z opvy ©iig Õ 6 ÍÏÀ i{ Ó‰n`‰©` h Ó‰o`‰©`y 7 Û Assume we execute this under the following conditions: . Í©Ö−Çð(©ÅÎ) = 4. . Array Ó begins at memory address nÓn and is stored in row-major order. . In each case below, the cache is initially empty. . The only memory accesses are to the entries of the array Ó. All other variables are stored in registers. Given these assumptions, estimate the miss rates for the following cases: A. Case 1: Assume the cache is 512 bytes, direct-mapped, with 16-byte cache blocks. What is the miss rate? B. Case 2: What is the miss rate if we double the cache size to 1,024 bytes? C. Case 3: Now assume the cache is 512 bytes, two-way set associative using an LRU replacement policy, with 16-byte cache blocks. What is the cache miss rate? D. For case 3, will a larger cache size help to reduce the miss rate? Why or why not? E. For case 3, will a larger block size help to reduce the miss rate? Why or why not? 6.37 ◆◆ This is another problem that tests your ability to analyze the cache behavior of C code. Assume we execute the three summation functions in Figure 6.47 under the following conditions: . Í©Ö−Çð(©ÅÎ) = 4. . The machine has a 4 KB direct-mapped cache with a 16-byte block size. . Within the two loops, the code uses memory accesses only for the array data. The loop indices and the value ÍÏÀ are held in registers. . Array þ is stored starting at memory address nÓnvnnnnnn. Fill in the table for the approximate cache miss rate for the two cases N = 64 and N = 60. Function N = 64 N = 60 ÍÏÀ¡ ÍÏÀ¢ ÍÏÀ£ 692 Chapter 6 The Memory Hierarchy 1 ÎÔÉ−®−ð ©ÅÎ þËËþÔˆÎ‰ﬁ`‰ﬁ`y 2 3 ©ÅÎ ÍÏÀ¡fþËËþÔˆÎ þg 4 Õ 5 ©ÅÎ ©j Áy 6 ©ÅÎ ÍÏÀ { ny 7 ðÇËf©{ny©zﬁy ©iig 8 ðÇËfÁ{nyÁzﬁy Áiig Õ 9 ÍÏÀ i{ þ‰©`‰Á`y 10 Û 11 Ë−ÎÏËÅ ÍÏÀy 12 Û 13 14 ©ÅÎ ÍÏÀ¢fþËËþÔˆÎ þg 15 Õ 16 ©ÅÎ ©j Áy 17 ©ÅÎ ÍÏÀ { ny 18 ðÇËfÁ{nyÁzﬁy Áiig 19 ðÇËf©{ny©zﬁy ©iig Õ 20 ÍÏÀ i{ þ‰©`‰Á`y 21 Û 22 Ë−ÎÏËÅ ÍÏÀy 23 Û 24 25 ©ÅÎ ÍÏÀ£fþËËþÔˆÎ þg 26 Õ 27 ©ÅÎ ©j Áy 28 ©ÅÎ ÍÏÀ { ny 29 ðÇËfÁ{nyÁzﬁy Ái{pg 30 ðÇËf©{ny©zﬁy ©i{pg Õ 31 ÍÏÀ i{ fþ‰©`‰Á` i þ‰©io`‰Á` 32 i þ‰©`‰Áio` i þ‰©io`‰Áio`gy 33 Û 34 Ë−ÎÏËÅ ÍÏÀy 35 Û Figure 6.47 Functions referenced in Problem 6.37. 6.38 ◆ 3M decides to make Post-its by printing yellow squares on white pieces of paper. As part of the printing process, they need to set the CMYK (cyan, magenta, yellow, black) value for every point in the square. 3M hires you to determine the efﬁciency of the following algorithms on a machine with a 1,024-byte direct-mapped data cache with 16-byte blocks. You are given the following deﬁnitions: Homework Problems 693 1 ÍÎËÏ²Î ÉÇ©ÅÎˆ²ÇÄÇË Õ 2 ©ÅÎ ²y 3 ©ÅÎ Ày 4 ©ÅÎ Ôy 5 ©ÅÎ Ây 6 Ûy 7 8 ÍÎËÏ²Î ÉÇ©ÅÎˆ²ÇÄÇË ÍÊÏþË−‰ot`‰ot`y 9 ©ÅÎ ©j Áy Assume the following: . Í©Ö−Çð(©ÅÎ) = 4. . ÍÊÏþË− begins at memory address 0. . The cache is initially empty. . The only memory accesses are to the entries of the array ÍÊÏþË−. Variables © and Á are stored in registers. Determine the cache performance of the following code: 1 ðÇË f© { osy © |{ ny ©kkgÕ 2 ðÇË fÁ { osy Á |{ ny Ákkg Õ 3 ÍÊÏþË−‰©`‰Á`l² { ny 4 ÍÊÏþË−‰©`‰Á`lÀ { ny 5 ÍÊÏþË−‰©`‰Á`lÔ { oy 6 ÍÊÏþË−‰©`‰Á`lÂ { ny 7 Û 8 Û A. What is the total number of writes? B. What is the total number of writes that hit in the cache? C. What is the hit rate? 6.39 ◆ Given the assumptions in Problem 6.38, determine the cache performance of the following code: 1 ðÇË f© { osy © |{ ny ©kkgÕ 2 ðÇË fÁ { osy Á |{ ny Ákkg Õ 3 ÍÊÏþË−‰Á`‰©`l² { ny 4 ÍÊÏþË−‰Á`‰©`lÀ { ny 5 ÍÊÏþË−‰Á`‰©`lÔ { oy 6 ÍÊÏþË−‰Á`‰©`lÂ { ny 7 Û 8 Û 694 Chapter 6 The Memory Hierarchy A. What is the total number of writes? B. What is the total number of writes that hit in the cache? C. What is the hit rate? 6.40 ◆ Given the assumptions in Problem 6.38, determine the cache performance of the following code: 1 ðÇË f© { osy © |{ ny ©kkg Õ 2 ðÇË fÁ { osy Á |{ ny Ákkg Õ 3 ÍÊÏþË−‰©`‰Á`lÔ { oy 4 Û 5 Û 6 ðÇË f© { osy © |{ ny ©kkg Õ 7 ðÇË fÁ { osy Á |{ ny Ákkg Õ 8 ÍÊÏþË−‰©`‰Á`l² { ny 9 ÍÊÏþË−‰©`‰Á`lÀ { ny 10 ÍÊÏþË−‰©`‰Á`lÂ { ny 11 Û 12 Û A. What is the total number of writes? B. What is the total number of writes that hit in the cache? C. What is the hit rate? 6.41 ◆◆ You are writing a new 3D game that you hope will earn you fame and fortune. You are currently working on a function to blank the screen buffer before drawing the next frame. The screen you are working with is a 640 × 480 array of pixels. The machine you are working on has a 32 KB direct-mapped cache with 8-byte lines. The C structures you are using are as follows: 1 ÍÎËÏ²Î É©Ó−Ä Õ 2 ²³þË Ëy 3 ²³þË ×y 4 ²³þË ¾y 5 ²³þË þy 6 Ûy 7 8 ÍÎËÏ²Î É©Ó−Ä ¾Ïðð−Ë‰rvn`‰trn`y 9 ©ÅÎ ©j Áy 10 ²³þË h²ÉÎËy 11 ©ÅÎ h©ÉÎËy Assume the following: . Í©Ö−Çð(²³þË) = 1 and Í©Ö−Çð(©ÅÎ) = 4. Homework Problems 695 . ¾Ïðð−Ë begins at memory address 0. . The cache is initially empty. . The only memory accesses are to the entries of the array ¾Ïðð−Ë. Variables ©, Á, ²ÉÎË, and ©ÉÎË are stored in registers. What percentage of writes in the following code will hit in the cache? 1 ðÇË fÁ { tqwy Á |{ ny Ákkg Õ 2 ðÇË f© { ruwy © |{ ny ©kkgÕ 3 ¾Ïðð−Ë‰©`‰Á`lË { ny 4 ¾Ïðð−Ë‰©`‰Á`l× { ny 5 ¾Ïðð−Ë‰©`‰Á`l¾ { ny 6 ¾Ïðð−Ë‰©`‰Á`lþ { ny 7 Û 8 Û 6.42 ◆◆ Given the assumptions in Problem 6.41, what percentage of writes in the following code will hit in the cache? 1 ²³þË h²ÉÎË { f²³þË hg ¾Ïðð−Ëy 2 ðÇË fy ²ÉÎË z fff²³þË hg ¾Ïðð−Ëg i trn h rvn h rgy ²ÉÎËiig 3 h²ÉÎË { ny 6.43 ◆◆ Given the assumptions in Problem 6.41, what percentage of writes in the following code will hit in the cache? 1 ©ÅÎ h©ÉÎË { f©ÅÎ hg¾Ïðð−Ëy 2 ðÇË fy ©ÉÎË z ff©ÅÎ hg¾Ïðð−Ë i trnhrvngy ©ÉÎËiig 3 h©ÉÎË { ny 6.44 ◆◆◆ Download the ÀÇÏÅÎþ©Å program from the CS:APP Web site and run it on your favorite PC/Linux system. Use the results to estimate the sizes of the caches on your system. 6.45 ◆◆◆◆ In this assignment, you will apply the concepts you learned in Chapters 5 and 6 to the problem of optimizing code for a memory-intensive application. Consider a procedure to copy and transpose the elements of an N × N matrix of type ©ÅÎ. That is, for source matrix S and destination matrix D, we want to copy each element si,j to dj,i. This code can be written with a simple loop, 1 ÌÇ©® ÎËþÅÍÉÇÍ−f©ÅÎ h®ÍÎj ©ÅÎ hÍË²j ©ÅÎ ®©Àg 2 Õ 3 ©ÅÎ ©j Áy 4 696 Chapter 6 The Memory Hierarchy 5 ðÇË f© { ny © z ®©Ày ©iig 6 ðÇË fÁ { ny Á z ®©Ày Áiig 7 ®ÍÎ‰Áh®©À i ©` { ÍË²‰©h®©À i Á`y 8 Û where the arguments to the procedure are pointers to the destination (®ÍÎ) and source (ÍË²) matrices, as well as the matrix size N (®©À). Your job is to devise a transpose routine that runs as fast as possible. 6.46 ◆◆◆◆ This assignment is an intriguing variation of Problem 6.45. Consider the problem of converting a directed graph g into its undirected counterpart g′. The graph g′ has an edge from vertex u to vertex v if and only if there is an edge from u to v or from v to u in the original graph g. The graph g is represented by its adjacency matrix G as follows. If N is the number of vertices in g, then G is an N × N matrix and its entries are all either 0 or 1. Suppose the vertices of g are named v0,v1,v2,...,vN −1. Then G[i][j ] is 1 if there is an edge from vi to vj and is 0 otherwise. Observe that the elements on the diagonal of an adjacency matrix are always 1 and that the adjacency matrix of an undirected graph is symmetric. This code can be written with a simple loop: 1 ÌÇ©® ²ÇÄˆ²ÇÅÌ−ËÎf©ÅÎ h§j ©ÅÎ ®©Àg Õ 2 ©ÅÎ ©j Áy 3 4 ðÇË f© { ny © z ®©Ày ©iig 5 ðÇË fÁ { ny Á z ®©Ày Áiig 6 §‰Áh®©À i ©` { §‰Áh®©À i ©` ÚÚ §‰©h®©À i Á`y 7 Û Your job is to devise a conversion routine that runs as fast as possible. As before, you will need to apply concepts you learned in Chapters 5 and 6 to come up with a good solution. Solutions to Practice Problems Solution to Problem 6.1 (page 620) The idea here is to minimize the number of address bits by minimizing the aspect ratio max(r, c)/ min(r, c). In other words, the squarer the array, the fewer the address bits. Organization rc br bc max(br,bc) 16 × 14 4 2 2 2 16 × 44 4 2 2 2 128 × 816 8 4 3 4 512 × 432 16 5 4 5 1,024 × 432 32 5 5 5 Solutions to Practice Problems 697 Solution to Problem 6.2 (page 628) The point of this little drill is to make sure you understand the relationship between cylinders and tracks. Once you have that straight, just plug and chug: Disk capacity = 1,024 bytes sector × 500 sectors track × 15,000 tracks surface × 2 surfaces platter × 3 platters disk = 46,080,000,000 bytes = 46.08 GB Solution to Problem 6.3 (page 631) The solution to this problem is a straightforward application of the formula for disk access time. The average rotational latency (in ms) is Tavg rotation = 1/2 × Tmax rotation = 1/2 × (60 secs/12,000 RPM) × 1,000 ms/sec ≈ 2.5ms The average transfer time is Tavg transfer = (60 secs/12,000 RPM) × 1/300 sectors/track × 1,000 ms/sec ≈ 0.016 ms Putting it all together, the total estimated access time is Taccess = Tavg seek + Tavg rotation + Tavg transfer = 5ms + 2.5ms + 0.016 ms ≈ 7.516 ms Solution to Problem 6.4 (page 631) This is a good check of your understanding of the factors that affect disk perfor- mance. First we need to determine a few basic properties of the ﬁle and the disk. The ﬁle consists of 10,000 512-byte logical blocks. For the disk, Tavg seek = 6 ms, Tmax rotation = 4.61 ms, and Tavg rotation = 2.30 ms. A. Best case: In the optimal case, the blocks are mapped to contiguous sectors, on the same cylinder, that can be read one after the other without moving the head. Once the head is positioned over the ﬁrst sector it takes two full rotations (5,000 sectors per rotation) of the disk to read all 10,000 blocks. So the total time to read the ﬁle is Tavg seek + Tavg rotation + 2 × Tmax rotation = 6 + 2.30 + 9.22 = 17.52 ms. B. Random case: In this case, where blocks are mapped randomly to sectors, reading each of the 10,000 blocks requires Tavg seek + Tavg rotation ms, so the total time to read the ﬁle is (Tavg seek + Tavg rotation) × 10,000 = 83,000 ms (83 seconds!). You can see now why it’s often a good idea to defragment your disk drive! 698 Chapter 6 The Memory Hierarchy Solution to Problem 6.5 (page 637) This is a simple problem that will give you some interesting insights into the feasi- bility of SSDs. Recall that for disks, 1 PB = 109 MB. Then the following straight- forward translation of units yields the following predicted times for each case: A. Worst-case sequential writes (520 MB/s): (109 × 128) × (1/520) × (1/(86,400 × 365)) ≈ 7 years B. Worst-case random writes (205 MB/s): (109 × 128) × (1/205) × (1/(86,400 × 365)) ≈ 19 years C. Average case (50 GB/day): (109 × 128) × (1/50,000) × (1/365) ≈ 6,912 years So even if the SSD operates continuously, it should last for at least 7 years, which is longer than the expected lifetime of most computers. Solution to Problem 6.6 (page 640) In the 10-year period between 2005 and 2015, the unit price of rotating disks dropped by a factor of 166, which means the price is dropping by roughly a factor of 2 every 18 months or so. Assuming this trend continues, a petabyte of storage, which costs about $30,000 in 2015, will drop below $200 after about eight of these factor-of-2 reductions. Since these are occurring every 18 months, we might expect a petabyte of storage to be available for $200 around the year 2027. Solution to Problem 6.7 (page 644) To create a stride-1 reference pattern, the loops must be permuted so that the rightmost indices change most rapidly. 1 ©ÅÎ ÉËÇ®Ï²ÎþËËþÔq®f©ÅÎ þ‰ﬁ`‰ﬁ`‰ﬁ`g 2 Õ 3 ©ÅÎ ©j Áj Âj ÉËÇ®Ï²Î { oy 4 5 ðÇË fÁ { ﬁkoy Á |{ ny Ákkg Õ 6 ðÇË fÂ { ﬁkoy Â |{ ny Âkkg Õ 7 ðÇË f© { ﬁkoy © |{ ny ©kkg Õ 8 ÉËÇ®Ï²Î h{ þ‰Á`‰Â`‰©`y 9 Û 10 Û 11 Û 12 Ë−ÎÏËÅ ÉËÇ®Ï²Îy 13 Û This is an important idea. Make sure you understand why this particular loop permutation results in a stride-1 access pattern. Solutions to Practice Problems 699 Solution to Problem 6.8 (page 645) The key to solving this problem is to visualize how the array is laid out in memory and then analyze the reference patterns. Function ²Ä−þËo accesses the array using a stride-1 reference pattern and thus clearly has the best spatial locality. Function ²Ä−þËp scans each of the N structs in order, which is good, but within each struct it hops around in a non-stride-1 pattern at the following offsets from the beginning of the struct: 0, 12, 4, 16, 8, 20. So ²Ä−þËp has worse spatial locality than ²Ä−þËo. Function ²Ä−þËq not only hops around within each struct, but also hops from struct to struct. So ²Ä−þËq exhibits worse spatial locality than ²Ä−þËp and ²Ä−þËo. Solution to Problem 6.9 (page 652) The solution is a straightforward application of the deﬁnitions of the various cache parameters in Figure 6.26. Not very exciting, but you need to understand how the cache organization induces these partitions in the address bits before you can really understand how caches work. Cache mC B E S t s b 1. 32 1,024 4 1 256 22 8 2 2. 32 1,024 8 4 32 24 5 3 3. 32 1,024 32 32 1 27 0 5 Solution to Problem 6.10 (page 660) The padding eliminates the conﬂict misses. Thus, three-fourths of the references are hits. Solution to Problem 6.11 (page 660) Sometimes, understanding why something is a bad idea helps you understand why the alternative is a good idea. Here, the bad idea we are looking at is indexing the cache with the high-order bits instead of the middle bits. A. With high-order bit indexing, each contiguous array chunk consists of 2t blocks, where t is the number of tag bits. Thus, the ﬁrst 2t contiguous blocks of the array would map to set 0, the next 2t blocks would map to set 1, and so on. B. For a direct-mapped cache where (S,E,B,m) = (512, 1, 32, 32), the cache capacity is 512 32-byte blocks with t = 18 tag bits in each cache line. Thus, the ﬁrst 218 blocks in the array would map to set 0, the next 218 blocks to set 1. Since our array consists of only (4,096 × 4)/32 = 512 blocks, all of the blocks in the array map to set 0. Thus, the cache will hold at most 1 array block at any point in time, even though the array is small enough to ﬁt entirely in the cache. Clearly, using high-order bit indexing makes poor use of the cache. Solution to Problem 6.12 (page 664) The 2 low-order bits are the block offset (CO), followed by 3 bits of set index (CI), with the remaining bits serving as the tag (CT): 700 Chapter 6 The Memory Hierarchy 12 11 10 9 8 7 6 5 4 3 2 1 0 CT CT CT CT CT CT CT CT CI CI CI CO CO Solution to Problem 6.13 (page 664) Address: nÓn⁄sq A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 0110101010011 CT CT CT CT CT CT CT CT CI CI CI CO CO B. Memory reference: Parameter Value Cache block offset (CO) nÓq Cache set index (CI) nÓr Cache tag (CT) nÓt¡ Cache hit? (Y/N) N Cache byte returned — Solution to Problem 6.14 (page 665) Address: nÓn£¢r A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 0110010110100 CT CT CT CT CT CT CT CT CI CI CI CO CO B. Memory reference: Parameter Value Cache block offset (CO) nÓn Cache set index (CI) nÓs Cache tag (CT) nÓts Cache hit? (Y/N) N Cache byte returned — Solution to Problem 6.15 (page 665) Address: nÓn¡qo A. Address format (1 bit per box): 12 11 10 9 8 7 6 5 4 3 2 1 0 0101000110001 CT CT CT CT CT CT CT CT CI CI CI CO CO B. Memory reference: Solutions to Practice Problems 701 Parameter Value Cache block offset nÓo Cache set index nÓr Cache tag nÓso Cache hit? (Y/N) N Cache byte returned —- Solution to Problem 6.16 (page 666) This problem is a sort of inverse version of Practice Problems 6.12–6.15 that requires you to work backward from the contents of the cache to derive the addresses that will hit in a particular set. In this case, set 3 contains one valid line with a tag of nÓqp. Since there is only one valid line in the set, four addresses will hit. These addresses have the binary form n noon nonn ooÓÓ. Thus, the four hex addresses that hit in set 3 are nÓntr£, nÓntr⁄, nÓntr¥, and nÓntrƒ Solution to Problem 6.17 (page 672) A. The key to solving this problem is to visualize the picture in Figure 6.48. Notice that each cache line holds exactly one row of the array, that the cache is exactly large enough to hold one array, and that for all i, row i of ÍË² and ®ÍÎ maps to the same cache line. Because the cache is too small to hold both arrays, references to one array keep evicting useful lines from the other array. For example, the write to ®ÍÎ‰n`‰n` evicts the line that was loaded when we read ÍË²‰n`‰n`. So when we next read ÍË²‰n`‰o`, we have a miss. ®ÍÎ array ÍË² array Col. 0 Col. 1 Col. 0 Col. 1 Row 0 m m Row 0 m m Row 1 m m Row 1 m h B. When the cache is 32 bytes, it is large enough to hold both arrays. Thus, the only misses are the initial cold misses. ®ÍÎ array ÍË² array Col. 0 Col. 1 Col. 0 Col. 1 Row 0 m h Row 0 m h Row 1 m h Row 1 m h Figure 6.48 Figure for solution to Problem 6.17. Main memory 0 16 Line 0 Line 1 src dst Cache 702 Chapter 6 The Memory Hierarchy Solution to Problem 6.18 (page 673) Each 32-byte cache line holds two contiguous þÄ×þ−ˆÉÇÍ©Î©ÇÅ structures. Each loop visits these structures in memory order, reading one integer element each time. So the pattern for each loop is miss, hit, miss, hit, and so on. Notice that for this problem we could have predicted the miss rate without actually enumerating the total number of reads and misses. A. What is the total number of read accesses? 2,048 reads. B. What is the total number of read accesses that miss in the cache? 1,024 misses. C. What is the miss rate? 1024/2048 = 50%. Solution to Problem 6.19 (page 674) The key to this problem is noticing that the cache can only hold 1/2 of the ar- ray. So the column-wise scan of the second half of the array evicts the lines that were loaded during the scan of the ﬁrst half. For example, reading the ﬁrst ele- ment of ×Ë©®‰v`‰n` evicts the line that was loaded when we read elements from ×Ë©®‰n`‰n`. This line also contained ×Ë©®‰n`‰o`. So when we begin scanning the next column, the reference to the ﬁrst element of ×Ë©®‰n`‰o` misses. A. What is the total number of read accesses? 2,048 reads. B. What is the total number of read accesses that hit in the cache? 1,024 misses. C. What is the hit rate? 1024/2048 = 50%. D. What would the hit rate be if the cache were twice as big? If the cache were twice as big, it could hold the entire ×Ë©® array. The only misses would be the initial cold misses, and the hit rate would be 3/4 = 75%. Solution to Problem 6.20 (page 674) This loop has a nice stride-1 reference pattern, and thus the only misses are the initial cold misses. A. What is the total number of read accesses? 2,048 reads. B. What is the total number of read accesses that hit in the cache? 1,536 misses. C. What is the hit rate? 1536/2048 = 75%. D. What would the hit rate be if the cache were twice as big? Increasing the cache size by any amount would not change the miss rate, since cold misses are unavoidable. Solution to Problem 6.21 (page 679) The sustained throughput using large strides from L1 is about 12,000 MB/s, the clock frequency is 2,100 MHz, and the individual read accesses are in units of 16-byte ÄÇÅ×s. Thus, from this graph we can estimate that it takes roughly 2,100/12,000 × 16 = 2.8 ≈ 3.0 cycles to access a word from L1 on this machine, which is roughly 1.25 times faster than the nominal 4-cycle latency from L1. This is due to the parallelism of the 4 × 4 unrolled loop, which allows multiple loads to be in ﬂight at the same time. Part II Running Programs on a System O ur exploration of computer systems continues with a closer look at the systems software that builds and runs application programs. The linker combines different parts of our programs into a sin- gle ﬁle that can be loaded into memory and executed by the processor. Modern operating systems cooperate with the hardware to provide each program with the illusion that it has exclusive use of a processor and the main memory, when in reality multiple programs are running on the sys- tem at any point in time. In the ﬁrst part of this book, you developed a good understanding of the interaction between your programs and the hardware. Part II of the book will broaden your view of systems by giving you a solid understand- ing of the interactions between your programs and the operating system. You will learn how to use services provided by the operating system to build system-level programs such as Unix shells and dynamic memory allocation packages. 703 This page is intentionally left blank. CHAPTER 7 Linking 7.1 Compiler Drivers 707 7.2 Static Linking 708 7.3 Object Files 709 7.4 Relocatable Object Files 710 7.5 Symbols and Symbol Tables 711 7.6 Symbol Resolution 715 7.7 Relocation 725 7.8 Executable Object Files 731 7.9 Loading Executable Object Files 733 7.10 Dynamic Linking with Shared Libraries 734 7.11 Loading and Linking Shared Libraries from Applications 737 7.12 Position-Independent Code (PIC) 740 7.13 Library Interpositioning 743 7.14 Tools for Manipulating Object Files 749 7.15 Summary 749 Bibliographic Notes 750 Homework Problems 750 Solutions to Practice Problems 753 705 706 Chapter 7 Linking L inking is the process of collecting and combining various pieces of code and data into a single ﬁle that can be loaded (copied) into memory and executed. Linking can be performed at compile time, when the source code is translated into machine code; at load time, when the program is loaded into memory and executed by the loader; and even at run time, by application programs. On early computer systems, linking was performed manually. On modern systems, linking is performed automatically by programs called linkers. Linkers play a crucial role in software development because they enable separate compilation. Instead of organizing a large application as one monolithic source ﬁle, we can decompose it into smaller, more manageable modules that can be modiﬁed and compiled separately. When we change one of these modules, we simply recompile it and relink the application, without having to recompile the other ﬁles. Linking is usually handled quietly by the linker and is not an important issue for students who are building small programs in introductory programming classes. So why bother learning about linking? . Understanding linkers will help you build large programs. Programmers who build large programs often encounter linker errors caused by missing modules, missing libraries, or incompatible library versions. Unless you understand how a linker resolves references, what a library is, and how a linker uses a library to resolve references, these kinds of errors will be bafﬂing and frustrating. . Understanding linkers will help you avoid dangerous programming errors.The decisions that Linux linkers make when they resolve symbol references can silently affect the correctness of your programs. Programs that incorrectly deﬁne multiple global variables can pass through the linker without any warn- ings in the default case. The resulting programs can exhibit bafﬂing run-time behavior and are extremely difﬁcult to debug. We will show you how this hap- pens and how to avoid it. . Understanding linking will help you understand how language scoping rules are implemented.For example, what is the difference between global and local variables? What does it really mean when you deﬁne a variable or function with the ÍÎþÎ©² attribute? . Understanding linking will help you understand other important systems con- cepts. The executable object ﬁles produced by linkers play key roles in impor- tant systems functions such as loading and running programs, virtual memory, paging, and memory mapping. . Understanding linking will enable you to exploit shared libraries. For many years, linking was considered to be fairly straightforward and uninteresting. However, with the increased importance of shared libraries and dynamic linking in modern operating systems, linking is a sophisticated process that provides the knowledgeable programmer with signiﬁcant power. For exam- ple, many software products use shared libraries to upgrade shrink-wrapped binaries at run time. Also, many Web servers rely on dynamic linking of shared libraries to serve dynamic content. Section 7.1 Compiler Drivers 707 (a) Àþ©Ål² code/link/main.c 1 ©ÅÎ ÍÏÀf©ÅÎ hþj ©ÅÎ Ågy 2 3 ©ÅÎ þËËþÔ‰p` { Õoj pÛy 4 5 ©ÅÎ Àþ©Åfg 6 Õ 7 ©ÅÎ ÌþÄ { ÍÏÀfþËËþÔj pgy 8 Ë−ÎÏËÅ ÌþÄy 9 Û code/link/main.c (b) ÍÏÀl² code/link/sum.c 1 ©ÅÎ ÍÏÀf©ÅÎ hþj ©ÅÎ Åg 2 Õ 3 ©ÅÎ©jÍ{ny 4 5 ðÇËf©{ny©zÅy ©iig Õ 6 Í i{ þ‰©`y 7 Û 8 Ë−ÎÏËÅ Íy 9 Û code/link/sum.c Figure 7.1 Example program 1. The example program consists of two source ﬁles, Àþ©Ål² and ÍÏÀl². The Àþ©Å function initializes an array of ©ÅÎs, and then calls the ÍÏÀ function to sum the array elements. This chapter provides a thorough discussion of all aspects of linking, from traditional static linking, to dynamic linking of shared libraries at load time, to dynamic linking of shared libraries at run time. We will describe the basic mechanisms using real examples, and we will identify situations in which linking issues can affect the performance and correctness of your programs. To keep things concrete and understandable, we will couch our discussion in the context of an x86- 64 system running Linux and using the standard ELF-64 (hereafter referred to as ELF) object ﬁle format. However, it is important to realize that the basic concepts of linking are universal, regardless of the operating system, the ISA, or the object ﬁle format. Details may vary, but the concepts are the same. 7.1 Compiler Drivers Consider the C program in Figure 7.1. It will serve as a simple running example throughout this chapter that will allow us to make some important points about how linkers work. Most compilation systems provide a compiler driver that invokes the language preprocessor, compiler, assembler, and linker, as needed on behalf of the user. For example, to build the example program using the GNU compilation system, we might invoke the gcc driver by typing the following command to the shell: Ä©ÅÏÓ| gcc -Og -o prog main.c sum.c Figure 7.2 summarizes the activities of the driver as it translates the example program from an ASCII source ﬁle into an executable object ﬁle. (If you want to see these steps for yourself, run gcc with the kÌ option.) The driver ﬁrst runs the C preprocessor (²ÉÉ),1 which translates the C source ﬁle Àþ©Ål² into an ASCII intermediate ﬁle Àþ©Ål©: 1. In some versions of gcc, the preprocessor is integrated into the compiler driver. 708 Chapter 7 Linking Figure 7.2 Static linking. The linker combines relocatable object ﬁles to form an executable object ﬁle ÉËÇ×. main.c main.o Translators (cpp, cc1, as) Linker (ld) prog Fully linked executable object file Relocatable object files Source filessum.c sum.o Translators (cpp, cc1, as) ²ÉÉ ‰other arguments` Àþ©Ål² mÎÀÉmÀþ©Ål© Next, the driver runs the C compiler (²²o), which translates Àþ©Ål© into an ASCII assembly-language ﬁle Àþ©ÅlÍ: ²²o mÎÀÉmÀþ©Ål© kﬂ× ‰other arguments` kÇ mÎÀÉmÀþ©ÅlÍ Then, the driver runs the assembler (þÍ), which translates Àþ©ÅlÍ into a binary relocatable object ﬁle Àþ©ÅlÇ: þÍ ‰other arguments` kÇ mÎÀÉmÀþ©ÅlÇ mÎÀÉmÀþ©ÅlÍ The driver goes through the same process to generate ÍÏÀlÇ. Finally, it runs the linker program Ä®, which combines Àþ©ÅlÇ and ÍÏÀlÇ, along with the necessary system object ﬁles, to create the binary executable object ﬁle ÉËÇ×: Ä® kÇ ÉËÇ× ‰system object ﬁles and args` mÎÀÉmÀþ©ÅlÇ mÎÀÉmÍÏÀlÇ To run the executable ÉËÇ×, we type its name on the Linux shell’s command line: Ä©ÅÏÓ| ./prog The shell invokes a function in the operating system called the loader, which copies the code and data in the executable ﬁle ÉËÇ× into memory, and then transfers control to the beginning of the program. 7.2 Static Linking Static linkers such as the Linux ld program take as input a collection of relocatable object ﬁles and command-line arguments and generate as output a fully linked executable object ﬁle that can be loaded and run. The input relocatable object ﬁles consist of various code and data sections, where each section is a contiguous sequence of bytes. Instructions are in one section, initialized global variables are in another section, and uninitialized variables are in yet another section. Section 7.3 Object Files 709 To build the executable, the linker must perform two main tasks: Step 1. Symbol resolution. Object ﬁles deﬁne and reference symbols, where each symbol corresponds to a function, a global variable, or a static variable (i.e., any C variable declared with the ÍÎþÎ©² attribute). The purpose of symbol resolution is to associate each symbol reference with exactly one symbol deﬁnition. Step 2. Relocation. Compilers and assemblers generate code and data sections that start at address 0. The linker relocates these sections by associating a memory location with each symbol deﬁnition, and then modifying all of the references to those symbols so that they point to this memory location. The linker blindly performs these relocations using detailed instructions, generated by the assembler, called relocation entries. The sections that follow describe these tasks in more detail. As you read, keep in mind some basic facts about linkers: Object ﬁles are merely collections of blocks of bytes. Some of these blocks contain program code, others contain program data, and others contain data structures that guide the linker and loader. A linker concatenates blocks together, decides on run-time locations for the concatenated blocks, and modiﬁes various locations within the code and data blocks. Linkers have minimal understanding of the target machine. The compilers and assemblers that generate the object ﬁles have already done most of the work. 7.3 Object Files Object ﬁles come in three forms: Relocatable object ﬁle. Contains binary code and data in a form that can be combined with other relocatable object ﬁles at compile time to create an executable object ﬁle. Executable object ﬁle. Contains binary code and data in a form that can be copied directly into memory and executed. Shared object ﬁle. A special type of relocatable object ﬁle that can be loaded into memory and linked dynamically, at either load time or run time. Compilers and assemblers generate relocatable object ﬁles (including shared object ﬁles). Linkers generate executable object ﬁles. Technically, an object module is a sequence of bytes, and an object ﬁle is an object module stored on disk in a ﬁle. However, we will use these terms interchangeably. Object ﬁles are organized according to speciﬁc object ﬁle formats, which vary from system to system. The ﬁrst Unix systems from Bell Labs used the þlÇÏÎ format. (To this day, executables are still referred to as þlÇÏÎ ﬁles.) Windows uses the Portable Executable (PE) format. Mac OS-X uses the Mach-O format. Modern x86-64 Linux and Unix systems use Executable and Linkable Format (ELF). Although our discussion will focus on ELF, the basic concepts are similar, regardless of the particular format. 710 Chapter 7 Linking Figure 7.3 Typical ELF relocatable object ﬁle. Section header table Describes object file sections Sections .strtab .line .debug .rel.data .rel.text .symtab .bss .data .rodata .text ELF header 0 7.4 Relocatable Object Files Figure 7.3 shows the format of a typical ELF relocatable object ﬁle. The ELF header begins with a 16-byte sequence that describes the word size and byte ordering of the system that generated the ﬁle. The rest of the ELF header contains information that allows a linker to parse and interpret the object ﬁle. This includes the size of the ELF header, the object ﬁle type (e.g., relocatable, executable, or shared), the machine type (e.g., x86-64), the ﬁle offset of the section header table, and the size and number of entries in the section header table. The locations and sizes of the various sections are described by the section header table, which contains a ﬁxed-size entry for each section in the object ﬁle. Sandwiched between the ELF header and the section header table are the sections themselves. A typical ELF relocatable object ﬁle contains the following sections: lÎ−ÓÎ The machine code of the compiled program. lËÇ®þÎþ Read-only data such as the format strings in ÉË©ÅÎð statements, and jump tables for switch statements. l®þÎþ Initialized global and static C variables. Local C variables are maintained at run time on the stack and do not appear in either the l®þÎþ or l¾ÍÍ sections. l¾ÍÍ Uninitialized global and static C variables, along with any global or static variables that are initialized to zero. This section occupies no actual space in the object ﬁle; it is merely a placeholder. Object ﬁle formats distinguish between initialized and uninitialized variables for space efﬁciency: unini- tialized variables do not have to occupy any actual disk space in the object ﬁle. At run time, these variables are allocated in memory with an initial value of zero. Section 7.5 Symbols and Symbol Tables 711 Aside Why is uninitialized data called l¾ÍÍ? The use of the term l¾ÍÍ to denote uninitialized data is universal. It was originally an acronym for the “block started by symbol” directive from the IBM 704 assembly language (circa 1957) and the acronym has stuck. A simple way to remember the difference between the l®þÎþ and l¾ÍÍ sections is to think of “bss” as an abbreviation for “Better Save Space!” lÍÔÀÎþ¾ A symbol table with information about functions and global variables that are deﬁned and referenced in the program. Some programmers mis- takenly believe that a program must be compiled with the k× option to get symbol table information. In fact, every relocatable object ﬁle has a symbol table in lÍÔÀÎþ¾ (unless the programmer has speciﬁcally re- moved it with the strip command). However, unlike the symbol table inside a compiler, the lÍÔÀÎþ¾ symbol table does not contain entries for local variables. lË−ÄlÎ−ÓÎ A list of locations in the lÎ−ÓÎ section that will need to be modiﬁed when the linker combines this object ﬁle with others. In general, any instruction that calls an external function or references a global variable will need to be modiﬁed. On the other hand, instructions that call local functions do not need to be modiﬁed. Note that relocation information is not needed in executable object ﬁles, and is usually omitted unless the user explicitly instructs the linker to include it. lË−Äl®þÎþ Relocation information for any global variables that are referenced or deﬁned by the module. In general, any initialized global variable whose initial value is the address of a global variable or externally deﬁned func- tion will need to be modiﬁed. l®−¾Ï× A debugging symbol table with entries for local variables and typedefs deﬁned in the program, global variables deﬁned and referenced in the program, and the original C source ﬁle. It is only present if the compiler driver is invoked with the k× option. lÄ©Å− A mapping between line numbers in the original C source program and machine code instructions in the lÎ−ÓÎ section. It is only present if the compiler driver is invoked with the k× option. lÍÎËÎþ¾ A string table for the symbol tables in the lÍÔÀÎþ¾ and l®−¾Ï× sec- tions and for the section names in the section headers. A string table is a sequence of null-terminated character strings. 7.5 Symbols and Symbol Tables Each relocatable object module, m, has a symbol table that contains information about the symbols that are deﬁned and referenced by m. In the context of a linker, there are three different kinds of symbols: 712 Chapter 7 Linking . Global symbols that are deﬁned by module m and that can be referenced by other modules. Global linker symbols correspond to nonstatic C functions and global variables. . Global symbols that are referenced by module m but deﬁned by some other module. Such symbols are called externals and correspond to nonstatic C functions and global variables that are deﬁned in other modules. . Local symbols that are deﬁned and referenced exclusively by module m. These correspond to static C functions and global variables that are deﬁned with the ÍÎþÎ©² attribute. These symbols are visible anywhere within module m, but cannot be referenced by other modules. It is important to realize that local linker symbols are not the same as local program variables. The symbol table in lÍÔÀÎþ¾ does not contain any symbols that correspond to local nonstatic program variables. These are managed at run time on the stack and are not of interest to the linker. Interestingly, local procedure variables that are deﬁned with the C ÍÎþÎ©² attribute are not managed on the stack. Instead, the compiler allocates space in l®þÎþ or l¾ÍÍ for each deﬁnition and creates a local linker symbol in the symbol table with a unique name. For example, suppose a pair of functions in the same module deﬁne a static local variable Ó: 1 ©ÅÎ ðfg 2 Õ 3 ÍÎþÎ©² ©ÅÎÓ{ny 4 Ë−ÎÏËÅ Óy 5 Û 6 7 ©ÅÎ ×fg 8 Õ 9 ÍÎþÎ©² ©ÅÎÓ{oy 10 Ë−ÎÏËÅ Óy 11 Û In this case, the compiler exports a pair of local linker symbols with different names to the assembler. For example, it might use Ólo for the deﬁnition in function ð and Ólp for the deﬁnition in function ×. Symbol tables are built by assemblers, using symbols exported by the compiler into the assembly-language lÍ ﬁle. An ELF symbol table is contained in the lÍÔÀÎþ¾ section. It contains an array of entries. Figure 7.4 shows the format of each entry. The ÅþÀ− is a byte offset into the string table that points to the null-terminated string name of the symbol. The ÌþÄÏ− is the symbol’s address. For relocatable modules, the ÌþÄÏ− is an offset from the beginning of the section where the object is deﬁned. For executable object ﬁles, the ÌþÄÏ− is an absolute run-time address. The Í©Ö− is the size (in bytes) of the object. The ÎÔÉ− is usually either ®þÎþ or ðÏÅ²Î©ÇÅ. The symbol table can also contain entries for the individual sections Section 7.5 Symbols and Symbol Tables 713 New to C? Hiding variable and function names with ÍÎþÎ©² C programmers use the ÍÎþÎ©² attribute to hide variable and function declarations inside modules, much as you would use public and private declarations in Java and C++. In C, source ﬁles play the role of modules. Any global variable or function declared with the ÍÎþÎ©² attribute is private to that module. Similarly, any global variable or function declared without the ÍÎþÎ©² attribute is public and can be accessed by any other module. It is good programming practice to protect your variables and functions with the ÍÎþÎ©² attribute wherever possible. code/link/elfstructs.c 1 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 2 ©ÅÎ ÅþÀ−y mh ·ÎË©Å× Îþ¾Ä− ÇððÍ−Î hm 3 ²³þË ÎÔÉ−xrj mh ƒÏÅ²Î©ÇÅ ÇË ®þÎþ fr ¾©ÎÍg hm 4 ¾©Å®©Å×xry mh ‹Ç²þÄ ÇË ×ÄÇ¾þÄ fr ¾©ÎÍg hm 5 ²³þË Ë−Í−ËÌ−®y mh •ÅÏÍ−® hm 6 Í³ÇËÎ Í−²Î©ÇÅy mh ·−²Î©ÇÅ ³−þ®−Ë ©Å®−Ó hm 7 ÄÇÅ× ÌþÄÏ−y mh ·−²Î©ÇÅ ÇððÍ−Î ÇË þ¾ÍÇÄÏÎ− þ®®Ë−ÍÍ hm 8 ÄÇÅ× Í©Ö−y mh ﬂ¾Á−²Î Í©Ö− ©Å ¾ÔÎ−Í hm 9 Û ¥Äðtrˆ·ÔÀ¾ÇÄy code/link/elfstructs.c Figure 7.4 ELF symbol table entry. The ÎÔÉ− and ¾©Å®©Å× ﬁelds are 4 bits each. and for the path name of the original source ﬁle. So there are distinct types for these objects as well. The ¾©Å®©Å× ﬁeld indicates whether the symbol is local or global. Each symbol is assigned to some section of the object ﬁle, denoted by the Í−²k Î©ÇÅ ﬁeld, which is an index into the section header table. There are three special pseudosections that don’t have entries in the section header table: ABS is for sym- bols that should not be relocated. UNDEF is for undeﬁned symbols—that is, sym- bols that are referenced in this object module but deﬁned elsewhere. COMMON is for uninitialized data objects that are not yet allocated. For COMMON symbols, the ÌþÄÏ− ﬁeld gives the alignment requirement, and Í©Ö− gives the minimum size. Note that these pseudosections exist only in relocatable object ﬁles; they do not exist in executable object ﬁles. The distinction between COMMON and l¾ÍÍ is subtle. Modern versions of gcc assign symbols in relocatable object ﬁles to COMMON and l¾ÍÍ using the following convention: COMMON Uninitialized global variables l¾ÍÍ Uninitialized static variables, and global or static variables that are initialized to zero 714 Chapter 7 Linking The reason for this seemingly arbitrary distinction stems from the way the linker performs symbol resolution, which we will explain in Section 7.6. The GNU readelf program is a handy tool for viewing the contents of object ﬁles. For example, here are the last three symbol table entries for the relocatable object ﬁle Àþ©ÅlÇ, from the example program in Figure 7.1. The ﬁrst eight entries, which are not shown, are local symbols that the linker uses internally. ﬁÏÀx ‚þÄÏ− ·©Ö− ¶ÔÉ− ¢©Å® ‚©Í ﬁ®Ó ﬁþÀ− vx nnnnnnnnnnnnnnnn pr ƒ•ﬁ£ §‹ﬂ¢¡‹ ⁄¥ƒ¡•‹¶ o Àþ©Å wx nnnnnnnnnnnnnnnn v ﬂ¢“¥£¶ §‹ﬂ¢¡‹ ⁄¥ƒ¡•‹¶ q þËËþÔ onx nnnnnnnnnnnnnnnn n ﬁﬂ¶»–¥ §‹ﬂ¢¡‹ ⁄¥ƒ¡•‹¶ •ﬁ⁄ ÍÏÀ In this example, we see an entry for the deﬁnition of global symbol Àþ©Å, a 24- byte function located at an offset (i.e., ÌþÄÏ−) of zero in the lÎ−ÓÎ section. This is followed by the deﬁnition of the global symbol þËËþÔ, an 8-byte object located at an offset of zero in the l®þÎþ section. The last entry comes from the reference to the external symbol ÍÏÀ. readelf identiﬁes each section by an integer index. ﬁ®Ó{o denotes the lÎ−ÓÎ section, and ﬁ®Ó{q denotes the l®þÎþ section. Practice Problem 7.1 (solution page 753) This problem concerns the ÀlÇ and ÍÑþÉlÇ modules from Figure 7.5. For each symbol that is deﬁned or referenced in ÍÑþÉlÇ, indicate whether or not it will have a symbol table entry in the lÍÔÀÎþ¾ section in module ÍÑþÉlÇ. If so, indicate the module that deﬁnes the symbol (ÍÑþÉlÇ or ÀlÇ), the symbol type (local, global, or extern), and the section (lÎ−ÓÎ, l®þÎþ, l¾ÍÍ, or COMMON) it is assigned to in the module. (a) Àl² code/link/m.c 1 ÌÇ©® ÍÑþÉfgy 2 3 ©ÅÎ ¾Ïð‰p` { Õoj pÛy 4 5 ©ÅÎ Àþ©Åfg 6 Õ 7 ÍÑþÉfgy 8 Ë−ÎÏËÅ ny 9 Û code/link/m.c (b) ÍÑþÉl² code/link/swap.c 1 −ÓÎ−ËÅ ©ÅÎ ¾Ïð‰`y 2 3 ©ÅÎ h¾ÏðÉn { d¾Ïð‰n`y 4 ©ÅÎ h¾ÏðÉoy 5 6 ÌÇ©® ÍÑþÉfg 7 Õ 8 ©ÅÎ Î−ÀÉy 9 10 ¾ÏðÉo { d¾Ïð‰o`y 11 Î−ÀÉ { h¾ÏðÉny 12 h¾ÏðÉn { h¾ÏðÉoy 13 h¾ÏðÉo { Î−ÀÉy 14 Û code/link/swap.c Figure 7.5 Example program for Practice Problem 7.1. Section 7.6 Symbol Resolution 715 Symbol lÍÔÀÎþ¾ entry? Symbol type Module where deﬁned Section ¾Ïð ¾ÏðÉn ¾ÏðÉo ÍÑþÉ Î−ÀÉ 7.6 Symbol Resolution The linker resolves symbol references by associating each reference with exactly one symbol deﬁnition from the symbol tables of its input relocatable object ﬁles. Symbol resolution is straightforward for references to local symbols that are de- ﬁned in the same module as the reference. The compiler allows only one deﬁnition of each local symbol per module. The compiler also ensures that static local vari- ables, which get local linker symbols, have unique names. Resolving references to global symbols, however, is trickier. When the com- piler encounters a symbol (either a variable or function name) that is not deﬁned in the current module, it assumes that it is deﬁned in some other module, gener- ates a linker symbol table entry, and leaves it for the linker to handle. If the linker is unable to ﬁnd a deﬁnition for the referenced symbol in any of its input modules, it prints an (often cryptic) error message and terminates. For example, if we try to compile and link the following source ﬁle on a Linux machine, 1 ÌÇ©® ðÇÇfÌÇ©®gy 2 3 ©ÅÎ Àþ©Åfg Õ 4 ðÇÇfgy 5 Ë−ÎÏËÅ ny 6 Û then the compiler runs without a hitch, but the linker terminates when it cannot resolve the reference to ðÇÇ: Ä©ÅÏÓ| gcc -Wall -Og -o linkerror linkerror.c mÎÀÉm²²·ÖsÏÎ©lÇx 'Å ðÏÅ²Î©ÇÅ ‘Àþ©Å’x mÎÀÉm²²·ÖsÏÎ©lÇflÎ−ÓÎinÓugx ÏÅ®−ð©Å−® Ë−ð−Ë−Å²− ÎÇ ‘ðÇÇ’ Symbol resolution for global symbols is also tricky because multiple object modules might deﬁne global symbols with the same name. In this case, the linker must either ﬂag an error or somehow choose one of the deﬁnitions and discard the rest. The approach adopted by Linux systems involves cooperation between the compiler, assembler, and linker and can introduce some bafﬂing bugs to the unwary programmer. 716 Chapter 7 Linking Aside Mangling of linker symbols in C++ and Java Both C++ and Java allow overloaded methods that have the same name in the source code but different parameter lists. So how does the linker tell the difference between these different overloaded functions? Overloaded functions in C++ and Java work because the compiler encodes each unique method and parameter list combination into a unique name for the linker. This encoding process is called mangling, and the inverse process is known as demangling. Happily, C++ and Java use compatible mangling schemes. A mangled class name consists of the integer number of characters in the name followed by the original name. For example, the class ƒÇÇ is encoded as qƒÇÇ. A method is encoded as the original method name, followed by ˆˆ, followed by the mangled class name, followed by single letter encodings of each argument. For example, ƒÇÇxx¾þËf©ÅÎj ÄÇÅ×g is encoded as ¾þËˆˆqƒÇÇ©Ä. Similar schemes are used to mangle global variable and template names. 7.6.1 How Linkers Resolve Duplicate Symbol Names The input to the linker is a collection of relocatable object modules. Each of these modules deﬁnes a set of symbols, some of which are local (visible only to the module that deﬁnes it), and some of which are global (visible to other modules). What happens if multiple modules deﬁne global symbols with the same name? Here is the approach that Linux compilation systems use. At compile time, the compiler exports each global symbol to the assembler as either strong or weak, and the assembler encodes this information implicitly in the symbol table of the relocatable object ﬁle. Functions and initialized global variables get strong symbols. Uninitialized global variables get weak symbols. Given this notion of strong and weak symbols, Linux linkers use the following rules for dealing with duplicate symbol names: Rule 1. Multiple strong symbols with the same name are not allowed. Rule 2. Given a strong symbol and multiple weak symbols with the same name, choose the strong symbol. Rule 3. Given multiple weak symbols with the same name, choose any of the weak symbols. For example, suppose we attempt to compile and link the following two C modules: 1 mh ðÇÇol² hm 2 ©ÅÎ Àþ©Åfg 3 Õ 4 Ë−ÎÏËÅ ny 5 Û 1 mh ¾þËol² hm 2 ©ÅÎ Àþ©Åfg 3 Õ 4 Ë−ÎÏËÅ ny 5 Û Section 7.6 Symbol Resolution 717 In this case, the linker will generate an error message because the strong symbol Àþ©Å is deﬁned multiple times (rule 1): Ä©ÅÏÓ| gcc foo1.c bar1.c mÎÀÉm²²Êp•ÓÅ®lÇx 'Å ðÏÅ²Î©ÇÅ ‘Àþ©Å’x ¾þËol²xflÎ−ÓÎinÓngx ÀÏÄÎ©ÉÄ− ®−ð©Å©Î©ÇÅ Çð ‘Àþ©Å’ Similarly, the linker will generate an error message for the following modules because the strong symbol Ó is deﬁned twice (rule 1): 1 mh ðÇÇpl² hm 2 ©ÅÎ Ó { ospoqy 3 4 ©ÅÎ Àþ©Åfg 5 Õ 6 Ë−ÎÏËÅ ny 7 Û 1 mh ¾þËpl² hm 2 ©ÅÎ Ó { ospoqy 3 4 ÌÇ©® ðfg 5 Õ 6 Û However, if Ó is uninitialized in one module, then the linker will quietly choose the strong symbol deﬁned in the other (rule 2): 1 mh ðÇÇql² hm 2 a©Å²ÄÏ®− zÍÎ®©Çl³| 3 ÌÇ©® ðfÌÇ©®gy 4 5 ©ÅÎ Ó { ospoqy 6 7 ©ÅÎ Àþ©Åfg 8 Õ 9 ðfgy 10 ÉË©ÅÎðf‘Ó { c®¿Å‘j Ógy 11 Ë−ÎÏËÅ ny 12 Û 1 mh ¾þËql² hm 2 ©ÅÎ Óy 3 4 ÌÇ©® ðfg 5 Õ 6 Ó { ospopy 7 Û 718 Chapter 7 Linking At run time, function ð changes the value of Ó from 15213 to 15212, which might come as an unwelcome surprise to the author of function Àþ©Å! Notice that the linker normally gives no indication that it has detected multiple deﬁnitions of Ó: Ä©ÅÏÓ| gcc -o foobar3 foo3.c bar3.c Ä©ÅÏÓ| ./foobar3 Ó { ospop The same thing can happen if there are two weak deﬁnitions of Ó (rule 3): 1 mh ðÇÇrl² hm 2 a©Å²ÄÏ®− zÍÎ®©Çl³| 3 ÌÇ©® ðfÌÇ©®gy 4 5 ©ÅÎ Óy 6 7 ©ÅÎ Àþ©Åfg 8 Õ 9 Ó { ospoqy 10 ðfgy 11 ÉË©ÅÎðf‘Ó { c®¿Å‘j Ógy 12 Ë−ÎÏËÅ ny 13 Û 1 mh ¾þËrl² hm 2 ©ÅÎ Óy 3 4 ÌÇ©® ðfg 5 Õ 6 Ó { ospopy 7 Û The application of rules 2 and 3 can introduce some insidious run-time bugs that are incomprehensible to the unwary programmer, especially if the duplicate symbol deﬁnitions have different types. Consider the following example, in which Ó is inadvertently deﬁned as an ©ÅÎ in one module and a ®ÇÏ¾Ä− in another: 1 mh ðÇÇsl² hm 2 a©Å²ÄÏ®− zÍÎ®©Çl³| 3 ÌÇ©® ðfÌÇ©®gy 4 5 ©ÅÎ Ô { ospopy 6 ©ÅÎ Ó { ospoqy 7 8 ©ÅÎ Àþ©Åfg 9 Õ 10 ðfgy Section 7.6 Symbol Resolution 719 11 ÉË©ÅÎðf‘Ó { nÓcÓ Ô { nÓcÓ ¿Å‘j 12 Ój Ôgy 13 Ë−ÎÏËÅ ny 14 Û 1 mh ¾þËsl² hm 2 ®ÇÏ¾Ä− Óy 3 4 ÌÇ©® ðfg 5 Õ 6 Ó { knlny 7 Û On an x86-64/Linux machine, ®ÇÏ¾Ä−s are 8 bytes and ©ÅÎs are 4 bytes. On our system, the address of Ó is nÓtnonpn and the address of Ô is nÓtnonpr. Thus, the assignment Ó { knln in line 6 of ¾þËsl² will overwrite the memory locations for Ó and Ô (lines 5 and 6 in ðÇÇsl²) with the double-precision ﬂoating-point representation of negative zero! Ä©ÅÏÓ| gcc -Wall -Og -o foobar5 foo5.c bar5.c mÏÍËm¾©ÅmÄ®x „þËÅ©Å×x þÄ©×ÅÀ−ÅÎ r Çð ÍÔÀ¾ÇÄ ‘Ó’ ©Å mÎÀÉm²²Ä•ƒ«s×lÇ ©Í ÍÀþÄÄ−Ë Î³þÅ v ©Å mÎÀÉm²²¾¶‹²¾wlÇ Ä©ÅÏÓ| ./foobar5 Ó { nÓn Ô { nÓvnnnnnnn This is a subtle and nasty bug, especially because it triggers only a warning from the linker, and because it typically manifests itself much later in the execution of the program, far away from where the error occurred. In a large system with hundreds of modules, a bug of this kind is extremely hard to ﬁx, especially because many programmers are not aware of how linkers work, and because they often ignore compiler warnings. When in doubt, invoke the linker with a ﬂag such as the gcc kðÅÇk²ÇÀÀÇÅ ﬂag, which triggers an error if it encounters multiply- deﬁned global symbols. Or use the k„−ËËÇË option, which turns all warnings into errors. In Section 7.5, we saw how the compiler assigns symbols to COMMON and l¾ÍÍ using a seemingly arbitrary convention. Actually, this convention is due to the fact that in some cases the linker allows multiple modules to deﬁne global symbols with the same name. When the compiler is translating some module and encounters a weak global symbol, say, Ó, it does not know if other modules also deﬁne Ó, and if so, it cannot predict which of the multiple instances of Ó the linker might choose. So the compiler defers the decision to the linker by assigning Ó to COMMON. On the other hand, if Ó is initialized to zero, then it is a strong symbol (and thus must be unique by rule 2), so the compiler can conﬁdently assign it to l¾ÍÍ. Similarly, static symbols are unique by construction, so the compiler can conﬁdently assign them to either l®þÎþ or l¾ÍÍ. 720 Chapter 7 Linking Practice Problem 7.2 (solution page 754) In this problem, let ‡¥ƒ(Ó.©) → ⁄¥ƒ(Ó.Â) denote that the linker will associate an arbitrary reference to symbol Ó in module © to the deﬁnition of Ó in module Â. For each example that follows, use this notation to indicate how the linker would resolve references to the multiply-deﬁned symbol in each module. If there is a link-time error (rule 1), write “error”. If the linker arbitrarily chooses one of the deﬁnitions (rule 3), write “unknown”. A. mh ›Ç®ÏÄ− o hm mh ›Ç®ÏÄ− p hm ©ÅÎ Àþ©Åfg ©ÅÎ Àþ©Åy Õ ©ÅÎ Épfg ÛÕ Û (a) ‡¥ƒ(Àþ©Å.o) → ⁄¥ƒ( . ) (b) ‡¥ƒ(Àþ©Å.p) → ⁄¥ƒ( . ) B. mh ›Ç®ÏÄ− o hm mh ›Ç®ÏÄ− p hm ÌÇ©® Àþ©Åfg ©ÅÎ Àþ©Å { oy Õ ©ÅÎ Épfg ÛÕ Û (a) ‡¥ƒ(Àþ©Å.o) → ⁄¥ƒ( . ) (b) ‡¥ƒ(Àþ©Å.p) → ⁄¥ƒ( . ) C. mh ›Ç®ÏÄ− o hm mh ›Ç®ÏÄ− p hm ©ÅÎ Óy ®ÇÏ¾Ä− Ó { olny ÌÇ©® Àþ©Åfg ©ÅÎ Épfg ÕÕ ÛÛ (a) ‡¥ƒ(Ó.o) → ⁄¥ƒ( . ) (b) ‡¥ƒ(Ó.p) → ⁄¥ƒ( . ) 7.6.2 Linking with Static Libraries So far, we have assumed that the linker reads a collection of relocatable object ﬁles and links them together into an output executable ﬁle. In practice, all compilation systems provide a mechanism for packaging related object modules into a single ﬁle called a static library, which can then be supplied as input to the linker. When it builds the output executable, the linker copies only the object modules in the library that are referenced by the application program. Why do systems support the notion of libraries? Consider ISO C99, which deﬁnes an extensive collection of standard I/O, string manipulation, and integer math functions such as þÎÇ©, ÉË©ÅÎð, Í²þÅð, ÍÎË²ÉÔ, and ËþÅ®. They are available Section 7.6 Symbol Resolution 721 to every C program in the Ä©¾²lþ library. ISO C99 also deﬁnes an extensive collection of ﬂoating-point math functions such as Í©Å, ²ÇÍ, and ÍÊËÎ in the Ä©¾Àlþ library. Consider the different approaches that compiler developers might use to pro- vide these functions to users without the beneﬁt of static libraries. One approach would be to have the compiler recognize calls to the standard functions and to generate the appropriate code directly. Pascal, which provides a small set of stan- dard functions, takes this approach, but it is not feasible for C, because of the large number of standard functions deﬁned by the C standard. It would add signiﬁcant complexity to the compiler and would require a new compiler version each time a function was added, deleted, or modiﬁed. To application programmers, however, this approach would be quite convenient because the standard functions would always be available. Another approach would be to put all of the standard C functions in a single relocatable object module, say, Ä©¾²lÇ, that application programmers could link into their executables: Ä©ÅÏÓ| gcc main.c /usr/lib/libc.o This approach has the advantage that it would decouple the implementation of the standard functions from the implementation of the compiler, and would still be reasonably convenient for programmers. However, a big disadvantage is that ev- ery executable ﬁle in a system would now contain a complete copy of the collection of standard functions, which would be extremely wasteful of disk space. (On our system, Ä©¾²lþ is about 5 MB and Ä©¾Àlþ is about 2 MB.) Worse, each running program would now contain its own copy of these functions in memory, which would be extremely wasteful of memory. Another big disadvantage is that any change to any standard function, no matter how small, would require the library developer to recompile the entire source ﬁle, a time-consuming operation that would complicate the development and maintenance of the standard functions. We could address some of these problems by creating a separate relocatable ﬁle for each standard function and storing them in a well-known directory. How- ever, this approach would require application programmers to explicitly link the appropriate object modules into their executables, a process that would be error prone and time consuming: Ä©ÅÏÓ| gcc main.c /usr/lib/printf.o /usr/lib/scanf.o ... The notion of a static library was developed to resolve the disadvantages of these various approaches. Related functions can be compiled into separate object modules and then packaged in a single static library ﬁle. Application programs can then use any of the functions deﬁned in the library by specifying a single ﬁlename on the command line. For example, a program that uses functions from the C standard library and the math library could be compiled and linked with a command of the form Ä©ÅÏÓ| gcc main.c /usr/lib/libm.a /usr/lib/libc.a 722 Chapter 7 Linking (a) þ®®Ì−²lÇ code/link/addvec.c 1 ©ÅÎ þ®®²ÅÎ { ny 2 3 ÌÇ©® þ®®Ì−²f©ÅÎ hÓj ©ÅÎ hÔj 4 ©ÅÎ hÖj ©ÅÎ Åg 5 Õ 6 ©ÅÎ ©y 7 8 þ®®²ÅÎiiy 9 10 ðÇËf©{ny©zÅy ©iig 11 Ö‰©` { Ó‰©` i Ô‰©`y 12 Û code/link/addvec.c (b) ÀÏÄÎÌ−²lÇ code/link/multvec.c 1 ©ÅÎ ÀÏÄÎ²ÅÎ { ny 2 3 ÌÇ©® ÀÏÄÎÌ−²f©ÅÎ hÓj ©ÅÎ hÔj 4 ©ÅÎ hÖj ©ÅÎ Åg 5 Õ 6 ©ÅÎ ©y 7 8 ÀÏÄÎ²ÅÎiiy 9 10 ðÇËf©{ny©zÅy ©iig 11 Ö‰©` { Ó‰©` h Ô‰©`y 12 Û code/link/multvec.c Figure 7.6 Member object ﬁles in the Ä©¾Ì−²ÎÇË library. At link time, the linker will only copy the object modules that are referenced by the program, which reduces the size of the executable on disk and in memory. On the other hand, the application programmer only needs to include the names of a few library ﬁles. (In fact, C compiler drivers always pass Ä©¾²lþ to the linker, so the reference to Ä©¾²lþ mentioned previously is unnecessary.) On Linux systems, static libraries are stored on disk in a particular ﬁle format known as an archive. An archive is a collection of concatenated relocatable object ﬁles, with a header that describes the size and location of each member object ﬁle. Archive ﬁlenames are denoted with the lþ sufﬁx. To make our discussion of libraries concrete, consider the pair of vector routines in Figure 7.6. Each routine, deﬁned in its own object module, performs a vector operation on a pair of input vectors and stores the result in an output vector. As a side effect, each routine records the number of times it has been called by incrementing a global variable. (This will be useful when we explain the idea of position-independent code in Section 7.12.) To create a static library of these functions, we would use the ar tool as follows: Ä©ÅÏÓ| gcc -c addvec.c multvec.c Ä©ÅÏÓ| ar rcs libvector.a addvec.o multvec.o To use the library, we might write an application such as Àþ©Åpl² in Figure 7.7, which invokes the þ®®Ì−² library routine. The include (or header) ﬁle Ì−²ÎÇËl³ deﬁnes the function prototypes for the routines in Ä©¾Ì−²ÎÇËlþ, To build the executable, we would compile and link the input ﬁles Àþ©ÅplÇ and Ä©¾Ì−²ÎÇËlþ: Ä©ÅÏÓ| gcc -c main2.c Ä©ÅÏÓ| gcc -static -o prog2c main2.o ./libvector.a Section 7.6 Symbol Resolution 723 code/link/main2.c 1 a©Å²ÄÏ®− zÍÎ®©Çl³| 2 a©Å²ÄÏ®− ‘Ì−²ÎÇËl³‘ 3 4 ©ÅÎ Ó‰p` { Õoj pÛy 5 ©ÅÎ Ô‰p` { Õqj rÛy 6 ©ÅÎ Ö‰p`y 7 8 ©ÅÎ Àþ©Åfg 9 Õ 10 þ®®Ì−²fÓj Ôj Öj pgy 11 ÉË©ÅÎðf‘Ö { ‰c® c®`¿Å‘j Ö‰n`j Ö‰o`gy 12 Ë−ÎÏËÅ ny 13 Û code/link/main2.c Figure 7.7 Example program 2. This program invokes a function in the Ä©¾Ì−²ÎÇË library. main2.c vector.h libvector.a libc.a addvec.o printf.o and any other modules called by printf.o main2.o Translators (cpp, cc1, as) Linker (ld) prog2c Fully linked executable object file Relocatable object files Source files Static libraries Figure 7.8 Linking with static libraries. or equivalently, Ä©ÅÏÓ| gcc -c main2.c Ä©ÅÏÓ| gcc -static -o prog2c main2.o -L. -lvector Figure 7.8 summarizes the activity of the linker. The kÍÎþÎ©² argument tells the compiler driver that the linker should build a fully linked executable object ﬁle that can be loaded into memory and run without any further linking at load time. The kÄÌ−²ÎÇË argument is a shorthand for Ä©¾Ì−²ÎÇËlþ, and the k‹l argument tells the linker to look for Ä©¾Ì−²ÎÇËlþ in the current directory. When the linker runs, it determines that the þ®®Ì−² symbol deﬁned by þ®®Ì−²lÇ is referenced by Àþ©ÅplÇ, so it copies þ®®Ì−²lÇ into the executable. 724 Chapter 7 Linking Since the program doesn’t reference any symbols deﬁned by ÀÏÄÎÌ−²lÇ, the linker does not copy this module into the executable. The linker also copies the ÉË©ÅÎðlÇ module from Ä©¾²lþ, along with a number of other modules from the C run-time system. 7.6.3 How Linkers Use Static Libraries to Resolve References While static libraries are useful, they are also a source of confusion to program- mers because of the way the Linux linker uses them to resolve external references. During the symbol resolution phase, the linker scans the relocatable object ﬁles and archives left to right in the same sequential order that they appear on the compiler driver’s command line. (The driver automatically translates any l² ﬁles on the command line into lÇ ﬁles.) During this scan, the linker maintains a set E of relocatable object ﬁles that will be merged to form the executable, a set U of unresolved symbols (i.e., symbols referred to but not yet deﬁned), and a set D of symbols that have been deﬁned in previous input ﬁles. Initially, E, U , and D are empty. . For each input ﬁle f on the command line, the linker determines if f is an object ﬁle or an archive. If f is an object ﬁle, the linker adds f to E, updates U and D to reﬂect the symbol deﬁnitions and references in f , and proceeds to the next input ﬁle. . If f is an archive, the linker attempts to match the unresolved symbols in U against the symbols deﬁned by the members of the archive. If some archive member m deﬁnes a symbol that resolves a reference in U , then m is added to E, and the linker updates U and D to reﬂect the symbol deﬁnitions and references in m. This process iterates over the member object ﬁles in the archive until a ﬁxed point is reached where U and D no longer change. At this point, any member object ﬁles not contained in E are simply discarded and the linker proceeds to the next input ﬁle. . If U is nonempty when the linker ﬁnishes scanning the input ﬁles on the command line, it prints an error and terminates. Otherwise, it merges and relocates the object ﬁles in E to build the output executable ﬁle. Unfortunately, this algorithm can result in some bafﬂing link-time errors because the ordering of libraries and object ﬁles on the command line is signiﬁcant. If the library that deﬁnes a symbol appears on the command line before the object ﬁle that references that symbol, then the reference will not be resolved and linking will fail. For example, consider the following: Ä©ÅÏÓ| gcc -static ./libvector.a main2.c mÎÀÉm²²w”¤t‡ÉlÇx 'Å ðÏÅ²Î©ÇÅ ‘Àþ©Å’x mÎÀÉm²²w”¤t‡ÉlÇflÎ−ÓÎinÓovgx ÏÅ®−ð©Å−® Ë−ð−Ë−Å²− ÎÇ ‘þ®®Ì−²’ What happened? When Ä©¾Ì−²ÎÇËlþ is processed, U is empty, so no member object ﬁles from Ä©¾Ì−²ÎÇËlþ are added to E. Thus, the reference to þ®®Ì−² is never resolved and the linker emits an error message and terminates. Section 7.7 Relocation 725 The general rule for libraries is to place them at the end of the command line. If the members of the different libraries are independent, in that no member references a symbol deﬁned by another member, then the libraries can be placed at the end of the command line in any order. If, on the other hand, the libraries are not independent, then they must be ordered so that for each symbol s that is referenced externally by a member of an archive, at least one deﬁnition of s follows a reference to s on the command line. For example, suppose ðÇÇl² calls functions in Ä©¾Ólþ and Ä©¾Ölþ that call functions in Ä©¾Ôlþ. Then Ä©¾Ólþ and Ä©¾Ölþ must precede Ä©¾Ôlþ on the command line: Ä©ÅÏÓ| gcc foo.c libx.a libz.a liby.a Libraries can be repeated on the command line if necessary to satisfy the dependence requirements. For example, suppose ðÇÇl² calls a function in Ä©¾Ólþ that calls a function in Ä©¾Ôlþ that calls a function in Ä©¾Ólþ. Then Ä©¾Ólþ must be repeated on the command line: Ä©ÅÏÓ| gcc foo.c libx.a liby.a libx.a Alternatively, we could combine Ä©¾Ólþ and Ä©¾Ôlþ into a single archive. Practice Problem 7.3 (solution page 754) Let þ and ¾ denote object modules or static libraries in the current directory, and let þ→¾ denote that þ depends on ¾, in the sense that ¾ deﬁnes a symbol that is referenced by þ. For each of the following scenarios, show the minimal command line (i.e., one with the least number of object ﬁle and library arguments) that will allow the static linker to resolve all symbol references. A. ÉlÇ → Ä©¾Ólþ B. ÉlÇ → Ä©¾Ólþ → Ä©¾Ôlþ C. ÉlÇ → Ä©¾Ólþ → Ä©¾Ôlþ and Ä©¾Ôlþ → Ä©¾Ólþ → ÉlÇ 7.7 Relocation Once the linker has completed the symbol resolution step, it has associated each symbol reference in the code with exactly one symbol deﬁnition (i.e., a symbol table entry in one of its input object modules). At this point, the linker knows the exact sizes of the code and data sections in its input object modules. It is now ready to begin the relocation step, where it merges the input modules and assigns run-time addresses to each symbol. Relocation consists of two steps: 1. Relocating sections and symbol deﬁnitions. In this step, the linker merges all sections of the same type into a new aggregate section of the same type. For example, the l®þÎþ sections from the input modules are all merged into one section that will become the l®þÎþ section for the output executable object 726 Chapter 7 Linking ﬁle. The linker then assigns run-time memory addresses to the new aggregate sections, to each section deﬁned by the input modules, and to each symbol deﬁned by the input modules. When this step is complete, each instruction and global variable in the program has a unique run-time memory address. 2. Relocating symbol references within sections. In this step, the linker modiﬁes every symbol reference in the bodies of the code and data sections so that they point to the correct run-time addresses. To perform this step, the linker relies on data structures in the relocatable object modules known as relocation entries, which we describe next. 7.7.1 Relocation Entries When an assembler generates an object module, it does not know where the code and data will ultimately be stored in memory. Nor does it know the locations of any externally deﬁned functions or global variables that are referenced by the module. So whenever the assembler encounters a reference to an object whose ultimate location is unknown, it generates a relocation entry that tells the linker how to modify the reference when it merges the object ﬁle into an executable. Relocation entries for code are placed in lË−ÄlÎ−ÓÎ. Relocation entries for data are placed in lË−Äl®þÎþ. Figure 7.9 shows the format of an ELF relocation entry. The ÇððÍ−Î is the section offset of the reference that will need to be modiﬁed. The ÍÔÀ¾ÇÄ identiﬁes the symbol that the modiﬁed reference should point to. The ÎÔÉ− tells the linker how to modify the new reference. The þ®®−Å® is a signed constant that is used by some types of relocations to bias the value of the modiﬁed reference. ELF deﬁnes 32 different relocation types, many quite arcane. We are con- cerned with only the two most basic relocation types: ‡ˆ”vtˆtrˆ–£qp. Relocate a reference that uses a 32-bit PC-relative address. Recall from Section 3.6.3 that a PC-relative address is an offset from the current run-time value of the program counter (PC). When the CPU executes an instruction using PC-relative addressing, it forms the effective address (e.g., the target of the ²þÄÄ instruction) by adding the 32-bit value code/link/elfstructs.c 1 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 2 ÄÇÅ× ÇððÍ−Îy mh ﬂððÍ−Î Çð Î³− Ë−ð−Ë−Å²− ÎÇ Ë−ÄÇ²þÎ− hm 3 ÄÇÅ× ÎÔÉ−xqpj mh ‡−ÄÇ²þÎ©ÇÅ ÎÔÉ− hm 4 ÍÔÀ¾ÇÄxqpy mh ·ÔÀ¾ÇÄ Îþ¾Ä− ©Å®−Ó hm 5 ÄÇÅ× þ®®−Å®y mh £ÇÅÍÎþÅÎ ÉþËÎ Çð Ë−ÄÇ²þÎ©ÇÅ −ÓÉË−ÍÍ©ÇÅ hm 6 Û ¥Äðtrˆ‡−Äþy code/link/elfstructs.c Figure 7.9 ELF relocation entry. Each entry identiﬁes a reference that must be relocated and speciﬁes how to compute the modiﬁed reference. Section 7.7 Relocation 727 encoded in the instruction to the current run-time value of the PC, which is always the address of the next instruction in memory. ‡ˆ”vtˆtrˆqp. Relocate a reference that uses a 32-bit absolute address. With absolute addressing, the CPU directly uses the 32-bit value encoded in the instruction as the effective address, without further modiﬁcations. These two relocation types support the x86-64 small code model, which as- sumes that the total size of the code and data in the executable object ﬁle is smaller than 2 GB, and thus can be accessed at run-time using 32-bit PC-relative addresses. The small code model is the default for gcc. Programs larger than 2 GB can be compiled using the kÀ²ÀÇ®−Ä{À−®©ÏÀ (medium code model) and kÀ²ÀÇ®−Ä{ÄþË×− (large code model) ﬂags, but we won’t discuss those. 7.7.2 Relocating Symbol References Figure 7.10 shows the pseudocode for the linker’s relocation algorithm. Lines 1 and 2 iterate over each section Í and each relocation entry Ë associated with each section. For concreteness, assume that each section Í is an array of bytes and that each relocation entry Ë is a ÍÎËÏ²Î of type ¥Äðtrˆ‡−Äþ, as deﬁned in Figure 7.9. Also, assume that when the algorithm runs, the linker has already chosen run- time addresses for each section (denoted ¡⁄⁄‡fÍg) and each symbol (denoted ¡⁄⁄‡fËlÍÔÀ¾ÇÄg). Line 3 computes the address in the Í array of the 4-byte ref- erence that needs to be relocated. If this reference uses PC-relative addressing, then it is relocated by lines 5–9. If the reference uses absolute addressing, then it is relocated by lines 11–13. 1 ðÇË−þ²³ Í−²Î©ÇÅ Í Õ 2 ðÇË−þ²³ Ë−ÄÇ²þÎ©ÇÅ −ÅÎËÔ Ë Õ 3 Ë−ðÉÎË{Íi ËlÇððÍ−Îy mh ÉÎË ÎÇ Ë−ð−Ë−Å²− ÎÇ ¾− Ë−ÄÇ²þÎ−® hm 4 5 mh ‡−ÄÇ²þÎ− þ –£kË−ÄþÎ©Ì− Ë−ð−Ë−Å²− hm 6 ©ð fËlÎÔÉ− {{ ‡ˆ”vtˆtrˆ–£qpg Õ 7 Ë−ðþ®®Ë { ¡⁄⁄‡fÍg i ËlÇððÍ−Îy mh Ë−ð’Í ËÏÅkÎ©À− þ®®Ë−ÍÍ hm 8 hË−ðÉÎË { fÏÅÍ©×Å−®g f¡⁄⁄‡fËlÍÔÀ¾ÇÄg i Ëlþ®®−Å® k Ë−ðþ®®Ëgy 9 Û 10 11 mh ‡−ÄÇ²þÎ− þÅ þ¾ÍÇÄÏÎ− Ë−ð−Ë−Å²− hm 12 ©ð fËlÎÔÉ− {{ ‡ˆ”vtˆtrˆqpg 13 hË−ðÉÎË { fÏÅÍ©×Å−®g f¡⁄⁄‡fËlÍÔÀ¾ÇÄg i Ëlþ®®−Å®gy 14 Û 15 Û Figure 7.10 Relocation algorithm. 728 Chapter 7 Linking code/link/main-relo.d 1 nnnnnnnnnnnnnnnn zÀþ©Å|x 2 nx rv vq −² nv ÍÏ¾ bnÓvjcËÍÉ 3 rx ¾− np nn nn nn ÀÇÌ bnÓpjc−Í© 4 wx ¾ð nn nn nn nn ÀÇÌ bnÓnjc−®© %edi = &array 5 þx ‡ˆ”vtˆtrˆqp þËËþÔ Relocation entry 6 −x −v nn nn nn nn ²þÄÄÊ oq zÀþ©ÅinÓoq| sum() 7 ðx ‡ˆ”vtˆtrˆ–£qp ÍÏÀknÓr Relocation entry 8 oqx rv vq ²r nv þ®® bnÓvjcËÍÉ 9 oux ²q Ë−ÎÊ code/link/main-relo.d Figure 7.11 Code and relocation entries from Àþ©ÅlÇ. The original C code is in Figure 7.1. Let’s see how the linker uses this algorithm to relocate the references in our example program in Figure 7.1. Figure 7.11 shows the disassembled code from Àþ©ÅlÇ, as generated by the GNU objdump tool (Ç¾Á®ÏÀÉ k®Ó Àþ©ÅlÇ). The Àþ©Å function references two global symbols, þËËþÔ and ÍÏÀ. For each reference, the assembler has generated a relocation entry, which is displayed on the following line.2 The relocation entries tell the linker that the reference to ÍÏÀ should be relocated using a 32-bit PC-relative address, and the reference to þËËþÔ should be relocated using a 32-bit absolute address. The next two sections detail how the linker relocates these references. Relocating PC-Relative References In line 6 in Figure 7.11, function Àþ©Å calls the ÍÏÀ function, which is deﬁned in module ÍÏÀlÇ.The ²þÄÄ instruction begins at section offset nÓ− and consists of the 1-byte opcode nÓ−v, followed by a placeholder for the 32-bit PC-relative reference to the target ÍÏÀ. The corresponding relocation entry Ë consists of four ﬁelds: ËlÇððÍ−Î { nÓð ËlÍÔÀ¾ÇÄ { ÍÏÀ ËlÎÔÉ− { ‡ˆ”vtˆtrˆ–£qp Ëlþ®®−Å® { kr These ﬁelds tell the linker to modify the 32-bit PC-relative reference starting at offset nÓð so that it will point to the ÍÏÀ routine at run time. Now, suppose that the linker has determined that ¡⁄⁄‡fÍg { ¡⁄⁄‡flÎ−ÓÎg { nÓrnnr®n 2. Recall that relocation entries and instructions are actually stored in different sections of the object ﬁle. The objdump tool displays them together for convenience. Section 7.7 Relocation 729 and ¡⁄⁄‡fËlÍÔÀ¾ÇÄg { ¡⁄⁄‡fÍÏÀg { nÓrnnr−v Using the algorithm in Figure 7.10, the linker ﬁrst computes the run-time address of the reference (line 7): Ë−ðþ®®Ë { ¡⁄⁄‡fÍg i ËlÇððÍ−Î { nÓrnnr®n i nÓð { nÓrnnr®ð It then updates the reference so that it will point to the ÍÏÀ routine at run time (line 8): hË−ðÉÎË { fÏÅÍ©×Å−®g f¡⁄⁄‡fËlÍÔÀ¾ÇÄg i Ëlþ®®−Å® k Ë−ðþ®®Ëg { fÏÅÍ©×Å−®g fnÓrnnr−v i fkrg k nÓrnnr®ðg { fÏÅÍ©×Å−®g fnÓsg In the resulting executable object ﬁle, the ²þÄÄ instruction has the following relocated form: rnnr®−x −v ns nn nn nn ²þÄÄÊ rnnr−v zÍÏÀ| sum() At run time, the ²þÄÄ instruction will be located at address nÓrnnr®−. When the CPU executes the ²þÄÄ instruction, the PC has a value of nÓrnnr−q, which is the address of the instruction immediately following the ²þÄÄ instruction. To execute the call instruction, the CPU performs the following steps: 1. Push PC onto stack 2. PC ← PC + nÓs = nÓrnnr−q + nÓs = nÓrnnr−v Thus, the next instruction to execute is the ﬁrst instruction of the ÍÏÀ routine, which of course is what we want! Relocating Absolute References Relocating absolute references is straightforward. For example, in line 4 in Fig- ure 7.11, the ÀÇÌ instruction copies the address of þËËþÔ (a 32-bit immediate value) into register c−®©.The ÀÇÌ instruction begins at section offset nÓw and consists of the 1-byte opcode nÓ¾ð, followed by a placeholder for the 32-bit absolute refer- ence to þËËþÔ. The corresponding relocation entry Ë consists of four ﬁelds: ËlÇððÍ−Î { nÓþ ËlÍÔÀ¾ÇÄ { þËËþÔ ËlÎÔÉ− { ‡ˆ”vtˆtrˆqp Ëlþ®®−Å® { n These ﬁelds tell the linker to modify the absolute reference starting at offset nÓþ so that it will point to the ﬁrst byte of þËËþÔ at run time. Now, suppose that the linker has determined that 730 Chapter 7 Linking (a) Relocated lÎ−ÓÎ section 1 nnnnnnnnnnrnnr®n zÀþ©Å|x 2 rnnr®nx rv vq −² nv ÍÏ¾ bnÓvjcËÍÉ 3 rnnr®rx ¾− np nn nn nn ÀÇÌ bnÓpjc−Í© 4 rnnr®wx ¾ð ov on tn nn ÀÇÌ bnÓtnonovjc−®© %edi = &array 5 rnnr®−x −v ns nn nn nn ²þÄÄÊ rnnr−v zÍÏÀ| sum() 6 rnnr−qx rv vq ²r nv þ®® bnÓvjcËÍÉ 7 rnnr−ux ²q Ë−ÎÊ 8 nnnnnnnnnnrnnr−v zÍÏÀ|x 9 rnnr−vx ¾v nn nn nn nn ÀÇÌ bnÓnjc−þÓ 10 rnnr−®x ¾þ nn nn nn nn ÀÇÌ bnÓnjc−®Ó 11 rnnrðpx −¾ nw ÁÀÉ rnnrð® zÍÏÀinÓos| 12 rnnrðrx rv tq ²þ ÀÇÌÍÄÊ c−®ÓjcË²Ó 13 rnnrðux nq nr vð þ®® fcË®©jcË²Ójrgjc−þÓ 14 rnnrðþx vq ²p no þ®® bnÓojc−®Ó 15 rnnrð®x qw ðp ²ÀÉ c−Í©jc−®Ó 16 rnnrððx u² ðq ÁÄ rnnrðr zÍÏÀinÓ²| 17 rnnsnox ðq ²q Ë−ÉÖ Ë−ÎÊ (b) Relocated l®þÎþ section 1 nnnnnnnnnntnonov zþËËþÔ|x 2 tnonovx no nn nn nn np nn nn nn Figure 7.12 Relocated lÎ−ÓÎ and l®þÎþ sections for the executable ﬁle ÉËÇ×. The original C code is in Figure 7.1. ¡⁄⁄‡fËlÍÔÀ¾ÇÄg { ¡⁄⁄‡fþËËþÔg { nÓtnonov The linker updates the reference using line 13 of the algorithm in Figure 7.10: hË−ðÉÎË { fÏÅÍ©×Å−®g f¡⁄⁄‡fËlÍÔÀ¾ÇÄg i Ëlþ®®−Å®g { fÏÅÍ©×Å−®g fnÓtnonov i ng { fÏÅÍ©×Å−®g fnÓtnonovg In the resulting executable object ﬁle, the reference has the following relocated form: rnnr®wx ¾ð ov on tn nn ÀÇÌ bnÓtnonovjc−®© %edi = &array Putting it all together, Figure 7.12 shows the relocated lÎ−ÓÎ and l®þÎþ sections in the ﬁnal executable object ﬁle. At load time, the loader can copy the bytes from these sections directly into memory and execute the instructions without any further modiﬁcations. Practice Problem 7.4 (solution page 754) This problem concerns the relocated program in Figure 7.12(a). Section 7.8 Executable Object Files 731 A. What is the hex address of the relocated reference to ÍÏÀ in line 5? B. What is the hex value of the relocated reference to ÍÏÀ in line 5? Practice Problem 7.5 (solution page 754) Consider the call to function ÍÑþÉ in object ﬁle ÀlÇ (Figure 7.5). wx −v nn nn nn nn ²þÄÄÊ − zÀþ©ÅinÓ−| swap() with the following relocation entry: ËlÇððÍ−Î { nÓþ ËlÍÔÀ¾ÇÄ { ÍÑþÉ ËlÎÔÉ− { ‡ˆ”vtˆtrˆ–£qp Ëlþ®®−Å® { kr Now suppose that the linker relocates lÎ−ÓÎ in ÀlÇ to address nÓrnnr®n and ÍÑþÉ to address nÓrnnr−v. Then what is the value of the relocated reference to ÍÑþÉ in the ²þÄÄÊ instruction? 7.8 Executable Object Files We have seen how the linker merges multiple object ﬁles into a single executable object ﬁle. Our example C program, which began life as a collection of ASCII text ﬁles, has been transformed into a single binary ﬁle that contains all of the information needed to load the program into memory and run it. Figure 7.13 summarizes the kinds of information in a typical ELF executable ﬁle. Section header table Describes object file sections Maps contiguous file sections to run-time memory segments .strtab .line .debug .symtab .bss .data .rodata .text .init Segment header table ELF header 0 Read-only memory segment (code segment) Read/write memory segment (data segment) Symbol table and debugging info are not loaded into memory Figure 7.13 Typical ELF executable object ﬁle. 732 Chapter 7 Linking code/link/prog-exe.d Read-only code segment 1 ‹ﬂ¡⁄ Çðð nÓnnnnnnnnnnnnnnnn Ìþ®®Ë nÓnnnnnnnnnnrnnnnn Éþ®®Ë nÓnnnnnnnnnnrnnnnn þÄ©×Å phhpo 2 ð©Ä−ÍÖ nÓnnnnnnnnnnnnntw² À−ÀÍÖ nÓnnnnnnnnnnnnntw² ðÄþ×Í ËkÓ Read/write data segment 3 ‹ﬂ¡⁄ Çðð nÓnnnnnnnnnnnnn®ðv Ìþ®®Ë nÓnnnnnnnnnntnn®ðv Éþ®®Ë nÓnnnnnnnnnntnn®ðv þÄ©×Å phhpo 4 ð©Ä−ÍÖ nÓnnnnnnnnnnnnnppv À−ÀÍÖ nÓnnnnnnnnnnnnnpqn ðÄþ×Í ËÑk code/link/prog-exe.d Figure 7.14 Program header table for the example executable ÉËÇ×. Çðð: offset in object ﬁle; Ìþ®®ËmÉþ®®Ë: memory address; þÄ©×Å: alignment requirement; ð©Ä−ÍÖ: segment size in object ﬁle; À−ÀÍÖ: segment size in memory; ðÄþ×Í: run-time permissions. The format of an executable object ﬁle is similar to that of a relocatable object ﬁle. The ELF header describes the overall format of the ﬁle. It also includes the program’s entry point, which is the address of the ﬁrst instruction to execute when the program runs. The lÎ−ÓÎ, lËÇ®þÎþ, and l®þÎþ sections are similar to those in a relocatable object ﬁle, except that these sections have been relocated to their eventual run-time memory addresses. The l©Å©Î section deﬁnes a small function, called ˆ©Å©Î, that will be called by the program’s initialization code. Since the executable is fully linked (relocated), it needs no lË−Ä sections. ELF executables are designed to be easy to load into memory, with contigu- ous chunks of the executable ﬁle mapped to contiguous memory segments. This mapping is described by the program header table. Figure 7.14 shows part of the program header table for our example executable ÉËÇ×, as displayed by objdump. From the program header table, we see that two memory segments will be initialized with the contents of the executable object ﬁle. Lines 1 and 2 tell us that the ﬁrst segment (the code segment) has read/execute permissions, starts at memory address nÓrnnnnn, has a total size in memory of nÓtw² bytes, and is initialized with the ﬁrst nÓtw² bytes of the executable object ﬁle, which includes the ELF header, the program header table, and the l©Å©Î, lÎ−ÓÎ, and lËÇ®þÎþ sections. Lines 3 and 4 tell us that the second segment (the data segment) has read/write permissions, starts at memory address nÓtnn®ðv, has a total memory size of nÓpqn bytes, and is initialized with the nÓppv bytes in the l®þÎþ section starting at offset nÓ®ðv in the object ﬁle. The remaining 8 bytes in the segment correspond to l¾ÍÍ data that will be initialized to zero at run time. For any segment s, the linker must choose a starting address, Ìþ®®Ë, such that Ìþ®®Ë mod þÄ©×Å = Çðð mod þÄ©×Å where Çðð is the offset of the segment’s ﬁrst section in the object ﬁle, and þÄ©×Å is the alignment speciﬁed in the program header (221 = nÓpnnnnn). For example, in the data segment in Figure 7.14, Section 7.9 Loading Executable Object Files 733 Ìþ®®Ë mod þÄ©×Å = nÓtnn®ðv mod nÓpnnnnn = nÓ®ðv and Çðð mod þÄ©×Å = nÓ®ðv mod nÓpnnnnn = nÓ®ðv This alignment requirement is an optimization that enables segments in the object ﬁle to be transferred efﬁciently to memory when the program executes. The reason is somewhat subtle and is due to the way that virtual memory is organized as large contiguous power-of-2 chunks of bytes. You will learn all about virtual memory in Chapter 9. 7.9 Loading Executable Object Files To run an executable object ﬁle ÉËÇ×, we can type its name to the Linux shell’s command line: Ä©ÅÏÓ| ./prog Since ÉËÇ× does not correspond to a built-in shell command, the shell assumes that ÉËÇ× is an executable object ﬁle, which it runs for us by invoking some memory- resident operating system code known as the ÄÇþ®−Ë. Any Linux program can invoke the loader by calling the −Ó−²Ì− function, which we will describe in detail in Section 8.4.6. The loader copies the code and data in the executable object ﬁle from disk into memory and then runs the program by jumping to its ﬁrst instruction, or entry point. This process of copying the program into memory and then running it is known as loading. Every running Linux program has a run-time memory image similar to the one in Figure 7.15. On Linux x86-64 systems, the code segment starts at address nÓrnnnnn, followed by the data segment. The run-time heap follows the data segment and grows upward via calls to the ÀþÄÄÇ² library. (We will describe ÀþÄÄÇ² and the heap in detail in Section 9.9.) This is followed by a region that is reserved for shared modules. The user stack starts below the largest legal user address (248 − 1) and grows down, toward smaller memory addresses. The region above the stack, starting at address 248, is reserved for the code and data in the kernel, which is the memory-resident part of the operating system. For simplicity, we’ve drawn the heap, data, and code segments as abutting each other, and we’ve placed the top of the stack at the largest legal user ad- dress. In practice, there is a gap between the code and data segments due to the alignment requirement on the l®þÎþ segment (Section 7.8). Also, the linker uses address-space layout randomization (ASLR, Section 3.10.4) when it assigns run- time addresses to the stack, shared library, and heap segments. Even though the locations of these regions change each time the program is run, their relative po- sitions are the same. When the loader runs, it creates a memory image similar to the one shown in Figure 7.15. Guided by the program header table, it copies chunks of the 734 Chapter 7 Linking Figure 7.15 Linux x86-64 run-time memory image. Gaps due to segment alignment requirements and address- space layout randomization (ASLR) are not shown. Not to scale. 0x400000 248-1 0 Memory invisible to user code %esp (stack pointer) brk Loaded from the executable file User stack (created at run time) Memory-mapped region for shared libraries Run-time heap (created by malloc) Read/write segment (.data,.bss) Read-only code segment (.init,.text,.rodata) Kernel memory executable object ﬁle into the code and data segments. Next, the loader jumps to the program’s entry point, which is always the address of the ˆÍÎþËÎ function. This function is deﬁned in the system object ﬁle ²ËÎolÇ and is the same for all C programs. The ˆÍÎþËÎ function calls the system startup function, ˆˆÄ©¾²ˆÍÎþËÎˆ Àþ©Å, which is deﬁned in Ä©¾²lÍÇ. It initializes the execution environment, calls the user-level Àþ©Å function, handles its return value, and if necessary returns control to the kernel. 7.10 Dynamic Linking with Shared Libraries The static libraries that we studied in Section 7.6.2 address many of the issues as- sociated with making large collections of related functions available to application programs. However, static libraries still have some signiﬁcant disadvantages. Static libraries, like all software, need to be maintained and updated periodically. If ap- plication programmers want to use the most recent version of a library, they must somehow become aware that the library has changed and then explicitly relink their programs against the updated library. Another issue is that almost every C program uses standard I/O functions such as ÉË©ÅÎð and Í²þÅð. At run time, the code for these functions is duplicated in the text segment of each running process. On a typical system that is running hundreds of processes, this can be a signiﬁcant waste of scarce memory system resources. (An interesting property of memory is that it is always a scarce resource, regardless Section 7.10 Dynamic Linking with Shared Libraries 735 Aside How do loaders really work? Our description of loading is conceptually correct but intentionally not entirely accurate. To understand how loading really works, you must understand the concepts of processes, virtual memory, and memory mapping, which we haven’t discussed yet. As we encounter these concepts later in Chapters 8 and 9, we will revisit loading and gradually reveal the mystery to you. For the impatient reader, here is a preview of how loading really works: Each program in a Linux system runs in the context of a process with its own virtual address space. When the shell runs a program, the parent shell process forks a child process that is a duplicate of the parent. The child process invokes the loader via the −Ó−²Ì− system call. The loader deletes the child’s existing virtual memory segments and creates a new set of code, data, heap, and stack segments. The new stack and heap segments are initialized to zero. The new code and data segments are initialized to the contents of the executable ﬁle by mapping pages in the virtual address space to page-size chunks of the executable ﬁle. Finally, the loader jumps to the ˆÍÎþËÎ address, which eventually calls the application’s Àþ©Å routine. Aside from some header information, there is no copying of data from disk to memory during loading. The copying is deferred until the CPU references a mapped virtual page, at which point the operating system automatically transfers the page from disk to memory using its paging mechanism. of how much there is in a system. Disk space and kitchen trash cans share this same property.) Shared libraries are modern innovations that address the disadvantages of static libraries. A shared library is an object module that, at either run time or load time, can be loaded at an arbitrary memory address and linked with a program in memory. This process is known as dynamic linking and is performed by a program called a dynamic linker. Shared libraries are also referred to as shared objects, and on Linux systems they are indicated by the lÍÇ sufﬁx. Microsoft operating systems make heavy use of shared libraries, which they refer to as DLLs (dynamic link libraries). Shared libraries are “shared” in two different ways. First, in any given ﬁle system, there is exactly one lÍÇ ﬁle for a particular library. The code and data in this lÍÇ ﬁle are shared by all of the executable object ﬁles that reference the library, as opposed to the contents of static libraries, which are copied and embedded in the executables that reference them. Second, a single copy of the lÎ−ÓÎ section of a shared library in memory can be shared by different running processes. We will explore this in more detail when we study virtual memory in Chapter 9. Figure 7.16 summarizes the dynamic linking process for the example program in Figure 7.7. To build a shared library Ä©¾Ì−²ÎÇËlÍÇ of our example vector routines in Figure 7.6, we invoke the compiler driver with some special directives to the compiler and linker: Ä©ÅÏÓ| gcc -shared -fpic -o libvector.so addvec.c multvec.c The kðÉ©² ﬂag directs the compiler to generate position-independent code (more on this in the next section). The kÍ³þË−® ﬂag directs the linker to create a shared 736 Chapter 7 Linking Figure 7.16 Dynamic linking with shared libraries. main2.c libc.so libvector.so libc.so libvector.so main2.o prog2l Translators (cpp,cc1,as) Linker (ld) Fully linked executable in memory Partially linked executable object file vector.h Loader (execve) Dynamic linker (ld-linux.so) Relocatable object file Relocation and symbol table info Code and data object ﬁle. Once we have created the library, we would then link it into our example program in Figure 7.7: Ä©ÅÏÓ| gcc -o prog2l main2.c ./libvector.so This creates an executable object ﬁle ÉËÇ×pÄ in a form that can be linked with Ä©¾Ì−²ÎÇËlÍÇ at run time. The basic idea is to do some of the linking statically when the executable ﬁle is created, and then complete the linking process dynami- cally when the program is loaded. It is important to realize that none of the code or data sections from Ä©¾Ì−²ÎÇËlÍÇ are actually copied into the executable ÉËÇ×pÄ at this point. Instead, the linker copies some relocation and symbol table informa- tion that will allow references to code and data in Ä©¾Ì−²ÎÇËlÍÇ to be resolved at load time. When the loader loads and runs the executable ÉËÇ×pÄ, it loads the partially linked executable ÉËÇ×pÄ, using the techniques discussed in Section 7.9. Next, it notices that ÉËÇ×pÄ contains a l©ÅÎ−ËÉ section, which contains the path name of the dynamic linker, which is itself a shared object (e.g., Ä®kÄ©ÅÏÓlÍÇ on Linux systems). Instead of passing control to the application, as it would normally do, the loader loads and runs the dynamic linker. The dynamic linker then ﬁnishes the linking task by performing the following relocations: . Relocating the text and data of Ä©¾²lÍÇ into some memory segment . Relocating the text and data of Ä©¾Ì−²ÎÇËlÍÇ into another memory segment . Relocating any references in ÉËÇ×pÄ to symbols deﬁned by Ä©¾²lÍÇ and Ä©¾Ì−²ÎÇËlÍÇ Section 7.11 Loading and Linking Shared Libraries from Applications 737 Finally, the dynamic linker passes control to the application. From this point on, the locations of the shared libraries are ﬁxed and do not change during execution of the program. 7.11 Loading and Linking Shared Libraries from Applications Up to this point, we have discussed the scenario in which the dynamic linker loads and links shared libraries when an application is loaded, just before it executes. However, it is also possible for an application to request the dynamic linker to load and link arbitrary shared libraries while the application is running, without having to link in the applications against those libraries at compile time. Dynamic linking is a powerful and useful technique. Here are some examples in the real world: . Distributing software. Developers of Microsoft Windows applications fre- quently use shared libraries to distribute software updates. They generate a new copy of a shared library, which users can then download and use as a replacement for the current version. The next time they run their application, it will automatically link and load the new shared library. . Building high-performance Web servers. Many Web servers generate dynamic content, such as personalized Web pages, account balances, and banner ads. Early Web servers generated dynamic content by using ðÇËÂ and −Ó−²Ì− to create a child process and run a “CGI program” in the context of the child. However, modern high-performance Web servers can generate dynamic content using a more efﬁcient and sophisticated approach based on dynamic linking. The idea is to package each function that generates dynamic content in a shared library. When a request arrives from a Web browser, the server dynamically loads and links the appropriate function and then calls it directly, as opposed to using ðÇËÂ and −Ó−²Ì− to run the function in the context of a child process. The function remains cached in the server’s address space, so subsequent requests can be handled at the cost of a simple function call. This can have a signiﬁcant impact on the throughput of a busy site. Further, existing functions can be updated and new functions can be added at run time, without stopping the server. Linux systems provide a simple interface to the dynamic linker that allows application programs to load and link shared libraries at run time. a©Å²ÄÏ®− z®Äð²Ål³| ÌÇ©® h®ÄÇÉ−Åf²ÇÅÍÎ ²³þË hð©Ä−ÅþÀ−j ©ÅÎ ðÄþ×gy Returns: pointer to handle if OK, NULL on error 738 Chapter 7 Linking The ®ÄÇÉ−Å function loads and links the shared library ð©Ä−ÅþÀ−. The external symbols in ð©Ä−ÅþÀ− are resolved using libraries previously opened with the ‡¶‹⁄ˆ §‹ﬂ¢¡‹ ﬂag. If the current executable was compiled with the kË®ÔÅþÀ©² ﬂag, then its global symbols are also available for symbol resolution. The ðÄþ× argument must include either ‡¶‹⁄ˆﬁﬂ„, which tells the linker to resolve references to external symbols immediately, or the ‡¶‹⁄ˆ‹¡…» ﬂag, which instructs the linker to defer symbol resolution until code from the library is executed. Either of these values can be ored with the ‡¶‹⁄ˆ§‹ﬂ¢¡‹ ﬂag. a©Å²ÄÏ®− z®Äð²Ål³| ÌÇ©® h®ÄÍÔÀfÌÇ©® h³þÅ®Ä−j ²³þË hÍÔÀ¾ÇÄgy Returns: pointer to symbol if OK, NULL on error The ®ÄÍÔÀ function takes a ³þÅ®Ä− to a previously opened shared library and a ÍÔÀ¾ÇÄ name and returns the address of the symbol, if it exists, or NULL otherwise. a©Å²ÄÏ®− z®Äð²Ål³| ©ÅÎ ®Ä²ÄÇÍ− fÌÇ©® h³þÅ®Ä−gy Returns: 0 if OK, −1 on error The ®Ä²ÄÇÍ− function unloads the shared library if no other shared libraries are still using it. a©Å²ÄÏ®− z®Äð²Ål³| ²ÇÅÍÎ ²³þË h®Ä−ËËÇËfÌÇ©®gy Returns: error message if previous call to ®ÄÇÉ−Å, ®ÄÍÔÀ,or ®Ä²ÄÇÍ− failed; NULL if previous call was OK The ®Ä−ËËÇË function returns a string describing the most recent error that oc- curred as a result of calling ®ÄÇÉ−Å, ®ÄÍÔÀ,or ®Ä²ÄÇÍ−, or NULL if no error occurred. Figure 7.17 shows how we would use this interface to dynamically link our Ä©¾Ì−²ÎÇËlÍÇ shared library at run time and then invoke its þ®®Ì−² routine. To compile the program, we would invoke gcc in the following way: Ä©ÅÏÓ| gcc -rdynamic -o prog2r dll.c -ldl Section 7.11 Loading and Linking Shared Libraries from Applications 739 code/link/dll.c 1 a©Å²ÄÏ®− zÍÎ®©Çl³| 2 a©Å²ÄÏ®− zÍÎ®Ä©¾l³| 3 a©Å²ÄÏ®− z®Äð²Ål³| 4 5 ©ÅÎ Ó‰p` { Õoj pÛy 6 ©ÅÎ Ô‰p` { Õqj rÛy 7 ©ÅÎ Ö‰p`y 8 9 ©ÅÎ Àþ©Åfg 10 Õ 11 ÌÇ©® h³þÅ®Ä−y 12 ÌÇ©® fhþ®®Ì−²gf©ÅÎ hj ©ÅÎ hj ©ÅÎ hj ©ÅÎgy 13 ²³þË h−ËËÇËy 14 15 mh ⁄ÔÅþÀ©²þÄÄÔ ÄÇþ® Î³− Í³þË−® Ä©¾ËþËÔ ²ÇÅÎþ©Å©Å× þ®®Ì−²fg hm 16 ³þÅ®Ä− { ®ÄÇÉ−Åf‘lmÄ©¾Ì−²ÎÇËlÍÇ‘j ‡¶‹⁄ˆ‹¡…»gy 17 ©ð f_³þÅ®Ä−g Õ 18 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍ¿Å‘j ®Ä−ËËÇËfggy 19 −Ó©Îfogy 20 Û 21 22 mh §−Î þ ÉÇ©ÅÎ−Ë ÎÇ Î³− þ®®Ì−²fg ðÏÅ²Î©ÇÅ Ñ− ÁÏÍÎ ÄÇþ®−® hm 23 þ®®Ì−² { ®ÄÍÔÀf³þÅ®Ä−j ‘þ®®Ì−²‘gy 24 ©ð ff−ËËÇË { ®Ä−ËËÇËfgg _{ ﬁ•‹‹g Õ 25 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍ¿Å‘j −ËËÇËgy 26 −Ó©Îfogy 27 Û 28 29 mh ﬁÇÑ Ñ− ²þÅ ²þÄÄ þ®®Ì−²fg ÁÏÍÎ Ä©Â− þÅÔ ÇÎ³−Ë ðÏÅ²Î©ÇÅ hm 30 þ®®Ì−²fÓj Ôj Öj pgy 31 ÉË©ÅÎðf‘Ö { ‰c® c®`¿Å‘j Ö‰n`j Ö‰o`gy 32 33 mh •ÅÄÇþ® Î³− Í³þË−® Ä©¾ËþËÔ hm 34 ©ð f®Ä²ÄÇÍ−f³þÅ®Ä−g z ng Õ 35 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍ¿Å‘j ®Ä−ËËÇËfggy 36 −Ó©Îfogy 37 Û 38 Ë−ÎÏËÅ ny 39 Û code/link/dll.c Figure 7.17 Example program 3. Dynamically loads and links the shared library Ä©¾Ì−²ÎÇËlÍÇ at run time. 740 Chapter 7 Linking Aside Shared libraries and the Java Native Interface Java deﬁnes a standard calling convention called Java Native Interface (JNI) that allows “native” C and C++ functions to be called from Java programs. The basic idea of JNI is to compile the native C function, say, ðÇÇ, into a shared library, say, ðÇÇlÍÇ. When a running Java program attempts to invoke function ðÇÇ, the Java interpreter uses the ®ÄÇÉ−Å interface (or something like it) to dynamically link and load ðÇÇlÍÇ and then call ðÇÇ. 7.12 Position-Independent Code (PIC) A key purpose of shared libraries is to allow multiple running processes to share the same library code in memory and thus save precious memory resources. So how can multiple processes share a single copy of a program? One approach would be to assign a priori a dedicated chunk of the address space to each shared library, and then require the loader to always load the shared library at that address. While straightforward, this approach creates some serious problems. It would be an inefﬁcient use of the address space because portions of the space would be allocated even if a process didn’t use the library. It would also be difﬁcult to manage. We would have to ensure that none of the chunks overlapped. Each time a library was modiﬁed, we would have to make sure that it still ﬁt in its assigned chunk. If not, then we would have to ﬁnd a new chunk. And if we created a new library, we would have to ﬁnd room for it. Over time, given the hundreds of libraries and versions of libraries in a system, it would be difﬁcult to keep the address space from fragmenting into lots of small unused but unusable holes. Even worse, the assignment of libraries to memory would be different for each system, thus creating even more management headaches. To avoid these problems, modern systems compile the code segments of shared modules so that they can be loaded anywhere in memory without having to be modiﬁed by the linker. With this approach, a single copy of a shared module’s code segment can be shared by an unlimited number of processes. (Of course, each process will still get its own copy of the read/write data segment.) Code that can be loaded without needing any relocations is known as position- independent code (PIC). Users direct GNU compilation systems to generate PIC code with the kðÉ©² option to gcc. Shared libraries must always be compiled with this option. On x86-64 systems, references to symbols in the same executable object mod- ule require no special treatment to be PIC. These references can be compiled using PC-relative addressing and relocated by the static linker when it builds the object ﬁle. However, references to external procedures and global variables that are de- ﬁned by shared modules require some special techniques, which we describe next. PIC Data References Compilers generate PIC references to global variables by exploiting the following interesting fact: no matter where we load an object module (including shared Section 7.12 Position-Independent Code (PIC) 741 Code segment Global offset table (GOT) Data segment Fixed distance of 0x2008b9 bytes at run time between GOT[3] and addl instruction GOT[0]: … GOT[1]: … GOT[2]: … GOT[3]: &addcnt addvec: mov 0x2008b9(%rip),% rax # %rax=*GOT[3]=&addcnt addl $0x1,(%rax) # addcnt++ Figure 7.18 Using the GOT to reference a global variable. The þ®®Ì−² routine in Ä©¾Ì−²ÎÇËlÍÇ references þ®®²ÅÎ indirectly through the GOT for Ä©¾Ì−²ÎÇËlÍÇ. object modules) in memory, the data segment is always the same distance from the code segment. Thus, the distance between any instruction in the code segment and any variable in the data segment is a run-time constant, independent of the absolute memory locations of the code and data segments. Compilers that want to generate PIC references to global variables exploit this fact by creating a table called the global offset table (GOT) at the beginning of the data segment. The GOT contains an 8-byte entry for each global data object (procedure or global variable) that is referenced by the object module. The compiler also generates a relocation record for each entry in the GOT. At load time, the dynamic linker relocates each GOT entry so that it contains the absolute address of the object. Each object module that references global objects has its own GOT. Figure 7.18 shows the GOT from our example Ä©¾Ì−²ÎÇËlÍÇ shared module. The þ®®Ì−² routine loads the address of the global variable þ®®²ÅÎ indirectly via §ﬂ¶‰q` and then increments þ®®²ÅÎ in memory. The key idea here is that the offset in the PC-relative reference to §ﬂ¶‰q` is a run-time constant. Since þ®®²ÅÎ is deﬁned by the Ä©¾Ì−²ÎÇËlÍÇ module, the compiler could have exploited the constant distance between the code and data segments by generating a direct PC-relative reference to þ®®²ÅÎ and adding a relocation for the linker to resolve when it builds the shared module. However, if þ®®²ÅÎ were deﬁned by another shared module, then the indirect access through the GOT would be necessary. In this case, the compiler has chosen to use the most general solution, the GOT, for all references. PIC Function Calls Suppose that a program calls a function that is deﬁned by a shared library. The compiler has no way of predicting the run-time address of the function, since the shared module that deﬁnes it could be loaded anywhere at run time. The normal approach would be to generate a relocation record for the reference, which 742 Chapter 7 Linking the dynamic linker could then resolve when the program was loaded. However, this approach would not be PIC, since it would require the linker to modify the code segment of the calling module. GNU compilation systems solve this problem using an interesting technique, called lazy binding, that defers the binding of each procedure address until the ﬁrst time the procedure is called. The motivation for lazy binding is that a typical application program will call only a handful of the hundreds or thousands of functions exported by a shared library such as Ä©¾²lÍÇ. By deferring the resolution of a function’s address until it is actually called, the dynamic linker can avoid hundreds or thousands of unnecessary relocations at load time. There is a nontrivial run-time overhead the ﬁrst time the function is called, but each call thereafter costs only a single instruction and a memory reference for the indirection. Lazy binding is implemented with a compact yet somewhat complex interac- tion between two data structures: the GOT and the procedure linkage table (PLT). If an object module calls any functions that are deﬁned in shared libraries, then it has its own GOT and PLT. The GOT is part of the data segment. The PLT is part of the code segment. Figure 7.19 shows how the PLT and GOT work together to resolve the address of a function at run time. First, let’s examine the contents of each of these tables. Procedure linkage table (PLT). The PLT is an array of 16-byte code entries. –‹¶‰n` is a special entry that jumps into the dynamic linker. Each shared library function called by the executable has its own PLT entry. Each of Global offset table (GOT) Data segment GOT[0]: GOT[1]: GOT[2]: GOT[3]: 0x4005b6 GOT[4]: 0x4005c6 GOT[5]: 0x4005d6 addr of .dynamic addr of reloc entries addr of dynamic linker # sys startup # addvec() # printf() addr of .dynamic addr of reloc entries addr of dynamic linker # sys startup &addvec()&addvec() # printf() Procedure linkage table (PLT) Code segment callq 0x4005c0 # call addvec() # PLT[0]: call dynamic linker 4005a0: pushq *GOT[1] 4005a6: jmpq *GOT[2] … # PLT[2]: call addvec() 4005c0: jmpq *GOT[4] 4005c6: pushq $0x1 4005cb: jmpq 4005a0 Procedure linkage table (PLT) Code segment callq 0x4005c0 # call addvec() # PLT[0]: call dynamic linker 4005a0: pushq *GOT[1] 4005a6: jmpq *GOT[2] … # PLT[2]: call addvec() 4005c0: jmpq *GOT[4] 4005c6: pushq $0x1 4005cb: jmpq 4005a0 Global offset table (GOT) Data segment GOT[0]: GOT[1]: GOT[2]: GOT[3]: 0x4005b6 GOT[4]: GOT[4]: GOT[5]: 0x4005d6 1 1 2 2 4 3 (a) First invocation of addvec (b) Subsequent invocations of addvec Figure 7.19 Using the PLT and GOT to call external functions. The dynamic linker resolves the address of þ®®Ì−² the ﬁrst time it is called. Section 7.13 Library Interpositioning 743 these entries is responsible for invoking a speciﬁc function. –‹¶‰o` (not shown here) invokes the system startup function (ˆˆÄ©¾²ˆÍÎþËÎˆÀþ©Å), which initializes the execution environment, calls the Àþ©Å function, and handles its return value. Entries starting at –‹¶‰p` invoke functions called by the user code. In our example, –‹¶‰p` invokes þ®®Ì−² and –‹¶‰q` (not shown) invokes ÉË©ÅÎð. Global offset table (GOT). As we have seen, the GOT is an array of 8-byte address entries. When used in conjunction with the PLT, §ﬂ¶‰n` and §ﬂ¶‰o` contain information that the dynamic linker uses when it resolves function addresses. §ﬂ¶‰p` is the entry point for the dynamic linker in the Ä®kÄ©ÅÏÓlÍÇ module. Each of the remaining entries corresponds to a called function whose address needs to be resolved at run time. Each has a matching PLT entry. For example, §ﬂ¶‰r` and –‹¶‰p` correspond to þ®®Ì−². Initially, each GOT entry points to the second instruction in the corresponding PLT entry. Figure 7.19(a) shows how the GOT and PLT work together to lazily resolve the run-time address of function þ®®Ì−² the ﬁrst time it is called: Step 1. Instead of directly calling þ®®Ì−², the program calls into –‹¶‰p`, which is the PLT entry for þ®®Ì−². Step 2. The ﬁrst PLT instruction does an indirect jump through §ﬂ¶‰r`. Since each GOT entry initially points to the second instruction in its correspond- ing PLT entry, the indirect jump simply transfers control back to the next instruction in –‹¶‰p`. Step 3. After pushing an ID for þ®®Ì−² (nÓo) onto the stack, –‹¶‰p` jumps to –‹¶‰n`. Step 4. –‹¶‰n` pushes an argument for the dynamic linker indirectly through §ﬂ¶‰o` and then jumps into the dynamic linker indirectly through §ﬂ¶‰p`. The dynamic linker uses the two stack entries to determine the run- time location of þ®®Ì−², overwrites §ﬂ¶‰r` with this address, and passes control to þ®®Ì−². Figure 7.19(b) shows the control ﬂow for any subsequent invocations of þ®®Ì−²: Step 1. Control passes to –‹¶‰p` as before. Step 2. However, this time the indirect jump through §ﬂ¶‰r` transfers control directly to þ®®Ì−². 7.13 Library Interpositioning Linux linkers support a powerful technique, called library interpositioning, that allows you to intercept calls to shared library functions and execute your own code instead. Using interpositioning, you could trace the number of times a particular 744 Chapter 7 Linking library function is called, validate and trace its input and output values, or even replace it with a completely different implementation. Here’s the basic idea: Given some target function to be interposed on, you create a wrapper function whose prototype is identical to the target function. Using some particular interpositioning mechanism, you then trick the system into calling the wrapper function instead of the target function. The wrapper function typically executes its own logic, then calls the target function and passes its return value back to the caller. Interpositioning can occur at compile time, link time, or run time as the program is being loaded and executed. To explore these different mechanisms, we will use the example program in Figure 7.20(a) as a running example. It calls the ÀþÄÄÇ² and ðË−− functions from the C standard library (Ä©¾²lÍÇ). The call to ÀþÄÄÇ² allocates a block of 32 bytes from the heap and returns a pointer to the block. The call to ðË−− gives the block back to the heap, for use by subsequent calls to ÀþÄÄÇ². Our goal is to use interpositioning to trace the calls to ÀþÄÄÇ² and ðË−− as the program runs. 7.13.1 Compile-Time Interpositioning Figure 7.20 shows how to use the C preprocessor to interpose at compile time. Each wrapper function in ÀÔÀþÄÄÇ²l² (Figure 7.20(c)) calls the target function, prints a trace, and returns. The local ÀþÄÄÇ²l³ header ﬁle (Figure 7.20(b)) instructs the preprocessor to replace each call to a target function with a call to its wrapper. Here is how to compile and link the program: Ä©ÅÏÓ| gcc -DCOMPILETIME -c mymalloc.c Ä©ÅÏÓ| gcc -I. -o intc int.c mymalloc.o The interpositioning happens because of the k'l argument, which tells the C preprocessor to look for ÀþÄÄÇ²l³ in the current directory before looking in the usual system directories. Notice that the wrappers in ÀÔÀþÄÄÇ²l² are compiled with the standard ÀþÄÄÇ²l³ header ﬁle. Running the program gives the following trace: Ä©ÅÏÓ| ./intc ÀþÄÄÇ²fqpg{nÓw−−non ðË−−fnÓw−−nong 7.13.2 Link-Time Interpositioning The Linux static linker supports link-time interpositioning with the kkÑËþÉ ð ﬂag. This ﬂag tells the linker to resolve references to symbol ð as ˆˆÑËþÉˆð (two underscores for the preﬁx), and to resolve references to symbol ˆˆË−þÄˆð (two underscores for the preﬁx) as ð. Figure 7.21 shows the wrappers for our example program. Here is how to compile the source ﬁles into relocatable object ﬁles: Ä©ÅÏÓ| gcc -DLINKTIME -c mymalloc.c Ä©ÅÏÓ| gcc -c int.c Section 7.13 Library Interpositioning 745 (a) Example program ©ÅÎl² code/link/interpose/int.c 1 a©Å²ÄÏ®− zÍÎ®©Çl³| 2 a©Å²ÄÏ®− zÀþÄÄÇ²l³| 3 4 ©ÅÎ Àþ©Åfg 5 Õ 6 ©ÅÎ hÉ { ÀþÄÄÇ²fqpgy 7 ðË−−fÉgy 8 Ë−ÎÏËÅfngy 9 Û code/link/interpose/int.c (b) Local ÀþÄÄÇ²l³ ﬁle code/link/interpose/malloc.h 1 a®−ð©Å− ÀþÄÄÇ²fÍ©Ö−g ÀÔÀþÄÄÇ²fÍ©Ö−g 2 a®−ð©Å− ðË−−fÉÎËg ÀÔðË−−fÉÎËg 3 4 ÌÇ©® hÀÔÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−gy 5 ÌÇ©® ÀÔðË−−fÌÇ©® hÉÎËgy code/link/interpose/malloc.h (c) Wrapper functions in ÀÔÀþÄÄÇ²l² code/link/interpose/mymalloc.c 1 a©ð®−ð £ﬂ›–'‹¥¶'›¥ 2 a©Å²ÄÏ®− zÍÎ®©Çl³| 3 a©Å²ÄÏ®− zÀþÄÄÇ²l³| 4 5 mh ÀþÄÄÇ² ÑËþÉÉ−Ë ðÏÅ²Î©ÇÅ hm 6 ÌÇ©® hÀÔÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−g 7 Õ 8 ÌÇ©® hÉÎË { ÀþÄÄÇ²fÍ©Ö−gy 9 ÉË©ÅÎðf‘ÀþÄÄÇ²fc®g{cÉ¿Å‘j 10 f©ÅÎgÍ©Ö−j ÉÎËgy 11 Ë−ÎÏËÅ ÉÎËy 12 Û 13 14 mh ðË−− ÑËþÉÉ−Ë ðÏÅ²Î©ÇÅ hm 15 ÌÇ©® ÀÔðË−−fÌÇ©® hÉÎËg 16 Õ 17 ðË−−fÉÎËgy 18 ÉË©ÅÎðf‘ðË−−fcÉg¿Å‘j ÉÎËgy 19 Û 20 a−Å®©ð code/link/interpose/mymalloc.c Figure 7.20 Compile-time interpositioning with the C preprocessor. 746 Chapter 7 Linking code/link/interpose/mymalloc.c 1 a©ð®−ð ‹'ﬁ«¶'›¥ 2 a©Å²ÄÏ®− zÍÎ®©Çl³| 3 4 ÌÇ©® hˆˆË−þÄˆÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−gy 5 ÌÇ©® ˆˆË−þÄˆðË−−fÌÇ©® hÉÎËgy 6 7 mh ÀþÄÄÇ² ÑËþÉÉ−Ë ðÏÅ²Î©ÇÅ hm 8 ÌÇ©® hˆˆÑËþÉˆÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−g 9 Õ 10 ÌÇ©® hÉÎË { ˆˆË−þÄˆÀþÄÄÇ²fÍ©Ö−gy mh £þÄÄ Ä©¾² ÀþÄÄÇ² hm 11 ÉË©ÅÎðf‘ÀþÄÄÇ²fc®g { cÉ¿Å‘j f©ÅÎgÍ©Ö−j ÉÎËgy 12 Ë−ÎÏËÅ ÉÎËy 13 Û 14 15 mh ðË−− ÑËþÉÉ−Ë ðÏÅ²Î©ÇÅ hm 16 ÌÇ©® ˆˆÑËþÉˆðË−−fÌÇ©® hÉÎËg 17 Õ 18 ˆˆË−þÄˆðË−−fÉÎËgy mh £þÄÄ Ä©¾² ðË−− hm 19 ÉË©ÅÎðf‘ðË−−fcÉg¿Å‘j ÉÎËgy 20 Û 21 a−Å®©ð code/link/interpose/mymalloc.c Figure 7.21 Link-time interpositioning with the kkÑËþÉ ﬂag. And here is how to link the object ﬁles into an executable: Ä©ÅÏÓ| gcc -Wl,--wrap,malloc -Wl,--wrap,free -o intl int.o mymalloc.o The k„ÄjÇÉÎ©ÇÅ ﬂag passes ÇÉÎ©ÇÅ to the linker. Each comma in ÇÉÎ©ÇÅ is replaced with a space. So k„ÄjkkÑËþÉjÀþÄÄÇ² passes kkÑËþÉ ÀþÄÄÇ² to the linker, and similarly for k„ÄjkkÑËþÉjðË−−. Running the program gives the following trace: Ä©ÅÏÓ| ./intl ÀþÄÄÇ²fqpg { nÓov²ðnon ðË−−fnÓov²ðnong 7.13.3 Run-Time Interpositioning Compile-time interpositioning requires access to a program’s source ﬁles. Link- time interpositioning requires access to its relocatable object ﬁles. However, there is a mechanism for interpositioning at run time that requires access only to the executable object ﬁle. This fascinating mechanism is based on the dynamic linker’s ‹⁄ˆ–‡¥‹ﬂ¡⁄ environment variable. Section 7.13 Library Interpositioning 747 If the ‹⁄ˆ–‡¥‹ﬂ¡⁄ environment variable is set to a list of shared library pathnames (separated by spaces or colons), then when you load and execute a program, the dynamic linker (ld-linux.so) will search the ‹⁄ˆ–‡¥‹ﬂ¡⁄ libraries ﬁrst, before any other shared libraries, when it resolves undeﬁned references. With this mechanism, you can interpose on any function in any shared library, including Ä©¾²lÍÇ, when you load and execute any executable. Figure 7.22 shows the wrappers for ÀþÄÄÇ² and ðË−−. In each wrapper, the call to ®ÄÍÔÀ returns the pointer to the target Ä©¾² function. The wrapper then calls the target function, prints a trace, and returns. Here is how to build the shared library that contains the wrapper functions: Ä©ÅÏÓ| gcc -DRUNTIME -shared -fpic -o mymalloc.so mymalloc.c -ldl Here is how to compile the main program: Ä©ÅÏÓ| gcc -o intr int.c Here is how to run the program from the ¾þÍ³ shell:3 Ä©ÅÏÓ| LD_PRELOAD=\"./mymalloc.so\" ./intr ÀþÄÄÇ²fqpg { nÓo¾ðunon ðË−−fnÓo¾ðunong And here is how to run it from the ²Í³ or Î²Í³ shells: Ä©ÅÏÓ| (setenv LD_PRELOAD \"./mymalloc.so\"; ./intr; unsetenv LD_PRELOAD) ÀþÄÄÇ²fqpg { nÓposunon ðË−−fnÓposunong Notice that you can use ‹⁄ˆ–‡¥‹ﬂ¡⁄ to interpose on the library calls of any executable program! Ä©ÅÏÓ| LD_PRELOAD=\"./mymalloc.so\" /usr/bin/uptime ÀþÄÄÇ²fstvg { nÓpo¾¾non ðË−−fnÓpo¾¾nong ÀþÄÄÇ²fosg { nÓpo¾¾non ÀþÄÄÇ²fstvg { nÓpo¾¾nqn ÀþÄÄÇ²fppssg { nÓpo¾¾pun ðË−−fnÓpo¾¾nqng ÀþÄÄÇ²fpng { nÓpo¾¾nqn ÀþÄÄÇ²fpng { nÓpo¾¾nsn ÀþÄÄÇ²fpng { nÓpo¾¾nun ÀþÄÄÇ²fpng { nÓpo¾¾nwn ÀþÄÄÇ²fpng { nÓpo¾¾n¾n ÀþÄÄÇ²fqvrg { nÓpo¾¾n®n pnxruxqt ÏÉ vs ®þÔÍj txnrj o ÏÍ−Ëj ÄÇþ® þÌ−Ëþ×−x nlonj nlnrj nlns 3. If you don’t know what shell you are running, type ÉË©ÅÎ−ÅÌ ·¤¥‹‹ at the command line. 748 Chapter 7 Linking code/link/interpose/mymalloc.c 1 a©ð®−ð ‡•ﬁ¶'›¥ 2 a®−ð©Å− ˆ§ﬁ•ˆ·ﬂ•‡£¥ 3 a©Å²ÄÏ®− zÍÎ®©Çl³| 4 a©Å²ÄÏ®− zÍÎ®Ä©¾l³| 5 a©Å²ÄÏ®− z®Äð²Ål³| 6 7 mh ÀþÄÄÇ² ÑËþÉÉ−Ë ðÏÅ²Î©ÇÅ hm 8 ÌÇ©® hÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−g 9 Õ 10 ÌÇ©® hfhÀþÄÄÇ²ÉgfÍ©Ö−ˆÎ Í©Ö−gy 11 ²³þË h−ËËÇËy 12 13 ÀþÄÄÇ²É { ®ÄÍÔÀf‡¶‹⁄ˆﬁ¥”¶j ‘ÀþÄÄÇ²‘gy mh §−Î þ®®Ë−ÍÍ Çð Ä©¾² ÀþÄÄÇ² hm 14 ©ð ff−ËËÇË { ®Ä−ËËÇËfgg _{ ﬁ•‹‹g Õ 15 ðÉÏÎÍf−ËËÇËj ÍÎ®−ËËgy 16 −Ó©Îfogy 17 Û 18 ²³þË hÉÎË { ÀþÄÄÇ²ÉfÍ©Ö−gy mh £þÄÄ Ä©¾² ÀþÄÄÇ² hm 19 ÉË©ÅÎðf‘ÀþÄÄÇ²fc®g { cÉ¿Å‘j f©ÅÎgÍ©Ö−j ÉÎËgy 20 Ë−ÎÏËÅ ÉÎËy 21 Û 22 23 mh ðË−− ÑËþÉÉ−Ë ðÏÅ²Î©ÇÅ hm 24 ÌÇ©® ðË−−fÌÇ©® hÉÎËg 25 Õ 26 ÌÇ©® fhðË−−ÉgfÌÇ©® hg { ﬁ•‹‹y 27 ²³þË h−ËËÇËy 28 29 ©ð f_ÉÎËg 30 Ë−ÎÏËÅy 31 32 ðË−−É { ®ÄÍÔÀf‡¶‹⁄ˆﬁ¥”¶j ‘ðË−−‘gy mh §−Î þ®®Ë−ÍÍ Çð Ä©¾² ðË−− hm 33 ©ð ff−ËËÇË { ®Ä−ËËÇËfgg _{ ﬁ•‹‹g Õ 34 ðÉÏÎÍf−ËËÇËj ÍÎ®−ËËgy 35 −Ó©Îfogy 36 Û 37 ðË−−ÉfÉÎËgy mh £þÄÄ Ä©¾² ðË−− hm 38 ÉË©ÅÎðf‘ðË−−fcÉg¿Å‘j ÉÎËgy 39 Û 40 a−Å®©ð code/link/interpose/mymalloc.c Figure 7.22 Run-time interpositioning with ‹⁄ˆ–‡¥‹ﬂ¡⁄. Section 7.15 Summary 749 7.14 Tools for Manipulating Object Files There are a number of tools available on Linux systems to help you understand and manipulate object ﬁles. In particular, the GNU binutils package is especially helpful and runs on every Linux platform. ar. Creates static libraries, and inserts, deletes, lists, and extracts members. strings. Lists all of the printable strings contained in an object ﬁle. strip. Deletes symbol table information from an object ﬁle. nm. Lists the symbols deﬁned in the symbol table of an object ﬁle. size. Lists the names and sizes of the sections in an object ﬁle. readelf. Displays the complete structure of an object ﬁle, including all of the information encoded in the ELF header. Subsumes the functionality of size and nm. objdump. The mother of all binary tools. Can display all of the information in an object ﬁle. Its most useful function is disassembling the binary instructions in the lÎ−ÓÎ section. Linux systems also provide the ldd program for manipulating shared libraries: ldd: Lists the shared libraries that an executable needs at run time. 7.15 Summary Linking can be performed at compile time by static linkers and at load time and run time by dynamic linkers. Linkers manipulate binary ﬁles called object ﬁles, which come in three different forms: relocatable, executable, and shared. Relocatable object ﬁles are combined by static linkers into an executable object ﬁle that can be loaded into memory and executed. Shared object ﬁles (shared libraries) are linked and loaded by dynamic linkers at run time, either implicitly when the calling program is loaded and begins executing, or on demand, when the program calls functions from the ®ÄÇÉ−Å library. The two main tasks of linkers are symbol resolution, where each global symbol in an object ﬁle is bound to a unique deﬁnition, and relocation, where the ultimate memory address for each symbol is determined and where references to those objects are modiﬁed. Static linkers are invoked by compiler drivers such as gcc. They combine multiple relocatable object ﬁles into a single executable object ﬁle. Multiple object ﬁles can deﬁne the same symbol, and the rules that linkers use for silently resolving these multiple deﬁnitions can introduce subtle bugs in user programs. Multiple object ﬁles can be concatenated in a single static library. Linkers use libraries to resolve symbol references in other object modules. The left-to- right sequential scan that many linkers use to resolve symbol references is another source of confusing link-time errors. 750 Chapter 7 Linking Loaders map the contents of executable ﬁles into memory and run the pro- gram. Linkers can also produce partially linked executable object ﬁles with un- resolved references to the routines and data deﬁned in a shared library. At load time, the loader maps the partially linked executable into memory and then calls a dynamic linker, which completes the linking task by loading the shared library and relocating the references in the program. Shared libraries that are compiled as position-independent code can be loaded anywhere and shared at run time by multiple processes. Applications can also use the dynamic linker at run time in order to load, link, and access the functions and data in shared libraries. Bibliographic Notes Linking is poorly documented in the computer systems literature. Since it lies at the intersection of compilers, computer architecture, and operating systems, link- ing requires an understanding of code generation, machine-language program- ming, program instantiation, and virtual memory. It does not ﬁt neatly into any of the usual computer systems specialties and thus is not well covered by the classic texts in these areas. However, Levine’s monograph provides a good general ref- erence on the subject [69]. The original IA32 speciﬁcations for ELF and DWARF (a speciﬁcation for the contents of the l®−¾Ï× and lÄ©Å− sections) are described in [54]. The x86-64 extensions to the ELF ﬁle format are described in [36]. The x86-64 application binary interface (ABI) describes the conventions for compil- ing, linking, and running x86-64 programs, including the rules for relocation and position-independent code [77]. Homework Problems 7.6 ◆ This problem concerns the ÀlÇ module from Figure 7.5 and the following version of the ÍÑþÉl² function that counts the number of times it has been called: 1 −ÓÎ−ËÅ ©ÅÎ ¾Ïð‰`y 2 3 ©ÅÎ h¾ÏðÉn { d¾Ïð‰n`y 4 ÍÎþÎ©² ©ÅÎ h¾ÏðÉoy 5 6 ÍÎþÎ©² ÌÇ©® ©Å²Ëfg 7 Õ 8 ÍÎþÎ©² ©ÅÎ ²ÇÏÅÎ{ny 9 10 ²ÇÏÅÎiiy 11 Û 12 13 ÌÇ©® ÍÑþÉfg 14 Õ Homework Problems 751 15 ©ÅÎ Î−ÀÉy 16 17 ©Å²Ëfgy 18 ¾ÏðÉo { d¾Ïð‰o`y 19 Î−ÀÉ { h¾ÏðÉny 20 h¾ÏðÉn { h¾ÏðÉoy 21 h¾ÏðÉo { Î−ÀÉy 22 Û For each symbol that is deﬁned and referenced in ÍÑþÉlÇ, indicate if it will have a symbol table entry in the lÍÔÀÎþ¾ section in module ÍÑþÉlÇ. If so, indicate the module that deﬁnes the symbol (ÍÑþÉlÇ or ÀlÇ), the symbol type (local, global, or extern), and the section (lÎ−ÓÎ, l®þÎþ,or l¾ÍÍ) it occupies in that module. Symbol ÍÑþÉlÇ lÍÔÀÎþ¾ entry? Symbol type Module where deﬁned Section ¾Ïð ¾ÏðÉn ¾ÏðÉo ÍÑþÉ Î−ÀÉ ©Å²Ë ²ÇÏÅÎ 7.7 ◆ Without changing any variable names, modify ¾þËsl² on page 719 so that ðÇÇsl² prints the correct values of Ó and Ô (i.e., the hex representations of integers 15213 and 15212). 7.8 ◆ In this problem, let ‡¥ƒ(Ó.©) → ⁄¥ƒ(Ó.Â) denote that the linker will associate an arbitrary reference to symbol Ó in module © to the deﬁnition of Ó in module Â.For each example below, use this notation to indicate how the linker would resolve references to the multiply-deﬁned symbol in each module. If there is a link-time error (rule 1), write “error”. If the linker arbitrarily chooses one of the deﬁnitions (rule 3), write “unknown”. A. mh ›Ç®ÏÄ− o hm mh ›Ç®ÏÄ− p hm ©ÅÎ Àþ©Åfg ÍÎþÎ©² ©ÅÎ Àþ©Å{o‰ Õ ©ÅÎ Épfg ÛÕ Û (a) ‡¥ƒ(Àþ©Å.o) → ⁄¥ƒ( . ) (b) ‡¥ƒ(Àþ©Å.p) → ⁄¥ƒ( . ) 752 Chapter 7 Linking B. mh ›Ç®ÏÄ− o hm mh ›Ç®ÏÄ− p hm ©ÅÎ Óy ®ÇÏ¾Ä− Óy ÌÇ©® Àþ©Åfg ©ÅÎ Épfg ÕÕ ÛÛ (a) ‡¥ƒ(Ó.o) → ⁄¥ƒ( . ) (b) ‡¥ƒ(Ó.p) → ⁄¥ƒ( . ) C. mh ›Ç®ÏÄ− o hm mh ›Ç®ÏÄ− p hm ©ÅÎ Ó{oy ®ÇÏ¾Ä− Ó{olny ÌÇ©® Àþ©Åfg ©ÅÎ Épfg ÕÕ ÛÛ (a) ‡¥ƒ(Ó.o) → ⁄¥ƒ( . ) (b) ‡¥ƒ(Ó.p) → ⁄¥ƒ( . ) 7.9 ◆ Consider the following program, which consists of two object modules: 1 mh ðÇÇtl² hm 2 ÌÇ©® ÉpfÌÇ©®gy 3 4 ©ÅÎ Àþ©Åfg 5 Õ 6 Épfgy 7 Ë−ÎÏËÅ ny 8 Û 1 mh ¾þËtl² hm 2 a©Å²ÄÏ®− zÍÎ®©Çl³| 3 4 ²³þË Àþ©Åy 5 6 ÌÇ©® Épfg 7 Õ 8 ÉË©ÅÎðf‘nÓcÓ¿Å‘j Àþ©Ågy 9 Û When this program is compiled and executed on an x86-64 Linux system, it prints the string nÓrv¿Å and terminates normally, even though function Ép never initializes variable Àþ©Å. Can you explain this? 7.10 ◆◆ Let þ and ¾ denote object modules or static libraries in the current directory, and let þ→¾ denote that þ depends on ¾, in the sense that ¾ deﬁnes a symbol that is Solutions to Practice Problems 753 referenced by þ. For each of the following scenarios, show the minimal command line (i.e., one with the least number of object ﬁle and library arguments) that will allow the static linker to resolve all symbol references: A. ÉlÇ → Ä©¾Ólþ → ÉlÇ B. ÉlÇ → Ä©¾Ólþ → Ä©¾Ôlþ and Ä©¾Ôlþ → Ä©¾Ólþ C. ÉlÇ → Ä©¾Ólþ → Ä©¾Ôlþ → Ä©¾Ölþ and Ä©¾Ôlþ → Ä©¾Ólþ → Ä©¾Ölþ 7.11 ◆◆ The program header in Figure 7.14 indicates that the data segment occupies nÓpqn bytes in memory. However, only the ﬁrst nÓppv bytes of these come from the sections of the executable ﬁle. What causes this discrepancy? 7.12 ◆◆ Consider the call to function ÍÑþÉ in object ﬁle ÀlÇ (Problem 7.6). wx −v nn nn nn nn ²þÄÄÊ − zÀþ©ÅinÓ−| swap() with the following relocation entry: ËlÇððÍ−Î { nÓþ ËlÍÔÀ¾ÇÄ { ÍÑþÉ ËlÎÔÉ− { ‡ˆ”vtˆtrˆ–£qp Ëlþ®®−Å® { kr A. Suppose that the linker relocates lÎ−ÓÎ in ÀlÇ to address nÓrnnr−n and ÍÑþÉ to address nÓrnnrðv. Then what is the value of the relocated reference to ÍÑþÉ in the ²þÄÄÊ instruction? B. Suppose that the linker relocates lÎ−ÓÎ in ÀlÇ to address nÓrnnr®n and ÍÑþÉ to address nÓrnnsnn. Then what is the value of the relocated reference to ÍÑþÉ in the ²þÄÄÊ instruction? 7.13 ◆◆ Performing the following tasks will help you become more familiar with the various tools for manipulating object ﬁles. A. How many object ﬁles are contained in the versions of Ä©¾²lþ and Ä©¾Àlþ on your system? B. Does ×²² kﬂ× produce different executable code than ×²² kﬂ× k×? C. What shared libraries does the gcc driver on your system use? Solutions to Practice Problems Solution to Problem 7.1 (page 714) The purpose of this problem is to help you understand the relationship between linker symbols and C variables and functions. Notice that the C local variable Î−ÀÉ does not have a symbol table entry. 754 Chapter 7 Linking Symbol lÍÔÀÎþ¾ entry? Symbol type Module where deﬁned Section ¾Ïð Yes extern ÀlÇ l®þÎþ ¾ÏðÉn Yes global ÍÑþÉlÇ l®þÎþ ¾ÏðÉo Yes global ÍÑþÉlÇ COMMON ÍÑþÉ Yes global ÍÑþÉlÇ lÎ−ÓÎ Î−ÀÉ No — — — Solution to Problem 7.2 (page 720) This is a simple drill that checks your understanding of the rules that a Unix linker uses when it resolves global symbols that are deﬁned in more than one module. Understanding these rules can help you avoid some nasty programming bugs. A. The linker chooses the strong symbol deﬁned in module 1 over the weak symbol deﬁned in module 2 (rule 2): (a) ‡¥ƒ(Àþ©Å.o) → ⁄¥ƒ(Àþ©Å.o) (b) ‡¥ƒ(Àþ©Å.p) → ⁄¥ƒ(Àþ©Å.o) B. This is an error, because each module deﬁnes a strong symbol Àþ©Å (rule 1). C. The linker chooses the strong symbol deﬁned in module 2 over the weak symbol deﬁned in module 1 (rule 2): (a) ‡¥ƒ(Ó.o) → ⁄¥ƒ(Ó.p) (b) ‡¥ƒ(Ó.p) → ⁄¥ƒ(Ó.p) Solution to Problem 7.3 (page 725) Placing static libraries in the wrong order on the command line is a common source of linker errors that confuses many programmers. However, once you understand how linkers use static libraries to resolve references, it’s pretty straightforward. This little drill checks your understanding of this idea: A. Ä©ÅÏÓ| ×²² ÉlÇ Ä©¾Ólþ B. Ä©ÅÏÓ| ×²² ÉlÇ Ä©¾Ólþ Ä©¾Ôlþ C. Ä©ÅÏÓ| ×²² ÉlÇ Ä©¾Ólþ Ä©¾Ôlþ Ä©¾Ólþ Solution to Problem 7.4 (page 730) This problem concerns the disassembly listing in Figure 7.12(a). Our purpose here is to give you some practice reading disassembly listings and to check your understanding of PC-relative addressing. A. The hex address of the relocated reference in line 5 is nÓrnnr®ð. B. The hex value of the relocated reference in line 5 is nÓs. Remember that the disassembly listing shows the value of the reference in little-endian byte order. Solution to Problem 7.5 (page 731) This problem tests your understanding of how the linker relocates PC-relative references. You were given that Solutions to Practice Problems 755 ¡⁄⁄‡fÍg { ¡⁄⁄‡flÎ−ÓÎg { nÓrnnr®n and ¡⁄⁄‡fËlÍÔÀ¾ÇÄg { ¡⁄⁄‡fÍÑþÉg { nÓrnnr−v Using the algorithm in Figure 7.10, the linker ﬁrst computes the run-time address of the reference: Ë−ðþ®®Ë { ¡⁄⁄‡fÍg i ËlÇððÍ−Î { nÓrnnr®n i nÓþ { nÓrnnr®þ It then updates the reference: hË−ðÉÎË { fÏÅÍ©×Å−®g f¡⁄⁄‡fËlÍÔÀ¾ÇÄg i Ëlþ®®−Å® k Ë−ðþ®®Ëg { fÏÅÍ©×Å−®g fnÓrnnr−v i fkrg k nÓrnnr®þg { fÏÅÍ©×Å−®g fnÓþg Thus, in the resulting executable object ﬁle, the PC-relative reference to ÍÑþÉ has a value of nÓþ: rnnr®wx −v nþ nn nn nn ²þÄÄÊ rnnr−v zÍÑþÉ|This page is intentionally left blank. CHAPTER 8 Exceptional Control Flow 8.1 Exceptions 759 8.2 Processes 768 8.3 System Call Error Handling 773 8.4 Process Control 774 8.5 Signals 792 8.6 Nonlocal Jumps 817 8.7 Tools for Manipulating Processes 822 8.8 Summary 823 Bibliographic Notes 823 Homework Problems 824 Solutions to Practice Problems 831 757 758 Chapter 8 Exceptional Control Flow F rom the time you ﬁrst apply power to a processor until the time you shut it off, the program counter assumes a sequence of values a0,a1,...,an−1 where each ak is the address of some corresponding instruction Ik. Each transition from ak to ak+1 is called a control transfer. A sequence of such control transfers is called the ﬂow of control,or control ﬂow, of the processor. The simplest kind of control ﬂow is a “smooth” sequence where each Ik and Ik+1 are adjacent in memory. Typically, abrupt changes to this smooth ﬂow, where Ik+1 is not adjacent to Ik, are caused by familiar program instructions such as jumps, calls, and returns. Such instructions are necessary mechanisms that allow programs to react to changes in internal program state represented by program variables. But systems must also be able to react to changes in system state that are not captured by internal program variables and are not necessarily related to the execution of the program. For example, a hardware timer goes off at regular intervals and must be dealt with. Packets arrive at the network adapter and must be stored in memory. Programs request data from a disk and then sleep until they are notiﬁed that the data are ready. Parent processes that create child processes must be notiﬁed when their children terminate. Modern systems react to these situations by making abrupt changes in the control ﬂow. In general, we refer to these abrupt changes as exceptional control ﬂow (ECF). ECF occurs at all levels of a computer system. For example, at the hardware level, events detected by the hardware trigger abrupt control transfers to exception handlers. At the operating systems level, the kernel transfers control from one user process to another via context switches. At the application level, a process can send a signal to another process that abruptly transfers control to a signal handler in the recipient. An individual program can react to errors by sidestepping the usual stack discipline and making nonlocal jumps to arbitrary locations in other functions. As programmers, there are a number of reasons why it is important for you to understand ECF: . Understanding ECF will help you understand important systems concepts.ECF is the basic mechanism that operating systems use to implement I/O, processes, and virtual memory. Before you can really understand these important ideas, you need to understand ECF. . Understanding ECF will help you understand how applications interact with the operating system. Applications request services from the operating system by using a form of ECF known as a trap or system call. For example, writing data to a disk, reading data from a network, creating a new process, and terminating the current process are all accomplished by application programs invoking system calls. Understanding the basic system call mechanism will help you understand how these services are provided to applications. . Understanding ECF will help you write interesting new application programs. The operating system provides application programs with powerful ECF Section 8.1 Exceptions 759 mechanisms for creating new processes, waiting for processes to terminate, notifying other processes of exceptional events in the system, and detecting and responding to these events. If you understand these ECF mechanisms, then you can use them to write interesting programs such as Unix shells and Web servers. . Understanding ECF will help you understand concurrency. ECF is a basic mechanism for implementing concurrency in computer systems. The following are all examples of concurrency in action: an exception handler that interrupts the execution of an application program; processes and threads whose exe- cution overlap in time; and a signal handler that interrupts the execution of an application program. Understanding ECF is a ﬁrst step to understanding concurrency. We will return to study it in more detail in Chapter 12. . Understanding ECF will help you understand how software exceptions work. Languages such as C++ and Java provide software exception mechanisms via ÎËÔ, ²þÎ²³, and Î³ËÇÑ statements. Software exceptions allow the program to make nonlocal jumps (i.e., jumps that violate the usual call/return stack discipline) in response to error conditions. Nonlocal jumps are a form of application-level ECF and are provided in C via the Í−ÎÁÀÉ and ÄÇÅ×ÁÀÉ functions. Understanding these low-level functions will help you understand how higher-level software exceptions can be implemented. Up to this point in your study of systems, you have learned how applications interact with the hardware. This chapter is pivotal in the sense that you will begin to learn how your applications interact with the operating system. Interestingly, these interactions all revolve around ECF. We describe the various forms of ECF that exist at all levels of a computer system. We start with exceptions, which lie at the intersection of the hardware and the operating system. We also discuss system calls, which are exceptions that provide applications with entry points into the operating system. We then move up a level of abstraction and describe processes and signals, which lie at the intersection of applications and the operating system. Finally, we discuss nonlocal jumps, which are an application-level form of ECF. 8.1 Exceptions Exceptions are a form of exceptional control ﬂow that are implemented partly by the hardware and partly by the operating system. Because they are partly implemented in hardware, the details vary from system to system. However, the basic ideas are the same for every system. Our aim in this section is to give you a general understanding of exceptions and exception handling and to help demystify what is often a confusing aspect of modern computer systems. An exception is an abrupt change in the control ﬂow in response to some change in the processor’s state. Figure 8.1 shows the basic idea. In the ﬁgure, the processor is executing some current instruction Icurr when a signiﬁcant change in the processor’s state occurs. The state is encoded in various bits and signals inside the processor. The change in state is known as an event. 760 Chapter 8 Exceptional Control Flow Aside Hardware versus software exceptions C++ and Java programmers will have noticed that the term “exception” is also used to describe the application-level ECF mechanism provided by C++ and Java in the form of ²þÎ²³, Î³ËÇÑ, and ÎËÔ statements. If we wanted to be perfectly clear, we might distinguish between “hardware” and “software” exceptions, but this is usually unnecessary because the meaning is clear from the context. Figure 8.1 Anatomy of an exception. A change in the processor’s state (an event) triggers an abrupt control transfer (an exception) from the application program to an exception handler. After it ﬁnishes processing, the handler either returns control to the interrupted program or aborts. Application program Exception handler Exception Exception processing Exception return (optional) Event occurs here Icurr Inext The event might be directly related to the execution of the current instruction. For example, a virtual memory page fault occurs, an arithmetic overﬂow occurs, or an instruction attempts a divide by zero. On the other hand, the event might be unrelated to the execution of the current instruction. For example, a system timer goes off or an I/O request completes. In any case, when the processor detects that the event has occurred, it makes an indirect procedure call (the exception), through a jump table called an exception table, to an operating system subroutine (the exception handler) that is speciﬁcally designed to process this particular kind of event. When the exception handler ﬁnishes processing, one of three things happens, depending on the type of event that caused the exception: 1. The handler returns control to the current instruction Icurr, the instruction that was executing when the event occurred. 2. The handler returns control to Inext, the instruction that would have executed next had the exception not occurred. 3. The handler aborts the interrupted program. Section 8.1.2 says more about these possibilities. 8.1.1 Exception Handling Exceptions can be difﬁcult to understand because handling them involves close cooperation between hardware and software. It is easy to get confused about Section 8.1 Exceptions 761 Figure 8.2 Exception table. The exception table is a jump table where entry k contains the address of the handler code for exception k. Code for exception handler 0 Code for exception handler 1 Code for exception handler 2 Code for exception handler n \u0002 1. . .. . . 0 1 2 n \u0002 1 Exception table Figure 8.3 Generating the address of an exception handler. The exception number is an index into the exception table.. . . 0 1 2 n – 1 Exception table Address of entry for exception # k Exception number (x 8) Exception table base register + which component performs which task. Let’s look at the division of labor between hardware and software in more detail. Each type of possible exception in a system is assigned a unique nonnegative integer exception number. Some of these numbers are assigned by the designers of the processor. Other numbers are assigned by the designers of the operating system kernel (the memory-resident part of the operating system). Examples of the former include divide by zero, page faults, memory access violations, break- points, and arithmetic overﬂows. Examples of the latter include system calls and signals from external I/O devices. At system boot time (when the computer is reset or powered on), the operat- ing system allocates and initializes a jump table called an exception table, so that entry k contains the address of the handler for exception k. Figure 8.2 shows the format of an exception table. At run time (when the system is executing some program), the processor detects that an event has occurred and determines the corresponding exception number k. The processor then triggers the exception by making an indirect pro- cedure call, through entry k of the exception table, to the corresponding handler. Figure 8.3 shows how the processor uses the exception table to form the address of the appropriate exception handler. The exception number is an index into the ex- ception table, whose starting address is contained in a special CPU register called the exception table base register. An exception is akin to a procedure call, but with some important differences: . As with a procedure call, the processor pushes a return address on the stack before branching to the handler. However, depending on the class of excep- tion, the return address is either the current instruction (the instruction that 762 Chapter 8 Exceptional Control Flow was executing when the event occurred) or the next instruction (the instruc- tion that would have executed after the current instruction had the event not occurred). . The processor also pushes some additional processor state onto the stack that will be necessary to restart the interrupted program when the handler returns. For example, an x86-64 system pushes the EFLAGS register containing the current condition codes, among other things, onto the stack. . When control is being transferred from a user program to the kernel, all of these items are pushed onto the kernel’s stack rather than onto the user’s stack. . Exception handlers run in kernel mode (Section 8.2.4), which means they have complete access to all system resources. Once the hardware triggers the exception, the rest of the work is done in software by the exception handler. After the handler has processed the event, it optionally returns to the interrupted program by executing a special “return from interrupt” instruction, which pops the appropriate state back into the processor’s control and data registers, restores the state to user mode (Section 8.2.4) if the exception interrupted a user program, and then returns control to the interrupted program. 8.1.2 Classes of Exceptions Exceptions can be divided into four classes: interrupts, traps, faults, and aborts. The table in Figure 8.4 summarizes the attributes of these classes. Interrupts Interrupts occur asynchronously as a result of signals from I/O devices that are external to the processor. Hardware interrupts are asynchronous in the sense that they are not caused by the execution of any particular instruction. Exception handlers for hardware interrupts are often called interrupt handlers. Figure 8.5 summarizes the processing for an interrupt. I/O devices such as network adapters, disk controllers, and timer chips trigger interrupts by signaling a pin on the processor chip and placing onto the system bus the exception number that identiﬁes the device that caused the interrupt. Class Cause Async/sync Return behavior Interrupt Signal from I/O device Async Always returns to next instruction Trap Intentional exception Sync Always returns to next instruction Fault Potentially recoverable error Sync Might return to current instruction Abort Nonrecoverable error Sync Never returns Figure 8.4 Classes of exceptions. Asynchronous exceptions occur as a result of events in I/O devices that are external to the processor. Synchronous exceptions occur as a direct result of executing an instruction. Section 8.1 Exceptions 763 Figure 8.5 Interrupt handling. The interrupt handler returns control to the next instruction in the application program’s control ﬂow. (2) Control passes to handler after current instruction finishes (3) Interrupt handler runs (4) Handler returns to next instruction (1) Interrupt pin goes high during execution of current instruction Icurr Inext Figure 8.6 Trap handling. The trap handler returns control to the next instruction in the application program’s control ﬂow. (2) Control passes to handler (3) Trap handler runs (4) Handler returns to instruction following the syscall (1) Application makes a system call syscall Inext After the current instruction ﬁnishes executing, the processor notices that the interrupt pin has gone high, reads the exception number from the system bus, and then calls the appropriate interrupt handler. When the handler returns, it returns control to the next instruction (i.e., the instruction that would have followed the current instruction in the control ﬂow had the interrupt not occurred). The effect is that the program continues executing as though the interrupt had never happened. The remaining classes of exceptions (traps, faults, and aborts) occur syn- chronously as a result of executing the current instruction. We refer to this in- struction as the faulting instruction. Traps and System Calls Traps are intentional exceptions that occur as a result of executing an instruction. Like interrupt handlers, trap handlers return control to the next instruction. The most important use of traps is to provide a procedure-like interface between user programs and the kernel, known as a system call. User programs often need to request services from the kernel such as reading a ﬁle (Ë−þ®), creating a new process (ðÇËÂ), loading a new program (−Ó−²Ì−), and terminating the current process (−Ó©Î). To allow controlled access to such kernel services, processors provide a special ÍÔÍ²þÄÄ n instruction that user programs can execute when they want to request service n. Executing the ÍÔÍ²þÄÄ instruction causes a trap to an exception handler that decodes the argument and calls the appropriate kernel routine. Figure 8.6 summarizes the processing for a system call. From a programmer’s perspective, a system call is identical to a regular func- tion call. However, their implementations are quite different. Regular functions 764 Chapter 8 Exceptional Control Flow Figure 8.7 Fault handling. Depending on whether the fault can be repaired or not, the fault handler either re-executes the faulting instruction or aborts. (2) Control passes to handler (3) Fault handler runs (4) Handler either re-executes current instruction or aborts (1) Current instruction causes a fault Icurr abort Figure 8.8 Abort handling. The abort handler passes control to a kernel þ¾ÇËÎ routine that terminates the application program. (2) Control passes to handler (3) Abort handler runs (4) Handler returns to abort routine (1) Fatal hardware error occurs Icurr abort run in user mode, which restricts the types of instructions they can execute, and they access the same stack as the calling function. A system call runs in kernel mode, which allows it to execute privileged instructions and access a stack deﬁned in the kernel. Section 8.2.4 discusses user and kernel modes in more detail. Faults Faults result from error conditions that a handler might be able to correct. When a fault occurs, the processor transfers control to the fault handler. If the handler is able to correct the error condition, it returns control to the faulting instruction, thereby re-executing it. Otherwise, the handler returns to an þ¾ÇËÎ routine in the kernel that terminates the application program that caused the fault. Figure 8.7 summarizes the processing for a fault. A classic example of a fault is the page fault exception, which occurs when an instruction references a virtual address whose corresponding page is not res- ident in memory and must therefore be retrieved from disk. As we will see in Chapter 9, a page is a contiguous block (typically 4 KB) of virtual memory. The page fault handler loads the appropriate page from disk and then returns control to the instruction that caused the fault. When the instruction executes again, the appropriate page is now resident in memory and the instruction is able to run to completion without faulting. Aborts Aborts result from unrecoverable fatal errors, typically hardware errors such as parity errors that occur when DRAM or SRAM bits are corrupted. Abort handlers never return control to the application program. As shown in Figure 8.8, the handler returns control to an þ¾ÇËÎ routine that terminates the application program. Section 8.1 Exceptions 765 Exception number Description Exception class 0 Divide error Fault 13 General protection fault Fault 14 Page fault Fault 18 Machine check Abort 32–255 OS-deﬁned exceptions Interrupt or trap Figure 8.9 Examples of exceptions in x86-64 systems. 8.1.3 Exceptions in Linux/x86-64 Systems To help make things more concrete, let’s look at some of the exceptions deﬁned for x86-64 systems. There are up to 256 different exception types [50]. Numbers in the range from 0 to 31 correspond to exceptions that are deﬁned by the Intel architects and thus are identical for any x86-64 system. Numbers in the range from 32 to 255 correspond to interrupts and traps that are deﬁned by the operating system. Figure 8.9 shows a few examples. Linux/x86-64 Faults and Aborts Divide error. A divide error (exception 0) occurs when an application attempts to divide by zero or when the result of a divide instruction is too big for the destination operand. Unix does not attempt to recover from divide errors, opting instead to abort the program. Linux shells typically report divide errors as “Floating exceptions.” General protection fault. The infamous general protection fault (exception 13) occurs for many reasons, usually because a program references an unde- ﬁned area of virtual memory or because the program attempts to write to a read-only text segment. Linux does not attempt to recover from this fault. Linux shells typically report general protection faults as “Segmentation faults.” Page fault. A page fault (exception 14) is an example of an exception where the faulting instruction is restarted. The handler maps the appropriate page of virtual memory on disk into a page of physical memory and then restarts the faulting instruction. We will see how page faults work in detail in Chapter 9. Machine check. A machine check (exception 18) occurs as a result of a fatal hardware error that is detected during the execution of the faulting in- struction. Machine check handlers never return control to the application program. Linux/x86-64 System Calls Linux provides hundreds of system calls that application programs use when they want to request services from the kernel, such as reading a ﬁle, writing a ﬁle, and 766 Chapter 8 Exceptional Control Flow Number Name Description Number Name Description 0 Ë−þ® Read ﬁle 33 ÉþÏÍ− Suspend process until signal arrives 1 ÑË©Î− Write ﬁle 37 þÄþËÀ Schedule delivery of alarm signal 2 ÇÉ−Å Open ﬁle 39 ×−ÎÉ©® Get process ID 3 ²ÄÇÍ− Close ﬁle 57 ðÇËÂ Create process 4 ÍÎþÎ Get info about ﬁle 59 −Ó−²Ì− Execute a program 9 ÀÀþÉ Map memory page to ﬁle 60 ˆ−Ó©Î Terminate process 12 ¾ËÂ Reset the top of the heap 61 Ñþ©Îr Wait for a process to terminate 32 ®ÏÉp Copy ﬁle descriptor 62 Â©ÄÄ Send signal to a process Figure 8.10 Examples of popular system calls in Linux x86-64 systems. creating a new process. Figure 8.10 lists some popular Linux system calls. Each system call has a unique integer number that corresponds to an offset in a jump table in the kernel. (Notice that this jump table is not the same as the exception table.) C programs can invoke any system call directly by using the ÍÔÍ²þÄÄ function. However, this is rarely necessary in practice. The C standard library provides a set of convenient wrapper functions for most system calls. The wrapper functions package up the arguments, trap to the kernel with the appropriate system call instruction, and then pass the return status of the system call back to the calling program. Throughout this text, we will refer to system calls and their associated wrapper functions interchangeably as system-level functions. System calls are provided on x86-64 systems via a trapping instruction called ÍÔÍ²þÄÄ. It is quite interesting to study how programs can use this instruction to invoke Linux system calls directly. All arguments to Linux system calls are passed through general-purpose registers rather than the stack. By convention, register cËþÓ contains the syscall number, with up to six arguments in cË®©, cËÍ©, cË®Ó, cËon, cËv, and cËw. The ﬁrst argument is in cË®©, the second in cËÍ©, and so on. On return from the system call, registers cË²Ó and cËoo are destroyed, and cËþÓ contains the return value. A negative return value between −4,095 and −1 indicates an error corresponding to negative −ËËÅÇ. For example, consider the following version of the familiar ³−ÄÄÇ program, written using the ÑË©Î− system-level function (Section 10.4) instead of ÉË©ÅÎð: 1 ©ÅÎ Àþ©Åfg 2 Õ 3 ÑË©Î−foj ‘³−ÄÄÇj ÑÇËÄ®¿Å‘j oqgy 4 ˆ−Ó©Îfngy 5 Û The ﬁrst argument to ÑË©Î− sends the output to ÍÎ®ÇÏÎ. The second argument is the sequence of bytes to write, and the third argument gives the number of bytes to write. Section 8.1 Exceptions 767 Aside A note on terminology The terminology for the various classes of exceptions varies from system to system. Processor ISA speciﬁcations often distinguish between asynchronous “interrupts” and synchronous “exceptions” yet provide no umbrella term to refer to these very similar concepts. To avoid having to constantly refer to “exceptions and interrupts” and “exceptions or interrupts,” we use the word “exception” as the general term and distinguish between asynchronous exceptions (interrupts) and synchronous exceptions (traps, faults, and aborts) only when it is appropriate. As we have noted, the basic ideas are the same for every system, but you should be aware that some manufacturers’ manuals use the word “exception” to refer only to those changes in control ﬂow caused by synchronous events. code/ecf/hello-asm64.sa 1 lÍ−²Î©ÇÅ l®þÎþ 2 ÍÎË©Å×x 3 lþÍ²©© ‘³−ÄÄÇj ÑÇËÄ®¿Å‘ 4 ÍÎË©Å×ˆ−Å®x 5 l−ÊÏ Ä−Åj ÍÎË©Å×ˆ−Å® k ÍÎË©Å× 6 lÍ−²Î©ÇÅ lÎ−ÓÎ 7 l×ÄÇ¾Ä Àþ©Å 8 Àþ©Åx First, call write(1, \"hello, world\\n\", 13) 9 ÀÇÌÊ boj cËþÓ write is system call 1 10 ÀÇÌÊ boj cË®© Arg1: stdout has descriptor 1 11 ÀÇÌÊ bÍÎË©Å×j cËÍ© Arg2: hello world string 12 ÀÇÌÊ bÄ−Åj cË®Ó Arg3: string length 13 ÍÔÍ²þÄÄ Make the system call Next, call _exit(0) 14 ÀÇÌÊ btnj cËþÓ _exit is system call 60 15 ÀÇÌÊ bnj cË®© Arg1: exit status is 0 16 ÍÔÍ²þÄÄ Make the system call code/ecf/hello-asm64.sa Figure 8.11 Implementing the ³−ÄÄÇ program directly with Linux system calls. Figure 8.11 shows an assembly-language version of ³−ÄÄÇ that uses the ÍÔÍ²þÄÄ instruction to invoke the ÑË©Î− and −Ó©Î system calls directly. Lines 9–13 invoke the ÑË©Î− function. First, line 9 stores the number of the ÑË©Î− sys- tem call in cËþÓ, and lines 10–12 set up the argument list. Then, line 13 uses the ÍÔÍ²þÄÄ instruction to invoke the system call. Similarly, lines 14–16 invoke the ˆ−Ó©Î system call. 768 Chapter 8 Exceptional Control Flow 8.2 Processes Exceptions are the basic building blocks that allow the operating system kernel to provide the notion of a process, one of the most profound and successful ideas in computer science. When we run a program on a modern system, we are presented with the illusion that our program is the only one currently running in the system. Our program appears to have exclusive use of both the processor and the memory. The processor appears to execute the instructions in our program, one after the other, without interruption. Finally, the code and data of our program appear to be the only objects in the system’s memory. These illusions are provided to us by the notion of a process. The classic deﬁnition of a process is an instance of a program in execution. Each program in the system runs in the context of some process. The context consists of the state that the program needs to run correctly. This state includes the program’s code and data stored in memory, its stack, the contents of its general- purpose registers, its program counter, environment variables, and the set of open ﬁle descriptors. Each time a user runs a program by typing the name of an executable object ﬁle to the shell, the shell creates a new process and then runs the executable object ﬁle in the context of this new process. Application programs can also create new processes and run either their own code or other applications in the context of the new process. A detailed discussion of how operating systems implement processes is be- yond our scope. Instead, we will focus on the key abstractions that a process provides to the application: . An independent logical control ﬂow that provides the illusion that our pro- gram has exclusive use of the processor. . A private address space that provides the illusion that our program has exclu- sive use of the memory system. Let’s look more closely at these abstractions. 8.2.1 Logical Control Flow A process provides each program with the illusion that it has exclusive use of the processor, even though many other programs are typically running concurrently on the system. If we were to use a debugger to single-step the execution of our program, we would observe a series of program counter (PC) values that corresponded exclusively to instructions contained in our program’s executable object ﬁle or in shared objects linked into our program dynamically at run time. This sequence of PC values is known as a logical control ﬂow, or simply logical ﬂow. Consider a system that runs three processes, as shown in Figure 8.12. The single physical control ﬂow of the processor is partitioned into three logical ﬂows, one for each process. Each vertical line represents a portion of the logical ﬂow for Section 8.2 Processes 769 Figure 8.12 Logical control ﬂows. Processes provide each program with the illusion that it has exclusive use of the processor. Each vertical bar represents a portion of the logical control ﬂow for a process. Process A Process B Process C Time a process. In the example, the execution of the three logical ﬂows is interleaved. Process A runs for a while, followed by B, which runs to completion. Process C then runs for a while, followed by A, which runs to completion. Finally, C is able to run to completion. The key point in Figure 8.12 is that processes take turns using the processor. Each process executes a portion of its ﬂow and then is preempted (temporarily suspended) while other processes take their turns. To a program running in the context of one of these processes, it appears to have exclusive use of the proces- sor. The only evidence to the contrary is that if we were to precisely measure the elapsed time of each instruction, we would notice that the CPU appears to peri- odically stall between the execution of some of the instructions in our program. However, each time the processor stalls, it subsequently resumes execution of our program without any change to the contents of the program’s memory locations or registers. 8.2.2 Concurrent Flows Logical ﬂows take many different forms in computer systems. Exception handlers, processes, signal handlers, threads, and Java processes are all examples of logical ﬂows. A logical ﬂow whose execution overlaps in time with another ﬂow is called a concurrent ﬂow, and the two ﬂows are said to run concurrently. More precisely, ﬂows X and Y are concurrent with respect to each other if and only if X begins after Y begins and before Y ﬁnishes, or Y begins after X begins and before X ﬁnishes. For example, in Figure 8.12, processes A and B run concurrently, as do A and C. On the other hand, B and C do not run concurrently, because the last instruction of B executes before the ﬁrst instruction of C. The general phenomenon of multiple ﬂows executing concurrently is known as concurrency. The notion of a process taking turns with other processes is also known as multitasking. Each time period that a process executes a portion of its ﬂow is called a time slice. Thus, multitasking is also referred to as time slicing.For example, in Figure 8.12, the ﬂow for process A consists of two time slices. Notice that the idea of concurrent ﬂows is independent of the number of processor cores or computers that the ﬂows are running on. If two ﬂows overlap in time, then they are concurrent, even if they are running on the same processor. However, we will sometimes ﬁnd it useful to identify a proper subset of concurrent 770 Chapter 8 Exceptional Control Flow ﬂows known as parallel ﬂows. If two ﬂows are running concurrently on different processor cores or computers, then we say that they are parallel ﬂows, that they are running in parallel, and have parallel execution. Practice Problem 8.1 (solution page 831) Consider three processes with the following starting and ending times: Process Start time End time A1 3 B2 5 C4 6 For each pair of processes, indicate whether they run concurrently (Y) or not (N): Process pair Concurrent? AB AC BC 8.2.3 Private Address Space A process provides each program with the illusion that it has exclusive use of the system’s address space. On a machine with n-bit addresses, the address space is the set of 2n possible addresses, 0, 1,..., 2n − 1. A process provides each program with its own private address space. This space is private in the sense that a byte of memory associated with a particular address in the space cannot in general be read or written by any other process. Although the contents of the memory associated with each private address space is different in general, each such space has the same general organization. For example, Figure 8.13 shows the organization of the address space for an x86-64 Linux process. The bottom portion of the address space is reserved for the user program, with the usual code, data, heap, and stack segments. The code segment always begins at address nÓrnnnnn. The top portion of the address space is reserved for the kernel (the memory-resident part of the operating system). This part of the address space contains the code, data, and stack that the kernel uses when it executes instructions on behalf of the process (e.g., when the application program executes a system call). 8.2.4 User and Kernel Modes In order for the operating system kernel to provide an airtight process abstraction, the processor must provide a mechanism that restricts the instructions that an Section 8.2 Processes 771 Figure 8.13 Process address space. 0x400000 0 Memory invisible to user code %esp (stack pointer) brk Loaded from the executable file User stack (created at run time) Memory-mapped region for shared libraries Run-time heap (created by malloc) Read/write segment (.data,.bss) Read-only code segment (.init,.text,.rodata) Kernel virtual memory (code, data, heap, stack) 248-1 application can execute, as well as the portions of the address space that it can access. Processors typically provide this capability with a mode bit in some control register that characterizes the privileges that the process currently enjoys. When the mode bit is set, the process is running in kernel mode (sometimes called supervisor mode). A process running in kernel mode can execute any instruction in the instruction set and access any memory location in the system. When the mode bit is not set, the process is running in user mode. A process in user mode is not allowed to execute privileged instructions that do things such as halt the processor, change the mode bit, or initiate an I/O operation. Nor is it allowed to directly reference code or data in the kernel area of the address space. Any such attempt results in a fatal protection fault. User programs must instead access kernel code and data indirectly via the system call interface. A process running application code is initially in user mode. The only way for the process to change from user mode to kernel mode is via an exception such as an interrupt, a fault, or a trapping system call. When the exception occurs, and control passes to the exception handler, the processor changes the mode from user mode to kernel mode. The handler runs in kernel mode. When it returns to the application code, the processor changes the mode from kernel mode back to user mode. Linux provides a clever mechanism, called the mÉËÇ² ﬁlesystem, that allows user mode processes to access the contents of kernel data structures. The mÉËÇ² ﬁlesystem exports the contents of many kernel data structures as a hierarchy of text 772 Chapter 8 Exceptional Control Flow ﬁles that can be read by user programs. For example, you can use the mÉËÇ² ﬁlesys- tem to ﬁnd out general system attributes such as CPU type (mÉËÇ²m²ÉÏ©ÅðÇ), or the memory segments used by a particular process (mÉËÇ²mprocess-idmÀþÉÍ). The 2.6 version of the Linux kernel introduced a mÍÔÍ ﬁlesystem, which exports addi- tional low-level information about system buses and devices. 8.2.5 Context Switches The operating system kernel implements multitasking using a higher-level form of exceptional control ﬂow known as a context switch. The context switch mecha- nism is built on top of the lower-level exception mechanism that we discussed in Section 8.1. The kernel maintains a context for each process. The context is the state that the kernel needs to restart a preempted process. It consists of the values of objects such as the general-purpose registers, the ﬂoating-point registers, the program counter, user’s stack, status registers, kernel’s stack, and various kernel data structures such as a page table that characterizes the address space, a process table that contains information about the current process, and a ﬁle table that contains information about the ﬁles that the process has opened. At certain points during the execution of a process, the kernel can decide to preempt the current process and restart a previously preempted process. This decision is known as scheduling and is handled by code in the kernel, called the scheduler. When the kernel selects a new process to run, we say that the kernel has scheduled that process. After the kernel has scheduled a new process to run, it preempts the current process and transfers control to the new process using a mechanism called a context switch that (1) saves the context of the current process, (2) restores the saved context of some previously preempted process, and (3) passes control to this newly restored process. A context switch can occur while the kernel is executing a system call on behalf of the user. If the system call blocks because it is waiting for some event to occur, then the kernel can put the current process to sleep and switch to another process. For example, if a Ë−þ® system call requires a disk access, the kernel can opt to perform a context switch and run another process instead of waiting for the data to arrive from the disk. Another example is the ÍÄ−−É system call, which is an explicit request to put the calling process to sleep. In general, even if a system call does not block, the kernel can decide to perform a context switch rather than return control to the calling process. A context switch can also occur as a result of an interrupt. For example, all systems have some mechanism for generating periodic timer interrupts, typically every 1 ms or 10 ms. Each time a timer interrupt occurs, the kernel can decide that the current process has run long enough and switch to a new process. Figure 8.14 shows an example of context switching between a pair of processes A and B. In this example, initially process A is running in user mode until it traps to the kernel by executing a Ë−þ® system call. The trap handler in the kernel requests a DMA transfer from the disk controller and arranges for the disk to interrupt the Section 8.3 System Call Error Handling 773 Figure 8.14 Anatomy of a process context switch. Process A Process B User code Kernel code Kernel code User code User code Context switch Context switch Time read Disk interrupt Return from read processor after the disk controller has ﬁnished transferring the data from disk to memory. The disk will take a relatively long time to fetch the data (on the order of tens of milliseconds), so instead of waiting and doing nothing in the interim, the kernel performs a context switch from process A to B. Note that, before the switch, the kernel is executing instructions in user mode on behalf of process A (i.e., there is no separate kernel process). During the ﬁrst part of the switch, the kernel is executing instructions in kernel mode on behalf of process A. Then at some point it begins executing instructions (still in kernel mode) on behalf of process B. And after the switch, the kernel is executing instructions in user mode on behalf of process B. Process B then runs for a while in user mode until the disk sends an interrupt to signal that data have been transferred from disk to memory. The kernel decides that process B has run long enough and performs a context switch from process B to A, returning control in process A to the instruction immediately following the Ë−þ® system call. Process A continues to run until the next exception occurs, and so on. 8.3 System Call Error Handling When Unix system-level functions encounter an error, they typically return −1 and set the global integer variable −ËËÅÇ to indicate what went wrong. Program- mers should always check for errors, but unfortunately, many skip error checking because it bloats the code and makes it harder to read. For example, here is how we might check for errors when we call the Linux ðÇËÂ function: 1 ©ð ffÉ©® { ðÇËÂfgg z ng Õ 2 ðÉË©ÅÎðfÍÎ®−ËËj ‘ðÇËÂ −ËËÇËx cÍ¿Å‘j ÍÎË−ËËÇËf−ËËÅÇggy 3 −Ó©Îfngy 4 Û The ÍÎË−ËËÇË function returns a text string that describes the error associated with a particular value of −ËËÅÇ. We can simplify this code somewhat by deﬁning the following error-reporting function: 774 Chapter 8 Exceptional Control Flow 1 ÌÇ©® ÏÅ©Óˆ−ËËÇËf²³þË hÀÍ×g mh •Å©ÓkÍÎÔÄ− −ËËÇË hm 2 Õ 3 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍx cÍ¿Å‘j ÀÍ×j ÍÎË−ËËÇËf−ËËÅÇggy 4 −Ó©Îfngy 5 Û Given this function, our call to ðÇËÂ reduces from four lines to two lines: 1 ©ð ffÉ©® { ðÇËÂfgg z ng 2 ÏÅ©Óˆ−ËËÇËf‘ðÇËÂ −ËËÇË‘gy We can simplify our code even further by using error-handling wrappers, as pioneered by Stevens in [110]. For a given base function ðÇÇ, we deﬁne a wrapper function ƒÇÇ with identical arguments but with the ﬁrst letter of the name capitalized. The wrapper calls the base function, checks for errors, and terminates if there are any problems. For example, here is the error-handling wrapper for the ðÇËÂ function: 1 É©®ˆÎ ƒÇËÂfÌÇ©®g 2 Õ 3 É©®ˆÎ É©®y 4 5 ©ð ffÉ©® { ðÇËÂfgg z ng 6 ÏÅ©Óˆ−ËËÇËf‘ƒÇËÂ −ËËÇË‘gy 7 Ë−ÎÏËÅ É©®y 8 Û Given this wrapper, our call to ðÇËÂ shrinks to a single compact line: 1 É©® { ƒÇËÂfgy We will use error-handling wrappers throughout the remainder of this book. They allow us to keep our code examples concise without giving you the mistaken impression that it is permissible to ignore error checking. Note that when we discuss system-level functions in the text, we will always refer to them by their lowercase base names, rather than by their uppercase wrapper names. See Appendix A for a discussion of Unix error handling and the error- handling wrappers used throughout this book. The wrappers are deﬁned in a ﬁle called ²ÍþÉÉl², and their prototypes are deﬁned in a header ﬁle called ²ÍþÉÉl³. These are available online from the CS:APP Web site. 8.4 Process Control Unix provides a number of system calls for manipulating processes from C pro- grams. This section describes the important functions and gives examples of how they are used. Section 8.4 Process Control 775 8.4.1 Obtaining Process IDs Each process has a unique positive (nonzero) process ID (PID).The ×−ÎÉ©® function returns the PID of the calling process. The ×−ÎÉÉ©® function returns the PID of its parent (i.e., the process that created the calling process). a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| É©®ˆÎ ×−ÎÉ©®fÌÇ©®gy É©®ˆÎ ×−ÎÉÉ©®fÌÇ©®gy Returns: PID of either the caller or the parent The ×−ÎÉ©® and ×−ÎÉÉ©® routines return an integer value of type É©®ˆÎ, which on Linux systems is deﬁned in ÎÔÉ−Íl³ as an ©ÅÎ. 8.4.2 Creating and Terminating Processes From a programmer’s perspective, we can think of a process as being in one of three states: Running. The process is either executing on the CPU or waiting to be executed and will eventually be scheduled by the kernel. Stopped. The execution of the process is suspended and will not be scheduled. A process stops as a result of receiving a SIGSTOP, SIGTSTP, SIGTTIN, or SIGTTOU signal, and it remains stopped until it receives a SIGCONT signal, at which point it becomes running again. (A signal is a form of software interrupt that we will describe in detail in Section 8.5.) Terminated. The process is stopped permanently. A process becomes termi- nated for one of three reasons: (1) receiving a signal whose default action is to terminate the process, (2) returning from the main routine, or (3) calling the −Ó©Î function. a©Å²ÄÏ®− zÍÎ®Ä©¾l³| ÌÇ©® −Ó©Îf©ÅÎ ÍÎþÎÏÍgy This function does not return The −Ó©Î function terminates the process with an exit status of ÍÎþÎÏÍ. (The other way to set the exit status is to return an integer value from the main routine.) 776 Chapter 8 Exceptional Control Flow A parent process creates a new running child process by calling the ðÇËÂ function. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| É©®ˆÎ ðÇËÂfÌÇ©®gy Returns: 0 to child, PID of child to parent, −1 on error The newly created child process is almost, but not quite, identical to the parent. The child gets an identical (but separate) copy of the parent’s user-level virtual address space, including the code and data segments, heap, shared libraries, and user stack. The child also gets identical copies of any of the parent’s open ﬁle descriptors, which means the child can read and write any ﬁles that were open in the parent when it called ðÇËÂ. The most signiﬁcant difference between the parent and the newly created child is that they have different PIDs. The ðÇËÂ function is interesting (and often confusing) because it is called once but it returns twice: once in the calling process (the parent), and once in the newly created child process. In the parent, ðÇËÂ returns the PID of the child. In the child, ðÇËÂ returns a value of 0. Since the PID of the child is always nonzero, the return value provides an unambiguous way to tell whether the program is executing in the parent or the child. Figure 8.15 shows a simple example of a parent process that uses ðÇËÂ to create a child process. When the ðÇËÂ call returns in line 6, Ó has a value of 1 in both the parent and child. The child increments and prints its copy of Ó in line 8. Similarly, the parent decrements and prints its copy of Ó in line 13. When we run the program on our Unix system, we get the following result: Ä©ÅÏÓ| ./fork ÉþË−ÅÎx Ó{n ²³©Ä® x Ó{p There are some subtle aspects to this simple example. Call once, return twice. The ðÇËÂ function is called once by the parent, but it returns twice: once to the parent and once to the newly created child. This is fairly straightforward for programs that create a single child. But programs with multiple instances of ðÇËÂ can be confusing and need to be reasoned about carefully. Concurrent execution. The parent and the child are separate processes that run concurrently. The instructions in their logical control ﬂows can be interleaved by the kernel in an arbitrary way. When we run the program on our system, the parent process completes its ÉË©ÅÎð statement ﬁrst, followed by the child. However, on another system the reverse might be true. In general, as programmers we can never make assumptions about the interleaving of the instructions in different processes. Section 8.4 Process Control 777 code/ecf/fork.c 1 ©ÅÎ Àþ©Åfg 2 Õ 3 É©®ˆÎ É©®y 4 ©ÅÎÓ{oy 5 6 É©® { ƒÇËÂfgy 7 ©ð fÉ©® {{ ng Õ mh £³©Ä® hm 8 ÉË©ÅÎðf‘²³©Ä® x Ó{c®¿Å‘j iiÓgy 9 −Ó©Îfngy 10 Û 11 12 mh –þË−ÅÎ hm 13 ÉË©ÅÎðf‘ÉþË−ÅÎx Ó{c®¿Å‘j kkÓgy 14 −Ó©Îfngy 15 Û code/ecf/fork.c Figure 8.15 Using ðÇËÂ to create a new process. Duplicate but separate address spaces. If we could halt both the parent and the child immediately after the ðÇËÂ function returned in each process, we would see that the address space of each process is identical. Each process has the same user stack, the same local variable values, the same heap, the same global variable values, and the same code. Thus, in our example program, local variable Ó has a value of 1 in both the parent and the child when the ðÇËÂ function returns in line 6. However, since the parent and the child are separate processes, they each have their own private address spaces. Any subsequent changes that a parent or child makes to Ó are private and are not reﬂected in the memory of the other process. This is why the variable Ó has different values in the parent and child when they call their respective ÉË©ÅÎð statements. Shared ﬁles. When we run the example program, we notice that both parent and child print their output on the screen. The reason is that the child inherits all of the parent’s open ﬁles. When the parent calls ðÇËÂ, the ÍÎ®ÇÏÎ ﬁle is open and directed to the screen. The child inherits this ﬁle, and thus its output is also directed to the screen. When you are ﬁrst learning about the ðÇËÂ function, it is often helpful to sketch the process graph, which is a simple kind of precedence graph that captures the partial ordering of program statements. Each vertex a corresponds to the execution of a program statement. A directed edge a → b denotes that statement a “happens before” statement b. Edges can be labeled with information such as the current value of a variable. Vertices corresponding to ÉË©ÅÎð statements can be labeled with the output of the ÉË©ÅÎð. Each graph begins with a vertex that 778 Chapter 8 Exceptional Control Flow Figure 8.16 Process graph for the example program in Figure 8.15. forkforkmainmain printfprintf exitexit child: x=2 Child Parent x==1 parent: x=0 printfprintf exitexit 1 ©ÅÎ Àþ©Åfg 2 Õ 3 ƒÇËÂfgy 4 ƒÇËÂfgy 5 ÉË©ÅÎðf‘³−ÄÄÇ¿Å‘gy 6 −Ó©Îfngy 7 Û forkforkforkfork printfprintf exitexit hello hello mainmain printfprintf exitexit forkfork printfprintf exitexit hello hello printfprintf exitexit Figure 8.17 Process graph for a nested ðÇËÂ. corresponds to the parent process calling Àþ©Å. This vertex has no inedges and exactly one outedge. The sequence of vertices for each process ends with a vertex corresponding to a call to −Ó©Î. This vertex has one inedge and no outedges. For example, Figure 8.16 shows the process graph for the example program in Figure 8.15. Initially, the parent sets variable Ó to 1. The parent calls ðÇËÂ, which creates a child process that runs concurrently with the parent in its own private address space. For a program running on a single processor, any topological sort of the vertices in the corresponding process graph represents a feasible total ordering of the statements in the program. Here’s a simple way to understand the idea of a topological sort: Given some permutation of the vertices in the process graph, draw the sequence of vertices in a line from left to right, and then draw each of the directed edges. The permutation is a topological sort if and only if each edge in the drawing goes from left to right. Thus, in our example program in Figure 8.15, the ÉË©ÅÎð statements in the parent and child can occur in either order because each of the orderings corresponds to some topological sort of the graph vertices. The process graph can be especially helpful in understanding programs with nested ðÇËÂ calls. For example, Figure 8.17 shows a program with two calls to ðÇËÂ in the source code. The corresponding process graph helps us see that this program runs four processes, each of which makes a call to ÉË©ÅÎð and which can execute in any order. Section 8.4 Process Control 779 Practice Problem 8.2 (solution page 831) Consider the following program: code/ecf/global-forkprob0.c 1 ©ÅÎ Àþ©Åfg 2 Õ 3 ©ÅÎþ{wy 4 5 ©ð fƒÇËÂfg {{ ng 6 ÉË©ÅÎðf‘Éox þ{c®¿Å‘j þkkgy 7 ÉË©ÅÎðf‘Épx þ{c®¿Å‘j þiigy 8 −Ó©Îfngy 9 Û code/ecf/global-forkprob0.c A. What is the output of the child process? B. What is the output of the parent process? 8.4.3 Reaping Child Processes When a process terminates for any reason, the kernel does not remove it from the system immediately. Instead, the process is kept around in a terminated state until it is reaped by its parent. When the parent reaps the terminated child, the kernel passes the child’s exit status to the parent and then discards the terminated process, at which point it ceases to exist. A terminated process that has not yet been reaped is called a zombie. When a parent process terminates, the kernel arranges for the ©Å©Î process to become the adopted parent of any orphaned children. The ©Å©Î process, which has a PID of 1, is created by the kernel during system start-up, never terminates, and is the ancestor of every process. If a parent process terminates without reaping its zombie children, then the kernel arranges for the ©Å©Î process to reap them. However, long-running programs such as shells or servers should always reap their zombie children. Even though zombies are not running, they still consume system memory resources. A process waits for its children to terminate or stop by calling the Ñþ©ÎÉ©® function. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÍÔÍmÑþ©Îl³| É©®ˆÎ Ñþ©ÎÉ©®fÉ©®ˆÎ É©®j ©ÅÎ hÍÎþÎÏÍÉj ©ÅÎ ÇÉÎ©ÇÅÍgy Returns: PID of child if OK, 0 (if WNOHANG), or −1 on error 780 Chapter 8 Exceptional Control Flow Aside Why are terminated children called zombies? In folklore, a zombie is a living corpse, an entity that is half alive and half dead. A zombie process is similar in the sense that although it has already terminated, the kernel maintains some of its state until it can be reaped by the parent. The Ñþ©ÎÉ©® function is complicated. By default (when ÇÉÎ©ÇÅÍ { n), Ñþ©ÎÉ©® suspends execution of the calling process until a child process in its wait set terminates. If a process in the wait set has already terminated at the time of the call, then Ñþ©ÎÉ©® returns immediately. In either case, Ñþ©ÎÉ©® returns the PID of the terminated child that caused Ñþ©ÎÉ©® to return. At this point, the terminated child has been reaped and the kernel removes all traces of it from the system. Determining the Members of the Wait Set The members of the wait set are determined by the É©® argument: . If É©®|n, then the wait set is the singleton child process whose process ID is equal to É©®. . If É©®{ko, then the wait set consists of all of the parent’s child processes. The Ñþ©ÎÉ©® function also supports other kinds of wait sets, involving Unix pro- cess groups, which we will not discuss. Modifying the Default Behavior The default behavior can be modiﬁed by setting ÇÉÎ©ÇÅÍ to various combinations of the WNOHANG, WUNTRACED, and WCONTINUED constants: WNOHANG. Return immediately (with a return value of 0) if none of the child processes in the wait set has terminated yet. The default behavior suspends the calling process until a child terminates; this option is useful in those cases where you want to continue doing useful work while waiting for a child to terminate. WUNTRACED. Suspend execution of the calling process until a process in the wait set becomes either terminated or stopped. Return the PID of the terminated or stopped child that caused the return. The default behavior returns only for terminated children; this option is useful when you want to check for both terminated and stopped children. WCONTINUED. Suspend execution of the calling process until a running process in the wait set is terminated or until a stopped process in the wait set has been resumed by the receipt of a SIGCONT signal. (Signals are explained in Section 8.5.) You can combine options by oring them together. For example: Section 8.4 Process Control 781 . WNOHANG Ú WUNTRACED: Return immediately, with a return value of 0, if none of the children in the wait set has stopped or terminated, or with a return value equal to the PID of one of the stopped or terminated children. Checking the Exit Status of a Reaped Child If the ÍÎþÎÏÍÉ argument is non-NULL, then Ñþ©ÎÉ©® encodes status information about the child that caused the return in ÍÎþÎÏÍ, which is the value pointed to by ÍÎþÎÏÍÉ.The Ñþ©Îl³ include ﬁle deﬁnes several macros for interpreting the ÍÎþÎÏÍ argument: WIFEXITED(ÍÎþÎÏÍ). Returns true if the child terminated normally, via a call to −Ó©Î or a return. WEXITSTATUS(ÍÎþÎÏÍ). Returns the exit status of a normally terminated child. This status is only deﬁned if WIFEXITED() returned true. WIFSIGNALED(ÍÎþÎÏÍ). Returns true if the child process terminated be- cause of a signal that was not caught. WTERMSIG(ÍÎþÎÏÍ). Returns the number of the signal that caused the child process to terminate. This status is only deﬁned if WIFSIGNALED() returned true. WIFSTOPPED(ÍÎþÎÏÍ). Returns true if the child that caused the return is currently stopped. WSTOPSIG(ÍÎþÎÏÍ). Returns the number of the signal that caused the child to stop. This status is only deﬁned if WIFSTOPPED() returned true. WIFCONTINUED(ÍÎþÎÏÍ). Returns true if the child process was restarted by receipt of a SIGCONT signal. Error Conditions If the calling process has no children, then Ñþ©ÎÉ©® returns −1 and sets −ËËÅÇ to ECHILD. If the Ñþ©ÎÉ©® function was interrupted by a signal, then it returns −1 and sets −ËËÅÇ to EINTR. Practice Problem 8.3 (solution page 833) List all of the possible output sequences for the following program: code/ecf/global-waitprob0.c 1 ©ÅÎ Àþ©Åfg 2 Õ 3 ©ð fƒÇËÂfg {{ ng Õ 4 ÉË©ÅÎðf‘w‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 5 Û 6 −ÄÍ− Õ 782 Chapter 8 Exceptional Control Flow 7 ÉË©ÅÎðf‘n‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 8 Ñþ©ÎÉ©®fkoj ﬁ•‹‹j ngy 9 Û 10 ÉË©ÅÎðf‘q‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 11 ÉË©ÅÎðf‘t‘gy −Ó©Îfngy 12 Û code/ecf/global-waitprob0.c The Ñþ©Î Function The Ñþ©Î function is a simpler version of Ñþ©ÎÉ©®. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÍÔÍmÑþ©Îl³| É©®ˆÎ Ñþ©Îf©ÅÎ hÍÎþÎÏÍÉgy Returns: PID of child if OK or −1 on error Calling Ñþ©ÎfdÍÎþÎÏÍg is equivalent to calling Ñþ©ÎÉ©®fkoj dÍÎþÎÏÍj ng. Examples of Using Ñþ©ÎÉ©® Because the Ñþ©ÎÉ©® function is somewhat complicated, it is helpful to look at a few examples. Figure 8.18 shows a program that uses Ñþ©ÎÉ©® to wait, in no particular order, for all of its N children to terminate. In line 11, the parent creates each of the N children, and in line 12, each child exits with a unique exit status. Aside Constants associated with Unix functions Constants such as WNOHANG and WUNTRACED are deﬁned by system header ﬁles. For example, WNOHANG and WUNTRACED are deﬁned (indirectly) by the Ñþ©Îl³ header ﬁle: mh ¢©ÎÍ ©Å Î³− Î³©Ë® þË×ÏÀ−ÅÎ ÎÇ ‘Ñþ©ÎÉ©®’l hm a®−ð©Å− „ﬁﬂ¤¡ﬁ§ o mh ⁄ÇÅ’Î ¾ÄÇ²Â Ñþ©Î©Å×l hm a®−ð©Å− „•ﬁ¶‡¡£¥⁄ p mh ‡−ÉÇËÎ ÍÎþÎÏÍ Çð ÍÎÇÉÉ−® ²³©Ä®Ë−Ål hm In order to use these constants, you must include the Ñþ©Îl³ header ﬁle in your code: a©Å²ÄÏ®− zÍÔÍmÑþ©Îl³| The ÀþÅ page for each Unix function lists the header ﬁles to include whenever you use that function in your code. Also, in order to check return codes such as ECHILD and EINTR, you must include −ËËÅÇl³. To simplify our code examples, we include a single header ﬁle called ²ÍþÉÉl³ that includes the header ﬁles for all of the functions used in the book. The ²ÍþÉÉl³ header ﬁle is available online from the CS:APP Web site. Section 8.4 Process Control 783 code/ecf/waitpid1.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ﬁ p 3 4 ©ÅÎ Àþ©Åfg 5 Õ 6 ©ÅÎ ÍÎþÎÏÍj ©y 7 É©®ˆÎ É©®y 8 9 mh –þË−ÅÎ ²Ë−þÎ−Í ﬁ ²³©Ä®Ë−Å hm 10 ðÇËf©{ny©zﬁy ©iig 11 ©ð ffÉ©® { ƒÇËÂfgg {{ ng mh £³©Ä® hm 12 −Ó©Îfonni©gy 13 14 mh –þË−ÅÎ Ë−þÉÍ ﬁ ²³©Ä®Ë−Å ©Å ÅÇ ÉþËÎ©²ÏÄþË ÇË®−Ë hm 15 Ñ³©Ä− ffÉ©® { Ñþ©ÎÉ©®fkoj dÍÎþÎÏÍj ngg | ng Õ 16 ©ð f„'ƒ¥”'¶¥⁄fÍÎþÎÏÍgg 17 ÉË©ÅÎðf‘²³©Ä® c® Î−ËÀ©ÅþÎ−® ÅÇËÀþÄÄÔ Ñ©Î³ −Ó©Î ÍÎþÎÏÍ{c®¿Å‘j 18 É©®j „¥”'¶·¶¡¶•·fÍÎþÎÏÍggy 19 −ÄÍ− 20 ÉË©ÅÎðf‘²³©Ä® c® Î−ËÀ©ÅþÎ−® þ¾ÅÇËÀþÄÄÔ¿Å‘j É©®gy 21 Û 22 23 mh ¶³− ÇÅÄÔ ÅÇËÀþÄ Î−ËÀ©ÅþÎ©ÇÅ ©Í ©ð Î³−Ë− þË− ÅÇ ÀÇË− ²³©Ä®Ë−Å hm 24 ©ð f−ËËÅÇ _{ ¥£¤'‹⁄g 25 ÏÅ©Óˆ−ËËÇËf‘Ñþ©ÎÉ©® −ËËÇË‘gy 26 27 −Ó©Îfngy 28 Û code/ecf/waitpid1.c Figure 8.18 Using the Ñþ©ÎÉ©® function to reap zombie children in no particular order. Before moving on, make sure you understand why line 12 is executed by each of the children, but not the parent. In line 15, the parent waits for all of its children to terminate by using Ñþ©ÎÉ©® as the test condition of a Ñ³©Ä− loop. Because the ﬁrst argument is −1, the call to Ñþ©ÎÉ©® blocks until an arbitrary child has terminated. As each child terminates, the call to Ñþ©ÎÉ©® returns with the nonzero PID of that child. Line 16 checks the exit status of the child. If the child terminated normally—in this case, by calling the −Ó©Î function—then the parent extracts the exit status and prints it on ÍÎ®ÇÏÎ. When all of the children have been reaped, the next call to Ñþ©ÎÉ©® returns −1 and sets −ËËÅÇ to ECHILD. Line 24 checks that the Ñþ©ÎÉ©® function terminated normally, and prints an error message otherwise. When we run the program on our Linux system, it produces the following output: 784 Chapter 8 Exceptional Control Flow Ä©ÅÏÓ| ./waitpid1 ²³©Ä® ppwtt Î−ËÀ©ÅþÎ−® ÅÇËÀþÄÄÔ Ñ©Î³ −Ó©Î ÍÎþÎÏÍ{onn ²³©Ä® ppwtu Î−ËÀ©ÅþÎ−® ÅÇËÀþÄÄÔ Ñ©Î³ −Ó©Î ÍÎþÎÏÍ{ono Notice that the program reaps its children in no particular order. The order that they were reaped is a property of this speciﬁc computer system. On another system, or even another execution on the same system, the two children might have been reaped in the opposite order. This is an example of the nondeterministic behavior that can make reasoning about concurrency so difﬁcult. Either of the two possible outcomes is equally correct, and as a programmer you may never assume that one outcome will always occur, no matter how unlikely the other outcome appears to be. The only correct assumption is that each possible outcome is equally likely. Figure 8.19 shows a simple change that eliminates this nondeterminism in the output order by reaping the children in the same order that they were created by the parent. In line 11, the parent stores the PIDs of its children in order and then waits for each child in this same order by calling Ñþ©ÎÉ©® with the appropriate PID in the ﬁrst argument. Practice Problem 8.4 (solution page 833) Consider the following program: code/ecf/global-waitprob1.c 1 ©ÅÎ Àþ©Åfg 2 Õ 3 ©ÅÎ ÍÎþÎÏÍy 4 É©®ˆÎ É©®y 5 6 ÉË©ÅÎðf‘·ÎþËÎ¿Å‘gy 7 É©® { ƒÇËÂfgy 8 ÉË©ÅÎðf‘c®¿Å‘j _É©®gy 9 ©ð fÉ©® {{ ng Õ 10 ÉË©ÅÎðf‘£³©Ä®¿Å‘gy 11 Û 12 −ÄÍ− ©ð ffÑþ©ÎÉ©®fkoj dÍÎþÎÏÍj ng | ng dd f„'ƒ¥”'¶¥⁄fÍÎþÎÏÍg _{ ngg Õ 13 ÉË©ÅÎðf‘c®¿Å‘j „¥”'¶·¶¡¶•·fÍÎþÎÏÍggy 14 Û 15 ÉË©ÅÎðf‘·ÎÇÉ¿Å‘gy 16 −Ó©Îfpgy 17 Û code/ecf/global-waitprob1.c A. How many output lines does this program generate? B. What is one possible ordering of these output lines? Section 8.4 Process Control 785 code/ecf/waitpid2.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ﬁ p 3 4 ©ÅÎ Àþ©Åfg 5 Õ 6 ©ÅÎ ÍÎþÎÏÍj ©y 7 É©®ˆÎ É©®‰ﬁ`j Ë−ÎÉ©®y 8 9 mh –þË−ÅÎ ²Ë−þÎ−Í ﬁ ²³©Ä®Ë−Å hm 10 ðÇËf©{ny©zﬁy ©iig 11 ©ð ffÉ©®‰©` { ƒÇËÂfgg {{ ng mh £³©Ä® hm 12 −Ó©Îfonni©gy 13 14 mh –þË−ÅÎ Ë−þÉÍ ﬁ ²³©Ä®Ë−Å ©Å ÇË®−Ë hm 15 ©{ny 16 Ñ³©Ä− ffË−ÎÉ©® { Ñþ©ÎÉ©®fÉ©®‰©ii`j dÍÎþÎÏÍj ngg | ng Õ 17 ©ð f„'ƒ¥”'¶¥⁄fÍÎþÎÏÍgg 18 ÉË©ÅÎðf‘²³©Ä® c® Î−ËÀ©ÅþÎ−® ÅÇËÀþÄÄÔ Ñ©Î³ −Ó©Î ÍÎþÎÏÍ{c®¿Å‘j 19 Ë−ÎÉ©®j „¥”'¶·¶¡¶•·fÍÎþÎÏÍggy 20 −ÄÍ− 21 ÉË©ÅÎðf‘²³©Ä® c® Î−ËÀ©ÅþÎ−® þ¾ÅÇËÀþÄÄÔ¿Å‘j Ë−ÎÉ©®gy 22 Û 23 24 mh ¶³− ÇÅÄÔ ÅÇËÀþÄ Î−ËÀ©ÅþÎ©ÇÅ ©Í ©ð Î³−Ë− þË− ÅÇ ÀÇË− ²³©Ä®Ë−Å hm 25 ©ð f−ËËÅÇ _{ ¥£¤'‹⁄g 26 ÏÅ©Óˆ−ËËÇËf‘Ñþ©ÎÉ©® −ËËÇË‘gy 27 28 −Ó©Îfngy 29 Û code/ecf/waitpid2.c Figure 8.19 Using Ñþ©ÎÉ©® to reap zombie children in the order they were created. 8.4.4 Putting Processes to Sleep The ÍÄ−−É function suspends a process for a speciﬁed period of time. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ÏÅÍ©×Å−® ©ÅÎ ÍÄ−−ÉfÏÅÍ©×Å−® ©ÅÎ Í−²Így Returns: seconds left to sleep ·Ä−−É returns zero if the requested amount of time has elapsed, and the number of seconds still left to sleep otherwise. The latter case is possible if the ÍÄ−−É function 786 Chapter 8 Exceptional Control Flow returns prematurely because it was interrupted by a signal. We will discuss signals in detail in Section 8.5. Another function that we will ﬁnd useful is the ÉþÏÍ− function, which puts the calling function to sleep until a signal is received by the process. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ©ÅÎ ÉþÏÍ−fÌÇ©®gy Always returns −1 Practice Problem 8.5 (solution page 833) Write a wrapper function for ÍÄ−−É, called ÑþÂ−ÏÉ, with the following interface: ÏÅÍ©×Å−® ©ÅÎ ÑþÂ−ÏÉfÏÅÍ©×Å−® ©ÅÎ Í−²Így The ÑþÂ−ÏÉ function behaves exactly as the ÍÄ−−É function, except that it prints a message describing when the process actually woke up: „ÇÂ− ÏÉ þÎ r Í−²Íl 8.4.5 Loading and Running Programs The −Ó−²Ì− function loads and runs a new program in the context of the current process. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ©ÅÎ −Ó−²Ì−f²ÇÅÍÎ ²³þË hð©Ä−ÅþÀ−j ²ÇÅÍÎ ²³þË hþË×Ì‰`j ²ÇÅÍÎ ²³þË h−ÅÌÉ‰`gy Does not return if OK; returns −1 on error The −Ó−²Ì− function loads and runs the executable object ﬁle ð©Ä−ÅþÀ− with the argument list þË×Ì and the environment variable list −ÅÌÉ. ¥Ó−²Ì− returns to the calling program only if there is an error, such as not being able to ﬁnd ð©Ä−ÅþÀ−. So unlike ðÇËÂ, which is called once but returns twice, −Ó−²Ì− is called once and never returns. The argument list is represented by the data structure shown in Figure 8.20. The þË×Ì variable points to a null-terminated array of pointers, each of which points to an argument string. By convention, þË×Ì‰n` is the name of the executable object ﬁle. The list of environment variables is represented by a similar data structure, shown in Figure 8.21. The −ÅÌÉ variable points to a null-terminated array of pointers to environment variable strings, each of which is a name-value pair of the form name{value. Section 8.4 Process Control 787 Figure 8.20 Organization of an argument list.… argv[]argv[] argv[0] \"ls\" \"-lt\" \"/user/include\" argv argv[1] argv[argc \u0002 1] NULL Figure 8.21 Organization of an environment variable list.… envp[]envp[] envp[0] \"PWD\u0005/usr/droh\" \"PRINTER\u0005iron\" \"USER\u0005droh\" envp envp[1] envp[n \u0002 1] NULL After −Ó−²Ì− loads ð©Ä−ÅþÀ−, it calls the start-up code described in Sec- tion 7.9. The start-up code sets up the stack and passes control to the main routine of the new program, which has a prototype of the form ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìj ²³þË hh−ÅÌÉgy or equivalently, ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hþË×Ì‰`j ²³þË h−ÅÌÉ‰`gy When Àþ©Å begins executing, the user stack has the organization shown in Fig- ure 8.22. Let’s work our way from the bottom of the stack (the highest address) to the top (the lowest address). First are the argument and environment strings. These are followed further up the stack by a null-terminated array of pointers, each of which points to an environment variable string on the stack. The global variable −ÅÌ©ËÇÅ points to the ﬁrst of these pointers, −ÅÌÉ‰n`. The environment array is followed by the null-terminated þË×Ì‰` array, with each element pointing to an argument string on the stack. At the top of the stack is the stack frame for the system start-up function, Ä©¾²ˆÍÎþËÎˆÀþ©Å (Section 7.9). There are three arguments to function Àþ©Å, each stored in a register accord- ing to the x86-64 stack discipline: (1) þË×², which gives the number of non-null pointers in the þË×Ì‰` array; (2) þË×Ì, which points to the ﬁrst entry in the þË×Ì‰` array; and (3) −ÅÌÉ, which points to the ﬁrst entry in the −ÅÌÉ‰` array. Linux provides several functions for manipulating the environment array: a©Å²ÄÏ®− zÍÎ®Ä©¾l³| ²³þË h×−Î−ÅÌf²ÇÅÍÎ ²³þË hÅþÀ−gy Returns: pointer to ÅþÀ− if it exists, NULL if no match 788 Chapter 8 Exceptional Control Flow Figure 8.22 Typical organization of the user stack when a new program starts. Bottom of stack Top of stack Null-terminated environment variable strings Null-terminated command-line arg strings Stack frame for libc_start_main Future stack frame for main envp[n] == NULL envp[n-1] … … envp[0] argv[argc] = NULL argv[argc-1] argv[0] environ (global var) envp (in %rdx) argv (in %rsi) argc (in %rdi) The ×−Î−ÅÌ function searches the environment array for a string ÅþÀ−{value.If found, it returns a pointer to value; otherwise, it returns ﬁ•‹‹. a©Å²ÄÏ®− zÍÎ®Ä©¾l³| ©ÅÎ Í−Î−ÅÌf²ÇÅÍÎ ²³þË hÅþÀ−j ²ÇÅÍÎ ²³þË hÅ−ÑÌþÄÏ−j ©ÅÎ ÇÌ−ËÑË©Î−gy Returns: 0 on success, −1 on error ÌÇ©® ÏÅÍ−Î−ÅÌf²ÇÅÍÎ ²³þË hÅþÀ−gy Returns: nothing If the environment array contains a string of the form ÅþÀ−{oldvalue, then ÏÅÍ−Î−ÅÌ deletes it and Í−Î−ÅÌ replaces oldvalue with Å−ÑÌþÄÏ−, but only if ÇÌ−ËÑË©Î− is nonzero. If ÅþÀ− does not exist, then Í−Î−ÅÌ adds ÅþÀ−{Å−ÑÌþÄÏ− to the array. Practice Problem 8.6 (solution page 833) Write a program called ÀÔ−²³Ç that prints its command-line arguments and envi- ronment variables. For example: Ä©ÅÏÓ| ./myecho arg1 arg2 £ÇÀÀþÅ®k©Å− þË×ÏÀ−ÅÎÍx þË×Ì‰ n`x ÀÔ−²³Ç þË×Ì‰ o`x þË×o þË×Ì‰ p`x þË×p Section 8.4 Process Control 789 ¥ÅÌ©ËÇÅÀ−ÅÎ ÌþË©þ¾Ä−Íx −ÅÌÉ‰ n`x –„⁄{mÏÍËnm®ËÇ³m©²Ím²Ç®−m−²ð −ÅÌÉ‰ o`x ¶¥‡›{−Àþ²Í l l l −ÅÌÉ‰ps`x •·¥‡{®ËÇ³ −ÅÌÉ‰pt`x ·¤¥‹‹{mÏÍËmÄÇ²þÄm¾©ÅmÎ²Í³ −ÅÌÉ‰pu`x ¤ﬂ›¥{mÏÍËnm®ËÇ³ 8.4.6 Using ðÇËÂ and −Ó−²Ì− to Run Programs Programs such as Unix shells and Web servers make heavy use of the ðÇËÂ and −Ó−²Ì− functions. A shell is an interactive application-level program that runs other programs on behalf of the user. The original shell was the Í³ program, which was followed by variants such as ²Í³, Î²Í³, ÂÍ³, and ¾þÍ³. A shell performs a sequence of read/evaluate steps and then terminates. The read step reads a command line from the user. The evaluate step parses the command line and runs programs on behalf of the user. Figure 8.23 shows the main routine of a simple shell. The shell prints a command-line prompt, waits for the user to type a command line on ÍÎ®©Å, and then evaluates the command line. Figure 8.24 shows the code that evaluates the command line. Its ﬁrst task is to call the ÉþËÍ−Ä©Å− function (Figure 8.25), which parses the space-separated command-line arguments and builds the þË×Ì vector that will eventually be passed to −Ó−²Ì−. The ﬁrst argument is assumed to be either the name of a built-in shell command that is interpreted immediately, or an executable object ﬁle that will be loaded and run in the context of a new child process. If the last argument is an ‘&’ character, then ÉþËÍ−Ä©Å− returns 1, indicating that the program should be executed in the background (the shell does not wait for it to complete). Otherwise, it returns 0, indicating that the program should be run in the foreground (the shell waits for it to complete). Aside Programs versus processes This is a good place to pause and make sure you understand the distinction between a program and a process. A program is a collection of code and data; programs can exist as object ﬁles on disk or as segments in an address space. A process is a speciﬁc instance of a program in execution; a program always runs in the context of some process. Understanding this distinction is important if you want to understand the ðÇËÂ and −Ó−²Ì− functions. The ðÇËÂ function runs the same program in a new child process that is a duplicate of the parent. The −Ó−²Ì− function loads and runs a new program in the context of the current process. While it overwrites the address space of the current process, it does not create a new process. The new program still has the same PID, and it inherits all of the ﬁle descriptors that were open at the time of the call to the −Ó−²Ì− function. 790 Chapter 8 Exceptional Control Flow code/ecf/shellex.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ›¡”¡‡§· opv 3 4 mh ƒÏÅ²Î©ÇÅ ÉËÇÎÇÎÔÉ−Í hm 5 ÌÇ©® −ÌþÄf²³þË h²À®Ä©Å−gy 6 ©ÅÎ ÉþËÍ−Ä©Å−f²³þË h¾Ïðj ²³þË hhþË×Ìgy 7 ©ÅÎ ¾Ï©ÄÎ©Åˆ²ÇÀÀþÅ®f²³þË hhþË×Ìgy 8 9 ©ÅÎ Àþ©Åfg 10 Õ 11 ²³þË ²À®Ä©Å−‰›¡”‹'ﬁ¥`y mh £ÇÀÀþÅ® Ä©Å− hm 12 13 Ñ³©Ä− fog Õ 14 mh ‡−þ® hm 15 ÉË©ÅÎðf‘| ‘gy 16 ƒ×−ÎÍf²À®Ä©Å−j ›¡”‹'ﬁ¥j ÍÎ®©Ågy 17 ©ð fð−ÇðfÍÎ®©Ågg 18 −Ó©Îfngy 19 20 mh ¥ÌþÄÏþÎ− hm 21 −ÌþÄf²À®Ä©Å−gy 22 Û 23 Û code/ecf/shellex.c Figure 8.23 The main routine for a simple shell program. After parsing the command line, the −ÌþÄ function calls the ¾Ï©ÄÎ©Åˆ²ÇÀÀþÅ® function, which checks whether the ﬁrst command-line argument is a built-in shell command. If so, it interprets the command immediately and returns 1. Otherwise, it returns 0. Our simple shell has just one built-in command, the ÊÏ©Î command, which terminates the shell. Real shells have numerous commands, such as ÉÑ®, ÁÇ¾Í, and ð×. If ¾Ï©ÄÎ©Åˆ²ÇÀÀþÅ® returns 0, then the shell creates a child process and executes the requested program inside the child. If the user has asked for the program to run in the background, then the shell returns to the top of the loop and waits for the next command line. Otherwise the shell uses the Ñþ©ÎÉ©® function to wait for the job to terminate. When the job terminates, the shell goes on to the next iteration. Notice that this simple shell is ﬂawed because it does not reap any of its background children. Correcting this ﬂaw requires the use of signals, which we describe in the next section. Section 8.4 Process Control 791 code/ecf/shellex.c 1 mh −ÌþÄ k ¥ÌþÄÏþÎ− þ ²ÇÀÀþÅ® Ä©Å− hm 2 ÌÇ©® −ÌþÄf²³þË h²À®Ä©Å−g 3 Õ 4 ²³þË hþË×Ì‰›¡”¡‡§·`y mh ¡Ë×ÏÀ−ÅÎ Ä©ÍÎ −Ó−²Ì−fg hm 5 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y mh ¤ÇÄ®Í ÀÇ®©ð©−® ²ÇÀÀþÅ® Ä©Å− hm 6 ©ÅÎ ¾×y mh ·³ÇÏÄ® Î³− ÁÇ¾ ËÏÅ ©Å ¾× ÇË ð×} hm 7 É©®ˆÎ É©®y mh –ËÇ²−ÍÍ ©® hm 8 9 ÍÎË²ÉÔf¾Ïðj ²À®Ä©Å−gy 10 ¾× { ÉþËÍ−Ä©Å−f¾Ïðj þË×Ìgy 11 ©ð fþË×Ì‰n` {{ ﬁ•‹‹g 12 Ë−ÎÏËÅy mh '×ÅÇË− −ÀÉÎÔ Ä©Å−Í hm 13 14 ©ð f_¾Ï©ÄÎ©Åˆ²ÇÀÀþÅ®fþË×Ìgg Õ 15 ©ð ffÉ©® { ƒÇËÂfgg {{ ng Õ mh £³©Ä® ËÏÅÍ ÏÍ−Ë ÁÇ¾ hm 16 ©ð f−Ó−²Ì−fþË×Ì‰n`j þË×Ìj −ÅÌ©ËÇÅg z ng Õ 17 ÉË©ÅÎðf‘cÍx £ÇÀÀþÅ® ÅÇÎ ðÇÏÅ®l¿Å‘j þË×Ì‰n`gy 18 −Ó©Îfngy 19 Û 20 Û 21 22 mh –þË−ÅÎ Ñþ©ÎÍ ðÇË ðÇË−×ËÇÏÅ® ÁÇ¾ ÎÇ Î−ËÀ©ÅþÎ− hm 23 ©ð f_¾×g Õ 24 ©ÅÎ ÍÎþÎÏÍy 25 ©ð fÑþ©ÎÉ©®fÉ©®j dÍÎþÎÏÍj ng z ng 26 ÏÅ©Óˆ−ËËÇËf‘Ñþ©Îð×x Ñþ©ÎÉ©® −ËËÇË‘gy 27 Û 28 −ÄÍ− 29 ÉË©ÅÎðf‘c® cÍ‘j É©®j ²À®Ä©Å−gy 30 Û 31 Ë−ÎÏËÅy 32 Û 33 34 mh 'ð ð©ËÍÎ þË× ©Í þ ¾Ï©ÄÎ©Å ²ÇÀÀþÅ®j ËÏÅ ©Î þÅ® Ë−ÎÏËÅ ÎËÏ− hm 35 ©ÅÎ ¾Ï©ÄÎ©Åˆ²ÇÀÀþÅ®f²³þË hhþË×Ìg 36 Õ 37 ©ð f_ÍÎË²ÀÉfþË×Ì‰n`j ‘ÊÏ©Î‘gg mh ÊÏ©Î ²ÇÀÀþÅ® hm 38 −Ó©Îfngy 39 ©ð f_ÍÎË²ÀÉfþË×Ì‰n`j ‘d‘gg mh '×ÅÇË− Í©Å×Ä−ÎÇÅ d hm 40 Ë−ÎÏËÅ oy 41 Ë−ÎÏËÅ ny mh ﬁÇÎ þ ¾Ï©ÄÎ©Å ²ÇÀÀþÅ® hm 42 Û code/ecf/shellex.c Figure 8.24 −ÌþÄ evaluates the shell command line. 792 Chapter 8 Exceptional Control Flow code/ecf/shellex.c 1 mh ÉþËÍ−Ä©Å− k –þËÍ− Î³− ²ÇÀÀþÅ® Ä©Å− þÅ® ¾Ï©Ä® Î³− þË×Ì þËËþÔ hm 2 ©ÅÎ ÉþËÍ−Ä©Å−f²³þË h¾Ïðj ²³þË hhþË×Ìg 3 Õ 4 ²³þË h®−Ä©Ày mh –Ç©ÅÎÍ ÎÇ ð©ËÍÎ ÍÉþ²− ®−Ä©À©Î−Ë hm 5 ©ÅÎ þË×²y mh ﬁÏÀ¾−Ë Çð þË×Í hm 6 ©ÅÎ ¾×y mh ¢þ²Â×ËÇÏÅ® ÁÇ¾} hm 7 8 ¾Ïð‰ÍÎËÄ−Åf¾Ïðgko`{’’y mh ‡−ÉÄþ²− ÎËþ©Ä©Å× ’¿Å’ Ñ©Î³ ÍÉþ²− hm 9 Ñ³©Ä− fh¾Ïð dd fh¾Ïð {{ ’ ’gg mh '×ÅÇË− Ä−þ®©Å× ÍÉþ²−Í hm 10 ¾Ïðiiy 11 12 mh ¢Ï©Ä® Î³− þË×Ì Ä©ÍÎ hm 13 þË×² { ny 14 Ñ³©Ä− ff®−Ä©À { ÍÎË²³Ëf¾Ïðj ’ ’ggg Õ 15 þË×Ì‰þË×²ii` { ¾Ïðy 16 h®−Ä©À { ’¿n’y 17 ¾Ïð { ®−Ä©À i oy 18 Ñ³©Ä− fh¾Ïð dd fh¾Ïð {{ ’ ’gg mh '×ÅÇË− ÍÉþ²−Í hm 19 ¾Ïðiiy 20 Û 21 þË×Ì‰þË×²` { ﬁ•‹‹y 22 23 ©ð fþË×² {{ ng mh '×ÅÇË− ¾ÄþÅÂ Ä©Å− hm 24 Ë−ÎÏËÅ oy 25 26 mh ·³ÇÏÄ® Î³− ÁÇ¾ ËÏÅ ©Å Î³− ¾þ²Â×ËÇÏÅ®} hm 27 ©ð ff¾× { fhþË×Ì‰þË×²ko` {{ ’d’gg _{ ng 28 þË×Ì‰kkþË×²` { ﬁ•‹‹y 29 30 Ë−ÎÏËÅ ¾×y 31 Û code/ecf/shellex.c Figure 8.25 ÉþËÍ−Ä©Å− parses a line of input for the shell. 8.5 Signals To this point in our study of exceptional control ﬂow, we have seen how hardware and software cooperate to provide the fundamental low-level exception mecha- nism. We have also seen how the operating system uses exceptions to support a form of exceptional control ﬂow known as the process context switch. In this sec- tion, we will study a higher-level software form of exceptional control ﬂow, known as a Linux signal, that allows processes and the kernel to interrupt other processes. Section 8.5 Signals 793 Number Name Default action Corresponding event 1 SIGHUP Terminate Terminal line hangup 2 SIGINT Terminate Interrupt from keyboard 3 SIGQUIT Terminate Quit from keyboard 4 SIGILL Terminate Illegal instruction 5 SIGTRAP Terminate and dump core a Trace trap 6 SIGABRT Terminate and dump core a Abort signal from þ¾ÇËÎ function 7 SIGBUS Terminate Bus error 8 SIGFPE Terminate and dump core a Floating-point exception 9 SIGKILL Terminate b Kill program 10 SIGUSR1 Terminate User-deﬁned signal 1 11 SIGSEGV Terminate and dump core a Invalid memory reference (seg fault) 12 SIGUSR2 Terminate User-deﬁned signal 2 13 SIGPIPE Terminate Wrote to a pipe with no reader 14 SIGALRM Terminate Timer signal from þÄþËÀ function 15 SIGTERM Terminate Software termination signal 16 SIGSTKFLT Terminate Stack fault on coprocessor 17 SIGCHLD Ignore A child process has stopped or terminated 18 SIGCONT Ignore Continue process if stopped 19 SIGSTOP Stop until next SIGCONT b Stop signal not from terminal 20 SIGTSTP Stop until next SIGCONT Stop signal from terminal 21 SIGTTIN Stop until next SIGCONT Background process read from terminal 22 SIGTTOU Stop until next SIGCONT Background process wrote to terminal 23 SIGURG Ignore Urgent condition on socket 24 SIGXCPU Terminate CPU time limit exceeded 25 SIGXFSZ Terminate File size limit exceeded 26 SIGVTALRM Terminate Virtual timer expired 27 SIGPROF Terminate Proﬁling timer expired 28 SIGWINCH Ignore Window size changed 29 SIGIO Terminate I/O now possible on a descriptor 30 SIGPWR Terminate Power failure Figure 8.26 Linux signals. Notes: (a) Years ago, main memory was implemented with a technology known as core memory. “Dumping core” is a historical term that means writing an image of the code and data memory segments to disk. (b) This signal can be neither caught nor ignored. (Source: ÀþÅ u Í©×ÅþÄ. Data from the Linux Foundation.) A signal is a small message that notiﬁes a process that an event of some type has occurred in the system. Figure 8.26 shows the 30 different types of signals that are supported on Linux systems. Each signal type corresponds to some kind of system event. Low-level hard- ware exceptions are processed by the kernel’s exception handlers and would not normally be visible to user processes. Signals provide a mechanism for exposing 794 Chapter 8 Exceptional Control Flow the occurrence of such exceptions to user processes. For example, if a process at- tempts to divide by zero, then the kernel sends it a SIGFPE signal (number 8). If a process executes an illegal instruction, the kernel sends it a SIGILL signal (number 4). If a process makes an illegal memory reference, the kernel sends it a SIGSEGV signal (number 11). Other signals correspond to higher-level software events in the kernel or in other user processes. For example, if you type Ctrl+C (i.e., press the Ctrl key and the ‘c’ key at the same time) while a process is running in the foreground, then the kernel sends a SIGINT (number 2) to each process in the foreground process group. A process can forcibly terminate another process by sending it a SIGKILL signal (number 9). When a child process terminates or stops, the kernel sends a SIGCHLD signal (number 17) to the parent. 8.5.1 Signal Terminology The transfer of a signal to a destination process occurs in two distinct steps: Sending a signal. The kernel sends (delivers) a signal to a destination process by updating some state in the context of the destination process. The signal is delivered for one of two reasons: (1) The kernel has detected a system event such as a divide-by-zero error or the termination of a child process. (2) A process has invoked the Â©ÄÄ function (discussed in the next section) to explicitly request the kernel to send a signal to the destination process. A process can send a signal to itself. Receiving a signal. A destination process receives a signal when it is forced by the kernel to react in some way to the delivery of the signal. The process can either ignore the signal, terminate, or catch the signal by executing a user-level function called a signal handler. Figure 8.27 shows the basic idea of a handler catching a signal. A signal that has been sent but not yet received is called a pending signal.At any point in time, there can be at most one pending signal of a particular type. If a process has a pending signal of type k, then any subsequent signals of type k sent to that process are not queued; they are simply discarded. A process can selectively block the receipt of certain signals. When a signal is blocked, it can be Figure 8.27 Signal handling. Receipt of a signal triggers a control transfer to a signal handler. After it ﬁnishes processing, the handler returns control to the interrupted program. (2) Control passes to signal handler (3) Signal handler runs (4) Signal handler returns to next instruction (1) Signal received by process Icurr Inext Section 8.5 Signals 795 delivered, but the resulting pending signal will not be received until the process unblocks the signal. A pending signal is received at most once. For each process, the kernel main- tains the set of pending signals in the É−Å®©Å× bit vector, and the set of blocked signals in the ¾ÄÇ²Â−® bit vector.1 The kernel sets bit k in É−Å®©Å× whenever a signal of type k is delivered and clears bit k in É−Å®©Å× whenever a signal of type k is received. 8.5.2 Sending Signals Unix systems provide a number of mechanisms for sending signals to processes. All of the mechanisms rely on the notion of a process group. Process Groups Every process belongs to exactly one process group, which is identiﬁed by a positive integer process group ID.The ×−ÎÉ×ËÉ function returns the process group ID of the current process. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| É©®ˆÎ ×−ÎÉ×ËÉfÌÇ©®gy Returns: process group ID of calling process By default, a child process belongs to the same process group as its parent. A process can change the process group of itself or another process by using the Í−ÎÉ×©® function: a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ©ÅÎ Í−ÎÉ×©®fÉ©®ˆÎ É©®j É©®ˆÎ É×©®gy Returns: 0 on success, −1 on error The Í−ÎÉ×©® function changes the process group of process É©® to É×©®.If É©® is zero, the PID of the current process is used. If É×©® is zero, the PID of the process speciﬁed by É©® is used for the process group ID. For example, if process 15213 is the calling process, then Í−ÎÉ×©®fnj ngy creates a new process group whose process group ID is 15213, and adds process 15213 to this new group. 1. Also known as the signal mask. 796 Chapter 8 Exceptional Control Flow Sending Signals with the m¾©ÅmÂ©ÄÄ Program The m¾©ÅmÂ©ÄÄ program sends an arbitrary signal to another process. For example, the command Ä©ÅÏÓ| /bin/kill -9 15213 sends signal 9 (SIGKILL) to process 15213. A negative PID causes the signal to be sent to every process in process group PID. For example, the command Ä©ÅÏÓ| /bin/kill -9 -15213 sends a SIGKILL signal to every process in process group 15213. Note that we use the complete path m¾©ÅmÂ©ÄÄ here because some Unix shells have their own built-in Â©ÄÄ command. Sending Signals from the Keyboard Unix shells use the abstraction of a job to represent the processes that are created as a result of evaluating a single command line. At any point in time, there is at most one foreground job and zero or more background jobs. For example, typing Ä©ÅÏÓ| ls | sort creates a foreground job consisting of two processes connected by a Unix pipe: one running the ÄÍ program, the other running the ÍÇËÎ program. The shell creates a separate process group for each job. Typically, the process group ID is taken from one of the parent processes in the job. For example, Figure 8.28 shows a shell with one foreground job and two background jobs. The parent process in the foreground job has a PID of 20 and a process group ID of 20. The parent process has created two children, each of which are also members of process group 20. Figure 8.28 Foreground and background process groups. Back- ground job #1 Fore- ground job Background process group 32 Foreground process group 20 Shell ChildChild Back- ground job #2 Background process group 40 pid\u000520 pgid\u000520 pid\u000510 pgid\u000510 pid\u000521 pgid\u000520 pid\u000522 pgid\u000520 pid\u000532 pgid\u000532 pid\u000540 pgid\u000540 Section 8.5 Signals 797 Typing Ctrl+C at the keyboard causes the kernel to send a SIGINT signal to every process in the foreground process group. In the default case, the result is to terminate the foreground job. Similarly, typing Ctrl+Z causes the kernel to send a SIGTSTP signal to every process in the foreground process group. In the default case, the result is to stop (suspend) the foreground job. Sending Signals with the Â©ÄÄ Function Processes send signals to other processes (including themselves) by calling the Â©ÄÄ function. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÍ©×ÅþÄl³| ©ÅÎ Â©ÄÄfÉ©®ˆÎ É©®j ©ÅÎ Í©×gy Returns: 0 if OK, −1 on error If É©® is greater than zero, then the Â©ÄÄ function sends signal number Í©× to process É©®.If É©® is equal to zero, then Â©ÄÄ sends signal Í©× to every process in the process group of the calling process, including the calling process itself. If É©® is less than zero, then Â©ÄÄ sends signal Í©× to every process in process group |É©®| (the absolute value of É©®). Figure 8.29 shows an example of a parent that uses the Â©ÄÄ function to send a SIGKILL signal to its child. code/ecf/kill.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 É©®ˆÎ É©®y 6 7 mh £³©Ä® ÍÄ−−ÉÍ ÏÅÎ©Ä ·'§«'‹‹ Í©×ÅþÄ Ë−²−©Ì−®j Î³−Å ®©−Í hm 8 ©ð ffÉ©® { ƒÇËÂfgg {{ ng Õ 9 –þÏÍ−fgy mh „þ©Î ðÇË þ Í©×ÅþÄ ÎÇ þËË©Ì− hm 10 ÉË©ÅÎðf‘²ÇÅÎËÇÄ Í³ÇÏÄ® Å−Ì−Ë Ë−þ²³ ³−Ë−_¿Å‘gy 11 −Ó©Îfngy 12 Û 13 14 mh –þË−ÅÎ Í−Å®Í þ ·'§«'‹‹ Í©×ÅþÄ ÎÇ þ ²³©Ä® hm 15 «©ÄÄfÉ©®j ·'§«'‹‹gy 16 −Ó©Îfngy 17 Û code/ecf/kill.c Figure 8.29 Using the Â©ÄÄ function to send a signal to a child. 798 Chapter 8 Exceptional Control Flow Sending Signals with the þÄþËÀ Function A process can send SIGALRM signals to itself by calling the þÄþËÀ function. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ÏÅÍ©×Å−® ©ÅÎ þÄþËÀfÏÅÍ©×Å−® ©ÅÎ Í−²Így Returns: remaining seconds of previous alarm, or 0 if no previous alarm The þÄþËÀ function arranges for the kernel to send a SIGALRM signal to the calling process in Í−²Í seconds. If Í−²Í is 0, then no new alarm is scheduled. In any event, the call to þÄþËÀ cancels any pending alarms and returns the number of seconds remaining until any pending alarm was due to be delivered (had not this call to þÄþËÀ canceled it), or 0 if there were no pending alarms. 8.5.3 Receiving Signals When the kernel switches a process p from kernel mode to user mode (e.g., returning from a system call or completing a context switch), it checks the set of unblocked pending signals (É−Å®©Å× d Ü¾ÄÇ²Â−®) for p. If this set is empty (the usual case), then the kernel passes control to the next instruction (Inext)inthe logical control ﬂow of p. However, if the set is nonempty, then the kernel chooses some signal k in the set (typically the smallest k) and forces p to receive signal k. The receipt of the signal triggers some action by the process. Once the process completes the action, then control passes back to the next instruction (Inext)inthe logical control ﬂow of p. Each signal type has a predeﬁned default action, which is one of the following: . The process terminates. . The process terminates and dumps core. . The process stops (suspends) until restarted by a SIGCONT signal. . The process ignores the signal. Figure 8.26 shows the default actions associated with each type of signal. For example, the default action for the receipt of a SIGKILL is to terminate the receiving process. On the other hand, the default action for the receipt of a SIGCHLD is to ignore the signal. A process can modify the default action associated with a signal by using the Í©×ÅþÄ function. The only exceptions are SIGSTOP and SIGKILL, whose default actions cannot be changed. a©Å²ÄÏ®− zÍ©×ÅþÄl³| ÎÔÉ−®−ð ÌÇ©® fhÍ©×³þÅ®Ä−ËˆÎgf©ÅÎgy Í©×³þÅ®Ä−ËˆÎ Í©×ÅþÄf©ÅÎ Í©×ÅÏÀj Í©×³þÅ®Ä−ËˆÎ ³þÅ®Ä−Ëgy Returns: pointer to previous handler if OK, SIG_ERR on error (does not set −ËËÅÇ) Section 8.5 Signals 799 The Í©×ÅþÄ function can change the action associated with a signal Í©×ÅÏÀ in one of three ways: . If ³þÅ®Ä−Ë is SIG_IGN, then signals of type Í©×ÅÏÀ are ignored. . If ³þÅ®Ä−Ë is SIG_DFL, then the action for signals of type Í©×ÅÏÀ reverts to the default action. . Otherwise, ³þÅ®Ä−Ë is the address of a user-deﬁned function, called a signal handler, that will be called whenever the process receives a signal of type Í©×ÅÏÀ. Changing the default action by passing the address of a handler to the Í©×ÅþÄ function is known as installing the handler. The invocation of the handler is called catching the signal. The execution of the handler is referred to as handling the signal. When a process catches a signal of type k, the handler installed for signal k is invoked with a single integer argument set to k. This argument allows the same handler function to catch different types of signals. When the handler executes its Ë−ÎÏËÅ statement, control (usually) passes back to the instruction in the control ﬂow where the process was interrupted by the receipt of the signal. We say “usually” because in some systems, interrupted system calls return immediately with an error. Figure 8.30 shows a program that catches the SIGINT signal that is sent whenever the user types Ctrl+C at the keyboard. The default action for SIGINT code/ecf/sigint.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® Í©×©ÅÎˆ³þÅ®Ä−Ëf©ÅÎ Í©×g mh ·'§'ﬁ¶ ³þÅ®Ä−Ë hm 4 Õ 5 ÉË©ÅÎðf‘£þÏ×³Î ·'§'ﬁ¶_¿Å‘gy 6 −Ó©Îfngy 7 Û 8 9 ©ÅÎ Àþ©Åfg 10 Õ 11 mh 'ÅÍÎþÄÄ Î³− ·'§'ﬁ¶ ³þÅ®Ä−Ë hm 12 ©ð fÍ©×ÅþÄf·'§'ﬁ¶j Í©×©ÅÎˆ³þÅ®Ä−Ëg {{ ·'§ˆ¥‡‡g 13 ÏÅ©Óˆ−ËËÇËf‘Í©×ÅþÄ −ËËÇË‘gy 14 15 ÉþÏÍ−fgy mh „þ©Î ðÇË Î³− Ë−²−©ÉÎ Çð þ Í©×ÅþÄ hm 16 17 Ë−ÎÏËÅ ny 18 Û code/ecf/sigint.c Figure 8.30 A program that uses a signal handler to catch a SIGINT signal. 800 Chapter 8 Exceptional Control Flow (4) Control passes to handler T (1) Program catches signal s (3) Program catches signal t (7) Main program resumes (5) Handler T returns to handler S (2) Control passes to handler S Main program Handler S Handler T (6) Handler S returns to main program Icurr Inext Figure 8.31 Handlers can be interrupted by other handlers. is to immediately terminate the process. In this example, we modify the default behavior to catch the signal, print a message, and then terminate the process. Signal handlers can be interrupted by other handlers, as shown in Figure 8.31. In this example, the main program catches signal s, which interrupts the main program and transfers control to handler S. While S is running, the program catches signal t ̸= s, which interrupts S and transfers control to handler T . When T returns, S resumes where it was interrupted. Eventually, S returns, transferring control back to the main program, which resumes where it left off. Practice Problem 8.7 (solution page 834) Write a program called ÍÅÇÇÖ− that takes a single command-line argument, calls the ÍÅÇÇÖ− function from Problem 8.5 with this argument, and then terminates. Write your program so that the user can interrupt the ÍÅÇÇÖ− function by typing Ctrl+C at the keyboard. For example: Ä©ÅÏÓ| ./snooze 5 CTRL+C User hits Crtl+C after 3 seconds ·Ä−ÉÎ ðÇË q Çð s Í−²Íl Ä©ÅÏÓ| 8.5.4 Blocking and Unblocking Signals Linux provides implicit and explicit mechanisms for blocking signals: Implicit blocking mechanism. By default, the kernel blocks any pending sig- nals of the type currently being processed by a handler. For example, in Figure 8.31, suppose the program has caught signal s and is currently run- ning handler S. If another signal s is sent to the process, then s will become pending but will not be received until after handler S returns. Explicit blocking mechanism. Applications can explicitly block and unblock selected signals using the Í©×ÉËÇ²ÀþÍÂ function and its helpers. Section 8.5 Signals 801 a©Å²ÄÏ®− zÍ©×ÅþÄl³| ©ÅÎ Í©×ÉËÇ²ÀþÍÂf©ÅÎ ³ÇÑj ²ÇÅÍÎ Í©×Í−ÎˆÎ hÍ−Îj Í©×Í−ÎˆÎ hÇÄ®Í−Îgy ©ÅÎ Í©×−ÀÉÎÔÍ−ÎfÍ©×Í−ÎˆÎ hÍ−Îgy ©ÅÎ Í©×ð©ÄÄÍ−ÎfÍ©×Í−ÎˆÎ hÍ−Îgy ©ÅÎ Í©×þ®®Í−ÎfÍ©×Í−ÎˆÎ hÍ−Îj ©ÅÎ Í©×ÅÏÀgy ©ÅÎ Í©×®−ÄÍ−ÎfÍ©×Í−ÎˆÎ hÍ−Îj ©ÅÎ Í©×ÅÏÀgy Returns: 0 if OK, −1 on error ©ÅÎ Í©×©ÍÀ−À¾−Ëf²ÇÅÍÎ Í©×Í−ÎˆÎ hÍ−Îj ©ÅÎ Í©×ÅÏÀgy Returns: 1 if member, 0 if not, −1 on error The Í©×ÉËÇ²ÀþÍÂ function changes the set of currently blocked signals (the ¾ÄÇ²Â−® bit vector described in Section 8.5.1). The speciﬁc behavior depends on the value of ³ÇÑ: SIG_BLOCK. Add the signals in Í−Î to ¾ÄÇ²Â−® (¾ÄÇ²Â−® { ¾ÄÇ²Â−® Ú Í−Î). SIG_UNBLOCK. Remove the signals in Í−Î from ¾ÄÇ²Â−® (¾ÄÇ²Â−® { ¾ÄÇ²Â−® d ÜÍ−Î). SIG_SETMASK. ¾ÄÇ²Â−® { Í−Î. If ÇÄ®Í−Î is non-NULL, the previous value of the ¾ÄÇ²Â−® bit vector is stored in ÇÄ®Í−Î. Signal sets such as Í−Î are manipulated using the following functions: The Í©×−ÀÉÎÔÍ−Î initializes Í−Î to the empty set. The Í©×ð©ÄÄÍ−Î function adds every signal to Í−Î.The Í©×þ®®Í−Î function adds Í©×ÅÏÀ to Í−Î, Í©×®−ÄÍ−Î deletes Í©×ÅÏÀ from Í−Î, and Í©×©ÍÀ−À¾−Ë returns 1 if Í©×ÅÏÀ is a member of Í−Î, and 0 if not. For example, Figure 8.32 shows how you would use Í©×ÉËÇ²ÀþÍÂ to tempo- rarily block the receipt of SIGINT signals. 1 Í©×Í−ÎˆÎ ÀþÍÂj ÉË−ÌˆÀþÍÂy 2 3 ·©×−ÀÉÎÔÍ−ÎfdÀþÍÂgy 4 ·©×þ®®Í−ÎfdÀþÍÂj ·'§'ﬁ¶gy 5 6 mh ¢ÄÇ²Â ·'§'ﬁ¶ þÅ® ÍþÌ− ÉË−Ì©ÇÏÍ ¾ÄÇ²Â−® Í−Î hm 7 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂj dÉË−ÌˆÀþÍÂgy 8 l l l // Code region that will not be interrupted by SIGINT 9 mh ‡−ÍÎÇË− ÉË−Ì©ÇÏÍ ¾ÄÇ²Â−® Í−Îj ÏÅ¾ÄÇ²Â©Å× ·'§'ﬁ¶ hm 10 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆÀþÍÂj ﬁ•‹‹gy 11 Figure 8.32 Temporarily blocking a signal from being received. 802 Chapter 8 Exceptional Control Flow 8.5.5 Writing Signal Handlers Signal handling is one of the thornier aspects of Linux system-level programming. Handlers have several attributes that make them difﬁcult to reason about: (1) Han- dlers run concurrently with the main program and share the same global variables, and thus can interfere with the main program and with other handlers. (2) The rules for how and when signals are received is often counterintuitive. (3) Different systems can have different signal-handling semantics. In this section, we address these issues and give you some basic guidelines for writing safe, correct, and portable signal handlers. Safe Signal Handling Signal handlers are tricky because they can run concurrently with the main pro- gram and with each other, as we saw in Figure 8.31. If a handler and the main program access the same global data structure concurrently, then the results can be unpredictable and often fatal. We will explore concurrent programming in detail in Chapter 12. Our aim here is to give you some conservative guidelines for writing handlers that are safe to run concurrently. If you ignore these guidelines, you run the risk of in- troducing subtle concurrency errors. With such errors, your program works cor- rectly most of the time. However, when it fails, it fails in unpredictable and unrepeatable ways that are horrendously difﬁcult to debug. Forewarned is fore- armed! G0. Keep handlers as simple as possible. The best way to avoid trouble is to keep your handlers as small and simple as possible. For example, the handler might simply set a global ﬂag and return immediately; all processing associated with the receipt of the signal is performed by the main program, which periodically checks (and resets) the ﬂag. G1. Call only async-signal-safe functions in your handlers. A function that is async-signal-safe, or simply safe, has the property that it can be safely called from a signal handler, either because it is reentrant (e.g., ac- cesses only local variables; see Section 12.7.2), or because it cannot be interrupted by a signal handler. Figure 8.33 lists the system-level functions that Linux guarantees to be safe. Notice that many popu- lar functions, such as ÉË©ÅÎð, ÍÉË©ÅÎð, ÀþÄÄÇ², and −Ó©Î, are not on this list. The only safe way to generate output from a signal handler is to use the ÑË©Î− function (see Section 10.1). In particular, calling ÉË©ÅÎð or ÍÉË©ÅÎð is unsafe. To work around this unfortunate restriction, we have developed some safe functions, called the Sio (Safe I/O) package, that you can use to print simple messages from signal handlers. Section 8.5 Signals 803 ˆ¥Ó©Î ð−Ó−²Ì− ÉÇÄÄ Í©×ÊÏ−Ï− ˆ−Ó©Î ðÇËÂ ÉÇÍ©ÓˆÎËþ²−ˆ−Ì−ÅÎ Í©×Í−Î þ¾ÇËÎ ðÍÎþÎ ÉÍ−Ä−²Î Í©×ÍÏÍÉ−Å® þ²²−ÉÎ ðÍÎþÎþÎ Ëþ©Í− ÍÄ−−É þ²²−ÍÍ ðÍÔÅ² Ë−þ® ÍÇ²ÂþÎÀþËÂ þ©Çˆ−ËËÇË ðÎËÏÅ²þÎ− Ë−þ®Ä©ÅÂ ÍÇ²Â−Î þ©ÇˆË−ÎÏËÅ ðÏÎ©À−ÅÍ Ë−þ®Ä©ÅÂþÎ ÍÇ²Â−ÎÉþ©Ë þ©ÇˆÍÏÍÉ−Å® ×−Î−×©® Ë−²Ì ÍÎþÎ þÄþËÀ ×−Î−Ï©® Ë−²ÌðËÇÀ ÍÔÀÄ©ÅÂ ¾©Å® ×−Î×©® Ë−²ÌÀÍ× ÍÔÀÄ©ÅÂþÎ ²ð×−Î©ÍÉ−−® ×−Î×ËÇÏÉÍ Ë−ÅþÀ− Î²®Ëþ©Å ²ð×−ÎÇÍÉ−−® ×−ÎÉ−−ËÅþÀ− Ë−ÅþÀ−þÎ Î²ðÄÇÑ ²ðÍ−Î©ÍÉ−−® ×−ÎÉ×ËÉ ËÀ®©Ë Î²ðÄÏÍ³ ²ðÍ−ÎÇÍÉ−−® ×−ÎÉ©® Í−Ä−²Î Î²×−ÎþÎÎË ²³®©Ë ×−ÎÉÉ©® Í−ÀˆÉÇÍÎ Î²×−ÎÉ×ËÉ ²³ÀÇ® ×−ÎÍÇ²ÂÅþÀ− Í−Å® Î²Í−Å®¾Ë−þÂ ²³ÇÑÅ ×−ÎÍÇ²ÂÇÉÎ Í−Å®ÀÍ× Î²Í−ÎþÎÎË ²ÄÇ²Âˆ×−ÎÎ©À− ×−ÎÏ©® Í−Å®ÎÇ Î²Í−ÎÉ×ËÉ ²ÄÇÍ− Â©ÄÄ Í−Î×©® Î©À− ²ÇÅÅ−²Î Ä©ÅÂ Í−ÎÉ×©® Î©À−Ëˆ×−ÎÇÌ−ËËÏÅ ²Ë−þÎ Ä©ÅÂþÎ Í−ÎÍ©® Î©À−Ëˆ×−ÎÎ©À− ®ÏÉ Ä©ÍÎ−Å Í−ÎÍÇ²ÂÇÉÎ Î©À−ËˆÍ−ÎÎ©À− ®ÏÉp ÄÍ−−Â Í−ÎÏ©® Î©À−Í −Ó−²Ä ÄÍÎþÎ Í³ÏÎ®ÇÑÅ ÏÀþÍÂ −Ó−²Ä− ÀÂ®©Ë Í©×þ²Î©ÇÅ ÏÅþÀ− −Ó−²Ì ÀÂ®©ËþÎ Í©×þ®®Í−Î ÏÅÄ©ÅÂ −Ó−²Ì− ÀÂð©ðÇ Í©×®−ÄÍ−Î ÏÅÄ©ÅÂþÎ ðþ²²−ÍÍþÎ ÀÂð©ðÇþÎ Í©×−ÀÉÎÔÍ−Î ÏÎ©À− ð²³ÀÇ® ÀÂÅÇ® Í©×ð©ÄÄÍ−Î ÏÎ©À−ÅÍþÎ ð²³ÀÇ®þÎ ÀÂÅÇ®þÎ Í©×©ÍÀ−À¾−Ë ÏÎ©À−Í ð²³ÇÑÅ ÇÉ−Å Í©×ÅþÄ Ñþ©Î ð²³ÇÑÅþÎ ÇÉ−ÅþÎ Í©×ÉþÏÍ− Ñþ©ÎÉ©® ð²ÅÎÄ ÉþÏÍ− Í©×É−Å®©Å× ÑË©Î− ð®þÎþÍÔÅ² É©É− Í©×ÉËÇ²ÀþÍÂ Figure 8.33 Async-signal-safe functions. (Source: ÀþÅ u Í©×ÅþÄ. Data from the Linux Foundation.) 804 Chapter 8 Exceptional Control Flow a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ÍÍ©Ö−ˆÎ Í©ÇˆÉÏÎÄfÄÇÅ× Ìgy ÍÍ©Ö−ˆÎ Í©ÇˆÉÏÎÍf²³þË Í‰`gy Returns: number of bytes transferred if OK, −1 on error ÌÇ©® Í©Çˆ−ËËÇËf²³þË Í‰`gy Returns: nothing The Í©ÇˆÉÏÎÄ and Í©ÇˆÉÏÎÍ functions emit a ÄÇÅ× and a string, respec- tively, to standard output. The Í©Çˆ−ËËÇË function prints an error mes- sage and terminates. Figure 8.34 shows the implementation of the Sio package, which uses two private reentrant functions from ²ÍþÉÉl².The Í©ÇˆÍÎËÄ−Å function in line 3 returns the length of string Í.The Í©ÇˆÄÎÇþ function in line 10, which is based on the ©ÎÇþ function from [61], converts Ì to its base ¾ string representation in Í.The ˆ−Ó©Î function in line 17 is an async-signal- safe variant of −Ó©Î. Figure 8.35 shows a safe version of the SIGINT handler from Fig- ure 8.30. G2. Save and restore −ËËÅÇ. Many of the Linux async-signal-safe functions set −ËËÅÇ when they return with an error. Calling such functions inside a handler might interfere with other parts of the program that rely on −ËËÅÇ. code/src/csapp.c 1 ÍÍ©Ö−ˆÎ Í©ÇˆÉÏÎÍf²³þË Í‰`g mh –ÏÎ ÍÎË©Å× hm 2 Õ 3 Ë−ÎÏËÅ ÑË©Î−f·¶⁄ﬂ•¶ˆƒ'‹¥ﬁﬂj Íj Í©ÇˆÍÎËÄ−ÅfÍggy 4 Û 5 6 ÍÍ©Ö−ˆÎ Í©ÇˆÉÏÎÄfÄÇÅ× Ìg mh –ÏÎ ÄÇÅ× hm 7 Õ 8 ²³þË Í‰opv`y 9 10 Í©ÇˆÄÎÇþfÌj Íj ongy mh ¢þÍ−® ÇÅ «d‡ ©ÎÇþfg hm 11 Ë−ÎÏËÅ Í©ÇˆÉÏÎÍfÍgy 12 Û 13 14 ÌÇ©® Í©Çˆ−ËËÇËf²³þË Í‰`g mh –ÏÎ −ËËÇË À−ÍÍþ×− þÅ® −Ó©Î hm 15 Õ 16 Í©ÇˆÉÏÎÍfÍgy 17 ˆ−Ó©Îfogy 18 Û code/src/csapp.c Figure 8.34 The Sio (Safe I/O) package for signal handlers. Section 8.5 Signals 805 code/ecf/sigintsafe.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® Í©×©ÅÎˆ³þÅ®Ä−Ëf©ÅÎ Í©×g mh ·þð− ·'§'ﬁ¶ ³þÅ®Ä−Ë hm 4 Õ 5 ·©ÇˆÉÏÎÍf‘£þÏ×³Î ·'§'ﬁ¶_¿Å‘gy mh ·þð− ÇÏÎÉÏÎ hm 6 ˆ−Ó©Îfngy mh ·þð− −Ó©Î hm 7 Û code/ecf/sigintsafe.c Figure 8.35 A safe version of the SIGINT handler from Figure 8.30. The workaround is to save −ËËÅÇ to a local variable on entry to the handler and restore it before the handler returns. Note that this is only necessary if the handler returns. It is not necessary if the handler terminates the process by calling ˆ−Ó©Î. G3. Protect accesses to shared global data structures by blocking all signals. If a handler shares a global data structure with the main program or with other handlers, then your handlers and main program should temporarily block all signals while accessing (reading or writing) that data structure. The reason for this rule is that accessing a data structure d from the main program typically requires a sequence of instructions. If this instruction sequence is interrupted by a handler that accesses d, then the handler might ﬁnd d in an inconsistent state, with unpredictable results. Tempo- rarily blocking signals while you access d guarantees that a handler will not interrupt the instruction sequence. G4. Declare global variables with ÌÇÄþÎ©Ä−. Consider a handler and Àþ©Å rou- tine that share a global variable g. The handler updates g, and Àþ©Å pe- riodically reads g. To an optimizing compiler, it would appear that the value of g never changes in Àþ©Å, and thus it would be safe to use a copy of g that is cached in a register to satisfy every reference to g. In this case, the Àþ©Å function would never see the updated values from the handler. You can tell the compiler not to cache a variable by declaring it with the ÌÇÄþÎ©Ä− type qualiﬁer. For example: ÌÇÄþÎ©Ä− ©ÅÎ ×y The ÌÇÄþÎ©Ä− qualiﬁer forces the compiler to read the value of × from memory each time it is referenced in the code. In general, as with any shared data structure, each access to a global variable should be protected by temporarily blocking signals. G5. Declare ﬂags with Í©×ˆþÎÇÀ©²ˆÎ. In one common handler design, the handler records the receipt of the signal by writing to a global ﬂag.The main program periodically reads the ﬂag, responds to the signal, and 806 Chapter 8 Exceptional Control Flow clears the ﬂag. For ﬂags that are shared in this way, C provides an integer data type, Í©×ˆþÎÇÀ©²ˆÎ, for which reads and writes are guaranteed to be atomic (uninterruptible) because they can be implemented with a single instruction: ÌÇÄþÎ©Ä− Í©×ˆþÎÇÀ©²ˆÎ ðÄþ×y Since they can’t be interrupted, you can safely read from and write to Í©×ˆþÎÇÀ©²ˆÎ variables without temporarily blocking signals. Note that the guarantee of atomicity only applies to individual reads and writes. It does not apply to updates such as ðÄþ×ii or ðÄþ× { ðÄþ× i on, which might require multiple instructions. Keep in mind that the guidelines we have presented are conservative, in the sense that they are not always strictly necessary. For example, if you know that a handler can never modify −ËËÅÇ, then you don’t need to save and restore −ËËÅÇ. Or if you can prove that no instance of ÉË©ÅÎð can ever be interrupted by a handler, then it is safe to call ÉË©ÅÎð from the handler. The same holds for accesses to shared global data structures. However, it is very difﬁcult to prove such assertions in general. So we recommend that you take the conservative approach and follow the guidelines by keeping your handlers as simple as possible, calling safe functions, saving and restoring −ËËÅÇ, protecting accesses to shared data structures, and using ÌÇÄþÎ©Ä− and Í©×ˆþÎÇÀ©²ˆÎ. Correct Signal Handling One of the nonintuitive aspects of signals is that pending signals are not queued. Because the É−Å®©Å× bit vector contains exactly one bit for each type of signal, there can be at most one pending signal of any particular type. Thus, if two signals of type k are sent to a destination process while signal k is blocked because the destination process is currently executing a handler for signal k, then the second signal is simply discarded; it is not queued. The key idea is that the existence of a pending signal merely indicates that at least one signal has arrived. To see how this affects correctness, let’s look at a simple application that is similar in nature to real programs such as shells and Web servers. The basic structure is that a parent process creates some children that run independently for a while and then terminate. The parent must reap the children to avoid leaving zombies in the system. But we also want the parent to be free to do other work while the children are running. So we decide to reap the children with a SIGCHLD handler, instead of explicitly waiting for the children to terminate. (Recall that the kernel sends a SIGCHLD signal to the parent whenever one of its children terminates or stops.) Figure 8.36 shows our ﬁrst attempt. The parent installs a SIGCHLD handler and then creates three children. In the meantime, the parent waits for a line of input from the terminal and then processes it. This processing is modeled by an inﬁnite loop. When each child terminates, the kernel notiﬁes the parent by sending it a SIGCHLD signal. The parent catches the SIGCHLD, reaps one child, Section 8.5 Signals 807 code/ecf/signal1.c 1 mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ô_ hm 2 3 ÌÇ©® ³þÅ®Ä−Ëof©ÅÎ Í©×g 4 Õ 5 ©ÅÎ ÇÄ®−ËËÅÇ { −ËËÅÇy 6 7 ©ð ffÑþ©ÎÉ©®fkoj ﬁ•‹‹j ngg z ng 8 Í©Çˆ−ËËÇËf‘Ñþ©ÎÉ©® −ËËÇË‘gy 9 ·©ÇˆÉÏÎÍf‘¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä®¿Å‘gy 10 ·Ä−−Éfogy 11 −ËËÅÇ { ÇÄ®−ËËÅÇy 12 Û 13 14 ©ÅÎ Àþ©Åfg 15 Õ 16 ©ÅÎ ©j Åy 17 ²³þË ¾Ïð‰›¡”¢•ƒ`y 18 19 ©ð fÍ©×ÅþÄf·'§£¤‹⁄j ³þÅ®Ä−Ëog {{ ·'§ˆ¥‡‡g 20 ÏÅ©Óˆ−ËËÇËf‘Í©×ÅþÄ −ËËÇË‘gy 21 22 mh –þË−ÅÎ ²Ë−þÎ−Í ²³©Ä®Ë−Å hm 23 ðÇËf©{ny©zqy ©iig Õ 24 ©ð fƒÇËÂfg {{ ng Õ 25 ÉË©ÅÎðf‘¤−ÄÄÇ ðËÇÀ ²³©Ä® c®¿Å‘j f©ÅÎg×−ÎÉ©®fggy 26 −Ó©Îfngy 27 Û 28 Û 29 30 mh –þË−ÅÎ Ñþ©ÎÍ ðÇË Î−ËÀ©ÅþÄ ©ÅÉÏÎ þÅ® Î³−Å ÉËÇ²−ÍÍ−Í ©Î hm 31 ©ð ffÅ { Ë−þ®f·¶⁄'ﬁˆƒ'‹¥ﬁﬂj ¾Ïðj Í©Ö−Çðf¾Ïðggg z ng 32 ÏÅ©Óˆ−ËËÇËf‘Ë−þ®‘gy 33 34 ÉË©ÅÎðf‘–þË−ÅÎ ÉËÇ²−ÍÍ©Å× ©ÅÉÏÎ¿Å‘gy 35 Ñ³©Ä− fog 36 y 37 38 −Ó©Îfngy 39 Û code/ecf/signal1.c Figure 8.36 Í©×ÅþÄo. This program is ﬂawed because it assumes that signals are queued. 808 Chapter 8 Exceptional Control Flow does some additional cleanup work (modeled by the ÍÄ−−É statement), and then returns. The Í©×ÅþÄo program in Figure 8.36 seems fairly straightforward. When we run it on our Linux system, however, we get the following output: Ä©ÅÏÓ| ./signal1 ¤−ÄÄÇ ðËÇÀ ²³©Ä® ornuq ¤−ÄÄÇ ðËÇÀ ²³©Ä® ornur ¤−ÄÄÇ ðËÇÀ ²³©Ä® ornus ¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä® ¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä® CR –þË−ÅÎ ÉËÇ²−ÍÍ©Å× ©ÅÉÏÎ From the output, we note that although three SIGCHLD signals were sent to the parent, only two of these signals were received, and thus the parent only reaped two children. If we suspend the parent process, we see that, indeed, child process 14075 was never reaped and remains a zombie (indicated by the string z®−ðÏÅ²Î| in the output of the ÉÍ command): Ctrl+Z ·ÏÍÉ−Å®−® Ä©ÅÏÓ| ps t –'⁄ ¶¶» ·¶¡¶ ¶'›¥ £ﬂ››¡ﬁ⁄ l l l ornup ÉÎÍmq ¶ nxnp lmÍ©×ÅþÄo ornus ÉÎÍmq … nxnn ‰Í©×ÅþÄo` z®−ðÏÅ²Î| ornut ÉÎÍmq ‡i nxnn ÉÍ Î What went wrong? The problem is that our code failed to account for the fact that signals are not queued. Here’s what happened: The ﬁrst signal is received and caught by the parent. While the handler is still processing the ﬁrst signal, the second signal is delivered and added to the set of pending signals. However, since SIGCHLD signals are blocked by the SIGCHLD handler, the second signal is not received. Shortly thereafter, while the handler is still processing the ﬁrst signal, the third signal arrives. Since there is already a pending SIGCHLD, this third SIGCHLD signal is discarded. Sometime later, after the handler has returned, the kernel notices that there is a pending SIGCHLD signal and forces the parent to receive the signal. The parent catches the signal and executes the handler a second time. After the handler ﬁnishes processing the second signal, there are no more pending SIGCHLD signals, and there never will be, because all knowledge of the third SIGCHLD has been lost. The crucial lesson is that signals cannot be used to count the occurrence of events in other processes. To ﬁx the problem, we must recall that the existence of a pending signal only implies that at least one signal has been delivered since the last time the process received a signal of that type. So we must modify the SIGCHLD handler to reap Section 8.5 Signals 809 code/ecf/signal2.c 1 ÌÇ©® ³þÅ®Ä−Ëpf©ÅÎ Í©×g 2 Õ 3 ©ÅÎ ÇÄ®−ËËÅÇ { −ËËÅÇy 4 5 Ñ³©Ä− fÑþ©ÎÉ©®fkoj ﬁ•‹‹j ng | ng Õ 6 ·©ÇˆÉÏÎÍf‘¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä®¿Å‘gy 7 Û 8 ©ð f−ËËÅÇ _{ ¥£¤'‹⁄g 9 ·©Çˆ−ËËÇËf‘Ñþ©ÎÉ©® −ËËÇË‘gy 10 ·Ä−−Éfogy 11 −ËËÅÇ { ÇÄ®−ËËÅÇy 12 Û code/ecf/signal2.c Figure 8.37 Í©×ÅþÄp. An improved version of Figure 8.36 that correctly accounts for the fact that signals are not queued. as many zombie children as possible each time it is invoked. Figure 8.37 shows the modiﬁed SIGCHLD handler. When we run Í©×ÅþÄp on our Linux system, it now correctly reaps all of the zombie children: Ä©ÅÏÓ| ./signal2 ¤−ÄÄÇ ðËÇÀ ²³©Ä® ospqu ¤−ÄÄÇ ðËÇÀ ²³©Ä® ospqv ¤−ÄÄÇ ðËÇÀ ²³©Ä® ospqw ¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä® ¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä® ¤þÅ®Ä−Ë Ë−þÉ−® ²³©Ä® CR –þË−ÅÎ ÉËÇ²−ÍÍ©Å× ©ÅÉÏÎ Practice Problem 8.8 (solution page 835) What is the output of the following program? code/ecf/signalprob0.c 1 ÌÇÄþÎ©Ä− ÄÇÅ× ²ÇÏÅÎ−Ë { py 2 3 ÌÇ©® ³þÅ®Ä−Ëof©ÅÎ Í©×g 4 Õ 5 Í©×Í−ÎˆÎ ÀþÍÂj ÉË−ÌˆÀþÍÂy 6 7 ·©×ð©ÄÄÍ−ÎfdÀþÍÂgy 8 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂj dÉË−ÌˆÀþÍÂgy mh ¢ÄÇ²Â Í©×Í hm 810 Chapter 8 Exceptional Control Flow 9 ·©ÇˆÉÏÎÄfkk²ÇÏÅÎ−Ëgy 10 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆÀþÍÂj ﬁ•‹‹gy mh ‡−ÍÎÇË− Í©×Í hm 11 12 ˆ−Ó©Îfngy 13 Û 14 15 ©ÅÎ Àþ©Åfg 16 Õ 17 É©®ˆÎ É©®y 18 Í©×Í−ÎˆÎ ÀþÍÂj ÉË−ÌˆÀþÍÂy 19 20 ÉË©ÅÎðf‘cÄ®‘j ²ÇÏÅÎ−Ëgy 21 ððÄÏÍ³fÍÎ®ÇÏÎgy 22 23 Í©×ÅþÄf·'§•·‡oj ³þÅ®Ä−Ëogy 24 ©ð ffÉ©® { ƒÇËÂfgg {{ ng Õ 25 Ñ³©Ä−fog ÕÛy 26 Û 27 «©ÄÄfÉ©®j ·'§•·‡ogy 28 „þ©ÎÉ©®fkoj ﬁ•‹‹j ngy 29 30 ·©×ð©ÄÄÍ−ÎfdÀþÍÂgy 31 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂj dÉË−ÌˆÀþÍÂgy mh ¢ÄÇ²Â Í©×Í hm 32 ÉË©ÅÎðf‘cÄ®‘j ii²ÇÏÅÎ−Ëgy 33 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆÀþÍÂj ﬁ•‹‹gy mh ‡−ÍÎÇË− Í©×Í hm 34 35 −Ó©Îfngy 36 Û code/ecf/signalprob0.c Portable Signal Handling Another ugly aspect of Unix signal handling is that different systems have different signal-handling semantics. For example: . The semantics of the Í©×ÅþÄ function varies. Some older Unix systems restore the action for signal k to its default after signal k has been caught by a handler. On these systems, the handler must explicitly reinstall itself, by calling Í©×ÅþÄ, each time it runs. . System calls can be interrupted. System calls such as Ë−þ®, Ñþ©Î, and þ²²−ÉÎ that can potentially block the process for a long period of time are called slow system calls. On some older versions of Unix, slow system calls that are interrupted when a handler catches a signal do not resume when the signal handler returns but instead return immediately to the user with an error condition and −ËËÅÇ set to EINTR. On these systems, programmers must include code that manually restarts interrupted system calls. Section 8.5 Signals 811 code/src/csapp.c 1 ³þÅ®Ä−ËˆÎ h·©×ÅþÄf©ÅÎ Í©×ÅÏÀj ³þÅ®Ä−ËˆÎ h³þÅ®Ä−Ëg 2 Õ 3 ÍÎËÏ²Î Í©×þ²Î©ÇÅ þ²Î©ÇÅj ÇÄ®ˆþ²Î©ÇÅy 4 5 þ²Î©ÇÅlÍþˆ³þÅ®Ä−Ë { ³þÅ®Ä−Ëy 6 Í©×−ÀÉÎÔÍ−Îfdþ²Î©ÇÅlÍþˆÀþÍÂgy mh ¢ÄÇ²Â Í©×Í Çð ÎÔÉ− ¾−©Å× ³þÅ®Ä−® hm 7 þ²Î©ÇÅlÍþˆðÄþ×Í { ·¡ˆ‡¥·¶¡‡¶y mh ‡−ÍÎþËÎ ÍÔÍ²þÄÄÍ ©ð ÉÇÍÍ©¾Ä− hm 8 9 ©ð fÍ©×þ²Î©ÇÅfÍ©×ÅÏÀj dþ²Î©ÇÅj dÇÄ®ˆþ²Î©ÇÅg z ng 10 ÏÅ©Óˆ−ËËÇËf‘·©×ÅþÄ −ËËÇË‘gy 11 Ë−ÎÏËÅ fÇÄ®ˆþ²Î©ÇÅlÍþˆ³þÅ®Ä−Ëgy 12 Û code/src/csapp.c Figure 8.38 ·©×ÅþÄ. A wrapper for Í©×þ²Î©ÇÅ that provides portable signal handling on Posix-compliant systems. To deal with these issues, the Posix standard deﬁnes the Í©×þ²Î©ÇÅ function, which allows users to clearly specify the signal-handling semantics they want when they install a handler. a©Å²ÄÏ®− zÍ©×ÅþÄl³| ©ÅÎ Í©×þ²Î©ÇÅf©ÅÎ Í©×ÅÏÀj ÍÎËÏ²Î Í©×þ²Î©ÇÅ hþ²Îj ÍÎËÏ²Î Í©×þ²Î©ÇÅ hÇÄ®þ²Îgy Returns: 0 if OK, −1 on error The Í©×þ²Î©ÇÅ function is unwieldy because it requires the user to set the entries of a complicated structure. A cleaner approach, originally proposed by W. Richard Stevens [110], is to deﬁne a wrapper function, called ·©×ÅþÄ, that calls Í©×þ²Î©ÇÅ for us. Figure 8.38 shows the deﬁnition of ·©×ÅþÄ, which is invoked in the same way as the Í©×ÅþÄ function. The ·©×ÅþÄ wrapper installs a signal handler with the following signal- handling semantics: . Only signals of the type currently being processed by the handler are blocked. . As with all signal implementations, signals are not queued. . Interrupted system calls are automatically restarted whenever possible. . Once the signal handler is installed, it remains installed until ·©×ÅþÄ is called with a ³þÅ®Ä−Ë argument of either SIG_IGN or SIG_DFL. We will use the ·©×ÅþÄ wrapper in all of our code. 812 Chapter 8 Exceptional Control Flow 8.5.6 Synchronizing Flows to Avoid Nasty Concurrency Bugs The problem of how to program concurrent ﬂows that read and write the same storage locations has challenged generations of computer scientists. In general, the number of potential interleavings of the ﬂows is exponential in the number of instructions. Some of those interleavings will produce correct answers, and others will not. The fundamental problem is to somehow synchronize the concurrent ﬂows so as to allow the largest set of feasible interleavings such that each of the feasible interleavings produces a correct answer. Concurrent programming is a deep and important problem that we will discuss in more detail in Chapter 12. However, we can use what you’ve learned about exceptional control ﬂow in this chapter to give you a sense of the interesting intellectual challenges associated with concurrency. For example, consider the program in Figure 8.39, which captures the structure of a typical Unix shell. The parent keeps track of its current children using entries in a global job list, with one entry per job. The þ®®ÁÇ¾ and ®−Ä−Î−ÁÇ¾ functions add and remove entries from the job list. After the parent creates a new child process, it adds the child to the job list. When the parent reaps a terminated (zombie) child in the SIGCHLD signal handler, it deletes the child from the job list. At ﬁrst glance, this code appears to be correct. Unfortunately, the following sequence of events is possible: 1. The parent executes the ðÇËÂ function and the kernel schedules the newly created child to run instead of the parent. 2. Before the parent is able to run again, the child terminates and becomes a zombie, causing the kernel to deliver a SIGCHLD signal to the parent. 3. Later, when the parent becomes runnable again but before it is executed, the kernel notices the pending SIGCHLD and causes it to be received by running the signal handler in the parent. 4. The signal handler reaps the terminated child and calls ®−Ä−Î−ÁÇ¾, which does nothing because the parent has not added the child to the list yet. 5. After the handler completes, the kernel then runs the parent, which returns from ðÇËÂ and incorrectly adds the (nonexistent) child to the job list by calling þ®®ÁÇ¾. Thus, for some interleavings of the parent’s main routine and signal-handling ﬂows, it is possible for ®−Ä−Î−ÁÇ¾ to be called before þ®®ÁÇ¾. This results in an incorrect entry on the job list, for a job that no longer exists and that will never be removed. On the other hand, there are also interleavings where events occur in the correct order. For example, if the kernel happens to schedule the parent to run when the ðÇËÂ call returns instead of the child, then the parent will correctly add the child to the job list before the child terminates and the signal handler removes the job from the list. This is an example of a classic synchronization error known as a race. In this case, the race is between the call to þ®®ÁÇ¾ in the main routine and the call to Section 8.5 Signals 813 code/ecf/procmask1.c 1 mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ô_ hm 2 ÌÇ©® ³þÅ®Ä−Ëf©ÅÎ Í©×g 3 Õ 4 ©ÅÎ ÇÄ®−ËËÅÇ { −ËËÅÇy 5 Í©×Í−ÎˆÎ ÀþÍÂˆþÄÄj ÉË−ÌˆþÄÄy 6 É©®ˆÎ É©®y 7 8 ·©×ð©ÄÄÍ−ÎfdÀþÍÂˆþÄÄgy 9 Ñ³©Ä− ffÉ©® { Ñþ©ÎÉ©®fkoj ﬁ•‹‹j ngg | ng Õ mh ‡−þÉ þ ÖÇÀ¾©− ²³©Ä® hm 10 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂˆþÄÄj dÉË−ÌˆþÄÄgy 11 ®−Ä−Î−ÁÇ¾fÉ©®gy mh ⁄−Ä−Î− Î³− ²³©Ä® ðËÇÀ Î³− ÁÇ¾ Ä©ÍÎ hm 12 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆþÄÄj ﬁ•‹‹gy 13 Û 14 ©ð f−ËËÅÇ _{ ¥£¤'‹⁄g 15 ·©Çˆ−ËËÇËf‘Ñþ©ÎÉ©® −ËËÇË‘gy 16 −ËËÅÇ { ÇÄ®−ËËÅÇy 17 Û 18 19 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 20 Õ 21 ©ÅÎ É©®y 22 Í©×Í−ÎˆÎ ÀþÍÂˆþÄÄj ÉË−ÌˆþÄÄy 23 24 ·©×ð©ÄÄÍ−ÎfdÀþÍÂˆþÄÄgy 25 ·©×ÅþÄf·'§£¤‹⁄j ³þÅ®Ä−Ëgy 26 ©Å©ÎÁÇ¾Ífgy mh 'Å©Î©þÄ©Ö− Î³− ÁÇ¾ Ä©ÍÎ hm 27 28 Ñ³©Ä− fog Õ 29 ©ð ffÉ©® { ƒÇËÂfgg {{ ng Õ mh £³©Ä® ÉËÇ²−ÍÍ hm 30 ¥Ó−²Ì−f‘m¾©Åm®þÎ−‘j þË×Ìj ﬁ•‹‹gy 31 Û 32 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂˆþÄÄj dÉË−ÌˆþÄÄgy mh –þË−ÅÎ ÉËÇ²−ÍÍ hm 33 þ®®ÁÇ¾fÉ©®gy mh ¡®® Î³− ²³©Ä® ÎÇ Î³− ÁÇ¾ Ä©ÍÎ hm 34 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆþÄÄj ﬁ•‹‹gy 35 Û 36 −Ó©Îfngy 37 Û code/ecf/procmask1.c Figure 8.39 A shell program with a subtle synchronization error. If the child terminates before the parent is able to run, then þ®®ÁÇ¾ and ®−Ä−Î−ÁÇ¾ will be called in the wrong order. 814 Chapter 8 Exceptional Control Flow ®−Ä−Î−ÁÇ¾ in the handler. If þ®®ÁÇ¾ wins the race, then the answer is correct. If not, the answer is incorrect. Such errors are enormously difﬁcult to debug because it is often impossible to test every interleaving. You might run the code a billion times without a problem, but then the next test results in an interleaving that triggers the race. Figure 8.40 shows one way to eliminate the race in Figure 8.39. By blocking SIGCHLD signals before the call to ðÇËÂ and then unblocking them only after we have called þ®®ÁÇ¾, we guarantee that the child will be reaped after it is added to the job list. Notice that children inherit the ¾ÄÇ²Â−® set of their parents, so we must be careful to unblock the SIGCHLD signal in the child before calling −Ó−²Ì−. 8.5.7 Explicitly Waiting for Signals Sometimes a main program needs to explicitly wait for a certain signal handler to run. For example, when a Linux shell creates a foreground job, it must wait for the job to terminate and be reaped by the SIGCHLD handler before accepting the next user command. Figure 8.41 shows the basic idea. The parent installs handlers for SIGINT and SIGCHLD and then enters an inﬁnite loop. It blocks SIGCHLD to avoid the race between parent and child that we discussed in Section 8.5.6. After creating the child, it resets É©® to zero, unblocks SIGCHLD, and then waits in a spin loop for É©® to become nonzero. After the child terminates, the handler reaps it and assigns its nonzero PID to the global É©® variable. This terminates the spin loop, and the parent continues with additional work before starting the next iteration. While this code is correct, the spin loop is wasteful of processor resources. We might be tempted to ﬁx this by inserting a ÉþÏÍ− in the body of the spin loop: Ñ³©Ä− f_É©®g mh ‡þ²−_ hm ÉþÏÍ−fgy Notice that we still need a loop because ÉþÏÍ− might be interrupted by the receipt of one or more SIGINT signals. However, this code has a serious race condition: if the SIGCHLD is received after the Ñ³©Ä− test but before the ÉþÏÍ−, the ÉþÏÍ− will sleep forever. Another option is to replace the ÉþÏÍ− with ÍÄ−−É: Ñ³©Ä− f_É©®g mh ¶ÇÇ ÍÄÇÑ_ hm ÍÄ−−Éfogy While correct, this code is too slow. If the signal is received after the Ñ³©Ä− and before the ÍÄ−−É, the program must wait a (relatively) long time before it can check the loop termination condition again. Using a higher-resolution sleep function such as ÅþÅÇÍÄ−−É isn’t acceptable, either, because there is no good rule for determining the sleep interval. Make it too small and the loop is too wasteful. Make it too high and the program is too slow. Section 8.5 Signals 815 code/ecf/procmask2.c 1 ÌÇ©® ³þÅ®Ä−Ëf©ÅÎ Í©×g 2 Õ 3 ©ÅÎ ÇÄ®−ËËÅÇ { −ËËÅÇy 4 Í©×Í−ÎˆÎ ÀþÍÂˆþÄÄj ÉË−ÌˆþÄÄy 5 É©®ˆÎ É©®y 6 7 ·©×ð©ÄÄÍ−ÎfdÀþÍÂˆþÄÄgy 8 Ñ³©Ä− ffÉ©® { Ñþ©ÎÉ©®fkoj ﬁ•‹‹j ngg | ng Õ mh ‡−þÉ þ ÖÇÀ¾©− ²³©Ä® hm 9 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂˆþÄÄj dÉË−ÌˆþÄÄgy 10 ®−Ä−Î−ÁÇ¾fÉ©®gy mh ⁄−Ä−Î− Î³− ²³©Ä® ðËÇÀ Î³− ÁÇ¾ Ä©ÍÎ hm 11 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆþÄÄj ﬁ•‹‹gy 12 Û 13 ©ð f−ËËÅÇ _{ ¥£¤'‹⁄g 14 ·©Çˆ−ËËÇËf‘Ñþ©ÎÉ©® −ËËÇË‘gy 15 −ËËÅÇ { ÇÄ®−ËËÅÇy 16 Û 17 18 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 19 Õ 20 ©ÅÎ É©®y 21 Í©×Í−ÎˆÎ ÀþÍÂˆþÄÄj ÀþÍÂˆÇÅ−j ÉË−ÌˆÇÅ−y 22 23 ·©×ð©ÄÄÍ−ÎfdÀþÍÂˆþÄÄgy 24 ·©×−ÀÉÎÔÍ−ÎfdÀþÍÂˆÇÅ−gy 25 ·©×þ®®Í−ÎfdÀþÍÂˆÇÅ−j ·'§£¤‹⁄gy 26 ·©×ÅþÄf·'§£¤‹⁄j ³þÅ®Ä−Ëgy 27 ©Å©ÎÁÇ¾Ífgy mh 'Å©Î©þÄ©Ö− Î³− ÁÇ¾ Ä©ÍÎ hm 28 29 Ñ³©Ä− fog Õ 30 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂˆÇÅ−j dÉË−ÌˆÇÅ−gy mh ¢ÄÇ²Â ·'§£¤‹⁄ hm 31 ©ð ffÉ©® { ƒÇËÂfgg {{ ng Õ mh £³©Ä® ÉËÇ²−ÍÍ hm 32 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆÇÅ−j ﬁ•‹‹gy mh •Å¾ÄÇ²Â ·'§£¤‹⁄ hm 33 ¥Ó−²Ì−f‘m¾©Åm®þÎ−‘j þË×Ìj ﬁ•‹‹gy 34 Û 35 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂˆþÄÄj ﬁ•‹‹gy mh –þË−ÅÎ ÉËÇ²−ÍÍ hm 36 þ®®ÁÇ¾fÉ©®gy mh ¡®® Î³− ²³©Ä® ÎÇ Î³− ÁÇ¾ Ä©ÍÎ hm 37 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−ÌˆÇÅ−j ﬁ•‹‹gy mh •Å¾ÄÇ²Â ·'§£¤‹⁄ hm 38 Û 39 −Ó©Îfngy 40 Û code/ecf/procmask2.c Figure 8.40 Using Í©×ÉËÇ²ÀþÍÂ to synchronize processes. In this example, the parent ensures that þ®®ÁÇ¾ executes before the corresponding ®−Ä−Î−ÁÇ¾. 816 Chapter 8 Exceptional Control Flow code/ecf/waitforsignal.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇÄþÎ©Ä− Í©×ˆþÎÇÀ©²ˆÎ É©®y 4 5 ÌÇ©® Í©×²³Ä®ˆ³þÅ®Ä−Ëf©ÅÎ Íg 6 Õ 7 ©ÅÎ ÇÄ®−ËËÅÇ { −ËËÅÇy 8 É©® { Ñþ©ÎÉ©®fkoj ﬁ•‹‹j ngy 9 −ËËÅÇ { ÇÄ®−ËËÅÇy 10 Û 11 12 ÌÇ©® Í©×©ÅÎˆ³þÅ®Ä−Ëf©ÅÎ Íg 13 Õ 14 Û 15 16 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 17 Õ 18 Í©×Í−ÎˆÎ ÀþÍÂj ÉË−Ìy 19 20 ·©×ÅþÄf·'§£¤‹⁄j Í©×²³Ä®ˆ³þÅ®Ä−Ëgy 21 ·©×ÅþÄf·'§'ﬁ¶j Í©×©ÅÎˆ³þÅ®Ä−Ëgy 22 ·©×−ÀÉÎÔÍ−ÎfdÀþÍÂgy 23 ·©×þ®®Í−ÎfdÀþÍÂj ·'§£¤‹⁄gy 24 25 Ñ³©Ä− fog Õ 26 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂj dÉË−Ìgy mh ¢ÄÇ²Â ·'§£¤‹⁄ hm 27 ©ð fƒÇËÂfg {{ ng mh £³©Ä® hm 28 −Ó©Îfngy 29 30 mh –þË−ÅÎ hm 31 É©®{ny 32 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−Ìj ﬁ•‹‹gy mh •Å¾ÄÇ²Â ·'§£¤‹⁄ hm 33 34 mh „þ©Î ðÇË ·'§£¤‹⁄ ÎÇ ¾− Ë−²−©Ì−® fÑþÍÎ−ðÏÄg hm 35 Ñ³©Ä− f_É©®g 36 y 37 38 mh ⁄Ç ÍÇÀ− ÑÇËÂ þðÎ−Ë Ë−²−©Ì©Å× ·'§£¤‹⁄ hm 39 ÉË©ÅÎðf‘l‘gy 40 Û 41 −Ó©Îfngy 42 Û code/ecf/waitforsignal.c Figure 8.41 Waiting for a signal with a spin loop. This code is correct, but the spin loop is wasteful. Section 8.6 Nonlocal Jumps 817 The proper solution is to use Í©×ÍÏÍÉ−Å®. a©Å²ÄÏ®− zÍ©×ÅþÄl³| ©ÅÎ Í©×ÍÏÍÉ−Å®f²ÇÅÍÎ Í©×Í−ÎˆÎ hÀþÍÂgy Returns: −1 The Í©×ÍÏÍÉ−Å® function temporarily replaces the current blocked set with ÀþÍÂ and then suspends the process until the receipt of a signal whose action is either to run a handler or to terminate the process. If the action is to terminate, then the process terminates without returning from Í©×ÍÏÍÉ−Å®. If the action is to run a handler, then Í©×ÍÏÍÉ−Å® returns after the handler returns, restoring the blocked set to its state when Í©×ÍÏÍÉ−Å® was called. The Í©×ÍÏÍÉ−Å® function is equivalent to an atomic (uninterruptible) version of the following: 1 Í©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂj dÉË−Ìgy 2 ÉþÏÍ−fgy 3 Í©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−Ìj ﬁ•‹‹gy The atomic property guarantees that the calls to Í©×ÉËÇ²ÀþÍÂ (line 1) and ÉþÏÍ− (line 2) occur together, without being interrupted. This eliminates the potential race where a signal is received after the call to Í©×ÉËÇ²ÀþÍÂ and before the call to ÉþÏÍ−. Figure 8.42 shows how we would use Í©×ÍÏÍÉ−Å® to replace the spin loop in Figure 8.41. Before each call to Í©×ÍÏÍÉ−Å®, SIGCHLD is blocked. The Í©×ÍÏÍÉ−Å® temporarily unblocks SIGCHLD, and then sleeps until the parent catches a signal. Before returning, it restores the original blocked set, which blocks SIGCHLD again. If the parent caught a SIGINT, then the loop test succeeds and the next iteration calls Í©×ÍÏÍÉ−Å® again. If the parent caught a SIGCHLD, then the loop test fails and we exit the loop. At this point, SIGCHLD is blocked, and so we can optionally unblock SIGCHLD. This might be useful in a real shell with background jobs that need to be reaped. The Í©×ÍÏÍÉ−Å® version is less wasteful than the original spin loop, avoids the race introduced by ÉþÏÍ−, and is more efﬁcient than ÍÄ−−É. 8.6 Nonlocal Jumps C provides a form of user-level exceptional control ﬂow, called a nonlocal jump, that transfers control directly from one function to another currently executing function without having to go through the normal call-and-return sequence. Non- local jumps are provided by the Í−ÎÁÀÉ and ÄÇÅ×ÁÀÉ functions. 818 Chapter 8 Exceptional Control Flow code/ecf/sigsuspend.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇÄþÎ©Ä− Í©×ˆþÎÇÀ©²ˆÎ É©®y 4 5 ÌÇ©® Í©×²³Ä®ˆ³þÅ®Ä−Ëf©ÅÎ Íg 6 Õ 7 ©ÅÎ ÇÄ®−ËËÅÇ { −ËËÅÇy 8 É©® { „þ©ÎÉ©®fkoj ﬁ•‹‹j ngy 9 −ËËÅÇ { ÇÄ®−ËËÅÇy 10 Û 11 12 ÌÇ©® Í©×©ÅÎˆ³þÅ®Ä−Ëf©ÅÎ Íg 13 Õ 14 Û 15 16 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 17 Õ 18 Í©×Í−ÎˆÎ ÀþÍÂj ÉË−Ìy 19 20 ·©×ÅþÄf·'§£¤‹⁄j Í©×²³Ä®ˆ³þÅ®Ä−Ëgy 21 ·©×ÅþÄf·'§'ﬁ¶j Í©×©ÅÎˆ³þÅ®Ä−Ëgy 22 ·©×−ÀÉÎÔÍ−ÎfdÀþÍÂgy 23 ·©×þ®®Í−ÎfdÀþÍÂj ·'§£¤‹⁄gy 24 25 Ñ³©Ä− fog Õ 26 ·©×ÉËÇ²ÀþÍÂf·'§ˆ¢‹ﬂ£«j dÀþÍÂj dÉË−Ìgy mh ¢ÄÇ²Â ·'§£¤‹⁄ hm 27 ©ð fƒÇËÂfg {{ ng mh £³©Ä® hm 28 −Ó©Îfngy 29 30 mh „þ©Î ðÇË ·'§£¤‹⁄ ÎÇ ¾− Ë−²−©Ì−® hm 31 É©®{ny 32 Ñ³©Ä− f_É©®g 33 Í©×ÍÏÍÉ−Å®fdÉË−Ìgy 34 35 mh ﬂÉÎ©ÇÅþÄÄÔ ÏÅ¾ÄÇ²Â ·'§£¤‹⁄ hm 36 ·©×ÉËÇ²ÀþÍÂf·'§ˆ·¥¶›¡·«j dÉË−Ìj ﬁ•‹‹gy 37 38 mh ⁄Ç ÍÇÀ− ÑÇËÂ þðÎ−Ë Ë−²−©Ì©Å× ·'§£¤‹⁄ hm 39 ÉË©ÅÎðf‘l‘gy 40 Û 41 −Ó©Îfngy 42 Û code/ecf/sigsuspend.c Figure 8.42 Waiting for a signal with Í©×ÍÏÍÉ−Å®. Section 8.6 Nonlocal Jumps 819 a©Å²ÄÏ®− zÍ−ÎÁÀÉl³| ©ÅÎ Í−ÎÁÀÉfÁÀÉˆ¾Ïð −ÅÌgy ©ÅÎ Í©×Í−ÎÁÀÉfÍ©×ÁÀÉˆ¾Ïð −ÅÌj ©ÅÎ ÍþÌ−Í©×Így Returns: 0 from Í−ÎÁÀÉ, nonzero from ÄÇÅ×ÁÀÉs The Í−ÎÁÀÉ function saves the current calling environment in the −ÅÌ buffer, for later use by ÄÇÅ×ÁÀÉ, and returns 0. The calling environment includes the program counter, stack pointer, and general-purpose registers. For subtle reasons beyond our scope, the value that Í−ÎÁÀÉ returns should not be assigned to a variable: Ë² { Í−ÎÁÀÉf−ÅÌgy mh „ËÇÅ×_ hm However, it can be safely used as a test in a ÍÑ©Î²³ or conditional statement [62]. a©Å²ÄÏ®− zÍ−ÎÁÀÉl³| ÌÇ©® ÄÇÅ×ÁÀÉfÁÀÉˆ¾Ïð −ÅÌj ©ÅÎ Ë−ÎÌþÄgy ÌÇ©® Í©×ÄÇÅ×ÁÀÉfÍ©×ÁÀÉˆ¾Ïð −ÅÌj ©ÅÎ Ë−ÎÌþÄgy Never returns The ÄÇÅ×ÁÀÉ function restores the calling environment from the −ÅÌ buffer and then triggers a return from the most recent Í−ÎÁÀÉ call that initialized −ÅÌ.The Í−ÎÁÀÉ then returns with the nonzero return value Ë−ÎÌþÄ. The interactions between Í−ÎÁÀÉ and ÄÇÅ×ÁÀÉ can be confusing at ﬁrst glance. The Í−ÎÁÀÉ function is called once but returns multiple times: once when the Í−ÎÁÀÉ is ﬁrst called and the calling environment is stored in the −ÅÌ buffer, and once for each corresponding ÄÇÅ×ÁÀÉ call. On the other hand, the ÄÇÅ×ÁÀÉ function is called once but never returns. An important application of nonlocal jumps is to permit an immediate return from a deeply nested function call, usually as a result of detecting some error condition. If an error condition is detected deep in a nested function call, we can use a nonlocal jump to return directly to a common localized error handler instead of laboriously unwinding the call stack. Figure 8.43 shows an example of how this might work. The Àþ©Å routine ﬁrst calls Í−ÎÁÀÉ to save the current calling environment, and then calls function ðÇÇ, which in turn calls function ¾þË.If ðÇÇ or ¾þË encounter an error, they return immediately from the Í−ÎÁÀÉ via a ÄÇÅ×ÁÀÉ call. The nonzero return value of the Í−ÎÁÀÉ indicates the error type, which can then be decoded and handled in one place in the code. The feature of ÄÇÅ×ÁÀÉ that allows it to skip up through all intermediate calls can have unintended consequences. For example, if some data structures were allocated in the intermediate function calls with the intention to deallocate them at the end of the function, the deallocation code gets skipped, thus creating a memory leak. 820 Chapter 8 Exceptional Control Flow code/ecf/setjmp.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÁÀÉˆ¾Ïð ¾Ïðy 4 5 ©ÅÎ −ËËÇËo { ny 6 ©ÅÎ −ËËÇËp { oy 7 8 ÌÇ©® ðÇÇfÌÇ©®gj ¾þËfÌÇ©®gy 9 10 ©ÅÎ Àþ©Åfg 11 Õ 12 ÍÑ©Î²³fÍ−ÎÁÀÉf¾Ïðgg Õ 13 ²þÍ− nx 14 ðÇÇfgy 15 ¾Ë−þÂy 16 ²þÍ− ox 17 ÉË©ÅÎðf‘⁄−Î−²Î−® þÅ −ËËÇËo ²ÇÅ®©Î©ÇÅ ©Å ðÇÇ¿Å‘gy 18 ¾Ë−þÂy 19 ²þÍ− px 20 ÉË©ÅÎðf‘⁄−Î−²Î−® þÅ −ËËÇËp ²ÇÅ®©Î©ÇÅ ©Å ðÇÇ¿Å‘gy 21 ¾Ë−þÂy 22 ®−ðþÏÄÎx 23 ÉË©ÅÎðf‘•ÅÂÅÇÑÅ −ËËÇË ²ÇÅ®©Î©ÇÅ ©Å ðÇÇ¿Å‘gy 24 Û 25 −Ó©Îfngy 26 Û 27 28 mh ⁄−−ÉÄÔ Å−ÍÎ−® ðÏÅ²Î©ÇÅ ðÇÇ hm 29 ÌÇ©® ðÇÇfÌÇ©®g 30 Õ 31 ©ð f−ËËÇËog 32 ÄÇÅ×ÁÀÉf¾Ïðj ogy 33 ¾þËfgy 34 Û 35 36 ÌÇ©® ¾þËfÌÇ©®g 37 Õ 38 ©ð f−ËËÇËpg 39 ÄÇÅ×ÁÀÉf¾Ïðj pgy 40 Û code/ecf/setjmp.c Figure 8.43 Nonlocal jump example. This example shows the framework for using nonlocal jumps to recover from error conditions in deeply nested functions without having to unwind the entire stack. Section 8.6 Nonlocal Jumps 821 code/ecf/restart.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 Í©×ÁÀÉˆ¾Ïð ¾Ïðy 4 5 ÌÇ©® ³þÅ®Ä−Ëf©ÅÎ Í©×g 6 Õ 7 Í©×ÄÇÅ×ÁÀÉf¾Ïðj ogy 8 Û 9 10 ©ÅÎ Àþ©Åfg 11 Õ 12 ©ð f_Í©×Í−ÎÁÀÉf¾Ïðj ogg Õ 13 ·©×ÅþÄf·'§'ﬁ¶j ³þÅ®Ä−Ëgy 14 ·©ÇˆÉÏÎÍf‘ÍÎþËÎ©Å×¿Å‘gy 15 Û 16 −ÄÍ− 17 ·©ÇˆÉÏÎÍf‘Ë−ÍÎþËÎ©Å×¿Å‘gy 18 19 Ñ³©Ä−fog Õ 20 ·Ä−−Éfogy 21 ·©ÇˆÉÏÎÍf‘ÉËÇ²−ÍÍ©Å×lll¿Å‘gy 22 Û 23 −Ó©Îfngy mh £ÇÅÎËÇÄ Å−Ì−Ë Ë−þ²³−Í ³−Ë− hm 24 Û code/ecf/restart.c Figure 8.44 A program that uses nonlocal jumps to restart itself when the user types Ctrl+C. Another important application of nonlocal jumps is to branch out of a signal handler to a speciﬁc code location, rather than returning to the instruction that was interrupted by the arrival of the signal. Figure 8.44 shows a simple program that illustrates this basic technique. The program uses signals and nonlocal jumps to do a soft restart whenever the user types Ctrl+C at the keyboard. The Í©×Í−ÎÁÀÉ and Í©×ÄÇÅ×ÁÀÉ functions are versions of Í−ÎÁÀÉ and ÄÇÅ×ÁÀÉ that can be used by signal handlers. The initial call to the Í©×Í−ÎÁÀÉ function saves the calling environment and signal context (including the pending and blocked signal vectors) when the pro- gram ﬁrst starts. The main routine then enters an inﬁnite processing loop. When the user types Ctrl+C, the kernel sends a SIGINT signal to the process, which catches it. Instead of returning from the signal handler, which would pass control back to the interrupted processing loop, the handler performs a nonlocal jump back to the beginning of the Àþ©Å program. When we run the program on our system, we get the following output: 822 Chapter 8 Exceptional Control Flow Aside Software exceptions in C++ and Java The exception mechanisms provided by C++ and Java are higher-level, more structured versions of the C Í−ÎÁÀÉ and ÄÇÅ×ÁÀÉ functions. You can think of a ²þÎ²³ clause inside a ÎËÔ statement as being akin to a Í−ÎÁÀÉ function. Similarly, a Î³ËÇÑ statement is similar to a ÄÇÅ×ÁÀÉ function. Ä©ÅÏÓ| ./restart ÍÎþËÎ©Å× ÉËÇ²−ÍÍ©Å×lll ÉËÇ²−ÍÍ©Å×lll Ctrl+C Ë−ÍÎþËÎ©Å× ÉËÇ²−ÍÍ©Å×lll Ctrl+C Ë−ÍÎþËÎ©Å× ÉËÇ²−ÍÍ©Å×lll There a couple of interesting things about this program. First, To avoid a race, we must install the handler after we call Í©×Í−ÎÁÀÉ. If not, we would run the risk of the handler running before the initial call to Í©×Í−ÎÁÀÉ sets up the calling environment for Í©×ÄÇÅ×ÁÀÉ. Second, you might have noticed that the Í©×Í−ÎÁÀÉ and Í©×ÄÇÅ×ÁÀÉ functions are not on the list of async-signal-safe functions in Figure 8.33. The reason is that in general Í©×ÄÇÅ×ÁÀÉ can jump into arbitrary code, so we must be careful to call only safe functions in any code reachable from a Í©×ÄÇÅ×ÁÀÉ. In our example, we call the safe Í©ÇˆÉÏÎÍ and ÍÄ−−É functions. The unsafe −Ó©Î function is unreachable. 8.7 Tools for Manipulating Processes Linux systems provide a number of useful tools for monitoring and manipulating processes: strace. Prints a trace of each system call invoked by a running program and its children. It is a fascinating tool for the curious student. Compile your program with kÍÎþÎ©² to get a cleaner trace without a lot of output related to shared libraries. ps. Lists processes (including zombies) currently in the system. top. Prints information about the resource usage of current processes. pmap. Displays the memory map of a process. mÉËÇ². A virtual ﬁlesystem that exports the contents of numerous kernel data structures in an ASCII text form that can be read by user programs. For example, type ²þÎ mÉËÇ²mÄÇþ®þÌ× to see the current load average on your Linux system. Bibliographic Notes 823 8.8 Summary Exceptional control ﬂow (ECF) occurs at all levels of a computer system and is a basic mechanism for providing concurrency in a computer system. At the hardware level, exceptions are abrupt changes in the control ﬂow that are triggered by events in the processor. The control ﬂow passes to a software handler, which does some processing and then returns control to the interrupted control ﬂow. There are four different types of exceptions: interrupts, faults, aborts, and traps. Interrupts occur asynchronously (with respect to any instructions) when an external I/O device such as a timer chip or a disk controller sets the in- terrupt pin on the processor chip. Control returns to the instruction follow- ing the faulting instruction. Faults and aborts occur synchronously as the re- sult of the execution of an instruction. Fault handlers restart the faulting in- struction, while abort handlers never return control to the interrupted ﬂow. Finally, traps are like function calls that are used to implement the system calls that provide applications with controlled entry points into the operating sys- tem code. At the operating system level, the kernel uses ECF to provide the funda- mental notion of a process. A process provides applications with two important abstractions: (1) logical control ﬂows that give each program the illusion that it has exclusive use of the processor, and (2) private address spaces that provide the illusion that each program has exclusive use of the main memory. At the interface between the operating system and applications, applications can create child processes, wait for their child processes to stop or terminate, run new programs, and catch signals from other processes. The semantics of signal handling is subtle and can vary from system to system. However, mechanisms exist on Posix-compliant systems that allow programs to clearly specify the expected signal-handling semantics. Finally, at the application level, C programs can use nonlocal jumps to bypass the normal call/return stack discipline and branch directly from one function to another. Bibliographic Notes Kerrisk is the essential reference for all aspects of programming in the Linux environment [62]. The Intel ISA speciﬁcation contains a detailed discussion of exceptions and interrupts on Intel processors [50]. Operating systems texts [102, 106, 113] contain additional information on exceptions, processes, and signals. The classic work by W. Richard Stevens [111] is a valuable and highly readable description of how to work with processes and signals from application programs. Bovet and Cesati [11] give a wonderfully clear description of the Linux kernel, including details of the process and signal implementations. 824 Chapter 8 Exceptional Control Flow Homework Problems 8.9 ◆ Consider four processes with the following starting and ending times: Process Start time End time A6 8 B3 5 C4 7 D2 9 For each pair of processes, indicate whether they run concurrently (Y) or not (N): Process pair Concurrent? AB AC AD BC BD CD 8.10 ◆ In this chapter, we have introduced some functions with unusual call and return behaviors: ×−Î−ÅÌ, Í−Î−ÅÌ, ÏÅÍ−Î−ÅÌ, and −Ó−²Ì−. Match each function with one of the following behaviors: A. Called once, returns only if there is an error B. Called once, returns nothing C. Called once, returns either a pointer or ﬁ•‹‹ 8.11 ◆ How many “Example” output lines does this program print? code/ecf/global-forkprob1.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ ©y 6 7 ðÇËf©{qy©|ny ©kkg 8 ƒÇËÂfgy 9 ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 10 −Ó©Îfngy 11 Û code/ecf/global-forkprob1.c Homework Problems 825 8.12 ◆ How many “Example” output lines does this program print? code/ecf/global-forkprob4.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® ÎËÔfg 4 Õ 5 ƒÇËÂfgy 6 ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 7 ƒÇËÂfgy 8 Ë−ÎÏËÅy 9 Û 10 11 ©ÅÎ Àþ©Åfg 12 Õ 13 ÎËÔfgy ƒÇËÂfgy 14 ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 15 −Ó©Îfngy 16 Û code/ecf/global-forkprob4.c 8.13 ◆ What is one possible output of the following program? code/ecf/global-forkprob3.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎþ{sy 6 7 ©ð fƒÇËÂfg _{ ng 8 ÉË©ÅÎðf‘þ{c®¿Å‘j kkþgy 9 10 ÉË©ÅÎðf‘þ{c®¿Å‘j iiþgy 11 −Ó©Îfngy 12 Û code/ecf/global-forkprob3.c 8.14 ◆ How many “Example” output lines does this program print? code/ecf/global-forkprob5.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® ÎËÔfg 4 Õ 826 Chapter 8 Exceptional Control Flow 5 ©ð fƒÇËÂfg _{ ng Õ 6 ƒÇËÂfgy 7 ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 8 −Ó©Îfngy 9 Û 10 Ë−ÎÏËÅy 11 Û 12 13 ©ÅÎ Àþ©Åfg 14 Õ 15 ÎËÔfgy 16 ðÇËÂfgy ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 17 −Ó©Îfngy 18 Û code/ecf/global-forkprob5.c 8.15 ◆ How many “Example” lines does this program print? code/ecf/global-forkprob6.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® ÎËÔfg 4 Õ 5 ©ð fƒÇËÂfg {{ ng Õ 6 ƒÇËÂfgy ƒÇËÂfgy 7 ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 8 Ë−ÎÏËÅy 9 Û 10 Ë−ÎÏËÅy 11 Û 12 13 ©ÅÎ Àþ©Åfg 14 Õ 15 ÎËÔfgy 16 ÉË©ÅÎðf‘¥ÓþÀÉÄ−¿Å‘gy 17 −Ó©Îfngy 18 Û code/ecf/global-forkprob6.c 8.16 ◆ What is the output of the following program? code/ecf/global-forkprob7.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 ©ÅÎ ²ÇÏÅÎ−Ë { oy 3 Homework Problems 827 4 ©ÅÎ Àþ©Åfg 5 Õ 6 ©ð fðÇËÂfg {{ ng Õ 7 ²ÇÏÅÎ−Ëiiy 8 −Ó©Îfngy 9 Û 10 −ÄÍ− Õ 11 „þ©Îfﬁ•‹‹gy 12 ²ÇÏÅÎ−Ëiiy ÉË©ÅÎðf‘²ÇÏÅÎ−Ë { c®¿Å‘j ²ÇÏÅÎ−Ëgy 13 Û 14 −Ó©Îfngy 15 Û code/ecf/global-forkprob7.c 8.17 ◆ Enumerate all of the possible outputs of the program in Practice Problem 8.4. 8.18 ◆◆ Consider the following program: code/ecf/forkprob2.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® −Å®fÌÇ©®g 4 Õ 5 ÉË©ÅÎðf‘p‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 6 Û 7 8 ©ÅÎ Àþ©Åfg 9 Õ 10 ©ð fƒÇËÂfg {{ ng 11 þÎ−Ó©Îf−Å®gy 12 ©ð fƒÇËÂfg {{ ng Õ 13 ÉË©ÅÎðf‘n‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 14 Û 15 −ÄÍ− Õ 16 ÉË©ÅÎðf‘o‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 17 Û 18 −Ó©Îfngy 19 Û code/ecf/forkprob2.c Determine which of the following outputs are possible. Note: The þÎ−Ó©Î function takes a pointer to a function and adds it to a list of functions (initially empty) that will be called when the −Ó©Î function is called. A. 112002 B. 211020 828 Chapter 8 Exceptional Control Flow C. 102120 D. 122001 E. 100212 8.19 ◆◆ How many lines of output does the following function print if the value of n entered by the user is 6? code/ecf/global-forkprob8.c 1 ÌÇ©® ðÇÇf©ÅÎ Åg 2 Õ 3 ©ÅÎ ©y 4 5 ðÇËf©{Åkoy©|{ny©k{pg 6 ƒÇËÂfgy 7 ÉË©ÅÎðf‘³−ÄÄÇ¿Å‘gy 8 −Ó©Îfngy 9 Û code/ecf/global-forkprob8.c 8.20 ◆◆ Use −Ó−²Ì− to write a program called ÀÔÄÍ whose behavior is identical to the m¾©ÅmÄÍ program. Your program should accept the same command-line argu- ments, interpret the identical environment variables, and produce the identical output. The ÄÍ program gets the width of the screen from the COLUMNS environ- ment variable. If COLUMNS is unset, then ÄÍ assumes that the screen is 80 columns wide. Thus, you can check your handling of the environment variables by setting the COLUMNS environment to something less than 80: Ä©ÅÏÓ| setenv COLUMNS 40 Ä©ÅÏÓ| ./myls l l l // Output is 40 columns wide Ä©ÅÏÓ| unsetenv COLUMNS Ä©ÅÏÓ| ./myls l l l // Output is now 80 columns wide 8.21 ◆◆ What are the possible output sequences from the following program? code/ecf/global-waitprob3.c 1 ©ÅÎ Àþ©Åfg 2 Õ 3 ÉË©ÅÎðf‘É‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy Homework Problems 829 4 ©ð fðÇËÂfg _{ ng Õ 5 ÉË©ÅÎðf‘Ê‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 6 Ë−ÎÏËÅ ny 7 Û 8 −ÄÍ− Õ 9 ÉË©ÅÎðf‘Ë‘gy ððÄÏÍ³fÍÎ®ÇÏÎgy 10 Ñþ©ÎÉ©®fkoj ﬁ•‹‹j ngy 11 Û 12 Ë−ÎÏËÅ ny 13 Û code/ecf/global-waitprob3.c 8.22 ◆◆◆ Write your own version of the Unix ÍÔÍÎ−À function ©ÅÎ ÀÔÍÔÍÎ−Àf²³þË h²ÇÀÀþÅ®gy The ÀÔÍÔÍÎ−À function executes ²ÇÀÀþÅ® by invoking m¾©ÅmÍ³ k² ²ÇÀÀþÅ®, and then returns after ²ÇÀÀþÅ® has completed. If ²ÇÀÀþÅ® exits normally (by calling the −Ó©Î function or executing a Ë−ÎÏËÅ statement), then ÀÔÍÔÍÎ−À returns the ²ÇÀÀþÅ® exit status. For example, if ²ÇÀÀþÅ® terminates by calling −Ó©Îfvg, then ÀÔÍÔÍÎ−À returns the value 8. Otherwise, if ²ÇÀÀþÅ® terminates abnormally, then ÀÔÍÔÍÎ−À returns the status returned by the shell. 8.23 ◆◆ One of your colleagues is thinking of using signals to allow a parent process to count events that occur in a child process. The idea is to notify the parent each time an event occurs by sending it a signal and letting the parent’s signal handler increment a global ²ÇÏÅÎ−Ë variable, which the parent can then inspect after the child has terminated. However, when he runs the test program in Figure 8.45 on his system, he discovers that when the parent calls ÉË©ÅÎð, ²ÇÏÅÎ−Ë always has a value of 2, even though the child has sent ﬁve signals to the parent. Perplexed, he comes to you for help. Can you explain the bug? 8.24 ◆◆◆ Modify the program in Figure 8.18 so that the following two conditions are met: 1. Each child terminates abnormally after attempting to write to a location in the read-only text segment. 2. The parent prints output that is identical (except for the PIDs) to the fol- lowing: ²³©Ä® oppss Î−ËÀ©ÅþÎ−® ¾Ô Í©×ÅþÄ oox ·−×À−ÅÎþÎ©ÇÅ ðþÏÄÎ ²³©Ä® oppsr Î−ËÀ©ÅþÎ−® ¾Ô Í©×ÅþÄ oox ·−×À−ÅÎþÎ©ÇÅ ðþÏÄÎ Hint: Read the ÀþÅ page for ÉÍ©×ÅþÄfqg. 830 Chapter 8 Exceptional Control Flow code/ecf/counterprob.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ ²ÇÏÅÎ−Ë { ny 4 5 ÌÇ©® ³þÅ®Ä−Ëf©ÅÎ Í©×g 6 Õ 7 ²ÇÏÅÎ−Ëiiy 8 ÍÄ−−Éfogy mh ⁄Ç ÍÇÀ− ÑÇËÂ ©Å Î³− ³þÅ®Ä−Ë hm 9 Ë−ÎÏËÅy 10 Û 11 12 ©ÅÎ Àþ©Åfg 13 Õ 14 ©ÅÎ ©y 15 16 ·©×ÅþÄf·'§•·‡pj ³þÅ®Ä−Ëgy 17 18 ©ð fƒÇËÂfg {{ ng Õ mh £³©Ä® hm 19 ðÇËf©{ny©zsy ©iig Õ 20 «©ÄÄf×−ÎÉÉ©®fgj ·'§•·‡pgy 21 ÉË©ÅÎðf‘Í−ÅÎ ·'§•·‡p ÎÇ ÉþË−ÅÎ¿Å‘gy 22 Û 23 −Ó©Îfngy 24 Û 25 26 „þ©Îfﬁ•‹‹gy 27 ÉË©ÅÎðf‘²ÇÏÅÎ−Ë{c®¿Å‘j ²ÇÏÅÎ−Ëgy 28 −Ó©Îfngy 29 Û code/ecf/counterprob.c Figure 8.45 Counter program referenced in Problem 8.23. 8.25 ◆◆◆ Write a version of the ð×−ÎÍ function, called Îð×−ÎÍ, that times out after 5 seconds. The Îð×−ÎÍ function accepts the same inputs as ð×−ÎÍ. If the user doesn’t type an input line within 5 seconds, Îð×−ÎÍ returns NULL. Otherwise, it returns a pointer to the input line. 8.26 ◆◆◆◆ Using the example in Figure 8.23 as a starting point, write a shell program that supports job control. Your shell should have the following features: . The command line typed by the user consists of a ÅþÀ− and zero or more argu- ments, all separated by one or more spaces. If ÅþÀ− is a built-in command, the Solutions to Practice Problems 831 shell handles it immediately and waits for the next command line. Otherwise, the shell assumes that ÅþÀ− is an executable ﬁle, which it loads and runs in the context of an initial child process (job). The process group ID for the job is identical to the PID of the child. . Each job is identiﬁed by either a process ID (PID) or a job ID (JID), which is a small arbitrary positive integer assigned by the shell. JIDs are denoted on the command line by the preﬁx ‘c’. For example, ‘cs’ denotes JID 5, and ‘s’ denotes PID 5. . If the command line ends with an ampersand, then the shell runs the job in the background. Otherwise, the shell runs the job in the foreground. . Typing Ctrl+C (Ctrl+Z) causes the kernel to send a SIGINT (SIGTSTP) signal to your shell, which then forwards it to every process in the foreground process group.2 . The ÁÇ¾Í built-in command lists all background jobs. . The ¾× job built-in command restarts job by sending it a SIGCONT signal and then runs it in the background. The job argument can be either a PID or a JID. . The ð× job built-in command restarts job by sending it a SIGCONT signal and then runs it in the foreground. . The shell reaps all of its zombie children. If any job terminates because it receives a signal that was not caught, then the shell prints a message to the terminal with the job’s PID and a description of the offending signal. Figure 8.46 shows an example shell session. Solutions to Practice Problems Solution to Problem 8.1 (page 770) Processes A and B are concurrent with respect to each other, as are B and C, because their respective executions overlap—that is, one process starts before the other ﬁnishes. Processes A and C are not concurrent because their executions do not overlap; A ﬁnishes before C begins. Solution to Problem 8.2 (page 779) In our example program in Figure 8.15, the parent and child execute disjoint sets of instructions. However, in this program, the parent and child execute nondisjoint sets of instructions, which is possible because the parent and child have identical code segments. This can be a difﬁcult conceptual hurdle, so be sure you understand the solution to this problem. Figure 8.47 shows the process graph. 2. Note that this is a simpliﬁcation of the way that real shells work. With real shells, the kernel responds to Ctrl+C (Ctrl+Z) by sending SIGINT (SIGTSTP) directly to each process in the terminal foreground process group. The shell manages the membership of this group using the Î²Í−ÎÉ×ËÉ function, and manages the attributes of the terminal using the Î²Í−ÎþÎÎË function, both of which are outside the scope of this book. See [62] for details. 832 Chapter 8 Exceptional Control Flow Ä©ÅÏÓ| ./shell Run your shell program |bogus ¾Ç×ÏÍx £ÇÀÀþÅ® ÅÇÎ ðÇÏÅ®l Execve can’t find executable |foo 10 “Ç¾ snqs Î−ËÀ©ÅþÎ−® ¾Ô Í©×ÅþÄx 'ÅÎ−ËËÏÉÎ User types Ctrl+C |foo 100 & ‰o` snqt ðÇÇ onn d |foo 200 & ‰p` snqu ðÇÇ pnn d |jobs ‰o` snqt ‡ÏÅÅ©Å× ðÇÇ onn d ‰p` snqu ‡ÏÅÅ©Å× ðÇÇ pnn d |fg %1 “Ç¾ ‰o` snqt ÍÎÇÉÉ−® ¾Ô Í©×ÅþÄx ·ÎÇÉÉ−® User types Ctrl+Z |jobs ‰o` snqt ·ÎÇÉÉ−® ðÇÇ onn d ‰p` snqu ‡ÏÅÅ©Å× ðÇÇ pnn d |bg 5035 snqsx ﬁÇ ÍÏ²³ ÉËÇ²−ÍÍ |bg 5036 ‰o` snqt ðÇÇ onn d |/bin/kill 5036 “Ç¾ snqt Î−ËÀ©ÅþÎ−® ¾Ô Í©×ÅþÄx ¶−ËÀ©ÅþÎ−® | fg %2 Wait for fg job to finish |quit Ä©ÅÏÓ| Back to the Unix shell Figure 8.46 Sample shell session for Problem 8.26. Figure 8.47 Process graph for Practice Problem 8.2. forkforkmainmain printfprintf exitexit p1: a=8 p2: a=9 Child Parent a==9 p2: a=10 printfprintf printfprintf exitexit A. The key idea here is that the child executes both ÉË©ÅÎð statements. After the ðÇËÂ returns, it executes the ÉË©ÅÎð in line 6. Then it falls out of the ©ð statement and executes the ÉË©ÅÎð in line 7. Here is the output produced by the child: Éox þ{v Épx þ{w B. The parent executes only the ÉË©ÅÎð in line 7: Éox þ{on Solutions to Practice Problems 833 Figure 8.48 Process graph for Practice Problem 8.3. forkforkmainmain printfprintf waitpidwaitpid 93 03 printfprintf 6 printfprintf exitexit printfprintf printfprintf 6 printfprintf exitexit Figure 8.49 Process graph for Practice Problem 8.4. forkforkmainmain printfprintfprintfprintf waitpidwaitpid 0 Stop 1Start 2 printfprintf Stop printfprintf exitexit printfprintf printfprintf Child printfprintf exit(2)exit(2) Solution to Problem 8.3 (page 781) We know that the sequences 936036, 903636, and 093636 are possible because they correspond to topological sorts of the process graph (Figure 8.48). However, sequences such as 036936 and 360369 do not correspond to any topological sort and thus are not feasible. Solution to Problem 8.4 (page 784) A. We can determine the number of lines of output by simply counting the number of ÉË©ÅÎð vertices in the process graph (Figure 8.49). In this case, there are seven such vertices, and thus the program will print seven lines of output. B. Any output sequence corresponding to a topological sort of the graph is possible. For example: ·ÎþËÎ, n, o, £³©Ä®, ·ÎÇÉ, p, ·ÎÇÉ is possible. Solution to Problem 8.5 (page 786) code/ecf/global-snooze.c 1 ÏÅÍ©×Å−® ©ÅÎ ÑþÂ−ÏÉfÏÅÍ©×Å−® ©ÅÎ Í−²Íg Õ 2 ÏÅÍ©×Å−® ©ÅÎ Ë² { ÍÄ−−ÉfÍ−²Így 3 4 ÉË©ÅÎðf‘„ÇÂ− ÏÉ þÎ c® Í−²Íl¿Å‘j Í−²ÍkË²iogy 5 Ë−ÎÏËÅ Ë²y 6 Û code/ecf/global-snooze.c Solution to Problem 8.6 (page 788) code/ecf/myecho.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hþË×Ì‰`j ²³þË h−ÅÌÉ‰`g 4 Õ 5 ©ÅÎ ©y 6 7 ÉË©ÅÎðf‘£ÇÀÀþÅ®kÄ©Å− þË×ÏÀ−ÅÎÍx¿Å‘gy 834 Chapter 8 Exceptional Control Flow 8 ðÇË f©{ny þË×Ì‰©` _{ ﬁ•‹‹y ©iig 9 ÉË©ÅÎðf‘ þË×Ì‰cp®`x cÍ¿Å‘j ©j þË×Ì‰©`gy 10 11 ÉË©ÅÎðf‘¿Å‘gy 12 ÉË©ÅÎðf‘¥ÅÌ©ËÇÅÀ−ÅÎ ÌþË©þ¾Ä−Íx¿Å‘gy 13 ðÇË f©{ny −ÅÌÉ‰©` _{ ﬁ•‹‹y ©iig 14 ÉË©ÅÎðf‘ −ÅÌÉ‰cp®`x cÍ¿Å‘j ©j −ÅÌÉ‰©`gy 15 16 −Ó©Îfngy 17 Û code/ecf/myecho.c Solution to Problem 8.7 (page 800) The ÍÄ−−É function returns prematurely whenever the sleeping process receives a signal that is not ignored. But since the default action upon receipt of a SIGINT is to terminate the process (Figure 8.26), we must install a SIGINT handler to allow the ÍÄ−−É function to return. The handler simply catches the SIGNAL and returns control to the ÍÄ−−É function, which returns immediately. code/ecf/snooze.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 mh ·'§'ﬁ¶ ³þÅ®Ä−Ë hm 4 ÌÇ©® ³þÅ®Ä−Ëf©ÅÎ Í©×g 5 Õ 6 Ë−ÎÏËÅy mh £þÎ²³ Î³− Í©×ÅþÄ þÅ® Ë−ÎÏËÅ hm 7 Û 8 9 ÏÅÍ©×Å−® ©ÅÎ ÍÅÇÇÖ−fÏÅÍ©×Å−® ©ÅÎ Í−²Íg Õ 10 ÏÅÍ©×Å−® ©ÅÎ Ë² { ÍÄ−−ÉfÍ−²Így 11 12 ÉË©ÅÎðf‘·Ä−ÉÎ ðÇË c® Çð c® Í−²Íl¿Å‘j Í−²ÍkË²j Í−²Így 13 Ë−ÎÏËÅ Ë²y 14 Û 15 16 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg Õ 17 18 ©ð fþË×² _{ pg Õ 19 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÍ−²Í|¿Å‘j þË×Ì‰n`gy 20 −Ó©Îfngy 21 Û 22 23 ©ð fÍ©×ÅþÄf·'§'ﬁ¶j ³þÅ®Ä−Ëg {{ ·'§ˆ¥‡‡g mh 'ÅÍÎþÄÄ ·'§'ﬁ¶ hm 24 ÏÅ©Óˆ−ËËÇËf‘Í©×ÅþÄ −ËËÇË¿Å‘gy mh ³þÅ®Ä−Ë hm 25 fÌÇ©®gÍÅÇÇÖ−fþÎÇ©fþË×Ì‰o`ggy Solutions to Practice Problems 835 26 −Ó©Îfngy 27 Û code/ecf/snooze.c Solution to Problem 8.8 (page 809) This program prints the string poq, which is the shorthand name of the CS:APP course at Carnegie Mellon. The parent starts by printing ‘p’, then forks the child, which spins in an inﬁnite loop. The parent then sends a signal to the child and waits for it to terminate. The child catches the signal (interrupting the inﬁnite loop), decrements the counter (from an initial value of 2), prints ‘o’, and then terminates. After the parent reaps the child, it increments the counter (from an initial value of 2), prints ‘q’, and terminates. This page is intentionally left blank. CHAPTER 9 Virtual Memory 9.1 Physical and Virtual Addressing 839 9.2 Address Spaces 840 9.3 VM as a Tool for Caching 841 9.4 VM as a Tool for Memory Management 847 9.5 VM as a Tool for Memory Protection 848 9.6 Address Translation 849 9.7 Case Study: The Intel Core i7/Linux Memory System 861 9.8 Memory Mapping 869 9.9 Dynamic Memory Allocation 875 9.10 Garbage Collection 901 9.11 Common Memory-Related Bugs in C Programs 906 9.12 Summary 911 Bibliographic Notes 912 Homework Problems 912 Solutions to Practice Problems 916 837 838 Chapter 9 Virtual Memory P rocesses in a system share the CPU and main memory with other processes. However, sharing the main memory poses some special challenges. As demand on the CPU increases, processes slow down in some reasonably smooth way. But if too many processes need too much memory, then some of them will simply not be able to run. When a program is out of space, it is out of luck. Memory is also vulnerable to corruption. If some process inadvertently writes to the memory used by another process, that process might fail in some bewildering fashion totally unrelated to the program logic. In order to manage memory more efﬁciently and with fewer errors, modern systems provide an abstraction of main memory known as virtual memory (VM). Virtual memory is an elegant interaction of hardware exceptions, hardware ad- dress translation, main memory, disk ﬁles, and kernel software that provides each process with a large, uniform, and private address space. With one clean mech- anism, virtual memory provides three important capabilities: (1) It uses main memory efﬁciently by treating it as a cache for an address space stored on disk, keeping only the active areas in main memory and transferring data back and forth between disk and memory as needed. (2) It simpliﬁes memory management by providing each process with a uniform address space. (3) It protects the address space of each process from corruption by other processes. Virtual memory is one of the great ideas in computer systems. A major reason for its success is that it works silently and automatically, without any intervention from the application programmer. Since virtual memory works so well behind the scenes, why would a programmer need to understand it? There are several reasons. . Virtual memory is central. Virtual memory pervades all levels of computer systems, playing key roles in the design of hardware exceptions, assemblers, linkers, loaders, shared objects, ﬁles, and processes. Understanding virtual memory will help you better understand how systems work in general. . Virtual memory is powerful. Virtual memory gives applications powerful ca- pabilities to create and destroy chunks of memory, map chunks of memory to portions of disk ﬁles, and share memory with other processes. For example, did you know that you can read or modify the contents of a disk ﬁle by reading and writing memory locations? Or that you can load the contents of a ﬁle into memory without doing any explicit copying? Understanding virtual memory will help you harness its powerful capabilities in your applications. . Virtual memory is dangerous. Applications interact with virtual memory ev- ery time they reference a variable, dereference a pointer, or make a call to a dynamic allocation package such as ÀþÄÄÇ². If virtual memory is used improp- erly, applications can suffer from perplexing and insidious memory-related bugs. For example, a program with a bad pointer can crash immediately with a “segmentation fault” or a “protection fault,” run silently for hours before crashing, or scariest of all, run to completion with incorrect results. Under- standing virtual memory, and the allocation packages such as ÀþÄÄÇ² that manage it, can help you avoid these errors. Section 9.1 Physical and Virtual Addressing 839 This chapter looks at virtual memory from two angles. The ﬁrst half of the chapter describes how virtual memory works. The second half describes how virtual memory is used and managed by applications. There is no avoiding the fact that VM is complicated, and the discussion reﬂects this in places. The good news is that if you work through the details, you will be able to simulate the virtual memory mechanism of a small system by hand, and the virtual memory idea will be forever demystiﬁed. The second half builds on this understanding, showing you how to use and manage virtual memory in your programs. You will learn how to manage virtual memory via explicit memory mapping and calls to dynamic storage allocators such as the ÀþÄÄÇ² package. You will also learn about a host of common memory- related errors in C programs and how to avoid them. 9.1 Physical and Virtual Addressing The main memory of a computer system is organized as an array of M contiguous byte-size cells. Each byte has a unique physical address (PA). The ﬁrst byte has an address of 0, the next byte an address of 1, the next byte an address of 2, and so on. Given this simple organization, the most natural way for a CPU to access memory would be to use physical addresses. We call this approach physical addressing. Figure 9.1 shows an example of physical addressing in the context of a load instruction that reads the 4-byte word starting at physical address 4. When the CPU executes the load instruction, it generates an effective physical address and passes it to main memory over the memory bus. The main memory fetches the 4-byte word starting at physical address 4 and returns it to the CPU, which stores it in a register. Early PCs used physical addressing, and systems such as digital signal pro- cessors, embedded microcontrollers, and Cray supercomputers continue to do so. However, modern processors use a form of addressing known as virtual address- ing, as shown in Figure 9.2. Figure 9.1 A system that uses physical addressing.. . . Main memory 0: 1: 2: 3: 4: 5: 6: 7: 8: Physical address (PA) CPU 4 M\u00021: Data word 840 Chapter 9 Virtual Memory Figure 9.2 A system that uses virtual addressing. Main memory 0: 1: 2: 3: 4: 5: 6: 7: Physical address (PA) Virtual address (VA) Address translation CPU CPU chip MMU 4100 4 M\u00021: Data word. . . With virtual addressing, the CPU accesses main memory by generating a vir- tual address (VA), which is converted to the appropriate physical address before being sent to main memory. The task of converting a virtual address to a physical one is known as address translation. Like exception handling, address translation requires close cooperation between the CPU hardware and the operating sys- tem. Dedicated hardware on the CPU chip called the memory management unit (MMU) translates virtual addresses on the ﬂy, using a lookup table stored in main memory whose contents are managed by the operating system. 9.2 Address Spaces An address space is an ordered set of nonnegative integer addresses {0, 1, 2,...} If the integers in the address space are consecutive, then we say that it is a linear address space. To simplify our discussion, we will always assume linear address spaces. In a system with virtual memory, the CPU generates virtual addresses from an address space of N = 2n addresses called the virtual address space: {0, 1, 2,...,N − 1} The size of an address space is characterized by the number of bits that are needed to represent the largest address. For example, a virtual address space with N = 2n addresses is called an n-bit address space. Modern systems typically support either 32-bit or 64-bit virtual address spaces. A system also has a physical address space that corresponds to the M bytes of physical memory in the system: {0, 1, 2,...,M − 1} M is not required to be a power of 2, but to simplify the discussion, we will assume that M = 2m. Section 9.3 VM as a Tool for Caching 841 The concept of an address space is important because it makes a clean dis- tinction between data objects (bytes) and their attributes (addresses). Once we recognize this distinction, then we can generalize and allow each data object to have multiple independent addresses, each chosen from a different address space. This is the basic idea of virtual memory. Each byte of main memory has a virtual address chosen from the virtual address space, and a physical address chosen from the physical address space. Practice Problem 9.1 (solution page 916) Complete the following table, ﬁlling in the missing entries and replacing each question mark with the appropriate integer. Use the following units: K = 210 (kilo), M = 220 (mega), G = 230 (giga), T = 240 (tera), P = 250 (peta), or E = 260 (exa). Number of Number of virtual address bits (n) virtual addresses (N ) Largest possible virtual address 4 2? = 16 K 224 − 1 =?M − 1 2? = 64 T 54 9.3 VM as a Tool for Caching Conceptually, a virtual memory is organized as an array of N contiguous byte-size cells stored on disk. Each byte has a unique virtual address that serves as an index into the array. The contents of the array on disk are cached in main memory. As with any other cache in the memory hierarchy, the data on disk (the lower level) is partitioned into blocks that serve as the transfer units between the disk and the main memory (the upper level). VM systems handle this by partitioning the virtual memory into ﬁxed-size blocks called virtual pages (VPs). Each virtual page is P = 2p bytes in size. Similarly, physical memory is partitioned into physical pages (PPs), also P bytes in size. (Physical pages are also referred to as page frames.) At any point in time, the set of virtual pages is partitioned into three disjoint subsets: Unallocated. Pages that have not yet been allocated (or created) by the VM system. Unallocated blocks do not have any data associated with them, and thus do not occupy any space on disk. Cached. Allocated pages that are currently cached in physical memory. Uncached. Allocated pages that are not cached in physical memory. The example in Figure 9.3 shows a small virtual memory with eight virtual pages. Virtual pages 0 and 3 have not been allocated yet, and thus do not yet exist 842 Chapter 9 Virtual Memory Figure 9.3 How a VM system uses main memory as a cache. VP 0 VP 1 PP 0 PP 1 PP 2 m–p – 1 VP 2 n–p – 1 Unallocated Virtual memory Physical memory Virtual pages (VPs) stored on disk Physical pages (PPs) cached in DRAM Cached Uncached Unallocated Cached Uncached Empty Empty Empty N – 1 M – 1 0 0 Cached Uncached on disk. Virtual pages 1, 4, and 6 are cached in physical memory. Pages 2, 5, and 7 are allocated but are not currently cached in physical memory. 9.3.1 DRAM Cache Organization To help us keep the different caches in the memory hierarchy straight, we will use the term SRAM cache to denote the L1, L2, and L3 cache memories between the CPU and main memory, and the term DRAM cache to denote the VM system’s cache that caches virtual pages in main memory. The position of the DRAM cache in the memory hierarchy has a big impact on the way that it is organized. Recall that a DRAM is at least 10 times slower than an SRAM and that disk is about 100,000 times slower than a DRAM. Thus, misses in DRAM caches are very expensive compared to misses in SRAM caches because DRAM cache misses are served from disk, while SRAM cache misses are usually served from DRAM-based main memory. Further, the cost of reading the ﬁrst byte from a disk sector is about 100,000 times slower than reading successive bytes in the sector. The bottom line is that the organization of the DRAM cache is driven entirely by the enormous cost of misses. Because of the large miss penalty and the expense of accessing the ﬁrst byte, virtual pages tend to be large—typically 4 KB to 2 MB. Due to the large miss penalty, DRAM caches are fully associative; that is, any virtual page can be placed in any physical page. The replacement policy on misses also assumes greater importance, because the penalty associated with replacing the wrong virtual page is so high. Thus, operating systems use much more sophisticated replacement algorithms for DRAM caches than the hardware does for SRAM caches. (These replacement algorithms are beyond our scope here.) Finally, because of the large access time of disk, DRAM caches always use write-back instead of write-through. 9.3.2 Page Tables As with any cache, the VM system must have some way to determine if a virtual page is cached somewhere in DRAM. If so, the system must determine which physical page it is cached in. If there is a miss, the system must determine Section 9.3 VM as a Tool for Caching 843 Figure 9.4 Page table. PTE 0 PP 0 PP 3 1 1 0 1 0 0 1 0 PTE 7 Null VP 1 VP 4 VP 7 VP 2 VP 1 VP 2 VP 3 VP 4 VP 6 VP 7 Null Physical page number or disk address Memory-resident page table (DRAM) Virtual memory (disk) Physical memory (DRAM) Valid where the virtual page is stored on disk, select a victim page in physical memory, and copy the virtual page from disk to DRAM, replacing the victim page. These capabilities are provided by a combination of operating system soft- ware, address translation hardware in the MMU (memory management unit), and a data structure stored in physical memory known as a page table that maps vir- tual pages to physical pages. The address translation hardware reads the page table each time it converts a virtual address to a physical address. The operating system is responsible for maintaining the contents of the page table and transferring pages back and forth between disk and DRAM. Figure 9.4 shows the basic organization of a page table. A page table is an array of page table entries (PTEs). Each page in the virtual address space has a PTE at a ﬁxed offset in the page table. For our purposes, we will assume that each PTE consists of a valid bit and an n-bit address ﬁeld. The valid bit indicates whether the virtual page is currently cached in DRAM. If the valid bit is set, the address ﬁeld indicates the start of the corresponding physical page in DRAM where the virtual page is cached. If the valid bit is not set, then a null address indicates that the virtual page has not yet been allocated. Otherwise, the address points to the start of the virtual page on disk. The example in Figure 9.4 shows a page table for a system with eight virtual pages and four physical pages. Four virtual pages (VP 1, VP 2, VP 4, and VP 7) are currently cached in DRAM. Two pages (VP 0 and VP 5) have not yet been allocated, and the rest (VP 3 and VP 6) have been allocated but are not currently cached. An important point to notice about Figure 9.4 is that because the DRAM cache is fully associative, any physical page can contain any virtual page. Practice Problem 9.2 (solution page 917) Determine the number of page table entries (PTEs) that are needed for the following combinations of virtual address size (n) and page size (P ): 844 Chapter 9 Virtual Memory nP = 2p Number of PTEs 12 1 K 16 16 K 24 2 M 36 1 G 9.3.3 Page Hits Consider what happens when the CPU reads a word of virtual memory contained in VP 2, which is cached in DRAM (Figure 9.5). Using a technique we will describe in detail in Section 9.6, the address translation hardware uses the virtual address as an index to locate PTE 2 and read it from memory. Since the valid bit is set, the address translation hardware knows that VP 2 is cached in memory. So it uses the physical memory address in the PTE (which points to the start of the cached page in PP 1) to construct the physical address of the word. 9.3.4 Page Faults In virtual memory parlance, a DRAM cache miss is known as a page fault. Fig- ure 9.6 shows the state of our example page table before the fault. The CPU has referenced a word in VP 3, which is not cached in DRAM. The address transla- tion hardware reads PTE 3 from memory, infers from the valid bit that VP 3 is not cached, and triggers a page fault exception. The page fault exception invokes a page fault exception handler in the kernel, which selects a victim page—in this case, VP 4 stored in PP 3. If VP 4 has been modiﬁed, then the kernel copies it back to disk. In either case, the kernel modiﬁes the page table entry for VP 4 to reﬂect the fact that VP 4 is no longer cached in main memory. Figure 9.5 VM page hit. The reference to a word in VP 2 is a hit. PTE 0 PP 0 PP 3 1 1 0 1 0 0 1 0 PTE 7 Null VP 1 VP 4 VP 7 VP 2 VP 1 VP 2 VP 3 VP 4 VP 6 VP 7 Null Physical page number or disk address Memory-resident page table (DRAM) Virtual memory (disk) Physical memory (DRAM)Virtual address Valid Section 9.3 VM as a Tool for Caching 845 Figure 9.6 VM page fault (before). The reference to a word in VP 3 is a miss and triggers a page fault. PTE 0 PP 0 PP 3 1 1 0 1 0 0 1 0 PTE 7 Null VP 1 VP 4 VP 7 VP 2 VP 1 VP 2 VP 3 VP 4 VP 6 VP 7 Null Physical page number or disk address Memory-resident page table (DRAM) Virtual memory (disk) Physical memory (DRAM)Virtual address Valid Figure 9.7 VM page fault (after). The page fault handler selects VP 4 as the victim and replaces it with a copy of VP 3 from disk. After the page fault handler restarts the faulting instruction, it will read the word from memory normally, without generating an exception. PTE 0 PP 0 PP 3 1 1 1 0 0 0 1 0 PTE 7 Null VP 1 VP 3 VP 7 VP 2 VP 1 VP 2 VP 3 VP 4 VP 6 VP 7 Null Physical page number or disk address Memory-resident page table (DRAM) Virtual memory (disk) Physical memory (DRAM) Valid Virtual address Next, the kernel copies VP 3 from disk to PP 3 in memory, updates PTE 3, and then returns. When the handler returns, it restarts the faulting instruction, which resends the faulting virtual address to the address translation hardware. But now, VP 3 is cached in main memory, and the page hit is handled normally by the address translation hardware. Figure 9.7 shows the state of our example page table after the page fault. Virtual memory was invented in the early 1960s, long before the widening CPU-memory gap spawned SRAM caches. As a result, virtual memory systems use a different terminology from SRAM caches, even though many of the ideas are similar. In virtual memory parlance, blocks are known as pages. The activity of transferring a page between disk and memory is known as swapping or paging. Pages are swapped in (paged in) from disk to DRAM, and swapped out (paged out) from DRAM to disk. The strategy of waiting until the last moment to swap 846 Chapter 9 Virtual Memory Figure 9.8 Allocating a new virtual page. The kernel allocates VP 5 on disk and points PTE 5 to this new location. PTE 0 PP 0 PP 3 1 1 1 0 0 0 1 0 PTE 7 Null VP 1 VP 3 VP 7 VP 2 VP 1 VP 2 VP 3 VP 4 VP 5 VP 6 VP 7 Physical page number or disk address Memory-resident page table (DRAM) Virtual memory (disk) Physical memory (DRAM) Valid in a page, when a miss occurs, is known as demand paging. Other approaches, such as trying to predict misses and swap pages in before they are actually referenced, are possible. However, all modern systems use demand paging. 9.3.5 Allocating Pages Figure 9.8 shows the effect on our example page table when the operating system allocates a new page of virtual memory—for example, as a result of calling ÀþÄÄÇ². In the example, VP 5 is allocated by creating room on disk and updating PTE 5 to point to the newly created page on disk. 9.3.6 Locality to the Rescue Again When many of us learn about the idea of virtual memory, our ﬁrst impression is often that it must be terribly inefﬁcient. Given the large miss penalties, we worry that paging will destroy program performance. In practice, virtual memory works well, mainly because of our old friend locality. Although the total number of distinct pages that programs reference during an entire run might exceed the total size of physical memory, the principle of locality promises that at any point in time they will tend to work on a smaller set of active pages known as the working set or resident set. After an initial overhead where the working set is paged into memory, subsequent references to the working set result in hits, with no additional disk trafﬁc. As long as our programs have good temporal locality, virtual memory systems work quite well. But of course, not all programs exhibit good temporal locality. If the working set size exceeds the size of physical memory, then the program can produce an unfortunate situation known as thrashing, where pages are swapped in and out continuously. Although virtual memory is usually efﬁcient, if a program’s performance slows to a crawl, the wise programmer will consider the possibility that it is thrashing. Section 9.4 VM as a Tool for Memory Management 847 Aside Counting page faults You can monitor the number of page faults (and lots of other information) with the Linux ×−ÎËÏÍþ×− function. Figure 9.9 How VM provides processes with separate address spaces. The operating system maintains a separate page table for each process in the system. Virtual address spaces Physical memory Shared page Address translation Process i: Process j: 0 N–1 0 VP 1 VP 2 VP 1 PP 2 PP 7 PP 10 VP 2 N–1 0 M–1 9.4 VM as a Tool for Memory Management In the last section, we saw how virtual memory provides a mechanism for using the DRAM to cache pages from a typically larger virtual address space. Interestingly, some early systems such as the DEC PDP-11/70 supported a virtual address space that was smaller than the available physical memory. Yet virtual memory was still a useful mechanism because it greatly simpliﬁed memory management and provided a natural way to protect memory. Thus far, we have assumed a single page table that maps a single virtual address space to the physical address space. In fact, operating systems provide a separate page table, and thus a separate virtual address space, for each process. Figure 9.9 shows the basic idea. In the example, the page table for process i maps VP 1 to PP 2 and VP 2 to PP 7. Similarly, the page table for process j maps VP 1 to PP 7 and VP 2 to PP 10. Notice that multiple virtual pages can be mapped to the same shared physical page. The combination of demand paging and separate virtual address spaces has a profound impact on the way that memory is used and managed in a system. In particular, VM simpliﬁes linking and loading, the sharing of code and data, and allocating memory to applications. . Simplifying linking. A separate address space allows each process to use the same basic format for its memory image, regardless of where the code and data actually reside in physical memory. For example, as we saw in Figure 8.13, ev- ery process on a given Linux system has a similar memory format. For 64-bit address spaces, the code segment always starts at virtual address nÓrnnnnn. The data segment follows the code segment after a suitable alignment gap. The stack occupies the highest portion of the user process address space and 848 Chapter 9 Virtual Memory grows downward. Such uniformity greatly simpliﬁes the design and implemen- tation of linkers, allowing them to produce fully linked executables that are independent of the ultimate location of the code and data in physical memory. . Simplifying loading. Virtual memory also makes it easy to load executable and shared object ﬁles into memory. To load the lÎ−ÓÎ and l®þÎþ sections of an object ﬁle into a newly created process, the Linux loader allocates virtual pages for the code and data segments, marks them as invalid (i.e., not cached), and points their page table entries to the appropriate locations in the object ﬁle. The interesting point is that the loader never actually copies any data from disk into memory. The data are paged in automatically and on demand by the virtual memory system the ﬁrst time each page is referenced, either by the CPU when it fetches an instruction or by an executing instruction when it references a memory location. This notion of mapping a set of contiguous virtual pages to an arbitrary location in an arbitrary ﬁle is known as memory mapping. Linux provides a system call called ÀÀþÉ that allows application programs to do their own memory mapping. We will describe application-level memory mapping in more detail in Section 9.8. . Simplifying sharing. Separate address spaces provide the operating system with a consistent mechanism for managing sharing between user processes and the operating system itself. In general, each process has its own private code, data, heap, and stack areas that are not shared with any other process. In this case, the operating system creates page tables that map the corresponding virtual pages to disjoint physical pages. However, in some instances it is desirable for processes to share code and data. For example, every process must call the same operating system kernel code, and every C program makes calls to routines in the standard C library such as ÉË©ÅÎð. Rather than including separate copies of the kernel and standard C library in each process, the operating system can arrange for multiple processes to share a single copy of this code by mapping the appropriate virtual pages in different processes to the same physical pages, as we saw in Figure 9.9. . Simplifying memory allocation.Virtual memory provides a simple mechanism for allocating additional memory to user processes. When a program running in a user process requests additional heap space (e.g., as a result of calling ÀþÄÄÇ²), the operating system allocates an appropriate number, say, k,of contiguous virtual memory pages, and maps them to k arbitrary physical pages located anywhere in physical memory. Because of the way page tables work, there is no need for the operating system to locate k contiguous pages of physical memory. The pages can be scattered randomly in physical memory. 9.5 VM as a Tool for Memory Protection Any modern computer system must provide the means for the operating system to control access to the memory system. A user process should not be allowed Section 9.6 Address Translation 849 Figure 9.10 Using VM to provide page-level memory protection. Physical memory PP 0 PP 2 PP 4 PP 6 PP 9 PP 11 Process i: Process j: Page tables with permission bits SUP READ WRITE Address VP 0: VP 1: VP 2: No No Yes Yes Yes Yes No Yes Yes PP 6 PP 4 PP 2 SUP READ WRITE Address VP 0: VP 1: VP 2: No Yes No Yes Yes Yes No Yes Yes PP 9 PP 6 PP 11. . .. . .. . . to modify its read-only code section. Nor should it be allowed to read or modify any of the code and data structures in the kernel. It should not be allowed to read or write the private memory of other processes, and it should not be allowed to modify any virtual pages that are shared with other processes, unless all parties explicitly allow it (via calls to explicit interprocess communication system calls). As we have seen, providing separate virtual address spaces makes it easy to isolate the private memories of different processes. But the address translation mechanism can be extended in a natural way to provide even ﬁner access control. Since the address translation hardware reads a PTE each time the CPU generates an address, it is straightforward to control access to the contents of a virtual page by adding some additional permission bits to the PTE. Figure 9.10 shows the general idea. In this example, we have added three permission bits to each PTE. The SUP bit indicates whether processes must be running in kernel (supervisor) mode to access the page. Processes running in kernel mode can access any page, but processes running in user mode are only allowed to access pages for which SUP is 0. The READ and WRITE bits control read and write access to the page. For example, if process i is running in user mode, then it has permission to read VP 0 and to read or write VP 1. However, it is not allowed to access VP 2. If an instruction violates these permissions, then the CPU triggers a general protection fault that transfers control to an exception handler in the kernel, which sends a SIGSEGV signal to the offending process. Linux shells typically report this exception as a “segmentation fault.” 9.6 Address Translation This section covers the basics of address translation. Our aim is to give you an appreciation of the hardware’s role in supporting virtual memory, with enough detail so that you can work through some concrete examples by hand. However, keep in mind that we are omitting a number of details, especially related to timing, 850 Chapter 9 Virtual Memory Symbol Description Basic parameters N = 2n Number of addresses in virtual address space M = 2m Number of addresses in physical address space P = 2p Page size (bytes) Components of a virtual address (VA) VPO Virtual page offset (bytes) VPN Virtual page number TLBI TLB index TLBT TLB tag Components of a physical address (PA) PPO Physical page offset (bytes) PPN Physical page number CO Byte offset within cache block CI Cache index CT Cache tag Figure 9.11 Summary of address translation symbols. that are important to hardware designers but are beyond our scope. For your reference, Figure 9.11 summarizes the symbols that we will be using throughout this section. Formally, address translation is a mapping between the elements of an N - element virtual address space (VAS) and an M-element physical address space (PAS), MAP: VAS → PAS ∪∅ where MAP(A) = { A′ if data at virtual addr. A are present at physical addr. A′ in PAS ∅ if data at virtual addr. A are not present in physical memory Figure 9.12 shows how the MMU uses the page table to perform this mapping. A control register in the CPU, the page table base register (PTBR) points to the current page table. The n-bit virtual address has two components: a p-bit virtual page offset (VPO) and an (n − p)-bit virtual page number (VPN). The MMU uses the VPN to select the appropriate PTE. For example, VPN 0 selects PTE 0, VPN 1 selects PTE 1, and so on. The corresponding physical address is the concatenation of the physical page number (PPN) from the page table entry and the VPO from the virtual address. Notice that since the physical and virtual pages are both P bytes, the physical page offset (PPO) is identical to the VPO. Section 9.6 Address Translation 851 Page table base register (PTBR) Physical address Virtual address Virtual page number (VPN) Virtual page offset (VPO) Page table Valid Physical page number (PPN) The VPN acts as an index into the page table If valid = 0, then page not in memory (page fault) Physical page number (PPN) Physical page offset (PPO) n–1 p p–1 p p–1 0 m–1 0 Figure 9.12 Address translation with a page table. Figure 9.13(a) shows the steps that the CPU hardware performs when there is a page hit. Step 1. The processor generates a virtual address and sends it to the MMU. Step 2. The MMU generates the PTE address and requests it from the cache/ main memory. Step 3. The cache/main memory returns the PTE to the MMU. Step 4. The MMU constructs the physical address and sends it to the cache/main memory. Step 5. The cache/main memory returns the requested data word to the pro- cessor. Unlike a page hit, which is handled entirely by hardware, handling a page fault requires cooperation between hardware and the operating system kernel (Figure 9.13(b)). Steps 1 to 3. The same as steps 1 to 3 in Figure 9.13(a). Step 4. The valid bit in the PTE is zero, so the MMU triggers an exception, which transfers control in the CPU to a page fault exception handler in the operating system kernel. Step 5. The fault handler identiﬁes a victim page in physical memory, and if that page has been modiﬁed, pages it out to disk. Step 6. The fault handler pages in the new page and updates the PTE in memory. 852 Chapter 9 Virtual Memory 5 CPU chip Processor MMU VA Data (a) Page hit PA PTE PTEA 2 1 3 4 Cache/ memory CPU chip Processor MMU Disk VA PTE Victim page New page PTEA 2 Exception 4 1 7 5 6 3 Cache/ memory Page fault exception handler (b) Page fault Figure 9.13 Operational view of page hits and page faults. VA: virtual address. PTEA: page table entry address. PTE: page table entry. PA: physical address. Step 7. The fault handler returns to the original process, causing the faulting instruction to be restarted. The CPU resends the offending virtual address to the MMU. Because the virtual page is now cached in physical memory, there is a hit, and after the MMU performs the steps in Figure 9.13(a), the main memory returns the requested word to the processor. Practice Problem 9.3 (solution page 917) Given a 64-bit virtual address space and a 32-bit physical address, determine the number of bits in the VPN, VPO, PPN, and PPO for the following page sizes P : Number of P VPN bits VPO bits PPN bits PPO bits 1KB 2KB 4KB 16 KB Section 9.6 Address Translation 853 CPU chip Processor MMU Memory VA Data L1 cache PA PTEA PTE PTE PTEA PA Data PTEA hit PA hit PTEA miss PA miss Figure 9.14 Integrating VM with a physically addressed cache. VA: virtual address. PTEA: page table entry address. PTE: page table entry. PA: physical address. 9.6.1 Integrating Caches and VM In any system that uses both virtual memory and SRAM caches, there is the issue of whether to use virtual or physical addresses to access the SRAM cache. Although a detailed discussion of the trade-offs is beyond our scope here, most systems opt for physical addressing. With physical addressing, it is straightforward for multiple processes to have blocks in the cache at the same time and to share blocks from the same virtual pages. Further, the cache does not have to deal with protection issues, because access rights are checked as part of the address translation process. Figure 9.14 shows how a physically addressed cache might be integrated with virtual memory. The main idea is that the address translation occurs before the cache lookup. Notice that page table entries can be cached, just like any other data words. 9.6.2 Speeding Up Address Translation with a TLB As we have seen, every time the CPU generates a virtual address, the MMU must refer to a PTE in order to translate the virtual address into a physical address. In the worst case, this requires an additional fetch from memory, at a cost of tens to hundreds of cycles. If the PTE happens to be cached in L1, then the cost goes down to a handful of cycles. However, many systems try to eliminate even this cost by including a small cache of PTEs in the MMU called a translation lookaside buffer (TLB). A TLB is a small, virtually addressed cache where each line holds a block consisting of a single PTE. A TLB usually has a high degree of associativity. As shown in Figure 9.15, the index and tag ﬁelds that are used for set selection and line matching are extracted from the virtual page number in the virtual address. If the TLB has T = 2t sets, then the TLB index (TLBI) consists of the t least signiﬁcant bits of the VPN, and the TLB tag (TLBT)consists of the remaining bits in the VPN. 854 Chapter 9 Virtual Memory Figure 9.15 Components of a virtual address that are used to access the TLB. n\u00021 p\u0003tp p\u000210p\u0003t\u00021 TLB tag (TLBT) TLB index (TLBI) VPO VPN Figure 9.16 Operational view of a TLB hit and miss. 2 1 3 4 5 CPU chip Processor Trans- lation TLB Cache/ memoryVA VPN PTE Data (a) TLB hit PA 2 1 4 3 5 6 (b) TLB miss CPU chip Processor Trans- lation TLB Cache/ memoryVA PA VPN PTE Data PTEA Figure 9.16(a) shows the steps involved when there is a TLB hit (the usual case). The key point here is that all of the address translation steps are performed inside the on-chip MMU and thus are fast. Step 1. The CPU generates a virtual address. Steps 2 and 3. The MMU fetches the appropriate PTE from the TLB. Section 9.6 Address Translation 855 Step 4. The MMU translates the virtual address to a physical address and sends it to the cache/main memory. Step 5. The cache/main memory returns the requested data word to the CPU. When there is a TLB miss, then the MMU must fetch the PTE from the L1 cache, as shown in Figure 9.16(b). The newly fetched PTE is stored in the TLB, possibly overwriting an existing entry. 9.6.3 Multi-Level Page Tables Thus far, we have assumed that the system uses a single page table to do address translation. But if we had a 32-bit address space, 4 KB pages, and a 4-byte PTE, then we would needa4MB page table resident in memory at all times, even if the application referenced only a small chunk of the virtual address space. The problem is compounded for systems with 64-bit address spaces. The common approach for compacting the page table is to use a hierarchy of page tables instead. The idea is easiest to understand with a concrete example. Consider a 32-bit virtual address space partitioned into 4 KB pages, with page table entries that are 4 bytes each. Suppose also that at this point in time the virtual address space has the following form: The ﬁrst 2 K pages of memory are allocated for code and data, the next 6 K pages are unallocated, the next 1,023 pages are also unallocated, and the next page is allocated for the user stack. Figure 9.17 shows how we might construct a two-level page table hierarchy for this virtual address space. Each PTE in the level 1 table is responsible for mappinga4MB chunk of the virtual address space, where each chunk consists of 1,024 contiguous pages. For example, PTE 0 maps the ﬁrst chunk, PTE 1 the next chunk, and so on. Given that the address space is 4 GB, 1,024 PTEs are sufﬁcient to cover the entire space. If every page in chunk i is unallocated, then level 1 PTE i is null. For example, in Figure 9.17, chunks 2–7 are unallocated. However, if at least one page in chunk i is allocated, then level 1 PTE i points to the base of a level 2 page table. For example, in Figure 9.17, all or portions of chunks 0, 1, and 8 are allocated, so their level 1 PTEs point to level 2 page tables. Each PTE in a level 2 page table is responsible for mapping a 4-KB page of virtual memory, just as before when we looked at single-level page tables. Notice that with 4-byte PTEs, each level 1 and level 2 page table is 4 kilobytes, which conveniently is the same size as a page. This scheme reduces memory requirements in two ways. First, if a PTE in the level 1 table is null, then the corresponding level 2 page table does not even have to exist. This represents a signiﬁcant potential savings, since most of the 4 GB virtual address space for a typical program is unallocated. Second, only the level 1 table needs to be in main memory at all times. The level 2 page tables can be created and paged in and out by the VM system as they are needed, which reduces pressure on main memory. Only the most heavily used level 2 page tables need to be cached in main memory. 856 Chapter 9 Virtual Memory. . . VP 1,023 VP 1,024 VP 2,047 Gap PTE 0 PTE 1 PTE 2 (null) VP 0 1,023 unallocated pages PTE 3 (null) PTE 4 (null) PTE 5 (null) PTE 6 (null) PTE 0 PTE 1,023 PTE 0 PTE 1,023 1,023 null PTEs PTE 7 (null) PTE 8 (1 K– 9) null PTEs PTE 1,023 . . . . . . VP 9,215 2 K allocated VM pages for code and data 6 K unallocated VM pages 1,023 unallocated pages 1 allocated VM page for the stack Virtual memory Level 2 page tables Level 1 page table 0 . . . . . . Figure 9.17 A two-level page table hierarchy. Notice that addresses increase from top to bottom. PPN PPO . . . . . . . . . m–1 n–1 p–1 0 p–1 0 Virtual address Physical address VPN 1 VPN 2 VPN k VPO Level 1 page table Level 2 page table Level k page table PPN Figure 9.18 Address translation with a k-level page table. Figure 9.18 summarizes address translation with a k-level page table hierarchy. The virtual address is partitioned into k VPNs and a VPO. Each VPN i,1 ≤ i ≤ k, is an index into a page table at level i. Each PTE in a level j table, 1 ≤ j ≤ k − 1, points to the base of some page table at level j + 1. Each PTE in a level k table contains either the PPN of some physical page or the address of a disk block. To construct the physical address, the MMU must access k PTEs before it can Section 9.6 Address Translation 857 determine the PPN. As with a single-level hierarchy, the PPO is identical to the VPO. Accessing k PTEs may seem expensive and impractical at ﬁrst glance. How- ever, the TLB comes to the rescue here by caching PTEs from the page tables at the different levels. In practice, address translation with multi-level page tables is not signiﬁcantly slower than with single-level page tables. 9.6.4 Putting It Together: End-to-End Address Translation In this section, we put it all together with a concrete example of end-to-end address translation on a small system with a TLB and L1 d-cache. To keep things manageable, we make the following assumptions: . The memory is byte addressable. . Memory accesses are to 1-byte words (not 4-byte words). . Virtual addresses are 14 bits wide (n = 14). . Physical addresses are 12 bits wide (m = 12). . The page size is 64 bytes (P = 64). . The TLB is 4-way set associative with 16 total entries. . The L1 d-cache is physically addressed and direct mapped, with a 4-byte line size and 16 total sets. Figure 9.19 shows the formats of the virtual and physical addresses. Since each page is 26 = 64 bytes, the low-order 6 bits of the virtual and physical addresses serve as the VPO and PPO, respectively. The high-order 8 bits of the virtual address serve as the VPN. The high-order 6 bits of the physical address serve as the PPN. Figure 9.20 shows a snapshot of our little memory system, including the TLB (Figure 9.20(a)), a portion of the page table (Figure 9.20(b)), and the L1 cache (Figure 9.20(c)). Above the ﬁgures of the TLB and cache, we have also shown how the bits of the virtual and physical addresses are partitioned by the hardware as it accesses these devices. 13 12 11 10 9 8 7 6 5 4 3 2 1 0 VPN (Virtual page number) VPO (Virtual page offset) Virtual address 11 10 9 8 7 65 432 10 PPN (Physical page number) PPO (Physical page offset) Physical address Figure 9.19 Addressing for small memory system. Assume 14-bit virtual addresses (n = 14), 12-bit physical addresses (m = 12), and 64-byte pages (P = 64). 13 03 12 11 10 9 8 7 6 5 4 3 2 1 0 VPN TLBT TLBI (a) TLB: 4 sets, 16 entries, 4-way set associative VPO Virtual address 03 02 07 – 2D – – 0 1 0 0 09 02 08 03 0D – – 0D 1 0 0 1 00 04 06 0A – – – 34 0 0 0 1 07 0A 03 02 02 – – – 1 Tag 0 1 2 3 Set PPN Valid Tag PPN Valid Tag PPN Valid Tag PPN Valid 0 0 0 28 — 33 02 1 0 1 1 — 16 — — 04 05 06 07 0 1 0 0 PPN 00 01 02 03 VPN Valid 13 17 09 — 1 1 1 0 — 2D 11 0D 0C 0D 0E 0F 0 1 1 1 PPN 08 09 0A 0B VPN Valid (b) Page table: Only the first 16 PTEs are shown 19 15 1B 36 1 0 1 0 32 0D 31 16 4 5 6 7 1 1 0 1 24 1 2D 0 2D 1 0B 0 12 0 16 1 13 1 14 8 9 A B C D E F0 Tag 0 1 2 3 Idx Valid 99 — 00 — 11 — 02 — 43 36 — 11 6D 72 — C2 3A 00 —— 93 15 —— —— 04 96 83 77 —— Blk 0 Blk 1 23 — 04 — 11 — 08 — 8F F0 — DF 09 1D — 03 51 89 —— DA 3B —— —— 34 15 1B D3 —— Blk 2 Blk 3 11 10 9 8 7 65 432 10 PPN CT CI CO PPO Physical address (c) Cache: 16 sets, 4-byte blocks, direct mapped Figure 9.20 TLB, page table, and cache for small memory system. All values in the TLB, page table, and cache are in hexadecimal notation. Section 9.6 Address Translation 859 TLB. The TLB is virtually addressed using the bits of the VPN. Since the TLB has four sets, the 2 low-order bits of the VPN serve as the set index (TLBI). The remaining 6 high-order bits serve as the tag (TLBT) that distinguishes the different VPNs that might map to the same TLB set. Page table. The page table is a single-level design with a total of 28 = 256 page table entries (PTEs). However, we are only interested in the ﬁrst 16 of these. For convenience, we have labeled each PTE with the VPN that indexes it; but keep in mind that these VPNs are not part of the page table and not stored in memory. Also, notice that the PPN of each invalid PTE is denoted with a dash to reinforce the idea that whatever bit values might happen to be stored there are not meaningful. Cache. The direct-mapped cache is addressed by the ﬁelds in the physical address. Since each block is 4 bytes, the low-order 2 bits of the physical address serve as the block offset (CO). Since there are 16 sets, the next 4 bits serve as the set index (CI). The remaining 6 bits serve as the tag (CT). Given this initial setup, let’s see what happens when the CPU executes a load instruction that reads the byte at address nÓnq®r. (Recall that our hypothetical CPU reads 1-byte words rather than 4-byte words.) To begin this kind of manual simulation, we ﬁnd it helpful to write down the bits in the virtual address, identify the various ﬁelds we will need, and determine their hex values. The hardware performs a similar task when it decodes the address. 11Bit position VA = 0x03d4 10 9 8 0x03 0x03 76543210 001010111100 12 0 13 0 0x0f 0x14 TLBT TLBI VPN VPO To begin, the MMU extracts the VPN (nÓnƒ) from the virtual address and checks with the TLB to see if it has cached a copy of PTE nÓnƒ from some previous memory reference. The TLB extracts the TLB index (nÓnq) and the TLB tag (nÓq) from the VPN, hits on a valid match in the second entry of set nÓq, and returns the cached PPN (nÓn⁄) to the MMU. If the TLB had missed, then the MMU would need to fetch the PTE from main memory. However, in this case, we got lucky and had a TLB hit. The MMU now has everything it needs to form the physical address. It does this by concatenating the PPN (nÓn⁄) from the PTE with the VPO (nÓor) from the virtual address, which forms the physical address (nÓqsr). Next, the MMU sends the physical address to the cache, which extracts the cache offset CO (nÓn), the cache set index CI (nÓs), and the cache tag CT (nÓn⁄) from the physical address. 860 Chapter 9 Virtual Memory 11Bit position PA = 0x354 10 9 8 0x0d 0x05 0x0 76543210 001010101100 0x0d 0x14 CT CI CO PPN PPO Since the tag in set nÓs matches CT, the cache detects a hit, reads out the data byte (nÓqt) at offset CO, and returns it to the MMU, which then passes it back to the CPU. Other paths through the translation process are also possible. For example, if the TLB misses, then the MMU must fetch the PPN from a PTE in the page table. If the resulting PTE is invalid, then there is a page fault and the kernel must page in the appropriate page and rerun the load instruction. Another possibility is that the PTE is valid, but the necessary memory block misses in the cache. Practice Problem 9.4 (solution page 917) Show how the example memory system in Section 9.6.4 translates a virtual address into a physical address and accesses the cache. For the given virtual address, indicate the TLB entry accessed, physical address, and cache byte value returned. Indicate whether the TLB misses, whether a page fault occurs, and whether a cache miss occurs. If there is a cache miss, enter “—” for “Cache byte returned.” If there is a page fault, enter “—” for “PPN” and leave parts C and D blank. Virtual address: nÓnq®u A. Virtual address format 12 1113 10 9 8 7 6 5 4 3 2 1 0 B. Address translation Parameter Value VPN TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN C. Physical address format 11 10 9 8 7 6 5 4 3 2 1 0 Section 9.7 Case Study: The Intel Core i7/Linux Memory System 861 D. Physical memory reference Parameter Value Byte offset Cache index Cache tag Cache hit? (Y/N) Cache byte returned 9.7 Case Study: The Intel Core i7/Linux Memory System We conclude our discussion of virtual memory mechanisms with a case study of a real system: an Intel Core i7 running Linux. Although the underlying Haswell microarchitecture allows for full 64-bit virtual and physical address spaces, the current Core i7 implementations (and those for the foreseeable future) support a 48-bit (256 TB) virtual address space and a 52-bit (4 PB) physical address space, along with a compatibility mode that supports 32-bit (4 GB) virtual and physical address spaces. Figure 9.21 gives the highlights of the Core i7 memory system. The processor package (chip) includes four cores, a large L3 cache shared by all of the cores, and DDR3 memory controller (shared by all cores) L2 unified TLB 512 entries, 4-way Main memory MMU (addr translation) To other cores To I/O bridge L1 i-TLB 128 entries, 4-way L1 d-TLB 64 entries, 4-way L2 unified cache 256 KB, 8-way L3 unified cache 8 MB, 16-way (shared by all cores) L1 i-cache 32 KB, 8-way L1 d-cache 32 KB, 8-way Instruction fetch Registers QuickPath interconnect Processor package Core ×4 Figure 9.21 The Core i7 memory system. 862 Chapter 9 Virtual Memory. . .. . . CPU VPN VPO 36 12 TLBT TLBI 32 4 VPN1 VPN2 PTEPTEPTEPTE PPN PPO 40 1299 VPN3 VPN4 99 TLB miss Virtual address (VA) TLB hit L1 TLB (16 sets, 4 entries/set) Page tables Result CR3 32/64 CT CI CO 40 66 L1 hit L1 d-cache (64 sets, 8 lines/set) L2, L3, and main memory L1 miss Physical address (PA) Figure 9.22 Summary of Core i7 address translation. For simplicity, the i-caches, i-TLB, and L2 uniﬁed TLB are not shown. a DDR3 memory controller. Each core contains a hierarchy of TLBs, a hierarchy of data and instruction caches, and a set of fast point-to-point links, based on the QuickPath technology, for communicating directly with the other cores and the external I/O bridge. The TLBs are virtually addressed, and 4-way set associative. The L1, L2, and L3 caches are physically addressed, with a block size of 64 bytes. L1 and L2 are 8-way set associative, and L3 is 16-way set associative. The page size can be conﬁgured at start-up time as either 4 KB or 4 MB. Linux uses 4 KB pages. 9.7.1 Core i7 Address Translation Figure 9.22 summarizes the entire Core i7 address translation process, from the time the CPU generates a virtual address until a data word arrives from memory. The Core i7 uses a four-level page table hierarchy. Each process has its own private page table hierarchy. When a Linux process is running, the page tables associated with allocated pages are all memory-resident, although the Core i7 architecture allows these page tables to be swapped in and out. The CR3 control register contains the physical address of the beginning of the level 1 (L1) page table. The value of CR3 is part of each process context, and is restored during each context switch. Section 9.7 Case Study: The Intel Core i7/Linux Memory System 863 R/WU/SWTCDAPSGPage table physical base addr UnusedUnused P=1 Available for OS (page table location on disk) P=0 0123 XD 63 4567891112515262 Field Description P Child page table present in physical memory (1) or not (0). R/W Read-only or read-write access permission for all reachable pages. U/S User or supervisor (kernel) mode access permission for all reachable pages. WT Write-through or write-back cache policy for the child page table. CD Caching disabled or enabled for the child page table. A Reference bit (set by MMU on reads and writes, cleared by software). PS Page size either 4 KB or 4 MB (deﬁned for level 1 PTEs only). Base addr 40 most signiﬁcant bits of physical base address of child page table. XD Disable or enable instruction fetches from all pages reachable from this PTE. Figure 9.23 Format of level 1, level 2, and level 3 page table entries. Each entry referencesa4KB child page table. Figure 9.23 shows the format of an entry in a level 1, level 2, or level 3 page table. When P = 1 (which is always the case with Linux), the address ﬁeld contains a 40-bit physical page number (PPN) that points to the beginning of the appropriate page table. Notice that this imposesa4KB alignment requirement on page tables. Figure 9.24 shows the format of an entry in a level 4 page table. When P = 1, the address ﬁeld contains a 40-bit PPN that points to the base of some page in physical memory. Again, this imposesa4KB alignment requirement on physical pages. The PTE has three permission bits that control access to the page. The R/W bit determines whether the contents of a page are read/write or read-only. The U/S bit, which determines whether the page can be accessed in user mode, protects code and data in the operating system kernel from user programs. The XD (exe- cute disable) bit, which was introduced in 64-bit systems, can be used to disable instruction fetches from individual memory pages. This is an important new fea- ture that allows the operating system kernel to reduce the risk of buffer overﬂow attacks by restricting execution to the read-only code segment. As the MMU translates each virtual address, it also updates two other bits that can be used by the kernel’s page fault handler. The MMU sets the A bit, which is known as a reference bit, each time a page is accessed. The kernel can use the reference bit to implement its page replacement algorithm. The MMU sets the D bit, or dirty bit, each time the page is written to. A page that has been modiﬁed is sometimes called a dirty page. The dirty bit tells the kernel whether or not it must 864 Chapter 9 Virtual Memory R/WU/SWTCDA0DGPage physical base addr UnusedUnused P=1 Available for OS (page table location on disk) P=0 0123 XD 63 4567891112515262 Field Description P Child page present in physical memory (1) or not (0). R/W Read-only or read/write access permission for child page. U/S User or supervisor mode (kernel mode) access permission for child page. WT Write-through or write-back cache policy for the child page. CD Cache disabled or enabled. A Reference bit (set by MMU on reads and writes, cleared by software). D Dirty bit (set by MMU on writes, cleared by software). G Global page (don’t evict from TLB on task switch). Base addr 40 most signiﬁcant bits of physical base address of child page. XD Disable or enable instruction fetches from the child page. Figure 9.24 Format of level 4 page table entries. Each entry referencesa4KB child page. write back a victim page before it copies in a replacement page. The kernel can call a special kernel-mode instruction to clear the reference or dirty bits. Figure 9.25 shows how the Core i7 MMU uses the four levels of page tables to translate a virtual address to a physical address. The 36-bit VPN is partitioned into four 9-bit chunks, each of which is used as an offset into a page table. The CR3 register contains the physical address of the L1 page table. VPN 1 provides an offset to an L1 PTE, which contains the base address of the L2 page table. VPN 2 provides an offset to an L2 PTE, and so on. 9.7.2 Linux Virtual Memory System A virtual memory system requires close cooperation between the hardware and the kernel. Details vary from version to version, and a complete description is beyond our scope. Nonetheless, our aim in this section is to describe enough of the Linux virtual memory system to give you a sense of how a real operating system organizes virtual memory and how it handles page faults. Linux maintains a separate virtual address space for each process of the form shown in Figure 9.26. We have seen this picture a number of times already, with its familiar code, data, heap, shared library, and stack segments. Now that we understand address translation, we can ﬁll in some more details about the kernel virtual memory that lies above the user stack. The kernel virtual memory contains the code and data structures in the kernel. Some regions of the kernel virtual memory are mapped to physical pages that VPO L4 PT Page table 4 KB region per entry 2 MB region per entry 1 GB region per entry 512 GB region per entry L3 PT Page middle directory L2 PT Page upper directory L1 PT Page global directory Physical address of L1 PT Physical address of page CR3 Physical address Virtual address PPN Offset into physical and virtual page L4 PTE 40 12 12 PPO 12 40 40 9 L3 PTE 40 9 L2 PTE 40 9 L1 PTE 40 9 VPN 4 9 VPN 3 9 VPN 2 9 VPN 1 9 Figure 9.25 Core i7 page table translation. PT: page table; PTE: page table entry; VPN: virtual page number; VPO: virtual page offset; PPN: physical page number; PPO: physical page offset. The Linux names for the four levels of page tables are also shown. Figure 9.26 The virtual memory of a Linux process. 0x400000 0 Process-specific data structures (e.g., page tables, task and mm structs, kernel stack) Physical memory Kernel code and data Memory-mapped region for shared libraries Run-time heap (via malloc) Uninitialized data (.bss) Initialized data (.data) Code (.text) User stack Different for each process Identical for each process Process virtual memory Kernel virtual memory %rsp brk 866 Chapter 9 Virtual Memory Aside Optimizing address translation In our discussion of address translation, we have described a sequential two-step process where the MMU (1) translates the virtual address to a physical address and then (2) passes the physical address to the L1 cache. However, real hardware implementations use a neat trick that allows these steps to be partially overlapped, thus speeding up accesses to the L1 cache. For example, a virtual address on a Core i7 with 4 KB pages has 12 bits of VPO, and these bits are identical to the 12 bits of PPO in the corresponding physical address. Since the 8-way set associative physically addressed L1 caches have 64 sets and 64-byte cache blocks, each physical address has 6 (log2 64) cache offset bits and 6 (log2 64) index bits. These 12 bits ﬁt exactly in the 12-bit VPO of a virtual address, which is no accident! When the CPU needs a virtual address translated, it sends the VPN to the MMU and the VPO to the L1 cache. While the MMU is requesting a page table entry from the TLB, the L1 cache is busy using the VPO bits to ﬁnd the appropriate set and read out the eight tags and corresponding data words in that set. When the MMU gets the PPN back from the TLB, the cache is ready to try to match the PPN to one of these eight tags. are shared by all processes. For example, each process shares the kernel’s code and global data structures. Interestingly, Linux also maps a set of contiguous virtual pages (equal in size to the total amount of DRAM in the system) to the corresponding set of contiguous physical pages. This provides the kernel with a convenient way to access any speciﬁc location in physical memory—for example, when it needs to access page tables or to perform memory-mapped I/O operations on devices that are mapped to particular physical memory locations. Other regions of kernel virtual memory contain data that differ for each process. Examples include page tables, the stack that the kernel uses when it is executing code in the context of the process, and various data structures that keep track of the current organization of the virtual address space. Linux Virtual Memory Areas Linux organizes the virtual memory as a collection of areas (also called segments). An area is a contiguous chunk of existing (allocated) virtual memory whose pages are related in some way. For example, the code segment, data segment, heap, shared library segment, and user stack are all distinct areas. Each existing virtual page is contained in some area, and any virtual page that is not part of some area does not exist and cannot be referenced by the process. The notion of an area is important because it allows the virtual address space to have gaps. The kernel does not keep track of virtual pages that do not exist, and such pages do not consume any additional resources in memory, on disk, or in the kernel itself. Figure 9.27 highlights the kernel data structures that keep track of the virtual memory areas in a process. The kernel maintains a distinct task structure (ÎþÍÂˆ ÍÎËÏ²Î in the source code) for each process in the system. The elements of the task structure either contain or point to all of the information that the kernel needs to Section 9.7 Case Study: The Intel Core i7/Linux Memory System 867 mm task_struct pgd vm_end vm_start vm_prot vm_flags vm_next vm_end vm_start vm_prot vm_flags vm_next vm_end Shared libraries 0 Data Text vm_start vm_prot vm_flags vm_next mmap mm_struct vm_area_struct Process virtual memory Figure 9.27 How Linux organizes virtual memory. run the process (e.g., the PID, pointer to the user stack, name of the executable object ﬁle, and program counter). One of the entries in the task structure points to an ÀÀˆÍÎËÏ²Î that charac- terizes the current state of the virtual memory. The two ﬁelds of interest to us are É×®, which points to the base of the level 1 table (the page global directory), and ÀÀþÉ, which points to a list of ÌÀˆþË−þˆÍÎËÏ²ÎÍ (area structs), each of which characterizes an area of the current virtual address space. When the kernel runs this process, it stores É×® in the CR3 control register. For our purposes, the area struct for a particular area contains the following ﬁelds: ðÌÀˆÍÎþËÎ. Points to the beginning of the area. ÌÀˆ−Å®. Points to the end of the area. ÌÀˆÉËÇÎ. Describes the read/write permissions for all of the pages contained in the area. ÌÀˆðÄþ×Í. Describes (among other things) whether the pages in the area are shared with other processes or private to this process. ÌÀˆÅ−ÓÎ. Points to the next area struct in the list. 868 Chapter 9 Virtual Memory Figure 9.28 Linux page fault handling. Process virtual memory Shared libraries Data Code Segmentation fault: Accessing a nonexistent page Normal page fault Protection exception (e.g., violating permission by writing to a read-only page) 1 3 2 vm_area_struct 0 vm_end vm_start r/o vm_next vm_end vm_start r/w vm_next vm_end vm_start r/o vm_next Linux Page Fault Exception Handling Suppose the MMU triggers a page fault while trying to translate some virtual address A. The exception results in a transfer of control to the kernel’s page fault handler, which then performs the following steps: 1. Is virtual address A legal? In other words, does A lie within an area deﬁned by some area struct? To answer this question, the fault handler searches the list of area structs, comparing A with the ÌÀˆÍÎþËÎ and ÌÀˆ−Å® in each area struct. If the instruction is not legal, then the fault handler triggers a segmentation fault, which terminates the process. This situation is labeled “1” in Figure 9.28. Because a process can create an arbitrary number of new virtual memory areas (using the ÀÀþÉ function described in the next section), a sequential search of the list of area structs might be very costly. So in practice, Linux superimposes a tree on the list, using some ﬁelds that we have not shown, and performs the search on this tree. 2. Is the attempted memory access legal? In other words, does the process have permission to read, write, or execute the pages in this area? For example, was the page fault the result of a store instruction trying to write to a read- only page in the code segment? Is the page fault the result of a process running in user mode that is attempting to read a word from kernel virtual memory? If the attempted access is not legal, then the fault handler triggers a protection exception, which terminates the process. This situation is labeled “2” in Figure 9.28. 3. At this point, the kernel knows that the page fault resulted from a legal operation on a legal virtual address. It handles the fault by selecting a victim page, swapping out the victim page if it is dirty, swapping in the new page, Section 9.8 Memory Mapping 869 and updating the page table. When the page fault handler returns, the CPU restarts the faulting instruction, which sends A to the MMU again. This time, the MMU translates A normally, without generating a page fault. 9.8 Memory Mapping Linux initializes the contents of a virtual memory area by associating it with an object on disk, a process known as memory mapping. Areas can be mapped to one of two types of objects: 1. Regular ﬁle in the Linux ﬁle system: An area can be mapped to a contiguous section of a regular disk ﬁle, such as an executable object ﬁle. The ﬁle section is divided into page-size pieces, with each piece containing the initial contents of a virtual page. Because of demand paging, none of these virtual pages is actually swapped into physical memory until the CPU ﬁrst touches the page (i.e., issues a virtual address that falls within that page’s region of the address space). If the area is larger than the ﬁle section, then the area is padded with zeros. 2. Anonymous ﬁle: An area can also be mapped to an anonymous ﬁle, created by the kernel, that contains all binary zeros. The ﬁrst time the CPU touches a virtual page in such an area, the kernel ﬁnds an appropriate victim page in physical memory, swaps out the victim page if it is dirty, overwrites the victim page with binary zeros, and updates the page table to mark the page as resident. Notice that no data are actually transferred between disk and memory. For this reason, pages in areas that are mapped to anonymous ﬁles are sometimes called demand-zero pages. In either case, once a virtual page is initialized, it is swapped back and forth between a special swap ﬁle maintained by the kernel. The swap ﬁle is also known as the swap space or the swap area. An important point to realize is that at any point in time, the swap space bounds the total amount of virtual pages that can be allocated by the currently running processes. 9.8.1 Shared Objects Revisited The idea of memory mapping resulted from a clever insight that if the virtual memory system could be integrated into the conventional ﬁle system, then it could provide a simple and efﬁcient way to load programs and data into memory. As we have seen, the process abstraction promises to provide each process with its own private virtual address space that is protected from errant writes or reads by other processes. However, many processes have identical read-only code areas. For example, each process that runs the Linux shell program ¾þÍ³ has the same code area. Further, many programs need to access identical copies of read-only run-time library code. For example, every C program requires functions from the standard C library such as ÉË©ÅÎð. It would be extremely wasteful for each process to keep duplicate copies of these commonly used codes in physical 870 Chapter 9 Virtual Memory memory. Fortunately, memory mapping provides us with a clean mechanism for controlling how objects are shared by multiple processes. An object can be mapped into an area of virtual memory as either a shared object or a private object. If a process maps a shared object into an area of its virtual address space, then any writes that the process makes to that area are visible to any other processes that have also mapped the shared object into their virtual memory. Further, the changes are also reﬂected in the original object on disk. Changes made to an area mapped to a private object, on the other hand, are not visible to other processes, and any writes that the process makes to the area are not reﬂected back to the object on disk. A virtual memory area into which a shared object is mapped is often called a shared area. Similarly for a private area. Suppose that process 1 maps a shared object into an area of its virtual memory, as shown in Figure 9.29(a). Now suppose that process 2 maps the same shared ob- Figure 9.29 A shared object. (a) After process 1 maps the shared object. (b) After process 2 maps the same shared object. (Note that the physical pages are not necessarily contiguous.) Process 1 virtual memory Process 2 virtual memory Physical memory Shared object (a) Process 1 virtual memory Process 2 virtual memory Physical memory Shared object (b) Section 9.8 Memory Mapping 871 Figure 9.30 A private copy-on-write object. (a) After both processes have mapped the private copy-on-write object. (b) After process 2 writes to a page in the private area. Process 1 virtual memory Process 2 virtual memory Physical memory Private copy-on-write object (a) Process 1 virtual memory Process 2 virtual memory Physical memory Private copy-on-write object (b) Copy-on-write Write to private copy-on-write page ject into its address space (not necessarily at the same virtual address as process 1), as shown in Figure 9.29(b). Since each object has a unique ﬁlename, the kernel can quickly determine that process 1 has already mapped this object and can point the page table entries in process 2 to the appropriate physical pages. The key point is that only a single copy of the shared object needs to be stored in physical memory, even though the object is mapped into multiple shared areas. For convenience, we have shown the physical pages as being contiguous, but of course this is not true in general. Private objects are mapped into virtual memory using a clever technique known as copy-on-write. A private object begins life in exactly the same way as a shared object, with only one copy of the private object stored in physical memory. For example, Figure 9.30(a) shows a case where two processes have mapped a private object into different areas of their virtual memories but share the same 872 Chapter 9 Virtual Memory physical copy of the object. For each process that maps the private object, the page table entries for the corresponding private area are ﬂagged as read-only, and the area struct is ﬂagged as private copy-on-write. So long as neither process attempts to write to its respective private area, they continue to share a single copy of the object in physical memory. However, as soon as a process attempts to write to some page in the private area, the write triggers a protection fault. When the fault handler notices that the protection exception was caused by the process trying to write to a page in a private copy-on-write area, it creates a new copy of the page in physical memory, updates the page table entry to point to the new copy, and then restores write permissions to the page, as shown in Figure 9.30(b). When the fault handler returns, the CPU re-executes the write, which now proceeds normally on the newly created page. By deferring the copying of the pages in private objects until the last possible moment, copy-on-write makes the most efﬁcient use of scarce physical memory. 9.8.2 The ðÇËÂ Function Revisited Now that we understand virtual memory and memory mapping, we can get a clear idea of how the ðÇËÂ function creates a new process with its own independent virtual address space. When the ðÇËÂ function is called by the current process, the kernel creates various data structures for the new process and assigns it a unique PID. To create the virtual memory for the new process, it creates exact copies of the current process’s ÀÀˆÍÎËÏ²Î, area structs, and page tables. It ﬂags each page in both processes as read-only, and ﬂags each area struct in both processes as private copy- on-write. When the ðÇËÂ returns in the new process, the new process now has an exact copy of the virtual memory as it existed when the fork was called. When either of the processes performs any subsequent writes, the copy-on-write mechanism creates new pages, thus preserving the abstraction of a private address space for each process. 9.8.3 The −Ó−²Ì− Function Revisited Virtual memory and memory mapping also play key roles in the process of loading programs into memory. Now that we understand these concepts, we can under- stand how the −Ó−²Ì− function really loads and executes programs. Suppose that the program running in the current process makes the following call: −Ó−²Ì−f‘þlÇÏÎ‘j ﬁ•‹‹j ﬁ•‹‹gy As you learned in Chapter 8, the −Ó−²Ì− function loads and runs the program contained in the executable object ﬁle þlÇÏÎ within the current process, effectively replacing the current program with the þlÇÏÎ program. Loading and running þlÇÏÎ requires the following steps: Section 9.8 Memory Mapping 873 Figure 9.31 How the loader maps the areas of the user address space. Memory-mapped region for shared libraries User stack 0 Run-time heap (via malloc) Uninitialized data (.bss) Initialized data (.data) Code (.text) Private, demand-zero Shared, file-backed Private, demand-zero Private, demand-zero Private, file-backed .data .text libc.so .data .text a.out 1. Delete existing user areas. Delete the existing area structs in the user portion of the current process’s virtual address. 2. Map private areas. Create new area structs for the code, data, bss, and stack areas of the new program. All of these new areas are private copy-on-write. The code and data areas are mapped to the lÎ−ÓÎ and l®þÎþ sections of the þlÇÏÎ ﬁle. The bss area is demand-zero, mapped to an anonymous ﬁle whose size is contained in þlÇÏÎ. The stack and heap area are also demand-zero, initially of zero length. Figure 9.31 summarizes the different mappings of the private areas. 3. Map shared areas. If the þlÇÏÎ program was linked with shared objects, such as the standard C library Ä©¾²lÍÇ, then these objects are dynamically linked into the program, and then mapped into the shared region of the user’s virtual address space. 4. Set the program counter (PC). The last thing that −Ó−²Ì− does is to set the program counter in the current process’s context to point to the entry point in the code area. The next time this process is scheduled, it will begin execution from the entry point. Linux will swap in code and data pages as needed. 9.8.4 User-Level Memory Mapping with the ÀÀþÉ Function Linux processes can use the ÀÀþÉ function to create new areas of virtual memory and to map objects into these areas. 874 Chapter 9 Virtual Memory Figure 9.32 Visual interpretation of ÀÀþÉ arguments. length (bytes) length (bytes) offset (bytes) Disk file specified by file descriptor fd Process virtual memory start (or address chosen by the kernel) 0 0 a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| a©Å²ÄÏ®− zÍÔÍmÀÀþÅl³| ÌÇ©® hÀÀþÉfÌÇ©® hÍÎþËÎj Í©Ö−ˆÎ Ä−Å×Î³j ©ÅÎ ÉËÇÎj ©ÅÎ ðÄþ×Íj ©ÅÎ ð®j ÇððˆÎ ÇððÍ−Îgy Returns: pointer to mapped area if OK, MAP_FAILED (−1) on error The ÀÀþÉ function asks the kernel to create a new virtual memory area, preferably one that starts at address ÍÎþËÎ, and to map a contiguous chunk of the object speciﬁed by ﬁle descriptor ð® to the new area. The contiguous object chunk has a size of Ä−Å×Î³ bytes and starts at an offset of ÇððÍ−Î bytes from the beginning of the ﬁle. The ÍÎþËÎ address is merely a hint, and is usually speciﬁed as NULL. For our purposes, we will always assume a NULL start address. Figure 9.32 depicts the meaning of these arguments. The ÉËÇÎ argument contains bits that describe the access permissions of the newly mapped virtual memory area (i.e., the ÌÀˆÉËÇÎ bits in the corresponding area struct). PROT_EXEC. Pages in the area consist of instructions that may be executed by the CPU. PROT_READ. Pages in the area may be read. PROT_WRITE. Pages in the area may be written. PROT_NONE. Pages in the area cannot be accessed. The ðÄþ×Í argument consists of bits that describe the type of the mapped object. If the MAP_ANON ﬂag bit is set, then the backing store is an anonymous object and the corresponding virtual pages are demand-zero. MAP_PRIVATE indicates a private copy-on-write object, and MAP_SHARED indicates a shared object. For example, ¾ÏðÉ { ›ÀþÉfﬁ•‹‹j Í©Ö−j –‡ﬂ¶ˆ‡¥¡⁄j ›¡–ˆ–‡'‚¡¶¥Ú›¡–ˆ¡ﬁﬂﬁj nj ngy Section 9.9 Dynamic Memory Allocation 875 asks the kernel to create a new read-only, private, demand-zero area of virtual memory containing Í©Ö− bytes. If the call is successful, then ¾ÏðÉ contains the address of the new area. The ÀÏÅÀþÉ function deletes regions of virtual memory: a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| a©Å²ÄÏ®− zÍÔÍmÀÀþÅl³| ©ÅÎ ÀÏÅÀþÉfÌÇ©® hÍÎþËÎj Í©Ö−ˆÎ Ä−Å×Î³gy Returns: 0 if OK, −1 on error The ÀÏÅÀþÉ function deletes the area starting at virtual address ÍÎþËÎ and consist- ing of the next Ä−Å×Î³ bytes. Subsequent references to the deleted region result in segmentation faults. Practice Problem 9.5 (solution page 918) Write a C program ÀÀþÉ²ÇÉÔl² that uses ÀÀþÉ to copy an arbitrary-size disk ﬁle to ÍÎ®ÇÏÎ. The name of the input ﬁle should be passed as a command-line argument. 9.9 Dynamic Memory Allocation While it is certainly possible to use the low-level ÀÀþÉ and ÀÏÅÀþÉ functions to create and delete areas of virtual memory, C programmers typically ﬁnd it more convenient and more portable to use a dynamic memory allocator when they need to acquire additional virtual memory at run time. A dynamic memory allocator maintains an area of a process’s virtual memory known as the heap (Figure 9.33). Details vary from system to system, but without loss of generality, we will assume that the heap is an area of demand-zero mem- ory that begins immediately after the uninitialized data area and grows upward (toward higher addresses). For each process, the kernel maintains a variable ¾ËÂ (pronounced “break”) that points to the top of the heap. An allocator maintains the heap as a collection of various-size blocks. Each block is a contiguous chunk of virtual memory that is either allocated or free.An allocated block has been explicitly reserved for use by the application. A free block is available to be allocated. A free block remains free until it is explicitly allocated by the application. An allocated block remains allocated until it is freed, either explicitly by the application or implicitly by the memory allocator itself. Allocators come in two basic styles. Both styles require the application to explicitly allocate blocks. They differ about which entity is responsible for freeing allocated blocks. . Explicit allocators require the application to explicitly free any allocated blocks. For example, the C standard library provides an explicit allocator called the ÀþÄÄÇ² package. C programs allocate a block by calling the ÀþÄÄÇ² 876 Chapter 9 Virtual Memory Figure 9.33 The heap. Memory-mapped region for shared libraries User stack 0 Heap Heap grows upward Uninitialized data (.bss) Initialized data (.data) Code (.text) Top of the heap (brk ptrbrk ptr) function, and free a block by calling the ðË−− function. The Å−Ñ and ®−Ä−Î− calls in £ii are comparable. . Implicit allocators, on the other hand, require the allocator to detect when an allocated block is no longer being used by the program and then free the block. Implicit allocators are also known as garbage collectors, and the process of automatically freeing unused allocated blocks is known as garbage collection. For example, higher-level languages such as Lisp, ML, and Java rely on garbage collection to free allocated blocks. The remainder of this section discusses the design and implementation of explicit allocators. We will discuss implicit allocators in Section 9.10. For concrete- ness, our discussion focuses on allocators that manage heap memory. However, you should be aware that memory allocation is a general idea that arises in a vari- ety of contexts. For example, applications that do intensive manipulation of graphs will often use the standard allocator to acquire a large block of virtual memory and then use an application-speciﬁc allocator to manage the memory within that block as the nodes of the graph are created and destroyed. 9.9.1 The ÀþÄÄÇ² and ðË−− Functions The C standard library provides an explicit allocator known as the ÀþÄÄÇ² package. Programs allocate blocks from the heap by calling the ÀþÄÄÇ² function. a©Å²ÄÏ®− zÍÎ®Ä©¾l³| ÌÇ©® hÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−gy Returns: pointer to allocated block if OK, NULL on error Section 9.9 Dynamic Memory Allocation 877 Aside How big is a word? Recall from our discussion of machine code in Chapter 3 that Intel refers to 4-byte objects as double words. However, throughout this section, we will assume that words are 4-byte objects and that double words are 8-byte objects, which is consistent with conventional terminology. The ÀþÄÄÇ² function returns a pointer to a block of memory of at least Í©Ö− bytes that is suitably aligned for any kind of data object that might be contained in the block. In practice, the alignment depends on whether the code is compiled to run in 32-bit mode (×²² kÀqp) or 64-bit mode (the default). In 32-bit mode, ÀþÄÄÇ² returns a block whose address is always a multiple of 8. In 64-bit mode, the address is always a multiple of 16. If ÀþÄÄÇ² encounters a problem (e.g., the program requests a block of memory that is larger than the available virtual memory), then it returns NULL and sets −ËËÅÇ. ›þÄÄÇ² does not initialize the memory it returns. Applications that want initialized dynamic memory can use ²þÄÄÇ², a thin wrapper around the ÀþÄÄÇ² function that initializes the allocated memory to zero. Applications that want to change the size of a previously allocated block can use the Ë−þÄÄÇ² function. Dynamic memory allocators such as ÀþÄÄÇ² can allocate or deallocate heap memory explicitly by using the ÀÀþÉ and ÀÏÅÀþÉ functions, or they can use the Í¾ËÂ function: a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ÌÇ©® hÍ¾ËÂf©ÅÎÉÎËˆÎ ©Å²Ëgy Returns: old ¾ËÂ pointer on success, −1 on error The Í¾ËÂ function grows or shrinks the heap by adding ©Å²Ë to the kernel’s ¾ËÂ pointer. If successful, it returns the old value of ¾ËÂ, otherwise it returns −1 and sets −ËËÅÇ to ENOMEM. If ©Å²Ë is zero, then Í¾ËÂ returns the current value of ¾ËÂ. Calling Í¾ËÂ with a negative ©Å²Ë is legal but tricky because the return value (the old value of ¾ËÂ) points to þ¾Íf©Å²Ëg bytes past the new top of the heap. Programs free allocated heap blocks by calling the ðË−− function. a©Å²ÄÏ®− zÍÎ®Ä©¾l³| ÌÇ©® ðË−−fÌÇ©® hÉÎËgy Returns: nothing The ÉÎË argument must point to the beginning of an allocated block that was obtained from ÀþÄÄÇ², ²þÄÄÇ²,or Ë−þÄÄÇ². If not, then the behavior of ðË−− is undeﬁned. Even worse, since it returns nothing, ðË−− gives no indication to the application that something is wrong. As we shall see in Section 9.11, this can produce some bafﬂing run-time errors. 878 Chapter 9 Virtual Memory Figure 9.34 Allocating and freeing blocks with ÀþÄÄÇ² and ðË−−. Each square corresponds to a word. Each heavy rectangle corresponds to a block. Allocated blocks are shaded. Padded regions of allocated blocks are shaded with a darker blue. Free blocks are unshaded. Heap addresses increase from left to right. p1 (a) p1 = malloc(4*sizeof(int)) p1 p2 (b) p2 = malloc(5*sizeof(int)) p1 p2 p3 (c) p3 = malloc(6*sizeof(int)) p1 p2 p3 (d) free(p2) p1 p2 p4 p3 (e) p4 = malloc(2*sizeof(int)) Figure 9.34 shows how an implementation of ÀþÄÄÇ² and ðË−− might manage a (very) small heap of 16 words for a C program. Each box represents a 4-byte word. The heavy-lined rectangles correspond to allocated blocks (shaded) and free blocks (unshaded). Initially, the heap consists of a single 16-word double- word-aligned free block.1 Figure 9.34(a). The program asks for a four-word block. ›þÄÄÇ² responds by carving out a four-word block from the front of the free block and return- ing a pointer to the ﬁrst word of the block. Figure 9.34(b). The program requests a ﬁve-word block. ›þÄÄÇ² responds by allocating a six-word block from the front of the free block. In this exam- ple, ÀþÄÄÇ² pads the block with an extra word in order to keep the free block aligned on a double-word boundary. Figure 9.34(c). The program requests a six-word block and ÀþÄÄÇ² responds by carving out a six-word block from the free block. Figure 9.34(d). The program frees the six-word block that was allocated in Figure 9.34(b). Notice that after the call to ðË−− returns, the pointer Ép 1. Throughout this section, we will assume that the allocator returns blocks aligned to 8-byte double- word boundaries. Section 9.9 Dynamic Memory Allocation 879 still points to the freed block. It is the responsibility of the application not to use Ép again until it is reinitialized by a new call to ÀþÄÄÇ². Figure 9.34(e). The program requests a two-word block. In this case, ÀþÄÄÇ² allocates a portion of the block that was freed in the previous step and returns a pointer to this new block. 9.9.2 Why Dynamic Memory Allocation? The most important reason that programs use dynamic memory allocation is that often they do not know the sizes of certain data structures until the program actually runs. For example, suppose we are asked to write a C program that reads a list of n ASCII integers, one integer per line, from ÍÎ®©Å into a C array. The input consists of the integer n, followed by the n integers to be read and stored into the array. The simplest approach is to deﬁne the array statically with some hard-coded maximum array size: 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ›¡”ﬁ ospoq 3 4 ©ÅÎ þËËþÔ‰›¡”ﬁ`y 5 6 ©ÅÎ Àþ©Åfg 7 Õ 8 ©ÅÎ ©j Åy 9 10 Í²þÅðf‘c®‘j dÅgy 11 ©ð fÅ | ›¡”ﬁg 12 þÉÉˆ−ËËÇËf‘'ÅÉÏÎ ð©Ä− ÎÇÇ ¾©×‘gy 13 ðÇËf©{ny©zÅy ©iig 14 Í²þÅðf‘c®‘j dþËËþÔ‰©`gy 15 −Ó©Îfngy 16 Û Allocating arrays with hard-coded sizes like this is often a bad idea. The value of MAXN is arbitrary and has no relation to the actual amount of available virtual memory on the machine. Further, if the user of this program wanted to read a ﬁle that was larger than MAXN, the only recourse would be to recompile the program with a larger value of MAXN. While not a problem for this simple example, the presence of hard-coded array bounds can become a maintenance nightmare for large software products with millions of lines of code and numerous users. A better approach is to allocate the array dynamically, at run time, after the value of n becomes known. With this approach, the maximum size of the array is limited only by the amount of available virtual memory. 880 Chapter 9 Virtual Memory 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ hþËËþÔj ©j Åy 6 7 Í²þÅðf‘c®‘j dÅgy 8 þËËþÔ { f©ÅÎ hg›þÄÄÇ²fÅ h Í©Ö−Çðf©ÅÎggy 9 ðÇËf©{ny©zÅy ©iig 10 Í²þÅðf‘c®‘j dþËËþÔ‰©`gy 11 ðË−−fþËËþÔgy 12 −Ó©Îfngy 13 Û Dynamic memory allocation is a useful and important programming tech- nique. However, in order to use allocators correctly and efﬁciently, programmers need to have an understanding of how they work. We will discuss some of the grue- some errors that can result from the improper use of allocators in Section 9.11. 9.9.3 Allocator Requirements and Goals Explicit allocators must operate within some rather stringent constraints: Handling arbitrary request sequences. An application can make an arbitrary se- quence of allocate and free requests, subject to the constraint that each free request must correspond to a currently allocated block obtained from a previous allocate request. Thus, the allocator cannot make any assump- tions about the ordering of allocate and free requests. For example, the allocator cannot assume that all allocate requests are accompanied by a matching free request, or that matching allocate and free requests are nested. Making immediate responses to requests. The allocator must respond immedi- ately to allocate requests. Thus, the allocator is not allowed to reorder or buffer requests in order to improve performance. Using only the heap. In order for the allocator to be scalable, any nonscalar data structures used by the allocator must be stored in the heap itself. Aligning blocks (alignment requirement). The allocator must align blocks in such a way that they can hold any type of data object. Not modifying allocated blocks. Allocators can only manipulate or change free blocks. In particular, they are not allowed to modify or move blocks once they are allocated. Thus, techniques such as compaction of allocated blocks are not permitted. Section 9.9 Dynamic Memory Allocation 881 Working within these constraints, the author of an allocator attempts to meet the often conﬂicting performance goals of maximizing throughput and memory utilization. Goal 1: Maximizing throughput. Given some sequence of n allocate and free requests R0,R1,...,Rk,...,Rn−1 we would like to maximize an allocator’s throughput, which is deﬁned as the number of requests that it completes per unit time. For example, if an alloca- tor completes 500 allocate requests and 500 free requests in 1 second, then its throughput is 1,000 operations per second. In general, we can maximize through- put by minimizing the average time to satisfy allocate and free requests. As we’ll see, it is not too difﬁcult to develop allocators with reasonably good performance where the worst-case running time of an allocate request is linear in the number of free blocks and the running time of a free request is constant. Goal 2: Maximizing memory utilization. Naive programmers often incorrectly assume that virtual memory is an unlimited resource. In fact, the total amount of virtual memory allocated by all of the processes in a system is limited by the amount of swap space on disk. Good programmers know that virtual memory is a ﬁnite resource that must be used efﬁciently. This is especially true for a dynamic memory allocator that might be asked to allocate and free large blocks of memory. There are a number of ways to characterize how efﬁciently an allocator uses the heap. In our experience, the most useful metric is peak utilization. As before, we are given some sequence of n allocate and free requests R0,R1,...,Rk,...,Rn−1 If an application requests a block of p bytes, then the resulting allocated block has a payload of p bytes. After request Rk has completed, let the aggregate payload, denoted Pk, be the sum of the payloads of the currently allocated blocks, and let Hk denote the current (monotonically nondecreasing) size of the heap. Then the peak utilization over the ﬁrst k + 1 requests, denoted by Uk,is given by Uk = maxi≤k Pi Hk The objective of the allocator, then, is to maximize the peak utilization Un−1 over the entire sequence. As we will see, there is a tension between maximizing throughput and utilization. In particular, it is easy to write an allocator that maximizes throughput at the expense of heap utilization. One of the interesting challenges in any allocator design is ﬁnding an appropriate balance between the two goals. 882 Chapter 9 Virtual Memory Aside Relaxing the monotonicity assumption We could relax the monotonically nondecreasing assumption in our deﬁnition of Uk and allow the heap to grow up and down by letting Hk be the high-water mark over the ﬁrst k + 1 requests. 9.9.4 Fragmentation The primary cause of poor heap utilization is a phenomenon known as fragmen- tation, which occurs when otherwise unused memory is not available to satisfy allocate requests. There are two forms of fragmentation: internal fragmentation and external fragmentation. Internal fragmentation occurs when an allocated block is larger than the pay- load. This might happen for a number of reasons. For example, the implementation of an allocator might impose a minimum size on allocated blocks that is greater than some requested payload. Or, as we saw in Figure 9.34(b), the allocator might increase the block size in order to satisfy alignment constraints. Internal fragmentation is straightforward to quantify. It is simply the sum of the differences between the sizes of the allocated blocks and their payloads. Thus, at any point in time, the amount of internal fragmentation depends only on the pattern of previous requests and the allocator implementation. External fragmentation occurs when there is enough aggregate free memory to satisfy an allocate request, but no single free block is large enough to handle the request. For example, if the request in Figure 9.34(e) were for eight words rather than two words, then the request could not be satisﬁed without requesting additional virtual memory from the kernel, even though there are eight free words remaining in the heap. The problem arises because these eight words are spread over two free blocks. External fragmentation is much more difﬁcult to quantify than internal frag- mentation because it depends not only on the pattern of previous requests and the allocator implementation but also on the pattern of future requests. For example, suppose that after k requests all of the free blocks are exactly four words in size. Does this heap suffer from external fragmentation? The answer depends on the pattern of future requests. If all of the future allocate requests are for blocks that are smaller than or equal to four words, then there is no external fragmentation. On the other hand, if one or more requests ask for blocks larger than four words, then the heap does suffer from external fragmentation. Since external fragmentation is difﬁcult to quantify and impossible to predict, allocators typically employ heuristics that attempt to maintain small numbers of larger free blocks rather than large numbers of smaller free blocks. 9.9.5 Implementation Issues The simplest imaginable allocator would organize the heap as a large array of bytes and a pointer É that initially points to the ﬁrst byte of the array. To allocate Section 9.9 Dynamic Memory Allocation 883 Í©Ö− bytes, ÀþÄÄÇ² would save the current value of É on the stack, increment É by Í©Ö−, and return the old value of É to the caller. ƒË−− would simply return to the caller without doing anything. This naive allocator is an extreme point in the design space. Since each ÀþÄÄÇ² and ðË−− execute only a handful of instructions, throughput would be extremely good. However, since the allocator never reuses any blocks, memory utilization would be extremely bad. A practical allocator that strikes a better balance between throughput and utilization must consider the following issues: Free block organization. How do we keep track of free blocks? Placement. How do we choose an appropriate free block in which to place a newly allocated block? Splitting. After we place a newly allocated block in some free block, what do we do with the remainder of the free block? Coalescing. What do we do with a block that has just been freed? The rest of this section looks at these issues in more detail. Since the basic techniques of placement, splitting, and coalescing cut across many different free block organizations, we will introduce them in the context of a simple free block organization known as an implicit free list. 9.9.6 Implicit Free Lists Any practical allocator needs some data structure that allows it to distinguish block boundaries and to distinguish between allocated and free blocks. Most allocators embed this information in the blocks themselves. One simple approach is shown in Figure 9.35. In this case, a block consists of a one-word header, the payload, and possibly some additional padding. The header encodes the block size (including the header and any padding) as well as whether the block is allocated or free. If we impose a double-word alignment constraint, then the block size is always a multiple of 8 and the 3 low-order bits of the block size are always zero. Thus, we need to store only the 29 high-order bits of the block size, freeing the remaining 3 bits to encode other information. In this case, we are using the least signiﬁcant of these bits Figure 9.35 Format of a simple heap block. Header Block size Payload (allocated block only) Padding (optional) 0 0 a The block size includes the header, payload, and any padding a = 1: Allocated a = 0: Free malloc returns a pointer to the beginning of the payload 31 3 2 1 0 884 Chapter 9 Virtual Memory Unused Start of heap 8/0 16/1 32/0 16/1 0/1 Double- word aligned Figure 9.36 Organizing the heap with an implicit free list. Allocated blocks are shaded. Free blocks are unshaded. Headers are labeled with (size (bytes)/allocated bit). (the allocated bit) to indicate whether the block is allocated or free. For example, suppose we have an allocated block with a block size of 24 (nÓov) bytes. Then its header would be nÓnnnnnnov Ú nÓo { nÓnnnnnnow Similarly, a free block with a block size of 40 (nÓpv) bytes would have a header of nÓnnnnnnpv Ú nÓn { nÓnnnnnnpv The header is followed by the payload that the application requested when it called ÀþÄÄÇ². The payload is followed by a chunk of unused padding that can be any size. There are a number of reasons for the padding. For example, the padding might be part of an allocator’s strategy for combating external fragmentation. Or it might be needed to satisfy the alignment requirement. Given the block format in Figure 9.35, we can organize the heap as a sequence of contiguous allocated and free blocks, as shown in Figure 9.36. We call this organization an implicit free list because the free blocks are linked implicitly by the size ﬁelds in the headers. The allocator can indirectly traverse the entire set of free blocks by traversing all of the blocks in the heap. Notice that we need some kind of specially marked end block—in this example, a terminating header with the allocated bit set and a size of zero. (As we will see in Section 9.9.12, setting the allocated bit simpliﬁes the coalescing of free blocks.) The advantage of an implicit free list is simplicity. A signiﬁcant disadvantage is that the cost of any operation that requires a search of the free list, such as placing allocated blocks, will be linear in the total number of allocated and free blocks in the heap. It is important to realize that the system’s alignment requirement and the allocator’s choice of block format impose a minimum block size on the allocator. No allocated or free block may be smaller than this minimum. For example, if we assume a double-word alignment requirement, then the size of each block must be a multiple of two words (8 bytes). Thus, the block format in Figure 9.35 induces a minimum block size of two words: one word for the header and another to maintain the alignment requirement. Even if the application were to request a single byte, the allocator would still create a two-word block. Section 9.9 Dynamic Memory Allocation 885 Practice Problem 9.6 (solution page 919) Determine the block sizes and header values that would result from the fol- lowing sequence of ÀþÄÄÇ² requests. Assumptions: (1) The allocator maintains double-word alignment and uses an implicit free list with the block format from Figure 9.35. (2) Block sizes are rounded up to the nearest multiple of 8 bytes. Request Block size (decimal bytes) Block header (hex) ÀþÄÄÇ²fpg ÀþÄÄÇ²fwg ÀþÄÄÇ²fosg ÀþÄÄÇ²fpng 9.9.7 Placing Allocated Blocks When an application requests a block of k bytes, the allocator searches the free list for a free block that is large enough to hold the requested block. The manner in which the allocator performs this search is determined by the placement policy. Some common policies are ﬁrst ﬁt, next ﬁt, and best ﬁt. First ﬁt searches the free list from the beginning and chooses the ﬁrst free block that ﬁts. Next ﬁt is similar to ﬁrst ﬁt, but instead of starting each search at the beginning of the list, it starts each search where the previous search left off. Best ﬁt examines every free block and chooses the free block with the smallest size that ﬁts. An advantage of ﬁrst ﬁt is that it tends to retain large free blocks at the end of the list. A disadvantage is that it tends to leave “splinters” of small free blocks toward the beginning of the list, which will increase the search time for larger blocks. Next ﬁt was ﬁrst proposed by Donald Knuth as an alternative to ﬁrst ﬁt, motivated by the idea that if we found a ﬁt in some free block the last time, there is a good chance that we will ﬁnd a ﬁt the next time in the remainder of the block. Next ﬁt can run signiﬁcantly faster than ﬁrst ﬁt, especially if the front of the list becomes littered with many small splinters. However, some studies suggest that next ﬁt suffers from worse memory utilization than ﬁrst ﬁt. Studies have found that best ﬁt generally enjoys better memory utilization than either ﬁrst ﬁt or next ﬁt. However, the disadvantage of using best ﬁt with simple free list organizations such as the implicit free list is that it requires an exhaustive search of the heap. Later, we will look at more sophisticated segregated free list organizations that approximate a best-ﬁt policy without an exhaustive search of the heap. 9.9.8 Splitting Free Blocks Once the allocator has located a free block that ﬁts, it must make another policy decision about how much of the free block to allocate. One option is to use the entire free block. Although simple and fast, the main disadvantage is that it 886 Chapter 9 Virtual Memory Unused Start of heap 8/0 16/1 16/1 16/116/0 0/1 Double- word aligned Figure 9.37 Splitting a free block to satisfy a three-word allocation request. Allocated blocks are shaded. Free blocks are unshaded. Headers are labeled with (size (bytes)/allocated bit). introduces internal fragmentation. If the placement policy tends to produce good ﬁts, then some additional internal fragmentation might be acceptable. However, if the ﬁt is not good, then the allocator will usually opt to split the free block into two parts. The ﬁrst part becomes the allocated block, and the remainder becomes a new free block. Figure 9.37 shows how the allocator might split the eight-word free block in Figure 9.36 to satisfy an application’s request for three words of heap memory. 9.9.9 Getting Additional Heap Memory What happens if the allocator is unable to ﬁnd a ﬁt for the requested block? One option is to try to create some larger free blocks by merging (coalescing) free blocks that are physically adjacent in memory (next section). However, if this does not yield a sufﬁciently large block, or if the free blocks are already maximally coalesced, then the allocator asks the kernel for additional heap memory by calling the Í¾ËÂ function. The allocator transforms the additional memory into one large free block, inserts the block into the free list, and then places the requested block in this new free block. 9.9.10 Coalescing Free Blocks When the allocator frees an allocated block, there might be other free blocks that are adjacent to the newly freed block. Such adjacent free blocks can cause a phenomenon known as false fragmentation, where there is a lot of available free memory chopped up into small, unusable free blocks. For example, Figure 9.38 shows the result of freeing the block that was allocated in Figure 9.37. The result is two adjacent free blocks with payloads of three words each. As a result, a subsequent request for a payload of four words would fail, even though the aggregate size of the two free blocks is large enough to satisfy the request. To combat false fragmentation, any practical allocator must merge adjacent free blocks in a process known as coalescing. This raises an important policy decision about when to perform coalescing. The allocator can opt for immediate coalescing by merging any adjacent blocks each time a block is freed. Or it can opt for deferred coalescing by waiting to coalesce free blocks at some later time. For example, the allocator might defer coalescing until some allocation request fails, and then scan the entire heap, coalescing all free blocks. Section 9.9 Dynamic Memory Allocation 887 Unused Start of heap 8/0 16/1 16/0 16/116/0 0/1 Double- word aligned Figure 9.38 An example of false fragmentation. Allocated blocks are shaded. Free blocks are unshaded. Headers are labeled with (size (bytes)/allocated bit). Immediate coalescing is straightforward and can be performed in constant time, but with some request patterns it can introduce a form of thrashing where a block is repeatedly coalesced and then split soon thereafter. For example, in Fig- ure 9.38, a repeated pattern of allocating and freeing a three-word block would introduce a lot of unnecessary splitting and coalescing. In our discussion of allo- cators, we will assume immediate coalescing, but you should be aware that fast allocators often opt for some form of deferred coalescing. 9.9.11 Coalescing with Boundary Tags How does an allocator implement coalescing? Let us refer to the block we want to free as the current block. Then coalescing the next free block (in memory) is straightforward and efﬁcient. The header of the current block points to the header of the next block, which can be checked to determine if the next block is free. If so, its size is simply added to the size of the current header and the blocks are coalesced in constant time. But how would we coalesce the previous block? Given an implicit free list of blocks with headers, the only option would be to search the entire list, remember- ing the location of the previous block, until we reached the current block. With an implicit free list, this means that each call to ðË−− would require time linear in the size of the heap. Even with more sophisticated free list organizations, the search time would not be constant. Knuth developed a clever and general technique, known as boundary tags, that allows for constant-time coalescing of the previous block. The idea, which is shown in Figure 9.39, is to add a footer (the boundary tag) at the end of each block, where the footer is a replica of the header. If each block includes such a footer, then the allocator can determine the starting location and status of the previous block by inspecting its footer, which is always one word away from the start of the current block. Consider all the cases that can exist when the allocator frees the current block: 1. The previous and next blocks are both allocated. 2. The previous block is allocated and the next block is free. 3. The previous block is free and the next block is allocated. 4. The previous and next blocks are both free. 888 Chapter 9 Virtual Memory Figure 9.39 Format of heap block that uses a boundary tag. Block size Payload (allocated block only) Padding (optional) a/f a = 001: Allocated a = 000: Free Block size a/f 31 3 2 1 0 Header Footer Figure 9.40 shows how we would coalesce each of the four cases. In case 1, both adjacent blocks are allocated and thus no coalescing is possible. So the status of the current block is simply changed from allocated to free. In case 2, the current block is merged with the next block. The header of the current block and the footer of the next block are updated with the combined sizes of the current and next blocks. In case 3, the previous block is merged with the current block. The header of the previous block and the footer of the current block are updated with the combined sizes of the two blocks. In case 4, all three blocks are merged to form a single free block, with the header of the previous block and the footer of the next block updated with the combined sizes of the three blocks. In each case, the coalescing is performed in constant time. The idea of boundary tags is a simple and elegant one that generalizes to many different types of allocators and free list organizations. However, there is a potential disadvantage. Requiring each block to contain both a header and a footer can introduce signiﬁcant memory overhead if an application manipulates many small blocks. For example, if a graph application dynamically creates and destroys graph nodes by making repeated calls to ÀþÄÄÇ² and ðË−−, and each graph node requires only a couple of words of memory, then the header and the footer will consume half of each allocated block. Fortunately, there is a clever optimization of boundary tags that eliminates the need for a footer in allocated blocks. Recall that when we attempt to coalesce the current block with the previous and next blocks in memory, the size ﬁeld in the footer of the previous block is only needed if the previous block is free.Ifwe were to store the allocated/free bit of the previous block in one of the excess low- order bits of the current block, then allocated blocks would not need footers, and we could use that extra space for payload. Note, however, that free blocks would still need footers. Practice Problem 9.7 (solution page 919) Determine the minimum block size for each of the following combinations of alignment requirements and block formats. Assumptions: Implicit free list, zero- size payloads are not allowed, and headers and footers are stored in 4-byte words. Section 9.9 Dynamic Memory Allocation 889 Figure 9.40 Coalescing with boundary tags. Case 1: prev and next allocated. Case 2: prev allocated, next free. Case 3: prev free, next allocated. Case 4: next and prev free. m1 a a a a a a n n m2 m2 m1 m1 a a f f a a n n m2 m2 Case 1 m1 m1 a a a a f f n n m2 m2 m1 m1 a a f f n\u0003m2 n\u0003m2 m1 Case 2 m1 f f a a a a n n m2 m2 m1 n\u0003m1 f f a a n\u0003m1 m2 m2 Case 3 m1 f f a a f f n n m2 m2 m1 n\u0003m1\u0003m2 f fn\u0003m1\u0003m2 Case 4 890 Chapter 9 Virtual Memory Minimum block Alignment Allocated block Free block size (bytes) Single word Header and footer Header and footer Single word Header, but no footer Header and footer Double word Header and footer Header and footer Double word Header, but no footer Header and footer 9.9.12 Putting It Together: Implementing a Simple Allocator Building an allocator is a challenging task. The design space is large, with nu- merous alternatives for block format and free list format, as well as placement, splitting, and coalescing policies. Another challenge is that you are often forced to program outside the safe, familiar conﬁnes of the type system, relying on the error-prone pointer casting and pointer arithmetic that is typical of low-level sys- tems programming. While allocators do not require enormous amounts of code, they are subtle and unforgiving. Students familiar with higher-level languages such as C++ or Java often hit a conceptual wall when they ﬁrst encounter this style of programming. To help you clear this hurdle, we will work through the implementation of a simple allocator based on an implicit free list with immediate boundary-tag coalescing. The maximum block size is 232 = 4 GB. The code is 64-bit clean, running without modiﬁcation in 32-bit (×²² kÀqp) or 64-bit (×²² kÀtr) processes. General Allocator Design Our allocator uses a model of the memory system provided by the À−ÀÄ©¾l² package shown in Figure 9.41. The purpose of the model is to allow us to run our allocator without interfering with the existing system-level ÀþÄÄÇ² package. The À−Àˆ©Å©Î function models the virtual memory available to the heap as a large double-word aligned array of bytes. The bytes between À−Àˆ³−þÉ and À−Àˆ ¾ËÂ represent allocated virtual memory. The bytes following À−Àˆ¾ËÂ represent unallocated virtual memory. The allocator requests additional heap memory by calling the À−ÀˆÍ¾ËÂ function, which has the same interface as the system’s Í¾ËÂ function, as well as the same semantics, except that it rejects requests to shrink the heap. The allocator itself is contained in a source ﬁle (ÀÀl²) that users can compile and link into their applications. The allocator exports three functions to applica- tion programs: 1 −ÓÎ−ËÅ ©ÅÎ ÀÀˆ©Å©ÎfÌÇ©®gy 2 −ÓÎ−ËÅ ÌÇ©® hÀÀˆÀþÄÄÇ² fÍ©Ö−ˆÎ Í©Ö−gy 3 −ÓÎ−ËÅ ÌÇ©® ÀÀˆðË−− fÌÇ©® hÉÎËgy The ÀÀˆ©Å©Î function initializes the allocator, returning 0 if successful and −1 otherwise. The ÀÀˆÀþÄÄÇ² and ÀÀˆðË−− functions have the same interfaces and semantics as their system counterparts. The allocator uses the block format Section 9.9 Dynamic Memory Allocation 891 code/vm/malloc/memlib.c 1 mh –Ë©ÌþÎ− ×ÄÇ¾þÄ ÌþË©þ¾Ä−Í hm 2 ÍÎþÎ©² ²³þË hÀ−Àˆ³−þÉy mh –Ç©ÅÎÍ ÎÇ ð©ËÍÎ ¾ÔÎ− Çð ³−þÉ hm 3 ÍÎþÎ©² ²³þË hÀ−Àˆ¾ËÂy mh –Ç©ÅÎÍ ÎÇ ÄþÍÎ ¾ÔÎ− Çð ³−þÉ ÉÄÏÍ o hm 4 ÍÎþÎ©² ²³þË hÀ−ÀˆÀþÓˆþ®®Ëy mh ›þÓ Ä−×þÄ ³−þÉ þ®®Ë ÉÄÏÍ ohm 5 6 mh 7 h À−Àˆ©Å©Î k 'Å©Î©þÄ©Ö− Î³− À−ÀÇËÔ ÍÔÍÎ−À ÀÇ®−Ä 8 hm 9 ÌÇ©® À−Àˆ©Å©ÎfÌÇ©®g 10 Õ 11 À−Àˆ³−þÉ { f²³þË hg›þÄÄÇ²f›¡”ˆ¤¥¡–gy 12 À−Àˆ¾ËÂ { f²³þË hgÀ−Àˆ³−þÉy 13 À−ÀˆÀþÓˆþ®®Ë { f²³þË hgfÀ−Àˆ³−þÉ i ›¡”ˆ¤¥¡–gy 14 Û 15 16 mh 17 h À−ÀˆÍ¾ËÂ k ·©ÀÉÄ− ÀÇ®−Ä Çð Î³− Í¾ËÂ ðÏÅ²Î©ÇÅl ¥ÓÎ−Å®Í Î³− ³−þÉ 18 h ¾Ô ©Å²Ë ¾ÔÎ−Í þÅ® Ë−ÎÏËÅÍ Î³− ÍÎþËÎ þ®®Ë−ÍÍ Çð Î³− Å−Ñ þË−þl 'Å 19 h Î³©Í ÀÇ®−Äj Î³− ³−þÉ ²þÅÅÇÎ ¾− Í³ËÏÅÂl 20 hm 21 ÌÇ©® hÀ−ÀˆÍ¾ËÂf©ÅÎ ©Å²Ëg 22 Õ 23 ²³þË hÇÄ®ˆ¾ËÂ { À−Àˆ¾ËÂy 24 25 ©ð f f©Å²Ë z ng ÚÚ ffÀ−Àˆ¾ËÂ i ©Å²Ëg | À−ÀˆÀþÓˆþ®®Ëgg Õ 26 −ËËÅÇ { ¥ﬁﬂ›¥›y 27 ðÉË©ÅÎðfÍÎ®−ËËj ‘¥‡‡ﬂ‡x À−ÀˆÍ¾ËÂ ðþ©Ä−®l ‡þÅ ÇÏÎ Çð À−ÀÇËÔlll¿Å‘gy 28 Ë−ÎÏËÅ fÌÇ©® hgkoy 29 Û 30 À−Àˆ¾ËÂ i{ ©Å²Ëy 31 Ë−ÎÏËÅ fÌÇ©® hgÇÄ®ˆ¾ËÂy 32 Û code/vm/malloc/memlib.c Figure 9.41 À−ÀÄ©¾l²: Memory system model. shown in Figure 9.39. The minimum block size is 16 bytes. The free list is organized as an implicit free list, with the invariant form shown in Figure 9.42. The ﬁrst word is an unused padding word aligned to a double-word boundary. The padding is followed by a special prologue block, which is an 8-byte allocated block consisting of only a header and a footer. The prologue block is created during initialization and is never freed. Following the prologue block are zero or more regular blocks that are created by calls to ÀþÄÄÇ² or ðË−−. The heap always ends with a special epilogue block, which is a zero-size allocated block 892 Chapter 9 Virtual Memory Prologue block Regular block 1 Regular block 2 Start of heap 8/1 8/1 hdr hdrftr ftr Regular block n Epilogue block hdr hdr ftr 0/1 static char *heap_listp Double- word aligned . . . Figure 9.42 Invariant form of the implicit free list. that consists of only a header. The prologue and epilogue blocks are tricks that eliminate the edge conditions during coalescing. The allocator uses a single private (ÍÎþÎ©²) global variable (³−þÉˆÄ©ÍÎÉ) that always points to the prologue block. (As a minor optimization, we could make it point to the next block instead of the prologue block.) Basic Constants and Macros for Manipulating the Free List Figure 9.43 shows some basic constants and macros that we will use throughout the allocator code. Lines 2–4 deﬁne some basic size constants: the sizes of words (WSIZE) and double words (DSIZE), and the size of the initial free block and the default size for expanding the heap (CHUNKSIZE). Manipulating the headers and footers in the free list can be troublesome because it demands extensive use of casting and pointer arithmetic. Thus, we ﬁnd it helpful to deﬁne a small set of macros for accessing and traversing the free list (lines 9–25). The PACK macro (line 9) combines a size and an allocate bit and returns a value that can be stored in a header or footer. The GET macro (line 12) reads and returns the word referenced by argu- ment É. The casting here is crucial. The argument É is typically a (ÌÇ©® h) pointer, which cannot be dereferenced directly. Similarly, the PUT macro (line 13) stores ÌþÄ in the word pointed at by argument É. The GET_SIZE and GET_ALLOC macros (lines 16–17) return the size and allocated bit, respectively, from a header or footer at address É. The remaining macros operate on block pointers (denoted ¾É) that point to the ﬁrst payload byte. Given a block pointer ¾É, the HDRP and FTRP macros (lines 20–21) return pointers to the block header and footer, respectively. The NEXT_BLKP and PREV_BLKP macros (lines 24–25) return the block pointers of the next and previous blocks, respectively. The macros can be composed in various ways to manipulate the free list. For example, given a pointer ¾É to the current block, we could use the following line of code to determine the size of the next block in memory: Í©Ö−ˆÎ Í©Ö− { §¥¶ˆ·'…¥f¤⁄‡–fﬁ¥”¶ˆ¢‹«–f¾Égggy Section 9.9 Dynamic Memory Allocation 893 code/vm/malloc/mm.c 1 mh ¢þÍ©² ²ÇÅÍÎþÅÎÍ þÅ® Àþ²ËÇÍ hm 2 a®−ð©Å− „·'…¥ r mh „ÇË® þÅ® ³−þ®−ËmðÇÇÎ−Ë Í©Ö− f¾ÔÎ−Íg hm 3 a®−ð©Å− ⁄·'…¥ v mh ⁄ÇÏ¾Ä− ÑÇË® Í©Ö− f¾ÔÎ−Íg hm 4 a®−ð©Å− £¤•ﬁ«·'…¥ fozzopg mh ¥ÓÎ−Å® ³−þÉ ¾Ô Î³©Í þÀÇÏÅÎ f¾ÔÎ−Íg hm 5 6 a®−ð©Å− ›¡”fÓj Ôg ffÓg | fÔg} fÓg x fÔgg 7 8 mh –þ²Â þ Í©Ö− þÅ® þÄÄÇ²þÎ−® ¾©Î ©ÅÎÇ þ ÑÇË® hm 9 a®−ð©Å− –¡£«fÍ©Ö−j þÄÄÇ²g ffÍ©Ö−g Ú fþÄÄÇ²gg 10 11 mh ‡−þ® þÅ® ÑË©Î− þ ÑÇË® þÎ þ®®Ë−ÍÍ É hm 12 a®−ð©Å− §¥¶fÉg fhfÏÅÍ©×Å−® ©ÅÎ hgfÉgg 13 a®−ð©Å− –•¶fÉj ÌþÄg fhfÏÅÍ©×Å−® ©ÅÎ hgfÉg { fÌþÄgg 14 15 mh ‡−þ® Î³− Í©Ö− þÅ® þÄÄÇ²þÎ−® ð©−Ä®Í ðËÇÀ þ®®Ë−ÍÍ É hm 16 a®−ð©Å− §¥¶ˆ·'…¥fÉg f§¥¶fÉg d ÜnÓug 17 a®−ð©Å− §¥¶ˆ¡‹‹ﬂ£fÉg f§¥¶fÉg d nÓog 18 19 mh §©Ì−Å ¾ÄÇ²Â ÉÎË ¾Éj ²ÇÀÉÏÎ− þ®®Ë−ÍÍ Çð ©ÎÍ ³−þ®−Ë þÅ® ðÇÇÎ−Ë hm 20 a®−ð©Å− ¤⁄‡–f¾Ég ff²³þË hgf¾Ég k „·'…¥g 21 a®−ð©Å− ƒ¶‡–f¾Ég ff²³þË hgf¾Ég i §¥¶ˆ·'…¥f¤⁄‡–f¾Égg k ⁄·'…¥g 22 23 mh §©Ì−Å ¾ÄÇ²Â ÉÎË ¾Éj ²ÇÀÉÏÎ− þ®®Ë−ÍÍ Çð Å−ÓÎ þÅ® ÉË−Ì©ÇÏÍ ¾ÄÇ²ÂÍ hm 24 a®−ð©Å− ﬁ¥”¶ˆ¢‹«–f¾Ég ff²³þË hgf¾Ég i §¥¶ˆ·'…¥fff²³þË hgf¾Ég k „·'…¥ggg 25 a®−ð©Å− –‡¥‚ˆ¢‹«–f¾Ég ff²³þË hgf¾Ég k §¥¶ˆ·'…¥fff²³þË hgf¾Ég k ⁄·'…¥ggg code/vm/malloc/mm.c Figure 9.43 Basic constants and macros for manipulating the free list. Creating the Initial Free List Before calling ÀÀˆÀþÄÄÇ² or ÀÀˆðË−−, the application must initialize the heap by calling the ÀÀˆ©Å©Î function (Figure 9.44). The ÀÀˆ©Å©Î function gets four words from the memory system and initializes them to create the empty free list (lines 4–10). It then calls the −ÓÎ−Å®ˆ³−þÉ function (Figure 9.45), which extends the heap by CHUNKSIZE bytes and creates the initial free block. At this point, the allocator is initialized and ready to accept allocate and free requests from the application. The −ÓÎ−Å®ˆ³−þÉ function is invoked in two different circumstances: (1) when the heap is initialized and (2) when ÀÀˆÀþÄÄÇ² is unable to ﬁnd a suitable ﬁt. To maintain alignment, −ÓÎ−Å®ˆ³−þÉ rounds up the requested size to the nearest 894 Chapter 9 Virtual Memory code/vm/malloc/mm.c 1 ©ÅÎ ÀÀˆ©Å©ÎfÌÇ©®g 2 Õ 3 mh £Ë−þÎ− Î³− ©Å©Î©þÄ −ÀÉÎÔ ³−þÉ hm 4 ©ð ff³−þÉˆÄ©ÍÎÉ { À−ÀˆÍ¾ËÂfrh„·'…¥gg {{ fÌÇ©® hgkog 5 Ë−ÎÏËÅ koy 6 –•¶f³−þÉˆÄ©ÍÎÉj ngy mh ¡Ä©×ÅÀ−ÅÎ Éþ®®©Å× hm 7 –•¶f³−þÉˆÄ©ÍÎÉ i foh„·'…¥gj –¡£«f⁄·'…¥j oggy mh –ËÇÄÇ×Ï− ³−þ®−Ë hm 8 –•¶f³−þÉˆÄ©ÍÎÉ i fph„·'…¥gj –¡£«f⁄·'…¥j oggy mh –ËÇÄÇ×Ï− ðÇÇÎ−Ë hm 9 –•¶f³−þÉˆÄ©ÍÎÉ i fqh„·'…¥gj –¡£«fnj oggy mh ¥É©ÄÇ×Ï− ³−þ®−Ë hm 10 ³−þÉˆÄ©ÍÎÉ i{ fph„·'…¥gy 11 12 mh ¥ÓÎ−Å® Î³− −ÀÉÎÔ ³−þÉ Ñ©Î³ þ ðË−− ¾ÄÇ²Â Çð £¤•ﬁ«·'…¥ ¾ÔÎ−Í hm 13 ©ð f−ÓÎ−Å®ˆ³−þÉf£¤•ﬁ«·'…¥m„·'…¥g {{ ﬁ•‹‹g 14 Ë−ÎÏËÅ koy 15 Ë−ÎÏËÅ ny 16 Û code/vm/malloc/mm.c Figure 9.44 ÀÀˆ©Å©Î creates a heap with an initial free block. code/vm/malloc/mm.c 1 ÍÎþÎ©² ÌÇ©® h−ÓÎ−Å®ˆ³−þÉfÍ©Ö−ˆÎ ÑÇË®Íg 2 Õ 3 ²³þË h¾Éy 4 Í©Ö−ˆÎ Í©Ö−y 5 6 mh ¡ÄÄÇ²þÎ− þÅ −Ì−Å ÅÏÀ¾−Ë Çð ÑÇË®Í ÎÇ Àþ©ÅÎþ©Å þÄ©×ÅÀ−ÅÎ hm 7 Í©Ö− { fÑÇË®Í c pg } fÑÇË®Íiog h „·'…¥ x ÑÇË®Í h „·'…¥y 8 ©ð ffÄÇÅ×gf¾É { À−ÀˆÍ¾ËÂfÍ©Ö−gg {{ kog 9 Ë−ÎÏËÅ ﬁ•‹‹y 10 11 mh 'Å©Î©þÄ©Ö− ðË−− ¾ÄÇ²Â ³−þ®−ËmðÇÇÎ−Ë þÅ® Î³− −É©ÄÇ×Ï− ³−þ®−Ë hm 12 –•¶f¤⁄‡–f¾Égj –¡£«fÍ©Ö−j nggy mh ƒË−− ¾ÄÇ²Â ³−þ®−Ë hm 13 –•¶fƒ¶‡–f¾Égj –¡£«fÍ©Ö−j nggy mh ƒË−− ¾ÄÇ²Â ðÇÇÎ−Ë hm 14 –•¶f¤⁄‡–fﬁ¥”¶ˆ¢‹«–f¾Éggj –¡£«fnj oggy mh ﬁ−Ñ −É©ÄÇ×Ï− ³−þ®−Ë hm 15 16 mh £ÇþÄ−Í²− ©ð Î³− ÉË−Ì©ÇÏÍ ¾ÄÇ²Â ÑþÍ ðË−− hm 17 Ë−ÎÏËÅ ²ÇþÄ−Í²−f¾Égy 18 Û code/vm/malloc/mm.c Figure 9.45 −ÓÎ−Å®ˆ³−þÉ extends the heap with a new free block. Section 9.9 Dynamic Memory Allocation 895 multiple of 2 words (8 bytes) and then requests the additional heap space from the memory system (lines 7–9). The remainder of the −ÓÎ−Å®ˆ³−þÉ function (lines 12–17) is somewhat subtle. The heap begins on a double-word aligned boundary, and every call to −ÓÎ−Å®ˆ ³−þÉ returns a block whose size is an integral number of double words. Thus, every call to À−ÀˆÍ¾ËÂ returns a double-word aligned chunk of memory immediately following the header of the epilogue block. This header becomes the header of the new free block (line 12), and the last word of the chunk becomes the new epilogue block header (line 14). Finally, in the likely case that the previous heap was terminated by a free block, we call the ²ÇþÄ−Í²− function to merge the two free blocks and return the block pointer of the merged blocks (line 17). Freeing and Coalescing Blocks An application frees a previously allocated block by calling the ÀÀˆðË−− function (Figure 9.46), which frees the requested block (¾É) and then merges adjacent free blocks using the boundary-tags coalescing technique described in Section 9.9.11. The code in the ²ÇþÄ−Í²− helper function is a straightforward implementation of the four cases outlined in Figure 9.40. There is one somewhat subtle aspect. The free list format we have chosen—with its prologue and epilogue blocks that are always marked as allocated—allows us to ignore the potentially troublesome edge conditions where the requested block ¾É is at the beginning or end of the heap. Without these special blocks, the code would be messier, more error prone, and slower because we would have to check for these rare edge conditions on each and every free request. Allocating Blocks An application requests a block of Í©Ö− bytes of memory by calling the ÀÀˆÀþÄÄÇ² function (Figure 9.47). After checking for spurious requests, the allocator must adjust the requested block size to allow room for the header and the footer, and to satisfy the double-word alignment requirement. Lines 12–13 enforce the minimum block size of 16 bytes: 8 bytes to satisfy the alignment requirement and 8 more bytes for the overhead of the header and footer. For requests over 8 bytes (line 15), the general rule is to add in the overhead bytes and then round up to the nearest multiple of 8. Once the allocator has adjusted the requested size, it searches the free list for a suitable free block (line 18). If there is a ﬁt, then the allocator places the requested block and optionally splits the excess (line 19) and then returns the address of the newly allocated block. If the allocator cannot ﬁnd a ﬁt, it extends the heap with a new free block (lines 24–26), places the requested block in the new free block, optionally splitting the block (line 27), and then returns a pointer to the newly allocated block. 896 Chapter 9 Virtual Memory code/vm/malloc/mm.c 1 ÌÇ©® ÀÀˆðË−−fÌÇ©® h¾Ég 2 Õ 3 Í©Ö−ˆÎ Í©Ö− { §¥¶ˆ·'…¥f¤⁄‡–f¾Éggy 4 5 –•¶f¤⁄‡–f¾Égj –¡£«fÍ©Ö−j nggy 6 –•¶fƒ¶‡–f¾Égj –¡£«fÍ©Ö−j nggy 7 ²ÇþÄ−Í²−f¾Égy 8 Û 9 10 ÍÎþÎ©² ÌÇ©® h²ÇþÄ−Í²−fÌÇ©® h¾Ég 11 Õ 12 Í©Ö−ˆÎ ÉË−ÌˆþÄÄÇ² { §¥¶ˆ¡‹‹ﬂ£fƒ¶‡–f–‡¥‚ˆ¢‹«–f¾Égggy 13 Í©Ö−ˆÎ Å−ÓÎˆþÄÄÇ² { §¥¶ˆ¡‹‹ﬂ£f¤⁄‡–fﬁ¥”¶ˆ¢‹«–f¾Égggy 14 Í©Ö−ˆÎ Í©Ö− { §¥¶ˆ·'…¥f¤⁄‡–f¾Éggy 15 16 ©ð fÉË−ÌˆþÄÄÇ² dd Å−ÓÎˆþÄÄÇ²g Õ mh £þÍ− o hm 17 Ë−ÎÏËÅ ¾Éy 18 Û 19 20 −ÄÍ− ©ð fÉË−ÌˆþÄÄÇ² dd _Å−ÓÎˆþÄÄÇ²g Õ mh £þÍ− p hm 21 Í©Ö− i{ §¥¶ˆ·'…¥f¤⁄‡–fﬁ¥”¶ˆ¢‹«–f¾Égggy 22 –•¶f¤⁄‡–f¾Égj –¡£«fÍ©Ö−j nggy 23 –•¶fƒ¶‡–f¾Égj –¡£«fÍ©Ö−jnggy 24 Û 25 26 −ÄÍ− ©ð f_ÉË−ÌˆþÄÄÇ² dd Å−ÓÎˆþÄÄÇ²g Õ mh £þÍ− q hm 27 Í©Ö− i{ §¥¶ˆ·'…¥f¤⁄‡–f–‡¥‚ˆ¢‹«–f¾Égggy 28 –•¶fƒ¶‡–f¾Égj –¡£«fÍ©Ö−j nggy 29 –•¶f¤⁄‡–f–‡¥‚ˆ¢‹«–f¾Éggj –¡£«fÍ©Ö−j nggy 30 ¾É { –‡¥‚ˆ¢‹«–f¾Égy 31 Û 32 33 −ÄÍ− Õ mh £þÍ− r hm 34 Í©Ö− i{ §¥¶ˆ·'…¥f¤⁄‡–f–‡¥‚ˆ¢‹«–f¾Éggg i 35 §¥¶ˆ·'…¥fƒ¶‡–fﬁ¥”¶ˆ¢‹«–f¾Égggy 36 –•¶f¤⁄‡–f–‡¥‚ˆ¢‹«–f¾Éggj –¡£«fÍ©Ö−j nggy 37 –•¶fƒ¶‡–fﬁ¥”¶ˆ¢‹«–f¾Éggj –¡£«fÍ©Ö−j nggy 38 ¾É { –‡¥‚ˆ¢‹«–f¾Égy 39 Û 40 Ë−ÎÏËÅ ¾Éy 41 Û code/vm/malloc/mm.c Figure 9.46 ÀÀˆðË−− frees a block and uses boundary-tag coalescing to merge it with any adjacent free blocks in constant time. Section 9.9 Dynamic Memory Allocation 897 code/vm/malloc/mm.c 1 ÌÇ©® hÀÀˆÀþÄÄÇ²fÍ©Ö−ˆÎ Í©Ö−g 2 Õ 3 Í©Ö−ˆÎ þÍ©Ö−y mh ¡®ÁÏÍÎ−® ¾ÄÇ²Â Í©Ö− hm 4 Í©Ö−ˆÎ −ÓÎ−Å®Í©Ö−y mh ¡ÀÇÏÅÎ ÎÇ −ÓÎ−Å® ³−þÉ ©ð ÅÇ ð©Î hm 5 ²³þË h¾Éy 6 7 mh '×ÅÇË− ÍÉÏË©ÇÏÍ Ë−ÊÏ−ÍÎÍ hm 8 ©ð fÍ©Ö− {{ ng 9 Ë−ÎÏËÅ ﬁ•‹‹y 10 11 mh ¡®ÁÏÍÎ ¾ÄÇ²Â Í©Ö− ÎÇ ©Å²ÄÏ®− ÇÌ−Ë³−þ® þÅ® þÄ©×ÅÀ−ÅÎ Ë−ÊÍl hm 12 ©ð fÍ©Ö− z{ ⁄·'…¥g 13 þÍ©Ö− { ph⁄·'…¥y 14 −ÄÍ− 15 þÍ©Ö− { ⁄·'…¥ h ffÍ©Ö− i f⁄·'…¥g i f⁄·'…¥kogg m ⁄·'…¥gy 16 17 mh ·−þË²³ Î³− ðË−− Ä©ÍÎ ðÇË þ ð©Î hm 18 ©ð ff¾É { ð©Å®ˆð©ÎfþÍ©Ö−gg _{ ﬁ•‹‹g Õ 19 ÉÄþ²−f¾Éj þÍ©Ö−gy 20 Ë−ÎÏËÅ ¾Éy 21 Û 22 23 mh ﬁÇ ð©Î ðÇÏÅ®l §−Î ÀÇË− À−ÀÇËÔ þÅ® ÉÄþ²− Î³− ¾ÄÇ²Â hm 24 −ÓÎ−Å®Í©Ö− { ›¡”fþÍ©Ö−j£¤•ﬁ«·'…¥gy 25 ©ð ff¾É { −ÓÎ−Å®ˆ³−þÉf−ÓÎ−Å®Í©Ö−m„·'…¥gg {{ ﬁ•‹‹g 26 Ë−ÎÏËÅ ﬁ•‹‹y 27 ÉÄþ²−f¾Éj þÍ©Ö−gy 28 Ë−ÎÏËÅ ¾Éy 29 Û code/vm/malloc/mm.c Figure 9.47 ÀÀˆÀþÄÄÇ² allocates a block from the free list. Practice Problem 9.8 (solution page 920) Implement a ð©Å®ˆð©Î function for the simple allocator described in Section 9.9.12. ÍÎþÎ©² ÌÇ©® hð©Å®ˆð©ÎfÍ©Ö−ˆÎ þÍ©Ö−g Your solution should perform a ﬁrst-ﬁt search of the implicit free list. Practice Problem 9.9 (solution page 920) Implement a ÉÄþ²− function for the example allocator. 898 Chapter 9 Virtual Memory ÍÎþÎ©² ÌÇ©® ÉÄþ²−fÌÇ©® h¾Éj Í©Ö−ˆÎ þÍ©Ö−g Your solution should place the requested block at the beginning of the free block, splitting only if the size of the remainder would equal or exceed the mini- mum block size. 9.9.13 Explicit Free Lists The implicit free list provides us with a simple way to introduce some basic allocator concepts. However, because block allocation time is linear in the total number of heap blocks, the implicit free list is not appropriate for a general- purpose allocator (although it might be ﬁne for a special-purpose allocator where the number of heap blocks is known beforehand to be small). A better approach is to organize the free blocks into some form of explicit data structure. Since by deﬁnition the body of a free block is not needed by the program, the pointers that implement the data structure can be stored within the bodies of the free blocks. For example, the heap can be organized as a doubly linked free list by including a ÉË−® (predecessor) and ÍÏ²² (successor) pointer in each free block, as shown in Figure 9.48. Using a doubly linked list instead of an implicit free list reduces the ﬁrst-ﬁt allocation time from linear in the total number of blocks to linear in the number of free blocks. However, the time to free a block can be either linear or constant, depending on the policy we choose for ordering the blocks in the free list. Block size Payload (a) Allocated block Padding (optional) a/f Block size a/f 31 3 2 1 0 Header Footer Block size pred (predecessor) (b) Free block succ (successor) Padding (optional) a/f Block size a/f 31 3 2 1 0 Header Old payload Footer Figure 9.48 Format of heap blocks that use doubly linked free lists. Section 9.9 Dynamic Memory Allocation 899 One approach is to maintain the list in last-in ﬁrst-out (LIFO) order by in- serting newly freed blocks at the beginning of the list. With a LIFO ordering and a ﬁrst-ﬁt placement policy, the allocator inspects the most recently used blocks ﬁrst. In this case, freeing a block can be performed in constant time. If boundary tags are used, then coalescing can also be performed in constant time. Another approach is to maintain the list in address order, where the address of each block in the list is less than the address of its successor. In this case, freeing a block requires a linear-time search to locate the appropriate predecessor. The trade-off is that address-ordered ﬁrst ﬁt enjoys better memory utilization than LIFO-ordered ﬁrst ﬁt, approaching the utilization of best ﬁt. A disadvantage of explicit lists in general is that free blocks must be large enough to contain all of the necessary pointers, as well as the header and possibly a footer. This results in a larger minimum block size and increases the potential for internal fragmentation. 9.9.14 Segregated Free Lists As we have seen, an allocator that uses a single linked list of free blocks requires time linear in the number of free blocks to allocate a block. A popular approach for reducing the allocation time, known generally as segregated storage, is to maintain multiple free lists, where each list holds blocks that are roughly the same size. The general idea is to partition the set of all possible block sizes into equivalence classes called size classes. There are many ways to deﬁne the size classes. For example, we might partition the block sizes by powers of 2: {1}, {2}, {3, 4}, {5–8}, ... , {1,025–2,048}, {2,049–4,096}, {4,097–∞} Or we might assign small blocks to their own size classes and partition large blocks by powers of 2: {1}, {2}, {3}, ... , {1,023}, {1,024}, {1,025–2,048}, {2,049–4,096}, {4,097–∞} The allocator maintains an array of free lists, with one free list per size class, ordered by increasing size. When the allocator needs a block of size n, it searches the appropriate free list. If it cannot ﬁnd a block that ﬁts, it searches the next list, and so on. The dynamic storage allocation literature describes dozens of variants of seg- regated storage that differ in how they deﬁne size classes, when they perform coalescing, when they request additional heap memory from the operating sys- tem, whether they allow splitting, and so forth. To give you a sense of what is possible, we will describe two of the basic approaches: simple segregated storage and segregated ﬁts. 900 Chapter 9 Virtual Memory Simple Segregated Storage With simple segregated storage, the free list for each size class contains same-size blocks, each the size of the largest element of the size class. For example, if some size class is deﬁned as {17–32}, then the free list for that class consists entirely of blocks of size 32. To allocate a block of some given size, we check the appropriate free list. If the list is not empty, we simply allocate the ﬁrst block in its entirety. Free blocks are never split to satisfy allocation requests. If the list is empty, the allocator requests a ﬁxed-size chunk of additional memory from the operating system (typically a multiple of the page size), divides the chunk into equal-size blocks, and links the blocks together to form the new free list. To free a block, the allocator simply inserts the block at the front of the appropriate free list. There are a number of advantages to this simple scheme. Allocating and freeing blocks are both fast constant-time operations. Further, the combination of the same-size blocks in each chunk, no splitting, and no coalescing means that there is very little per-block memory overhead. Since each chunk has only same- size blocks, the size of an allocated block can be inferred from its address. Since there is no coalescing, allocated blocks do not need an allocated/free ﬂag in the header. Thus, allocated blocks require no headers, and since there is no coalescing, they do not require any footers either. Since allocate and free operations insert and delete blocks at the beginning of the free list, the list need only be singly linked instead of doubly linked. The bottom line is that the only required ﬁeld in any block is a one-word ÍÏ²² pointer in each free block, and thus the minimum block size is only one word. A signiﬁcant disadvantage is that simple segregated storage is susceptible to internal and external fragmentation. Internal fragmentation is possible because free blocks are never split. Worse, certain reference patterns can cause extreme external fragmentation because free blocks are never coalesced (Practice Prob- lem 9.10). Practice Problem 9.10 (solution page 921) Describe a reference pattern that results in severe external fragmentation in an allocator based on simple segregated storage. Segregated Fits With this approach, the allocator maintains an array of free lists. Each free list is associated with a size class and is organized as some kind of explicit or implicit list. Each list contains potentially different-size blocks whose sizes are members of the size class. There are many variants of segregated ﬁts allocators. Here we describe a simple version. To allocate a block, we determine the size class of the request and do a ﬁrst- ﬁt search of the appropriate free list for a block that ﬁts. If we ﬁnd one, then we (optionally) split it and insert the fragment in the appropriate free list. If we cannot ﬁnd a block that ﬁts, then we search the free list for the next larger size class. We Section 9.10 Garbage Collection 901 repeat until we ﬁnd a block that ﬁts. If none of the free lists yields a block that ﬁts, then we request additional heap memory from the operating system, allocate the block out of this new heap memory, and place the remainder in the appropriate size class. To free a block, we coalesce and place the result on the appropriate free list. The segregated ﬁts approach is a popular choice with production-quality allocators such as the GNU ÀþÄÄÇ² package provided in the C standard library because it is both fast and memory efﬁcient. Search times are reduced because searches are limited to particular parts of the heap instead of the entire heap. Memory utilization can improve because of the interesting fact that a simple ﬁrst- ﬁt search of a segregated free list approximates a best-ﬁt search of the entire heap. Buddy Systems A buddy system is a special case of segregated ﬁts where each size class is a power of 2. The basic idea is that, given a heap of 2m words, we maintain a separate free list for each block size 2k, where 0 ≤ k ≤ m. Requested block sizes are rounded up to the nearest power of 2. Originally, there is one free block of size 2m words. To allocate a block of size 2k, we ﬁnd the ﬁrst available block of size 2j , such that k ≤ j ≤ m.If j = k, then we are done. Otherwise, we recursively split the block in half until j = k. As we perform this splitting, each remaining half (known as a buddy) is placed on the appropriate free list. To free a block of size 2k, we continue coalescing with the free buddies. When we encounter an allocated buddy, we stop the coalescing. A key fact about buddy systems is that, given the address and size of a block, it is easy to compute the address of its buddy. For example, a block of size 32 bytes with address xxx ...x00000 has its buddy at address xxx ...x10000 In other words, the addresses of a block and its buddy differ in exactly one bit position. The major advantage of a buddy system allocator is its fast searching and coalescing. The major disadvantage is that the power-of-2 requirement on the block size can cause signiﬁcant internal fragmentation. For this reason, buddy system allocators are not appropriate for general-purpose workloads. However, for certain application-speciﬁc workloads, where the block sizes are known in advance to be powers of 2, buddy system allocators have a certain appeal. 9.10 Garbage Collection With an explicit allocator such as the C ÀþÄÄÇ² package, an application allocates and frees heap blocks by making calls to ÀþÄÄÇ² and ðË−−. It is the application’s responsibility to free any allocated blocks that it no longer needs. 902 Chapter 9 Virtual Memory Failing to free allocated blocks is a common programming error. For example, consider the following C function that allocates a block of temporary storage as part of its processing: 1 ÌÇ©® ×þË¾þ×−fg 2 Õ 3 ©ÅÎ hÉ { f©ÅÎ hg›þÄÄÇ²fospoqgy 4 5 Ë−ÎÏËÅy mh ¡ËËþÔ É ©Í ×þË¾þ×− þÎ Î³©Í ÉÇ©ÅÎ hm 6 Û Since É is no longer needed by the program, it should have been freed before ×þË¾þ×− returned. Unfortunately, the programmer has forgotten to free the block. It remains allocated for the lifetime of the program, needlessly occupying heap space that could be used to satisfy subsequent allocation requests. A garbage collector is a dynamic storage allocator that automatically frees al- located blocks that are no longer needed by the program. Such blocks are known as garbage (hence the term “garbage collector”). The process of automatically reclaiming heap storage is known as garbage collection. In a system that supports garbage collection, applications explicitly allocate heap blocks but never explic- itly free them. In the context of a C program, the application calls ÀþÄÄÇ² but never calls ðË−−. Instead, the garbage collector periodically identiﬁes the garbage blocks and makes the appropriate calls to ðË−− to place those blocks back on the free list. Garbage collection dates back to Lisp systems developed by John McCarthy at MIT in the early 1960s. It is an important part of modern language systems such as Java, ML, Perl, and Mathematica, and it remains an active and important area of research. The literature describes an amazing number of approaches for garbage collection. We will limit our discussion to McCarthy’s original Mark&Sweep al- gorithm, which is interesting because it can be built on top of an existing ÀþÄÄÇ² package to provide garbage collection for C and C++ programs. 9.10.1 Garbage Collector Basics A garbage collector views memory as a directed reachability graph of the form shown in Figure 9.49. The nodes of the graph are partitioned into a set of root nodes and a set of heap nodes. Each heap node corresponds to an allocated block in the heap. A directed edge p → q means that some location in block p points to some location in block q. Root nodes correspond to locations not in the heap that contain pointers into the heap. These locations can be registers, variables on the stack, or global variables in the read/write data area of virtual memory. We say that a node p is reachable if there exists a directed path from any root node to p. At any point in time, the unreachable nodes correspond to garbage that can never be used again by the application. The role of a garbage collector is to maintain some representation of the reachability graph and periodically reclaim the unreachable nodes by freeing them and returning them to the free list. Section 9.10 Garbage Collection 903 Figure 9.49 A garbage collector’s view of memory as a directed graph. Root nodes Heap nodes Reachable Unreachable (garbage) Figure 9.50 Integrating a conserva- tive garbage collector andaC ÀþÄÄÇ² package. C application program malloc() Conservative garbage collector free() Dynamic storage allocator Garbage collectors for languages like ML and Java, which exert tight control over how applications create and use pointers, can maintain an exact representa- tion of the reachability graph and thus can reclaim all garbage. However, collectors for languages like C and C++ cannot in general maintain exact representations of the reachability graph. Such collectors are known as conservative garbage col- lectors. They are conservative in the sense that each reachable block is correctly identiﬁed as reachable, while some unreachable nodes might be incorrectly iden- tiﬁed as reachable. Collectors can provide their service on demand, or they can run as separate threads in parallel with the application, continuously updating the reachability graph and reclaiming garbage. For example, consider how we might incorporate a conservative collector for C programs into an existing ÀþÄÄÇ² package, as shown in Figure 9.50. The application calls ÀþÄÄÇ² in the usual manner whenever it needs heap space. If ÀþÄÄÇ² is unable to ﬁnd a free block that ﬁts, then it calls the garbage col- lector in hopes of reclaiming some garbage to the free list. The collector identiﬁes the garbage blocks and returns them to the heap by calling the ðË−− function. The key idea is that the collector calls ðË−− instead of the application. When the call to the collector returns, ÀþÄÄÇ² tries again to ﬁnd a free block that ﬁts. If that fails, then it can ask the operating system for additional memory. Eventually, ÀþÄÄÇ² returns a pointer to the requested block (if successful) or the NULL pointer (if unsuccessful). 9.10.2 Mark&Sweep Garbage Collectors A Mark&Sweep garbage collector consists of a mark phase, which marks all reachable and allocated descendants of the root nodes, followed by a sweep phase, which frees each unmarked allocated block. Typically, one of the spare low-order bits in the block header is used to indicate whether a block is marked or not. 904 Chapter 9 Virtual Memory (a) ÀþËÂ function ÌÇ©® ÀþËÂfÉÎË Ég Õ ©ð ff¾ { ©Í–ÎËfÉgg {{ ﬁ•‹‹g Ë−ÎÏËÅy ©ð f¾ÄÇ²Â›þËÂ−®f¾gg Ë−ÎÏËÅy ÀþËÂ¢ÄÇ²Âf¾gy Ä−Å { Ä−Å×Î³f¾gy ðÇË f©{ny © z Ä−Åy ©iig ÀþËÂf¾‰©`gy Ë−ÎÏËÅy Û (b) ÍÑ−−É function ÌÇ©® ÍÑ−−ÉfÉÎË ¾j ÉÎË −Å®g Õ Ñ³©Ä− f¾ z −Å®g Õ ©ð f¾ÄÇ²Â›þËÂ−®f¾gg ÏÅÀþËÂ¢ÄÇ²Âf¾gy −ÄÍ− ©ð f¾ÄÇ²Â¡ÄÄÇ²þÎ−®f¾gg ðË−−f¾gy ¾ { Å−ÓÎ¢ÄÇ²Âf¾gy Û Ë−ÎÏËÅy Û Figure 9.51 Pseudocode for the ÀþËÂ and ÍÑ−−É functions. Our description of Mark&Sweep will assume the following functions, where ÉÎË is deﬁned as ÎÔÉ−®−ð ÌÇ©® hÉÎË: ÉÎË ©Í–ÎËfÉÎË Ég.If É points to some word in an allocated block, it returns a pointer ¾ to the beginning of that block. Returns NULL otherwise. ©ÅÎ ¾ÄÇ²Â›þËÂ−®fÉÎË ¾g. Returns ÎËÏ− if block ¾ is already marked. ©ÅÎ ¾ÄÇ²Â¡ÄÄÇ²þÎ−®fÉÎË ¾g. Returns ÎËÏ− if block ¾ is allocated. ÌÇ©® ÀþËÂ¢ÄÇ²ÂfÉÎË ¾g. Marks block ¾. ©ÅÎ Ä−Å×Î³fÉÎË ¾g. Returns the length in words (excluding the header) of block ¾. ÌÇ©® ÏÅÀþËÂ¢ÄÇ²ÂfÉÎË ¾g. Changes the status of block ¾ from marked to un- marked. ÉÎË Å−ÓÎ¢ÄÇ²ÂfÉÎË ¾g. Returns the successor of block b in the heap. The mark phase calls the ÀþËÂ function shown in Figure 9.51(a) once for each root node. The ÀþËÂ function returns immediately if É does not point to an allocated and unmarked heap block. Otherwise, it marks the block and calls itself recursively on each word in block. Each call to the ÀþËÂ function marks any unmarked and reachable descendants of some root node. At the end of the mark phase, any allocated block that is not marked is guaranteed to be unreachable and, hence, garbage that can be reclaimed in the sweep phase. The sweep phase is a single call to the ÍÑ−−É function shown in Figure 9.51(b). The ÍÑ−−É function iterates over each block in the heap, freeing any unmarked allocated blocks (i.e., garbage) that it encounters. Figure 9.52 shows a graphical interpretation of Mark&Sweep for a small heap. Block boundaries are indicated by heavy lines. Each square corresponds to a word of memory. Each block has a one-word header, which is either marked or unmarked. Section 9.10 Garbage Collection 905 Figure 9.52 Mark&Sweep example. Note that the arrows in this example denote memory references, not free list pointers. 12 34 5 6 Before mark: Root After mark: Unmarked block header Marked block header After sweep: FreeFree Figure 9.53 Left and right pointers in a balanced tree of allocated blocks. Size Left Right Remainder of block Allocated block header \u0006\u0007 Initially, the heap in Figure 9.52 consists of six allocated blocks, each of which is unmarked. Block 3 contains a pointer to block 1. Block 4 contains pointers to blocks 3 and 6. The root points to block 4. After the mark phase, blocks 1, 3, 4, and 6 are marked because they are reachable from the root. Blocks 2 and 5 are unmarked because they are unreachable. After the sweep phase, the two unreachable blocks are reclaimed to the free list. 9.10.3 Conservative Mark&Sweep for C Programs Mark&Sweep is an appropriate approach for garbage collecting C programs be- cause it works in place without moving any blocks. However, the C language poses some interesting challenges for the implementation of the ©Í–ÎË function. First, C does not tag memory locations with any type information. Thus, there is no obvious way for ©Í–ÎË to determine if its input parameter É is a pointer or not. Second, even if we were to know that É was a pointer, there would be no obvious way for ©Í–ÎË to determine whether É points to some location in the payload of an allocated block. One solution to the latter problem is to maintain the set of allocated blocks as a balanced binary tree that maintains the invariant that all blocks in the left subtree are located at smaller addresses and all blocks in the right subtree are located in larger addresses. As shown in Figure 9.53, this requires two additional ﬁelds (Ä−ðÎ and Ë©×³Î) in the header of each allocated block. Each ﬁeld points to the header of some allocated block. The ©Í–ÎËfÉÎË Ég function uses the tree to perform a binary search of the allocated blocks. At each step, it relies on the size ﬁeld in the block header to determine if É falls within the extent of the block. 906 Chapter 9 Virtual Memory The balanced tree approach is correct in the sense that it is guaranteed to mark all of the nodes that are reachable from the roots. This is a necessary guarantee, as application users would certainly not appreciate having their allocated blocks prematurely returned to the free list. However, it is conservative in the sense that it may incorrectly mark blocks that are actually unreachable, and thus it may fail to free some garbage. While this does not affect the correctness of application programs, it can result in unnecessary external fragmentation. The fundamental reason that Mark&Sweep collectors for C programs must be conservative is that the C language does not tag memory locations with type information. Thus, scalars like ©ÅÎsor ðÄÇþÎs can masquerade as pointers. For example, suppose that some reachable allocated block contains an ©ÅÎ in its payload whose value happens to correspond to an address in the payload of some other allocated block b. There is no way for the collector to infer that the data is really an ©ÅÎ and not a pointer. Therefore, the allocator must conservatively mark block b as reachable, when in fact it might not be. 9.11 Common Memory-Related Bugs in C Programs Managing and using virtual memory can be a difﬁcult and error-prone task for C programmers. Memory-related bugs are among the most frightening because they often manifest themselves at a distance, in both time and space, from the source of the bug. Write the wrong data to the wrong location, and your program can run for hours before it ﬁnally fails in some distant part of the program. We conclude our discussion of virtual memory with a look at of some of the common memory-related bugs. 9.11.1 Dereferencing Bad Pointers As we learned in Section 9.7.2, there are large holes in the virtual address space of a process that are not mapped to any meaningful data. If we attempt to dereference a pointer into one of these holes, the operating system will terminate our program with a segmentation exception. Also, some areas of virtual memory are read-only. Attempting to write to one of these areas terminates the program with a protection exception. A common example of dereferencing a bad pointer is the classic Í²þÅð bug. Suppose we want to use Í²þÅð to read an integer from ÍÎ®©Å into a variable. The correct way to do this is to pass Í²þÅð a format string and the address of the variable: Í²þÅðf‘c®‘j dÌþÄg However, it is easy for new C programmers (and experienced ones too!) to pass the contents of ÌþÄ instead of its address: Í²þÅðf‘c®‘j ÌþÄg Section 9.11 Common Memory-Related Bugs in C Programs 907 In this case, Í²þÅð will interpret the contents of ÌþÄ as an address and attempt to write a word to that location. In the best case, the program terminates immediately with an exception. In the worst case, the contents of ÌþÄ correspond to some valid read/write area of virtual memory, and we overwrite memory, usually with disastrous and bafﬂing consequences much later. 9.11.2 Reading Uninitialized Memory While bss memory locations (such as uninitialized global C variables) are always initialized to zeros by the loader, this is not true for heap memory. A common error is to assume that heap memory is initialized to zero: 1 mh ‡−ÎÏËÅÔ{¡Óhm 2 ©ÅÎ hÀþÎÌ−²f©ÅÎ hh¡j ©ÅÎ hÓj ©ÅÎ Åg 3 Õ 4 ©ÅÎ ©j Áy 5 6 ©ÅÎ hÔ { f©ÅÎ hg›þÄÄÇ²fÅ h Í©Ö−Çðf©ÅÎggy 7 8 ðÇËf©{ny©zÅy ©iig 9 ðÇËfÁ{nyÁzÅy Áiig 10 Ô‰©` i{ ¡‰©`‰Á` h Ó‰Á`y 11 Ë−ÎÏËÅ Ôy 12 Û In this example, the programmer has incorrectly assumed that vector Ô has been initialized to zero. A correct implementation would explicitly zero Ô‰©` or use ²þÄÄÇ². 9.11.3 Allowing Stack Buffer Overﬂows As we saw in Section 3.10.3, a program has a buffer overﬂow bug if it writes to a target buffer on the stack without examining the size of the input string. For example, the following function has a buffer overﬂow bug because the ×−ÎÍ function copies an arbitrary-length string to the buffer. To ﬁx this, we would need to use the ð×−ÎÍ function, which limits the size of the input string. 1 ÌÇ©® ¾ÏðÇÌ−ËðÄÇÑfg 2 Õ 3 ²³þË ¾Ïð‰tr`y 4 5 ×−ÎÍf¾Ïðgy mh ¤−Ë− ©Í Î³− ÍÎþ²Â ¾Ïðð−Ë ÇÌ−ËðÄÇÑ ¾Ï× hm 6 Ë−ÎÏËÅy 7 Û 908 Chapter 9 Virtual Memory 9.11.4 Assuming That Pointers and the Objects They Point to Are the Same Size One common mistake is to assume that pointers to objects are the same size as the objects they point to: 1 mh £Ë−þÎ− þÅ ÅÓÀ þËËþÔ hm 2 ©ÅÎ hhÀþÂ−¡ËËþÔof©ÅÎ Åj ©ÅÎ Àg 3 Õ 4 ©ÅÎ ©y 5 ©ÅÎ hh¡ { f©ÅÎ hhg›þÄÄÇ²fÅ h Í©Ö−Çðf©ÅÎggy 6 7 ðÇËf©{ny©zÅy ©iig 8 ¡‰©` { f©ÅÎ hg›þÄÄÇ²fÀ h Í©Ö−Çðf©ÅÎggy 9 Ë−ÎÏËÅ ¡y 10 Û The intent here is to create an array of n pointers, each of which points to an array of m ©ÅÎs. However, because the programmer has written Í©Ö−Çðf©ÅÎg instead of Í©Ö−Çðf©ÅÎ hg in line 5, the code actually creates an array of ©ÅÎs. This code will run ﬁne on machines where ©ÅÎs and pointers to ©ÅÎs are the same size. But if we run this code on a machine like the Core i7, where a pointer is larger than an ©ÅÎ, then the loop in lines 7–8 will write past the end of the ¡ array. Since one of these words will likely be the boundary-tag footer of the allocated block, we may not discover the error until we free the block much later in the program, at which point the coalescing code in the allocator will fail dramatically and for no apparent reason. This is an insidious example of the kind of “action at a distance” that is so typical of memory-related programming bugs. 9.11.5 Making Off-by-One Errors Off-by-one errors are another common source of overwriting bugs: 1 mh £Ë−þÎ− þÅ ÅÓÀ þËËþÔ hm 2 ©ÅÎ hhÀþÂ−¡ËËþÔpf©ÅÎ Åj ©ÅÎ Àg 3 Õ 4 ©ÅÎ ©y 5 ©ÅÎ hh¡ { f©ÅÎ hhg›þÄÄÇ²fÅ h Í©Ö−Çðf©ÅÎ hggy 6 7 ðÇË f© { ny © z{ Åy ©iig 8 ¡‰©` { f©ÅÎ hg›þÄÄÇ²fÀ h Í©Ö−Çðf©ÅÎggy 9 Ë−ÎÏËÅ ¡y 10 Û This is another version of the program in the previous section. Here we have created an n-element array of pointers in line 5 but then tried to initialize n + 1of its elements in lines 7 and 8, in the process overwriting some memory that follows the ¡ array. Section 9.11 Common Memory-Related Bugs in C Programs 909 9.11.6 Referencing a Pointer Instead of the Object It Points To If we are not careful about the precedence and associativity of C operators, then we incorrectly manipulate a pointer instead of the object it points to. For example, consider the following function, whose purpose is to remove the ﬁrst item in a binary heap of hÍ©Ö− items and then reheapify the remaining hÍ©Ö− k o items: 1 ©ÅÎ h¾©Å³−þÉ⁄−Ä−Î−f©ÅÎ hh¾©Å³−þÉj ©ÅÎ hÍ©Ö−g 2 Õ 3 ©ÅÎ hÉþ²Â−Î { ¾©Å³−þÉ‰n`y 4 5 ¾©Å³−þÉ‰n` { ¾©Å³−þÉ‰hÍ©Ö− k o`y 6 hÍ©Ö−kky mh ¶³©Í Í³ÇÏÄ® ¾− fhÍ©Ö−gkk hm 7 ³−þÉ©ðÔf¾©Å³−þÉj hÍ©Ö−j ngy 8 Ë−ÎÏËÅfÉþ²Â−Îgy 9 Û In line 6, the intent is to decrement the integer value pointed to by the Í©Ö− pointer. However, because the unary kk and h operators have the same precedence and associate from right to left, the code in line 6 actually decrements the pointer itself instead of the integer value that it points to. If we are lucky, the program will crash immediately. But more likely we will be left scratching our heads when the program produces an incorrect answer much later in its execution. The moral here is to use parentheses whenever in doubt about precedence and associativity. For example, in line 6, we should have clearly stated our intent by using the expression fhÍ©Ö−gkk. 9.11.7 Misunderstanding Pointer Arithmetic Another common mistake is to forget that arithmetic operations on pointers are performed in units that are the size of the objects they point to, which are not necessarily bytes. For example, the intent of the following function is to scan an array of ©ÅÎs and return a pointer to the ﬁrst occurrence of ÌþÄ: 1 ©ÅÎ hÍ−þË²³f©ÅÎ hÉj ©ÅÎ ÌþÄg 2 Õ 3 Ñ³©Ä− fhÉ dd hÉ _{ ÌþÄg 4 É i{ Í©Ö−Çðf©ÅÎgy mh ·³ÇÏÄ® ¾− Éii hm 5 Ë−ÎÏËÅ Éy 6 Û However, because line 4 increments the pointer by 4 (the number of bytes in an integer) each time through the loop, the function incorrectly scans every fourth integer in the array. 910 Chapter 9 Virtual Memory 9.11.8 Referencing Nonexistent Variables Naive C programmers who do not understand the stack discipline will sometimes reference local variables that are no longer valid, as in the following example: 1 ©ÅÎ hÍÎþ²ÂË−ð fg 2 Õ 3 ©ÅÎ ÌþÄy 4 5 Ë−ÎÏËÅ dÌþÄy 6 Û This function returns a pointer (say, É) to a local variable on the stack and then pops its stack frame. Although É still points to a valid memory address, it no longer points to a valid variable. When other functions are called later in the program, the memory will be reused for their stack frames. Later, if the program assigns some value to hÉ, then it might actually be modifying an entry in another function’s stack frame, with potentially disastrous and bafﬂing consequences. 9.11.9 Referencing Data in Free Heap Blocks A similar error is to reference data in heap blocks that have already been freed. Consider the following example, which allocates an integer array Ó in line 6, prematurely frees block Ó in line 10, and then later references it in line 14: 1 ©ÅÎ h³−þÉË−ðf©ÅÎ Åj ©ÅÎ Àg 2 Õ 3 ©ÅÎ ©y 4 ©ÅÎ hÓj hÔy 5 6 Ó { f©ÅÎ hg›þÄÄÇ²fÅ h Í©Ö−Çðf©ÅÎggy 7 8 l l l // Other calls to malloc and free go here 9 10 ðË−−fÓgy 11 12 Ô { f©ÅÎ hg›þÄÄÇ²fÀ h Í©Ö−Çðf©ÅÎggy 13 ðÇËf©{ny©zÀy ©iig 14 Ô‰©` { Ó‰©`iiy mh ﬂÇÉÍ_ Ó‰©` ©Í þ ÑÇË® ©Å þ ðË−− ¾ÄÇ²Â hm 15 16 Ë−ÎÏËÅ Ôy 17 Û Depending on the pattern of ÀþÄÄÇ² and ðË−− calls that occur between lines 6 and 10, when the program references Ó‰©` in line 14, the array Ó might be part of some other allocated heap block and may have been overwritten. As with many Section 9.12 Summary 911 memory-related bugs, the error will only become evident later in the program when we notice that the values in Ô are corrupted. 9.11.10 Introducing Memory Leaks Memory leaks are slow, silent killers that occur when programmers inadvertently create garbage in the heap by forgetting to free allocated blocks. For example, the following function allocates a heap block Ó and then returns without freeing it: 1 ÌÇ©® Ä−þÂf©ÅÎ Åg 2 Õ 3 ©ÅÎ hÓ { f©ÅÎ hg›þÄÄÇ²fÅ h Í©Ö−Çðf©ÅÎggy 4 5 Ë−ÎÏËÅy mh Ó ©Í ×þË¾þ×− þÎ Î³©Í ÉÇ©ÅÎ hm 6 Û If Ä−þÂ is called frequently, then the heap will gradually ﬁll up with garbage, in the worst case consuming the entire virtual address space. Memory leaks are particularly serious for programs such as daemons and servers, which by deﬁnition never terminate. 9.12 Summary Virtual memory is an abstraction of main memory. Processors that support vir- tual memory reference main memory using a form of indirection known as virtual addressing. The processor generates a virtual address, which is translated into a physical address before being sent to the main memory. The translation of ad- dresses from a virtual address space to a physical address space requires close cooperation between hardware and software. Dedicated hardware translates vir- tual addresses using page tables whose contents are supplied by the operating system. Virtual memory provides three important capabilities. First, it automatically caches recently used contents of the virtual address space stored on disk in main memory. The block in a virtual memory cache is known as a page. A reference to a page on disk triggers a page fault that transfers control to a fault handler in the operating system. The fault handler copies the page from disk to the main memory cache, writing back the evicted page if necessary. Second, virtual memory simpliﬁes memory management, which in turn simpliﬁes linking, sharing data between processes, the allocation of memory for processes, and program loading. Finally, virtual memory simpliﬁes memory protection by incorporating protection bits into every page table entry. The process of address translation must be integrated with the operation of any hardware caches in the system. Most page table entries are located in the L1 cache, but the cost of accessing page table entries from L1 is usually eliminated by an on-chip cache of page table entries called a TLB. 912 Chapter 9 Virtual Memory Modern systems initialize chunks of virtual memory by associating them with chunks of ﬁles on disk, a process known as memory mapping. Memory mapping provides an efﬁcient mechanism for sharing data, creating new processes, and loading programs. Applications can manually create and delete areas of the virtual address space using the ÀÀþÉ function. However, most programs rely on a dynamic memory allocator such as ÀþÄÄÇ², which manages memory in an area of the virtual address space called the heap. Dynamic memory allocators are application-level programs with a system-level feel, directly manipulating memory without much help from the type system. Allocators come in two ﬂavors. Explicit allocators require applications to explicitly free their memory blocks. Implicit allocators (garbage collectors) free any unused and unreachable blocks automatically. Managing and using memory is a difﬁcult and error-prone task for C program- mers. Examples of common errors include dereferencing bad pointers, reading uninitialized memory, allowing stack buffer overﬂows, assuming that pointers and the objects they point to are the same size, referencing a pointer instead of the object it points to, misunderstanding pointer arithmetic, referencing nonexistent variables, and introducing memory leaks. Bibliographic Notes Kilburn and his colleagues published the ﬁrst description of virtual memory [63]. Architecture texts contain additional details about the hardware’s role in virtual memory [46]. Operating systems texts contain additional information about the operating system’s role [102, 106, 113]. Bovet and Cesati [11] give a detailed de- scription of the Linux virtual memory system. Intel Corporation provides detailed documentation on 32-bit and 64-bit address translation on IA processors [52]. Knuth wrote the classic work on storage allocation in 1968 [64]. Since that time, there has been a tremendous amount of work in the area. Wilson, Johnstone, Neely, and Boles have written a beautiful survey and performance evaluation of explicit allocators [118]. The general comments in this book about the throughput and utilization of different allocator strategies are paraphrased from their sur- vey. Jones and Lins provide a comprehensive survey of garbage collection [56]. Kernighan and Ritchie [61] show the complete code for a simple allocator based on an explicit free list with a block size and successor pointer in each free block. The code is interesting in that it uses unions to eliminate a lot of the complicated pointer arithmetic, but at the expense of a linear-time (rather than constant-time) free operation. Doug Lea developed a widely used open-source malloc package called ®ÄÀþÄÄÇ² [67]. Homework Problems 9.11 ◆ In the following series of problems, you are to show how the example memory system in Section 9.6.4 translates a virtual address into a physical address and accesses the cache. For the given virtual address, indicate the TLB entry accessed, Homework Problems 913 the physical address, and the cache byte value returned. Indicate whether the TLB misses, whether a page fault occurs, and whether a cache miss occurs. If there is a cache miss, enter “—” for “Cache byte returned.” If there is a page fault, enter “—” for “PPN” and leave parts C and D blank. Virtual address: nÓnpu² A. Virtual address format 12 1113 10 9 8 7 6 5 4 3 2 1 0 B. Address translation Parameter Value VPN TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN C. Physical address format 11 10 9 8 7 6 5 4 3 2 1 0 D. Physical memory reference Parameter Value Byte offset Cache index Cache tag Cache hit? (Y/N) Cache byte returned 9.12 ◆ Repeat Problem 9.11 for the following address. Virtual address: nÓnqþw A. Virtual address format 12 1113 10 9 8 7 6 5 4 3 2 1 0 914 Chapter 9 Virtual Memory B. Address translation Parameter Value VPN TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN C. Physical address format 11 10 9 8 7 6 5 4 3 2 1 0 D. Physical memory reference Parameter Value Byte offset Cache index Cache tag Cache hit? (Y/N) Cache byte returned 9.13 ◆ Repeat Problem 9.11 for the following address. Virtual address: nÓnnrn 12 1113 10 9 8 7 6 5 4 3 2 1 0 A. Address translation Parameter Value VPN TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN B. Physical address format 11 10 9 8 7 6 5 4 3 2 1 0 Homework Problems 915 C. Physical memory reference Parameter Value Byte offset Cache index Cache tag Cache hit? (Y/N) Cache byte returned 9.14 ◆◆ Given an input ﬁle ³−ÄÄÇlÎÓÎ that consists of the string ¤−ÄÄÇj ÑÇËÄ®_¿Å, write a C program that uses ÀÀþÉ to change the contents of ³−ÄÄÇlÎÓÎ to “−ÄÄÇj ÑÇËÄ®_¿Å. 9.15 ◆ Determine the block sizes and header values that would result from the fol- lowing sequence of ÀþÄÄÇ² requests. Assumptions: (1) The allocator maintains double-word alignment and uses an implicit free list with the block format from Figure 9.35. (2) Block sizes are rounded up to the nearest multiple of 8 bytes. Request Block size (decimal bytes) Block header (hex) ÀþÄÄÇ²frg ÀþÄÄÇ²fug ÀþÄÄÇ²fowg ÀþÄÄÇ²fppg 9.16 ◆ Determine the minimum block size for each of the following combinations of alignment requirements and block formats. Assumptions: Explicit free list, 4-byte ÉË−® and ÍÏ²² pointers in each free block, zero-size payloads are not allowed, and headers and footers are stored in 4-byte words. Minimum block Alignment Allocated block Free block size (bytes) Single word Header and footer Header and footer Single word Header, but no footer Header and footer Double word Header and footer Header and footer Double word Header, but no footer Header and footer 9.17 ◆◆◆ Develop a version of the allocator in Section 9.9.12 that performs a next-ﬁt search instead of a ﬁrst-ﬁt search. 9.18 ◆◆◆ The allocator in Section 9.9.12 requires both a header and a footer for each block in order to perform constant-time coalescing. Modify the allocator so that free blocks require a header and a footer, but allocated blocks require only a header. 916 Chapter 9 Virtual Memory 9.19 ◆ You are given three groups of statements relating to memory management and garbage collection below. In each group, only one statement is true. Your task is to indicate which statement is true. 1. (a) In a buddy system, up to 50% of the space can be wasted due to internal fragmentation. (b) The ﬁrst-ﬁt memory allocation algorithm is slower than the best-ﬁt algo- rithm (on average). (c) Deallocation using boundary tags is fast only when the list of free blocks is ordered according to increasing memory addresses. (d) The buddy system suffers from internal fragmentation, but not from external fragmentation. 2. (a) Using the ﬁrst-ﬁt algorithm on a free list that is ordered according to decreasing block sizes results in low performance for allocations, but avoids external fragmentation. (b) For the best-ﬁt method, the list of free blocks should be ordered according to increasing memory addresses. (c) The best-ﬁt method chooses the largest free block into which the re- quested segment ﬁts. (d) Using the ﬁrst-ﬁt algorithm on a free list that is ordered according to increasing block sizes is equivalent to using the best-ﬁt algorithm. 3. Mark&Sweep garbage collectors are called conservative if (a) They coalesce freed memory only when a memory request cannot be satisﬁed. (b) They treat everything that looks like a pointer as a pointer. (c) They perform garbage collection only when they run out of memory. (d) They do not free memory blocks forming a cyclic list. 9.20 ◆◆◆◆ Write your own version of ÀþÄÄÇ² and ðË−−, and compare its running time and space utilization to the version of ÀþÄÄÇ² provided in the standard C library. Solutions to Practice Problems Solution to Problem 9.1 (page 841) This problem gives you some appreciation for the sizes of different address spaces. At one point in time, a 32-bit address space seemed impossibly large. But now there are database and scientiﬁc applications that need more, and you can expect this trend to continue. At some point in your lifetime, expect to ﬁnd yourself complaining about the cramped 64-bit address space on your personal computer! Solutions to Practice Problems 917 Number of Number of address bits (n) virtual addresses (N ) Largest possible virtual address 424 = 16 24 − 1 = 15 14 214 = 16 K 214 − 1 = 16 K − 1 24 224 = 16 M 224 − 1 = 16 M − 1 46 246 = 64 T 246 − 1 = 64 T − 1 54 254 = 16 P 254 − 1 = 16 P − 1 Solution to Problem 9.2 (page 843) Since each virtual page is P = 2p bytes, there are a total of 2n/2p = 2n−p possible pages in the system, each of which needs a page table entry (PTE). nP = 2p Number of PTEs 12 1 K 4 16 16 K 4 24 2 M 8 36 1 G 64 Solution to Problem 9.3 (page 852) You need to understand this kind of problem well in order to fully grasp address translation. Here is how to solve the ﬁrst subproblem: We are given n = 64 virtual address bits and m = 32 physical address bits. A page size of P = 1 KB means we need log2(1K) = 10 bits for both the VPO and PPO. (Recall that the VPO and PPO are identical.) The remaining address bits are the VPN and PPN, respectively. Number of P VPN bits VPO bits PPN bits PPO bits 1KB 54 10 22 10 2KB 53 11 21 11 4KB 52 12 20 12 16 KB 50 14 18 14 Solution to Problem 9.4 (page 860) Doing a few of these manual simulations is a great way to ﬁrm up your understand- ing of address translation. You might ﬁnd it helpful to write out all the bits in the addresses and then draw boxes around the different bit ﬁelds, such as VPN, TLBI, and so on. In this particular problem, there are no misses of any kind: the TLB has a copy of the PTE and the cache has a copy of the requested data words. See Problems 9.11, 9.12, and 9.13 for some different combinations of hits and misses. 918 Chapter 9 Virtual Memory A. nn nnoo oono nooo B. Parameter Value VPN nÓð TLB index nÓq TLB tag nÓq TLB hit? (Y/N) Y Page fault? (Y/N) N PPN nÓ® C. nnoo nono nooo D. Parameter Value Byte offset nÓq Cache index nÓs Cache tag nÓ® Cache hit? (Y/N) Y Cache byte returned nÓo® Solution to Problem 9.5 (page 875) Solving this problem will give you a good feel for the idea of memory mapping. Try it yourself. We haven’t discussed the ÇÉ−Å, ðÍÎþÎ,or ÑË©Î− functions, so you’ll need to read their ÀþÅ pages to see how they work. code/vm/mmapcopy.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 mh 4 h ÀÀþÉ²ÇÉÔ k ÏÍ−Í ÀÀþÉ ÎÇ ²ÇÉÔ ð©Ä− ð® ÎÇ ÍÎ®ÇÏÎ 5 hm 6 ÌÇ©® ÀÀþÉ²ÇÉÔf©ÅÎ ð®j ©ÅÎ Í©Ö−g 7 Õ 8 ²³þË h¾ÏðÉy mh ÉÎË ÎÇ À−ÀÇËÔkÀþÉÉ−® ‚› þË−þ hm 9 10 ¾ÏðÉ { ›ÀþÉfﬁ•‹‹j Í©Ö−j –‡ﬂ¶ˆ‡¥¡⁄j ›¡–ˆ–‡'‚¡¶¥j ð®j ngy 11 „Ë©Î−foj ¾ÏðÉj Í©Ö−gy 12 Ë−ÎÏËÅy 13 Û 14 15 mh ÀÀþÉ²ÇÉÔ ®Ë©Ì−Ë hm 16 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 17 Õ 18 ÍÎËÏ²Î ÍÎþÎ ÍÎþÎy 19 ©ÅÎ ð®y 20 Solutions to Practice Problems 919 21 mh £³−²Â ðÇË Ë−ÊÏ©Ë−® ²ÇÀÀþÅ®kÄ©Å− þË×ÏÀ−ÅÎ hm 22 ©ð fþË×² _{ pg Õ 23 ÉË©ÅÎðf‘ÏÍþ×−x cÍ zð©Ä−ÅþÀ−|¿Å‘j þË×Ì‰n`gy 24 −Ó©Îfngy 25 Û 26 27 mh £ÇÉÔ Î³− ©ÅÉÏÎ þË×ÏÀ−ÅÎ ÎÇ ÍÎ®ÇÏÎ hm 28 ð® { ﬂÉ−ÅfþË×Ì‰o`j ﬂˆ‡⁄ﬂﬁ‹»j ngy 29 ðÍÎþÎfð®j dÍÎþÎgy 30 ÀÀþÉ²ÇÉÔfð®j ÍÎþÎlÍÎˆÍ©Ö−gy 31 −Ó©Îfngy 32 Û code/vm/mmapcopy.c Solution to Problem 9.6 (page 885) This problem touches on some core ideas such as alignment requirements, min- imum block sizes, and header encodings. The general approach for determining the block size is to round the sum of the requested payload and the header size to the nearest multiple of the alignment requirement (in this case, 8 bytes). For example, the block size for the ÀþÄÄÇ²fpg request is 4 + 2 = 6 rounded up to 8. The block size for the ÀþÄÄÇ²fpng request is 20 + 4 = 24 rounded up to 24. No need to round up since it is already aligned. Request Block size (decimal bytes) Block header (hex) ÀþÄÄÇ²fpg 8 nÓw ÀþÄÄÇ²fwg 16 nÓoo ÀþÄÄÇ²fosg 24 nÓow ÀþÄÄÇ²fpng 24 nÓow Solution to Problem 9.7 (page 888) The minimum block size can have a signiﬁcant effect on internal fragmentation. Thus, it is good to understand the minimum block sizes associated with different allocator designs and alignment requirements. The tricky part is to realize that the same block can be allocated or free at different points in time. Thus, the minimum block size is the maximum of the minimum allocated block size and the minimum free block size. For example, in the last subproblem, the minimum allocated block size is a 4-byte header and a 1-byte payload rounded up to 8 bytes. The minimum free block size is a 4-byte header and 4-byte footer, which is already a multiple of 8 and doesn’t need to be rounded. So the minimum block size for this allocator is 8 bytes. Minimum block Alignment Allocated block Free block size (bytes) Single word Header and footer Header and footer 12 Single word Header, but no footer Header and footer 8 Double word Header and footer Header and footer 16 Double word Header, but no footer Header and footer 8 920 Chapter 9 Virtual Memory Solution to Problem 9.8 (page 897) There is nothing very tricky here. But the solution requires you to understand how the rest of our simple implicit-list allocator works and how to manipulate and traverse blocks. code/vm/malloc/mm.c 1 ÍÎþÎ©² ÌÇ©® hð©Å®ˆð©ÎfÍ©Ö−ˆÎ þÍ©Ö−g 2 Õ 3 mh ƒ©ËÍÎkð©Î Í−þË²³ hm 4 ÌÇ©® h¾Éy 5 6 ðÇË f¾É { ³−þÉˆÄ©ÍÎÉy §¥¶ˆ·'…¥f¤⁄‡–f¾Égg | ny ¾É { ﬁ¥”¶ˆ¢‹«–f¾Égg Õ 7 ©ð f_§¥¶ˆ¡‹‹ﬂ£f¤⁄‡–f¾Égg dd fþÍ©Ö− z{ §¥¶ˆ·'…¥f¤⁄‡–f¾Égggg Õ 8 Ë−ÎÏËÅ ¾Éy 9 Û 10 Û 11 Ë−ÎÏËÅ ﬁ•‹‹y mh ﬁÇ ð©Î hm 12 a−Å®©ð 13 Û code/vm/malloc/mm.c Solution to Problem 9.9 (page 897) This is another warm-up exercise to help you become familiar with allocators. Notice that for this allocator the minimum block size is 16 bytes. If the remainder of the block after splitting would be greater than or equal to the minimum block size, then we go ahead and split the block (lines 6–10). The only tricky part here is to realize that you need to place the new allocated block (lines 6 and 7) before moving to the next block (line 8). code/vm/malloc/mm.c 1 ÍÎþÎ©² ÌÇ©® ÉÄþ²−fÌÇ©® h¾Éj Í©Ö−ˆÎ þÍ©Ö−g 2 Õ 3 Í©Ö−ˆÎ ²Í©Ö− { §¥¶ˆ·'…¥f¤⁄‡–f¾Éggy 4 5 ©ð ff²Í©Ö− k þÍ©Ö−g |{ fph⁄·'…¥gg Õ 6 –•¶f¤⁄‡–f¾Égj –¡£«fþÍ©Ö−j oggy 7 –•¶fƒ¶‡–f¾Égj –¡£«fþÍ©Ö−j oggy 8 ¾É { ﬁ¥”¶ˆ¢‹«–f¾Égy 9 –•¶f¤⁄‡–f¾Égj –¡£«f²Í©Ö−kþÍ©Ö−j nggy 10 –•¶fƒ¶‡–f¾Égj –¡£«f²Í©Ö−kþÍ©Ö−j nggy 11 Û 12 −ÄÍ− Õ 13 –•¶f¤⁄‡–f¾Égj –¡£«f²Í©Ö−j oggy 14 –•¶fƒ¶‡–f¾Égj –¡£«f²Í©Ö−j oggy 15 Û 16 Û code/vm/malloc/mm.c Solutions to Practice Problems 921 Solution to Problem 9.10 (page 900) Here is one pattern that will cause external fragmentation: The application makes numerous allocation and free requests to the ﬁrst size class, followed by numer- ous allocation and free requests to the second size class, followed by numerous allocation and free requests to the third size class, and so on. For each size class, the allocator creates a lot of memory that is never reclaimed because the allocator doesn’t coalesce, and because the application never requests blocks from that size class again. This page is intentionally left blank. Part III Interaction and Communication between Programs T o this point in our study of computer systems, we have assumed that pro- grams run in isolation, with minimal input and output. However, in the real world, application programs use services provided by the operating system to communicate with I/O devices and with other programs. This part of the book will give you an understanding of the basic I/O services provided by Unix operating systems and how to use these services to build appli- cations such as Web clients and servers that communicate with each other over the Internet. You will learn techniques for writing concurrent programs, such as Web servers that can service multiple clients at the same time. Writing concurrent application programs can also allow them to execute faster on modern multi-core processors. When you ﬁnish this part, you will be well on your way to becoming a power programmer with a mature understanding of computer systems and their impact on your programs. 923 This page is intentionally left blank. CHAPTER 10 System-Level I/O 10.1 Unix I/O 926 10.2 Files 927 10.3 Opening and Closing Files 929 10.4 Reading and Writing Files 931 10.5 Robust Reading and Writing with the Rio Package 933 10.6 Reading File Metadata 939 10.7 Reading Directory Contents 941 10.8 Sharing Files 942 10.9 I/O Redirection 945 10.10 Standard I/O 947 10.11 Putting It Together: Which I/O Functions Should I Use? 947 10.12 Summary 949 Bibliographic Notes 950 Homework Problems 950 Solutions to Practice Problems 951 925 926 Chapter 10 System-Level I/O I nput/output (I/O) is the process of copying data between main memory and ex- ternal devices such as disk drives, terminals, and networks. An input operation copies data from an I/O device to main memory, and an output operation copies data from memory to a device. All language run-time systems provide higher-level facilities for performing I/O. For example, ANSI C provides the standard I/O library, with functions such as ÉË©ÅÎð and Í²þÅð that perform buffered I/O. The C++ language provides similar functionality with its overloaded zz (“put to”) and || (“get from”) operators. On Linux systems, these higher-level I/O functions are implemented using system- level Unix I/O functions provided by the kernel. Most of the time, the higher-level I/O functions work quite well and there is no need to use Unix I/O directly. So why bother learning about Unix I/O? . Understanding Unix I/O will help you understand other systems concepts.I/O is integral to the operation of a system, and because of this, we often encounter circular dependencies between I/O and other systems ideas. For example, I/O plays a key role in process creation and execution. Conversely, process creation plays a key role in how ﬁles are shared by different processes. Thus, to really understand I/O, you need to understand processes, and vice versa. We have already touched on aspects of I/O in our discussions of the memory hierarchy, linking and loading, processes, and virtual memory. Now that you have a better understanding of these ideas, we can close the circle and delve into I/O in more detail. . Sometimes you have no choice but to use Unix I/O. There are some important cases where using higher-level I/O functions is either impossible or inappro- priate. For example, the standard I/O library provides no way to access ﬁle metadata such as ﬁle size or ﬁle creation time. Further, there are problems with the standard I/O library that make it risky to use for network program- ming. This chapter introduces you to the general concepts of Unix I/O and standard I/O and shows you how to use them reliably from your C programs. Besides serving as a general introduction, this chapter lays a ﬁrm foundation for our subsequent study of network programming and concurrency. 10.1 Unix I/O A Linux ﬁle is a sequence of m bytes: B0,B1,...,Bk,...,Bm−1 All I/O devices, such as networks, disks, and terminals, are modeled as ﬁles, and all input and output is performed by reading and writing the appropriate ﬁles. This elegant mapping of devices to ﬁles allows the Linux kernel to export a simple, low- level application interface, known as Unix I/O, that enables all input and output to be performed in a uniform and consistent way: Section 10.2 Files 927 Opening ﬁles. An application announces its intention to access an I/O device by asking the kernel to open the corresponding ﬁle. The kernel returns a small nonnegative integer, called a descriptor, that identiﬁes the ﬁle in all subsequent operations on the ﬁle. The kernel keeps track of all information about the open ﬁle. The application only keeps track of the descriptor. Each process created by a Linux shell begins life with three open ﬁles: standard input (descriptor 0), standard output (descriptor 1), and standard error (descriptor 2). The header ﬁle zÏÅ©ÍÎ®l³| deﬁnes constants ·¶⁄'ﬁˆ ƒ'‹¥ﬁﬂ, ·¶⁄ﬂ•¶ˆƒ'‹¥ﬁﬂ, and ·¶⁄¥‡‡ˆƒ'‹¥ﬁﬂ, which can be used instead of the explicit descriptor values. Changing the current ﬁle position. The kernel maintains a ﬁle position k, initially 0, for each open ﬁle. The ﬁle position is a byte offset from the beginning of a ﬁle. An application can set the current ﬁle position k explicitly by performing a seek operation. Reading and writing ﬁles. A read operation copies n> 0 bytes from a ﬁle to memory, starting at the current ﬁle position k and then incrementing k by n. Given a ﬁle with a size of m bytes, performing a read operation when k ≥ m triggers a condition known as end-of-ﬁle (EOF), which can be detected by the application. There is no explicit “EOF character” at the end of a ﬁle. Similarly, a write operation copies n> 0 bytes from memory to a ﬁle, starting at the current ﬁle position k and then updating k. Closing ﬁles. When an application has ﬁnished accessing a ﬁle, it informs the kernel by asking it to close the ﬁle. The kernel responds by freeing the data structures it created when the ﬁle was opened and restoring the descriptor to a pool of available descriptors. When a process terminates for any reason, the kernel closes all open ﬁles and frees their memory resources. 10.2 Files Each Linux ﬁle has a type that indicates its role in the system: . A regular ﬁle contains arbitrary data. Application programs often distinguish between text ﬁles, which are regular ﬁles that contain only ASCII or Unicode characters, and binary ﬁles, which are everything else. To the kernel there is no difference between text and binary ﬁles. A Linux text ﬁle consists of a sequence of text lines, where each line is a sequence of characters terminated by a newline character (‘¿Å’). The newline character is the same as the ASCII line feed character (LF) and has a numeric value of nÓnþ. . A directory is a ﬁle consisting of an array of links, where each link maps a ﬁlename to a ﬁle, which may be another directory. Each directory contains at 928 Chapter 10 System-Level I/O Aside End of line (EOL) indicators One of the clumsy aspects of working with text ﬁles is that different systems use different characters to mark the end of a line. Linux and Mac OS X use ’¿Å’(nÓþ), which is the ASCII line feed (LF) character. However, MS Windows and Internet protocols such as HTTP use the sequence ‘¿Ë¿Å’(nÓ® nÓþ), which is the ASCII carriage return (CR) character followed by a line feed (LF). If you create a ﬁle ðÇÇlÎÓÎ in Windows and then view it in a Linux text editor, you’ll see an annoying ´› at the end of each line, which is how Linux tools display the CR character. You can remove these unwanted CR characters from ðÇÇlÎÓÎ in place by running the following command: Ä©ÅÏÓ| perl -pi -e \"s/\\r\\n/\\n/g\" foo.txt least two entries: l (dot) is a link to the directory itself, and ll (dot-dot) is a link to the parent directory in the directory hierarchy (see below). You can create a directory with the ÀÂ®©Ë command, view its contents with ÄÍ, and delete it with ËÀ®©Ë. . A socket is a ﬁle that is used to communicate with another process across a network (Section 11.4). Other ﬁle types include named pipes, symbolic links, and character and block devices, which are beyond our scope. The Linux kernel organizes all ﬁles in a single directory hierarchy anchored by the root directory named m (slash). Each ﬁle in the system is a direct or indirect descendant of the root directory. Figure 10.1 shows a portion of the directory hierarchy on our Linux system. As part of its context, each process has a current working directory that identiﬁes its current location in the directory hierarchy. You can change the shell’s current working directory with the ²® command. bash bin/ tty1 group passwd droh/ bryant/ stdio.h include/ bin/ dev/ etc/ / home/ usr/ hello.c sys/ vim unistd.h Figure 10.1 Portion of the Linux directory hierarchy. A trailing slash denotes a directory. Section 10.3 Opening and Closing Files 929 Locations in the directory hierarchy are speciﬁed by pathnames. A pathname is a string consisting of an optional slash followed by a sequence of ﬁlenames separated by slashes. Pathnames have two forms: . An absolute pathname starts with a slash and denotes a path from the root node. For example, in Figure 10.1, the absolute pathname for ³−ÄÄÇl² is m³ÇÀ−m®ËÇ³m³−ÄÄÇl². . A relative pathname starts with a ﬁlename and denotes a path from the current working directory. For example, in Figure 10.1, if m³ÇÀ−m®ËÇ³ is the current working directory, then the relative pathname for ³−ÄÄÇl² is lm³−ÄÄÇl².On the other hand, if m³ÇÀ−m¾ËÔþÅÎ is the current working directory, then the relative pathname is llm³ÇÀ−m®ËÇ³m³−ÄÄÇl². 10.3 Opening and Closing Files A process opens an existing ﬁle or creates a new ﬁle by calling the ÇÉ−Å function. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÍÔÍmÍÎþÎl³| a©Å²ÄÏ®− zð²ÅÎÄl³| ©ÅÎ ÇÉ−Åf²³þË hð©Ä−ÅþÀ−j ©ÅÎ ðÄþ×Íj ÀÇ®−ˆÎ ÀÇ®−gy Returns: new ﬁle descriptor if OK, −1 on error The ÇÉ−Å function converts a ð©Ä−ÅþÀ− to a ﬁle descriptor and returns the de- scriptor number. The descriptor returned is always the smallest descriptor that is not currently open in the process. The ðÄþ×Í argument indicates how the process intends to access the ﬁle: O_RDONLY. Reading only O_WRONLY. Writing only O_RDWR. Reading and writing For example, here is how to open an existing ﬁle for reading: ð® { ﬂÉ−Åf‘ðÇÇlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy The ðÄþ×Í argument can also be ored with one or more bit masks that provide additional instructions for writing: O_CREAT. If the ﬁle doesn’t exist, then create a truncated (empty) version of it. O_TRUNC. If the ﬁle already exists, then truncate it. O_APPEND. Before each write operation, set the ﬁle position to the end of the ﬁle. 930 Chapter 10 System-Level I/O Mask Description S_IRUSR User (owner) can read this ﬁle S_IWUSR User (owner) can write this ﬁle S_IXUSR User (owner) can execute this ﬁle S_IRGRP Members of the owner’s group can read this ﬁle S_IWGRP Members of the owner’s group can write this ﬁle S_IXGRP Members of the owner’s group can execute this ﬁle S_IROTH Others (anyone) can read this ﬁle S_IWOTH Others (anyone) can write this ﬁle S_IXOTH Others (anyone) can execute this ﬁle Figure 10.2 Access permission bits. Deﬁned in ÍÔÍmÍÎþÎl³. For example, here is how you might open an existing ﬁle with the intent of appending some data: ð® { ﬂÉ−Åf‘ðÇÇlÎÓÎ‘j ﬂˆ„‡ﬂﬁ‹»Úﬂˆ¡––¥ﬁ⁄j ngy The ÀÇ®− argument speciﬁes the access permission bits of new ﬁles. The symbolic names for these bits are shown in Figure 10.2. As part of its context, each process has a ÏÀþÍÂ that is set by calling the ÏÀþÍÂ function. When a process creates a new ﬁle by calling the ÇÉ−Å function with some ÀÇ®− argument, then the access permission bits of the ﬁle are set to ÀÇ®− d ÜÏÀþÍÂ. For example, suppose we are given the following default values for ÀÇ®− and ÏÀþÍÂ: a®−ð©Å− ⁄¥ƒˆ›ﬂ⁄¥ ·ˆ'‡•·‡Ú·ˆ'„•·‡Ú·ˆ'‡§‡–Ú·ˆ'„§‡–Ú·ˆ'‡ﬂ¶¤Ú·ˆ'„ﬂ¶¤ a®−ð©Å− ⁄¥ƒˆ•›¡·« ·ˆ'„§‡–Ú·ˆ'„ﬂ¶¤ Then the following code fragment creates a new ﬁle in which the owner of the ﬁle has read and write permissions, and all other users have read permissions: ÏÀþÍÂf⁄¥ƒˆ•›¡·«gy ð® { ﬂÉ−Åf‘ðÇÇlÎÓÎ‘j ﬂˆ£‡¥¡¶Úﬂˆ¶‡•ﬁ£Úﬂˆ„‡ﬂﬁ‹»j ⁄¥ƒˆ›ﬂ⁄¥gy Finally, a process closes an open ﬁle by calling the ²ÄÇÍ− function. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ©ÅÎ ²ÄÇÍ−f©ÅÎ ð®gy Returns: 0 if OK, −1 on error Closing a descriptor that is already closed is an error. Section 10.4 Reading and Writing Files 931 Practice Problem 10.1 (solution page 951) What is the output of the following program? 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ ð®oj ð®py 6 7 ð®o { ﬂÉ−Åf‘ðÇÇlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 8 £ÄÇÍ−fð®ogy 9 ð®p { ﬂÉ−Åf‘¾þÖlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 10 ÉË©ÅÎðf‘ð®p { c®¿Å‘j ð®pgy 11 −Ó©Îfngy 12 Û 10.4 Reading and Writing Files Applications perform input and output by calling the Ë−þ® and ÑË©Î− functions, respectively. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ÍÍ©Ö−ˆÎ Ë−þ®f©ÅÎ ð®j ÌÇ©® h¾Ïðj Í©Ö−ˆÎ Ågy Returns: number of bytes read if OK, 0 on EOF, −1 on error ÍÍ©Ö−ˆÎ ÑË©Î−f©ÅÎ ð®j ²ÇÅÍÎ ÌÇ©® h¾Ïðj Í©Ö−ˆÎ Ågy Returns: number of bytes written if OK, −1 on error The Ë−þ® function copies at most Å bytes from the current ﬁle position of descriptor ð® to memory location ¾Ïð. A return value of −1 indicates an error, and a return value of 0 indicates EOF. Otherwise, the return value indicates the number of bytes that were actually transferred. The ÑË©Î− function copies at most Å bytes from memory location ¾Ïð to the current ﬁle position of descriptor ð®. Figure 10.3 shows a program that uses Ë−þ® and ÑË©Î− calls to copy the standard input to the standard output, 1 byte at a time. Applications can explicitly modify the current ﬁle position by calling the ÄÍ−−Â function, which is beyond our scope. In some situations, Ë−þ® and ÑË©Î− transfer fewer bytes than the application requests. Such short counts do not indicate an error. They occur for a number of reasons: 932 Chapter 10 System-Level I/O Aside What’s the difference between ÍÍ©Ö−ˆÎ and Í©Ö−ˆÎ? You might have noticed that the Ë−þ® function has a Í©Ö−ˆÎ input argument and an ÍÍ©Ö−ˆÎ return value. So what’s the difference between these two types? On x86-64 systems, a Í©Ö−ˆÎ is deﬁned as an ÏÅÍ©×Å−® ÄÇÅ×, and an ÍÍ©Ö−ˆÎ (signed size) is deﬁned as a ÄÇÅ×.The Ë−þ® function returns a signed size rather than an unsigned size because it must return a −1 on error. Interestingly, the possibility of returning a single −1 reduces the maximum size of a Ë−þ® by a factor of 2. code/io/cpstdin.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©ÅfÌÇ©®g 4 Õ 5 ²³þË ²y 6 7 Ñ³©Ä−f‡−þ®f·¶⁄'ﬁˆƒ'‹¥ﬁﬂj d²j og _{ ng 8 „Ë©Î−f·¶⁄ﬂ•¶ˆƒ'‹¥ﬁﬂj d²j ogy 9 −Ó©Îfngy 10 Û code/io/cpstdin.c Figure 10.3 Using read and write to copy standard input to standard output 1 byte at a time. Encountering EOF on reads. Suppose that we are ready to read from a ﬁle that contains only 20 more bytes from the current ﬁle position and that we are reading the ﬁle in 50-byte chunks. Then the next Ë−þ® will return a short count of 20, and the Ë−þ® after that will signal EOF by returning a short count of 0. Reading text lines from a terminal. If the open ﬁle is associated with a terminal (i.e., a keyboard and display), then each Ë−þ® function will transfer one text line at a time, returning a short count equal to the size of the text line. Reading and writing network sockets. If the open ﬁle corresponds to a network socket (Section 11.4), then internal buffering constraints and long net- work delays can cause Ë−þ® and ÑË©Î− to return short counts. Short counts can also occur when you call Ë−þ® and ÑË©Î− on a Linux pipe, an inter- process communication mechanism that is beyond our scope. In practice, you will never encounter short counts when you read from disk ﬁles except on EOF, and you will never encounter short counts when you write to disk ﬁles. However, if you want to build robust (reliable) network applications Section 10.5 Robust Reading and Writing with the Rio Package 933 such as Web servers, then you must deal with short counts by repeatedly calling Ë−þ® and ÑË©Î− until all requested bytes have been transferred. 10.5 Robust Reading and Writing with the Rio Package In this section, we will develop an I/O package, called the Rio (Robust I/O) package, that handles these short counts for you automatically. The Rio package provides convenient, robust, and efﬁcient I/O in applications such as network programs that are subject to short counts. Rio provides two different kinds of functions: Unbuffered input and output functions. These functions transfer data directly between memory and a ﬁle, with no application-level buffering. They are especially useful for reading and writing binary data to and from networks. Buffered input functions. These functions allow you to efﬁciently read text lines and binary data from a ﬁle whose contents are cached in an application- level buffer, similar to the one provided for standard I/O functions such as ÉË©ÅÎð. Unlike the buffered I/O routines presented in [110], the buffered Rio input functions are thread-safe (Section 12.7.1) and can be inter- leaved arbitrarily on the same descriptor. For example, you can read some text lines from a descriptor, then some binary data, and then some more text lines. We are presenting the Rio routines for two reasons. First, we will be using them in the network applications we develop in the next two chapters. Second, by studying the code for these routines, you will gain a deeper understanding of Unix I/O in general. 10.5.1 Rio Unbuffered Input and Output Functions Applications can transfer data directly between memory and a ﬁle by calling the Ë©ÇˆË−þ®Å and Ë©ÇˆÑË©Î−Å functions. a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®Åf©ÅÎ ð®j ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ Ågy ÍÍ©Ö−ˆÎ Ë©ÇˆÑË©Î−Åf©ÅÎ ð®j ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ Ågy Returns: number of bytes transferred if OK, 0 on EOF (Ë©ÇˆË−þ®Å only), −1 on error The Ë©ÇˆË−þ®Å function transfers up to Å bytes from the current ﬁle position of descriptor ð® to memory location ÏÍË¾Ïð. Similarly, the Ë©ÇˆÑË©Î−Å function transfers Å bytes from location ÏÍË¾Ïð to descriptor ð®.The Ë©ÇˆË−þ®Å function can only return a short count if it encounters EOF. The Ë©ÇˆÑË©Î−Å function never returns a short count. Calls to Ë©ÇˆË−þ®Å and Ë©ÇˆÑË©Î−Å can be interleaved arbitrarily on the same descriptor. 934 Chapter 10 System-Level I/O Figure 10.4 shows the code for Ë©ÇˆË−þ®Å and Ë©ÇˆÑË©Î−Å. Notice that each function manually restarts the Ë−þ® or ÑË©Î− function if it is interrupted by the return from an application signal handler. To be as portable as possible, we allow for interrupted system calls and restart them when necessary. 10.5.2 Rio Buffered Input Functions Suppose we wanted to write a program that counts the number of lines in a text ﬁle. How might we do this? One approach is to use the Ë−þ® function to transfer 1 byte at a time from the ﬁle to the user’s memory, checking each byte for the newline character. The disadvantage of this approach is that it is inefﬁcient, requiring a trap to the kernel to read each byte in the ﬁle. A better approach is to call a wrapper function (Ë©ÇˆË−þ®Ä©Å−¾) that copies the text line from an internal read buffer, automatically making a Ë−þ® call to reﬁll the buffer whenever it becomes empty. For ﬁles that contain both text lines and binary data (such as the HTTP responses described in Section 11.5.3), we also provide a buffered version of Ë©ÇˆË−þ®Å, called Ë©ÇˆË−þ®Å¾, that transfers raw bytes from the same read buffer as Ë©ÇˆË−þ®Ä©Å−¾. a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ÌÇ©® Ë©ÇˆË−þ®©Å©Î¾fË©ÇˆÎ hËÉj ©ÅÎ ð®gy Returns: nothing ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®Ä©Å−¾fË©ÇˆÎ hËÉj ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ ÀþÓÄ−Ågy ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®Å¾fË©ÇˆÎ hËÉj ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ Ågy Returns: number of bytes read if OK, 0 on EOF, −1 on error The Ë©ÇˆË−þ®©Å©Î¾ function is called once per open descriptor. It associates the descriptor ð® with a read buffer of type Ë©ÇˆÎ at address ËÉ. The Ë©ÇˆË−þ®Ä©Å−¾ function reads the next text line from ﬁle ËÉ (including the terminating newline character), copies it to memory location ÏÍË¾Ïð, and terminates the text line with the NULL (zero) character. The Ë©ÇˆË−þ®Ä©Å−¾ function reads at most ÀþÓÄ−Åko bytes, leaving room for the terminating NULL character. Text lines that exceed ÀþÓÄ−Åko bytes are truncated and terminated with a NULL character. The Ë©ÇˆË−þ®Å¾ function reads up to Å bytes from ﬁle ËÉ to memory location ÏÍË¾Ïð. Calls to Ë©ÇˆË−þ®Ä©Å−¾ and Ë©ÇˆË−þ®Å¾ can be interleaved arbitrarily on the same descriptor. However, calls to these buffered functions should not be interleaved with calls to the unbuffered Ë©ÇˆË−þ®Å function. You will encounter numerous examples of the Rio functions in the remainder of this text. Figure 10.5 shows how to use the Rio functions to copy a text ﬁle from standard input to standard output, one line at a time. Figure 10.6 shows the format of a read buffer, along with the code for the Ë©ÇˆË−þ®©Å©Î¾ function that initializes it. The Ë©ÇˆË−þ®©Å©Î¾ function sets up an empty read buffer and associates an open ﬁle descriptor with that buffer. Section 10.5 Robust Reading and Writing with the Rio Package 935 code/src/csapp.c 1 ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®Åf©ÅÎ ð®j ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ Åg 2 Õ 3 Í©Ö−ˆÎ ÅÄ−ðÎ { Åy 4 ÍÍ©Ö−ˆÎ ÅË−þ®y 5 ²³þË h¾ÏðÉ { ÏÍË¾Ïðy 6 7 Ñ³©Ä− fÅÄ−ðÎ | ng Õ 8 ©ð ffÅË−þ® { Ë−þ®fð®j ¾ÏðÉj ÅÄ−ðÎgg z ng Õ 9 ©ð f−ËËÅÇ {{ ¥'ﬁ¶‡g mh 'ÅÎ−ËËÏÉÎ−® ¾Ô Í©× ³þÅ®Ä−Ë Ë−ÎÏËÅ hm 10 ÅË−þ® { ny mh þÅ® ²þÄÄ Ë−þ®fg þ×þ©Å hm 11 −ÄÍ− 12 Ë−ÎÏËÅ koy mh −ËËÅÇ Í−Î ¾Ô Ë−þ®fg hm 13 Û 14 −ÄÍ− ©ð fÅË−þ® {{ ng 15 ¾Ë−þÂy mh ¥ﬂƒ hm 16 ÅÄ−ðÎ k{ ÅË−þ®y 17 ¾ÏðÉ i{ ÅË−þ®y 18 Û 19 Ë−ÎÏËÅ fÅ k ÅÄ−ðÎgy mh ‡−ÎÏËÅ |{ n hm 20 Û code/src/csapp.c code/src/csapp.c 1 ÍÍ©Ö−ˆÎ Ë©ÇˆÑË©Î−Åf©ÅÎ ð®j ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ Åg 2 Õ 3 Í©Ö−ˆÎ ÅÄ−ðÎ { Åy 4 ÍÍ©Ö−ˆÎ ÅÑË©ÎÎ−Åy 5 ²³þË h¾ÏðÉ { ÏÍË¾Ïðy 6 7 Ñ³©Ä− fÅÄ−ðÎ | ng Õ 8 ©ð ffÅÑË©ÎÎ−Å { ÑË©Î−fð®j ¾ÏðÉj ÅÄ−ðÎgg z{ ng Õ 9 ©ð f−ËËÅÇ {{ ¥'ﬁ¶‡g mh 'ÅÎ−ËËÏÉÎ−® ¾Ô Í©× ³þÅ®Ä−Ë Ë−ÎÏËÅ hm 10 ÅÑË©ÎÎ−Å { ny mh þÅ® ²þÄÄ ÑË©Î−fg þ×þ©Å hm 11 −ÄÍ− 12 Ë−ÎÏËÅ koy mh −ËËÅÇ Í−Î ¾Ô ÑË©Î−fg hm 13 Û 14 ÅÄ−ðÎ k{ ÅÑË©ÎÎ−Åy 15 ¾ÏðÉ i{ ÅÑË©ÎÎ−Åy 16 Û 17 Ë−ÎÏËÅ Åy 18 Û code/src/csapp.c Figure 10.4 The Ë©ÇˆË−þ®Å and Ë©ÇˆÑË©Î−Å functions. 936 Chapter 10 System-Level I/O code/io/cpﬁle.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ©ÅÎ Åy 6 Ë©ÇˆÎ Ë©Çy 7 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 8 9 ‡©ÇˆË−þ®©Å©Î¾fdË©Çj ·¶⁄'ﬁˆƒ'‹¥ﬁﬂgy 10 Ñ³©Ä−ffÅ { ‡©ÇˆË−þ®Ä©Å−¾fdË©Çj ¾Ïðj ›¡”‹'ﬁ¥gg _{ ng 11 ‡©ÇˆÑË©Î−Åf·¶⁄ﬂ•¶ˆƒ'‹¥ﬁﬂj ¾Ïðj Ågy 12 Û code/io/cpﬁle.c Figure 10.5 Copying a text ﬁle from standard input to standard output. code/include/csapp.h 1 a®−ð©Å− ‡'ﬂˆ¢•ƒ·'…¥ vowp 2 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 3 ©ÅÎ Ë©Çˆð®y mh ⁄−Í²Ë©ÉÎÇË ðÇË Î³©Í ©ÅÎ−ËÅþÄ ¾Ïð hm 4 ©ÅÎ Ë©Çˆ²ÅÎy mh •ÅË−þ® ¾ÔÎ−Í ©Å ©ÅÎ−ËÅþÄ ¾Ïð hm 5 ²³þË hË©Çˆ¾ÏðÉÎËy mh ﬁ−ÓÎ ÏÅË−þ® ¾ÔÎ− ©Å ©ÅÎ−ËÅþÄ ¾Ïð hm 6 ²³þË Ë©Çˆ¾Ïð‰‡'ﬂˆ¢•ƒ·'…¥`y mh 'ÅÎ−ËÅþÄ ¾Ïðð−Ë hm 7 Û Ë©ÇˆÎy code/include/csapp.h code/src/csapp.c 1 ÌÇ©® Ë©ÇˆË−þ®©Å©Î¾fË©ÇˆÎ hËÉj ©ÅÎ ð®g 2 Õ 3 ËÉk|Ë©Çˆð® { ð®y 4 ËÉk|Ë©Çˆ²ÅÎ { ny 5 ËÉk|Ë©Çˆ¾ÏðÉÎË { ËÉk|Ë©Çˆ¾Ïðy 6 Û code/src/csapp.c Figure 10.6 A read buffer of type Ë©ÇˆÎ and the Ë©ÇˆË−þ®©Å©Î¾ function that initializes it. The heart of the Rio read routines is the Ë©ÇˆË−þ® function shown in Fig- ure 10.7. The Ë©ÇˆË−þ® function is a buffered version of the Linux Ë−þ® function. When Ë©ÇˆË−þ® is called with a request to read Å bytes, there are ËÉk|Ë©Çˆ²ÅÎ unread bytes in the read buffer. If the buffer is empty, then it is replenished with a call to Ë−þ®. Receiving a short count from this invocation of Ë−þ® is not an er- ror; it simply has the effect of partially ﬁlling the read buffer. Once the buffer is Section 10.5 Robust Reading and Writing with the Rio Package 937 code/src/csapp.c 1 ÍÎþÎ©² ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®fË©ÇˆÎ hËÉj ²³þË hÏÍË¾Ïðj Í©Ö−ˆÎ Åg 2 Õ 3 ©ÅÎ ²ÅÎy 4 5 Ñ³©Ä− fËÉk|Ë©Çˆ²ÅÎ z{ ng Õ mh ‡−ð©ÄÄ ©ð ¾Ïð ©Í −ÀÉÎÔ hm 6 ËÉk|Ë©Çˆ²ÅÎ { Ë−þ®fËÉk|Ë©Çˆð®j ËÉk|Ë©Çˆ¾Ïðj 7 Í©Ö−ÇðfËÉk|Ë©Çˆ¾Ïðggy 8 ©ð fËÉk|Ë©Çˆ²ÅÎ z ng Õ 9 ©ð f−ËËÅÇ _{ ¥'ﬁ¶‡g mh 'ÅÎ−ËËÏÉÎ−® ¾Ô Í©× ³þÅ®Ä−Ë Ë−ÎÏËÅ hm 10 Ë−ÎÏËÅ koy 11 Û 12 −ÄÍ− ©ð fËÉk|Ë©Çˆ²ÅÎ {{ ng mh ¥ﬂƒ hm 13 Ë−ÎÏËÅ ny 14 −ÄÍ− 15 ËÉk|Ë©Çˆ¾ÏðÉÎË { ËÉk|Ë©Çˆ¾Ïðy mh ‡−Í−Î ¾Ïðð−Ë ÉÎË hm 16 Û 17 18 mh £ÇÉÔ À©ÅfÅj ËÉk|Ë©Çˆ²ÅÎg ¾ÔÎ−Í ðËÇÀ ©ÅÎ−ËÅþÄ ¾Ïð ÎÇ ÏÍ−Ë ¾Ïð hm 19 ²ÅÎ{Åy 20 ©ð fËÉk|Ë©Çˆ²ÅÎ z Åg 21 ²ÅÎ { ËÉk|Ë©Çˆ²ÅÎy 22 À−À²ÉÔfÏÍË¾Ïðj ËÉk|Ë©Çˆ¾ÏðÉÎËj ²ÅÎgy 23 ËÉk|Ë©Çˆ¾ÏðÉÎË i{ ²ÅÎy 24 ËÉk|Ë©Çˆ²ÅÎ k{ ²ÅÎy 25 Ë−ÎÏËÅ ²ÅÎy 26 Û code/src/csapp.c Figure 10.7 The internal Ë©ÇˆË−þ® function. nonempty, Ë©ÇˆË−þ® copies the minimum of Å and ËÉk|Ë©Çˆ²ÅÎ bytes from the read buffer to the user buffer and returns the number of bytes copied. To an application program, the Ë©ÇˆË−þ® function has the same semantics as the Linux Ë−þ® function. On error, it returns −1 and sets −ËËÅÇ appropriately. On EOF, it returns 0. It returns a short count if the number of requested bytes exceeds the number of unread bytes in the read buffer. The similarity of the two functions makes it easy to build different kinds of buffered read functions by substituting Ë©ÇˆË−þ® for Ë−þ®. For example, the Ë©ÇˆË−þ®Å¾ function in Figure 10.8 has the same structure as Ë©ÇˆË−þ®Å, with Ë©ÇˆË−þ® substituted for Ë−þ®. Similarly, the Ë©ÇˆË−þ®Ä©Å−¾ routine in Figure 10.8 calls Ë©ÇˆË−þ® at most ÀþÓÄ−Åko times. Each call returns 1 byte from the read buffer, which is then checked for being the terminating newline. 938 Chapter 10 System-Level I/O code/src/csapp.c 1 ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®Ä©Å−¾fË©ÇˆÎ hËÉj ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ ÀþÓÄ−Åg 2 Õ 3 ©ÅÎ Åj Ë²y 4 ²³þË ²j h¾ÏðÉ { ÏÍË¾Ïðy 5 6 ðÇË fÅ { oy Å z ÀþÓÄ−Åy Åiig Õ 7 ©ð ffË² { Ë©ÇˆË−þ®fËÉj d²j ogg {{ og Õ 8 h¾ÏðÉii { ²y 9 ©ð f² {{ ’¿Å’g Õ 10 Åiiy 11 ¾Ë−þÂy 12 Û 13 Û −ÄÍ− ©ð fË² {{ ng Õ 14 ©ð fÅ {{ og 15 Ë−ÎÏËÅ ny mh ¥ﬂƒj ÅÇ ®þÎþ Ë−þ® hm 16 −ÄÍ− 17 ¾Ë−þÂy mh ¥ﬂƒj ÍÇÀ− ®þÎþ ÑþÍ Ë−þ® hm 18 Û −ÄÍ− 19 Ë−ÎÏËÅ koy mh ¥ËËÇË hm 20 Û 21 h¾ÏðÉ { ny 22 Ë−ÎÏËÅ Åkoy 23 Û code/src/csapp.c code/src/csapp.c 1 ÍÍ©Ö−ˆÎ Ë©ÇˆË−þ®Å¾fË©ÇˆÎ hËÉj ÌÇ©® hÏÍË¾Ïðj Í©Ö−ˆÎ Åg 2 Õ 3 Í©Ö−ˆÎ ÅÄ−ðÎ { Åy 4 ÍÍ©Ö−ˆÎ ÅË−þ®y 5 ²³þË h¾ÏðÉ { ÏÍË¾Ïðy 6 7 Ñ³©Ä− fÅÄ−ðÎ | ng Õ 8 ©ð ffÅË−þ® { Ë©ÇˆË−þ®fËÉj ¾ÏðÉj ÅÄ−ðÎgg z ng 9 Ë−ÎÏËÅ koy mh −ËËÅÇ Í−Î ¾Ô Ë−þ®fg hm 10 −ÄÍ− ©ð fÅË−þ® {{ ng 11 ¾Ë−þÂy mh ¥ﬂƒ hm 12 ÅÄ−ðÎ k{ ÅË−þ®y 13 ¾ÏðÉ i{ ÅË−þ®y 14 Û 15 Ë−ÎÏËÅ fÅ k ÅÄ−ðÎgy mh ‡−ÎÏËÅ |{ n hm 16 Û code/src/csapp.c Figure 10.8 The Ë©ÇˆË−þ®Ä©Å−¾ and Ë©ÇˆË−þ®Å¾ functions. Section 10.6 Reading File Metadata 939 Aside Origins of the Rio package The Rio functions are inspired by the Ë−þ®Ä©Å−, Ë−þ®Å, and ÑË©Î−Å functions described by W. Richard Stevens in his classic network programming text [110]. The Ë©ÇˆË−þ®Å and Ë©ÇˆÑË©Î−Å functions are identical to the Stevens Ë−þ®Å and ÑË©Î−Å functions. However, the Stevens Ë−þ®Ä©Å− function has some limitations that are corrected in Rio. First, because Ë−þ®Ä©Å− is buffered and Ë−þ®Å is not, these two functions cannot be used together on the same descriptor. Second, because it uses a ÍÎþÎ©² buffer, the Stevens Ë−þ®Ä©Å− function is not thread-safe, which required Stevens to introduce a different thread- safe version called Ë−þ®Ä©Å−ˆË. We have corrected both of these ﬂaws with the Ë©ÇˆË−þ®Ä©Å−¾ and Ë©ÇˆË−þ®Å¾ functions, which are mutually compatible and thread-safe. 10.6 Reading File Metadata An application can retrieve information about a ﬁle (sometimes called the ﬁle’s metadata) by calling the ÍÎþÎ and ðÍÎþÎ functions. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| a©Å²ÄÏ®− zÍÔÍmÍÎþÎl³| ©ÅÎ ÍÎþÎf²ÇÅÍÎ ²³þË hð©Ä−ÅþÀ−j ÍÎËÏ²Î ÍÎþÎ h¾Ïðgy ©ÅÎ ðÍÎþÎf©ÅÎ ð®j ÍÎËÏ²Î ÍÎþÎ h¾Ïðgy Returns: 0 if OK, −1 on error The ÍÎþÎ function takes as input a ﬁlename and ﬁlls in the members of a ÍÎþÎ structure shown in Figure 10.9. The ðÍÎþÎ function is similar, but it takes a ﬁle descriptor instead of a ﬁlename. We will need the ÍÎˆÀÇ®− and ÍÎˆÍ©Ö− members of the ÍÎþÎ structure when we discuss Web servers in Section 11.5. The other members are beyond our scope. The ÍÎˆÍ©Ö− member contains the ﬁle size in bytes. The ÍÎˆÀÇ®− member encodes both the ﬁle permission bits (Figure 10.2) and the ﬁle type (Section 10.2). Linux deﬁnes macro predicates in ÍÔÍmÍÎþÎl³ for determining the ﬁle type from the ÍÎˆÀÇ®− member: S_ISREG(m). Is this a regular ﬁle? S_ISDIR(m). Is this a directory ﬁle? S_ISSOCK(m). Is this a network socket? Figure 10.10 shows how we might use these macros and the ÍÎþÎ function to read and interpret a ﬁle’s ÍÎˆÀÇ®− bits. 940 Chapter 10 System-Level I/O statbuf.h (included by sys/stat.h) mh ›−Îþ®þÎþ Ë−ÎÏËÅ−® ¾Ô Î³− ÍÎþÎ þÅ® ðÍÎþÎ ðÏÅ²Î©ÇÅÍ hm ÍÎËÏ²Î ÍÎþÎ Õ ®−ÌˆÎ ÍÎˆ®−Ìy mh ⁄−Ì©²− hm ©ÅÇˆÎ ÍÎˆ©ÅÇy mh ©ÅÇ®− hm ÀÇ®−ˆÎ ÍÎˆÀÇ®−y mh –ËÇÎ−²Î©ÇÅ þÅ® ð©Ä− ÎÔÉ− hm ÅÄ©ÅÂˆÎ ÍÎˆÅÄ©ÅÂy mh ﬁÏÀ¾−Ë Çð ³þË® Ä©ÅÂÍ hm Ï©®ˆÎ ÍÎˆÏ©®y mh •Í−Ë '⁄ Çð ÇÑÅ−Ë hm ×©®ˆÎ ÍÎˆ×©®y mh §ËÇÏÉ '⁄ Çð ÇÑÅ−Ë hm ®−ÌˆÎ ÍÎˆË®−Ìy mh ⁄−Ì©²− ÎÔÉ− f©ð ©ÅÇ®− ®−Ì©²−g hm ÇððˆÎ ÍÎˆÍ©Ö−y mh ¶ÇÎþÄ Í©Ö−j ©Å ¾ÔÎ−Í hm ÏÅÍ©×Å−® ÄÇÅ× ÍÎˆ¾ÄÂÍ©Ö−y mh ¢ÄÇ²Â Í©Ö− ðÇË ð©Ä−ÍÔÍÎ−À 'mﬂ hm ÏÅÍ©×Å−® ÄÇÅ× ÍÎˆ¾ÄÇ²ÂÍy mh ﬁÏÀ¾−Ë Çð ¾ÄÇ²ÂÍ þÄÄÇ²þÎ−® hm Î©À−ˆÎ ÍÎˆþÎ©À−y mh ¶©À− Çð ÄþÍÎ þ²²−ÍÍ hm Î©À−ˆÎ ÍÎˆÀÎ©À−y mh ¶©À− Çð ÄþÍÎ ÀÇ®©ð©²þÎ©ÇÅ hm Î©À−ˆÎ ÍÎˆ²Î©À−y mh ¶©À− Çð ÄþÍÎ ²³þÅ×− hm Ûy statbuf.h (included by sys/stat.h) Figure 10.9 The ÍÎþÎ structure. code/io/statcheck.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Å f©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ÍÎËÏ²Î ÍÎþÎ ÍÎþÎy 6 ²³þË hÎÔÉ−j hË−þ®ÇÂy 7 8 ·ÎþÎfþË×Ì‰o`j dÍÎþÎgy 9 ©ð f·ˆ'·‡¥§fÍÎþÎlÍÎˆÀÇ®−gg mh ⁄−Î−ËÀ©Å− ð©Ä− ÎÔÉ− hm 10 ÎÔÉ− { ‘Ë−×ÏÄþË‘y 11 −ÄÍ− ©ð f·ˆ'·⁄'‡fÍÎþÎlÍÎˆÀÇ®−gg 12 ÎÔÉ− { ‘®©Ë−²ÎÇËÔ‘y 13 −ÄÍ− 14 ÎÔÉ− { ‘ÇÎ³−Ë‘y 15 ©ð ffÍÎþÎlÍÎˆÀÇ®− d ·ˆ'‡•·‡gg mh £³−²Â Ë−þ® þ²²−ÍÍ hm 16 Ë−þ®ÇÂ { ‘Ô−Í‘y 17 −ÄÍ− 18 Ë−þ®ÇÂ { ‘ÅÇ‘y 19 20 ÉË©ÅÎðf‘ÎÔÉ−x cÍj Ë−þ®x cÍ¿Å‘j ÎÔÉ−j Ë−þ®ÇÂgy 21 −Ó©Îfngy 22 Û code/io/statcheck.c Figure 10.10 Querying and manipulating a ﬁle’s ÍÎˆÀÇ®− bits. Section 10.7 Reading Directory Contents 941 10.7 Reading Directory Contents Applications can read the contents of a directory with the Ë−þ®®©Ë family of functions. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− z®©Ë−ÅÎl³| ⁄'‡ hÇÉ−Å®©Ëf²ÇÅÍÎ ²³þË hÅþÀ−gy Returns: pointer to handle if OK, NULL on error The ÇÉ−Å®©Ë function takes a pathname and returns a pointer to a directory stream. A stream is an abstraction for an ordered list of items, in this case a list of directory entries. a©Å²ÄÏ®− z®©Ë−ÅÎl³| ÍÎËÏ²Î ®©Ë−ÅÎ hË−þ®®©Ëf⁄'‡ h®©ËÉgy Returns: pointer to next directory entry if OK, NULL if no more entries or error Each call to Ë−þ®®©Ë returns a pointer to the next directory entry in the stream ®©ËÉ, or NULL if there are no more entries. Each directory entry is a structure of the form ÍÎËÏ²Î ®©Ë−ÅÎ Õ ©ÅÇˆÎ ®ˆ©ÅÇy mh ©ÅÇ®− ÅÏÀ¾−Ë hm ²³þË ®ˆÅþÀ−‰pst`y mh ƒ©Ä−ÅþÀ− hm Ûy Although some versions of Linux include other structure members, these are the only two that are standard across all systems. The ®ˆÅþÀ− member is the ﬁlename, and ®ˆ©ÅÇ is the ﬁle location. On error, Ë−þ®®©Ë returns NULL and sets −ËËÅÇ. Unfortunately, the only way to distinguish an error from the end-of-stream condition is to check if −ËËÅÇ has been modiﬁed since the call to Ë−þ®®©Ë. a©Å²ÄÏ®− z®©Ë−ÅÎl³| ©ÅÎ ²ÄÇÍ−®©Ëf⁄'‡ h®©ËÉgy Returns: 0 on success, −1 on error The ²ÄÇÍ−®©Ë function closes the stream and frees up any of its resources. Fig- ure 10.11 shows how we might use Ë−þ®®©Ë to read the contents of a directory. 942 Chapter 10 System-Level I/O code/io/readdir.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ⁄'‡ hÍÎË−þÀÉy 6 ÍÎËÏ²Î ®©Ë−ÅÎ h®−Éy 7 8 ÍÎË−þÀÉ { ﬂÉ−Å®©ËfþË×Ì‰o`gy 9 10 −ËËÅÇ { ny 11 Ñ³©Ä− ff®−É { Ë−þ®®©ËfÍÎË−þÀÉgg _{ ﬁ•‹‹g Õ 12 ÉË©ÅÎðf‘ƒÇÏÅ® ð©Ä−x cÍ¿Å‘j ®−Ék|®ˆÅþÀ−gy 13 Û 14 ©ð f−ËËÅÇ _{ ng 15 ÏÅ©Óˆ−ËËÇËf‘Ë−þ®®©Ë −ËËÇË‘gy 16 17 £ÄÇÍ−®©ËfÍÎË−þÀÉgy 18 −Ó©Îfngy 19 Û code/io/readdir.c Figure 10.11 Reading the contents of a directory. 10.8 Sharing Files Linux ﬁles can be shared in a number of different ways. Unless you have a clear picture of how the kernel represents open ﬁles, the idea of ﬁle sharing can be quite confusing. The kernel represents open ﬁles using three related data structures: Descriptor table. Each process has its own separate descriptor table whose en- tries are indexed by the process’s open ﬁle descriptors. Each open descrip- tor entry points to an entry in the ﬁle table. File table. The set of open ﬁles is represented by a ﬁle table that is shared by all processes. Each ﬁle table entry consists of (for our purposes) the current ﬁle position, a reference count of the number of descriptor entries that currently point to it, and a pointer to an entry in the v-node table. Closing a descriptor decrements the reference count in the associated ﬁle table entry. The kernel will not delete the ﬁle table entry until its reference count is zero. v-node table. Like the ﬁle table, the v-node table is shared by all processes. Each entry contains most of the information in the ÍÎþÎ structure, including the ÍÎˆÀÇ®− and ÍÎˆÍ©Ö− members. Section 10.8 Sharing Files 943 Figure 10.12 Typical kernel data structures for open ﬁles. In this example, two descriptors reference distinct ﬁles. There is no sharing. Descriptor table (one table per process) Open file table (shared by all processes) v-node table (shared by all processes) stdin fd 0 stdout fd 1 stderr fd 2 fd 3 fd 4 File size File access File type File B File pos refcnt\u00051… File A File pos refcnt\u00051…… File size File access File type… Figure 10.13 File sharing. This example shows two descriptors sharing the same disk ﬁle through two open ﬁle table entries. Descriptor table (one table per process) Open file table (shared by all processes) v-node table (shared by all processes) fd 0 fd 1 fd 2 fd 3 fd 4 File size File access File type File B File pos refcnt\u00051… File A File pos refcnt\u00051…… Figure 10.12 shows an example where descriptors 1 and 4 reference two different ﬁles through distinct open ﬁle table entries. This is the typical situation, where ﬁles are not shared and where each descriptor corresponds to a distinct ﬁle. Multiple descriptors can also reference the same ﬁle through different ﬁle table entries, as shown in Figure 10.13. This might happen, for example, if you were to call the ÇÉ−Å function twice with the same ﬁlename. The key idea is that each descriptor has its own distinct ﬁle position, so different reads on different descriptors can fetch data from different locations in the ﬁle. We can also understand how parent and child processes share ﬁles. Suppose that before a call to ðÇËÂ, the parent process has the open ﬁles shown in Fig- ure 10.12. Then Figure 10.14 shows the situation after the call to ðÇËÂ. The child gets its own duplicate copy of the parent’s descriptor table. Parent and child share the same set of open ﬁle tables and thus share the same ﬁle pos- ition. An important consequence is that the parent and child must both close their descriptors before the kernel will delete the corresponding ﬁle table entry. 944 Chapter 10 System-Level I/O Figure 10.14 How a child process inherits the parent’s open ﬁles. The initial situation is in Figure 10.12. Descriptor tables Open file table (shared by all processes) v-node table (shared by all processes) fd 0 fd 1 fd 2 fd 3 fd 4 File size File access File type File B File pos refcnt\u00052… File AParent’s table fd 0 fd 1 fd 2 fd 3 fd 4 Child’s table File pos refcnt\u00052…… File size File access File type… Practice Problem 10.2 (solution page 951) Suppose the disk ﬁle ðÇÇ¾þËlÎÓÎ consists of the six ASCII characters ðÇÇ¾þË. Then what is the output of the following program? 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ ð®oj ð®py 6 ²³þË ²y 7 8 ð®o { ﬂÉ−Åf‘ðÇÇ¾þËlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 9 ð®p { ﬂÉ−Åf‘ðÇÇ¾þËlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 10 ‡−þ®fð®oj d²j ogy 11 ‡−þ®fð®pj d²j ogy 12 ÉË©ÅÎðf‘² { c²¿Å‘j ²gy 13 −Ó©Îfngy 14 Û Practice Problem 10.3 (solution page 951) As before, suppose the disk ﬁle ðÇÇ¾þËlÎÓÎ consists of the six ASCII characters ðÇÇ¾þË. Then what is the output of the following program? 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ ð®y 6 ²³þË ²y Section 10.9 I/O Redirection 945 7 8 ð® { ﬂÉ−Åf‘ðÇÇ¾þËlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 9 ©ð fƒÇËÂfg {{ ng Õ 10 ‡−þ®fð®j d²j ogy 11 −Ó©Îfngy 12 Û 13 „þ©Îfﬁ•‹‹gy 14 ‡−þ®fð®j d²j ogy 15 ÉË©ÅÎðf‘² { c²¿Å‘j ²gy 16 −Ó©Îfngy 17 Û 10.9 I/O Redirection Linux shells provide I/O redirection operators that allow users to associate stan- dard input and output with disk ﬁles. For example, typing Ä©ÅÏÓ| ls > foo.txt causes the shell to load and execute the ÄÍ program, with standard output redi- rected to disk ﬁle ðÇÇlÎÓÎ. As we will see in Section 11.5, a Web server performs a similar kind of redirection when it runs a CGI program on behalf of the client. So how does I/O redirection work? One way is to use the ®ÏÉp function. a©Å²ÄÏ®− zÏÅ©ÍÎ®l³| ©ÅÎ ®ÏÉpf©ÅÎ ÇÄ®ð®j ©ÅÎ Å−Ñð®gy Returns: nonnegative descriptor if OK, −1 on error The ®ÏÉp function copies descriptor table entry ÇÄ®ð® to descriptor table entry Å−Ñð®, overwriting the previous contents of descriptor table entry Å−Ñð®.If Å−Ñð® was already open, then ®ÏÉp closes Å−Ñð® before it copies ÇÄ®ð®. Suppose that before calling ®ÏÉpfrjog, we have the situation in Figure 10.12, where descriptor 1 (standard output) corresponds to ﬁle A (say, a terminal) and descriptor 4 corresponds to ﬁle B (say, a disk ﬁle). The reference counts for A and B are both equal to 1. Figure 10.15 shows the situation after calling ®ÏÉpfrjog. Both descriptors now point to ﬁle B; ﬁle A has been closed and its ﬁle table and v-node table entries deleted; and the reference count for ﬁle B has been incremented. From this point on, any data written to standard output are redirected to ﬁle B. Practice Problem 10.4 (solution page 951) How would you use ®ÏÉp to redirect standard input to descriptor 5? 946 Chapter 10 System-Level I/O Aside Right and left hoinkies To avoid confusion with other bracket-type operators such as ‘`’ and ‘‰’, we have always referred to the shell’s ‘|’ operator as a “right hoinky” and the ‘z’ operator as a “left hoinky.” Figure 10.15 Kernel data structures after redirecting standard output by calling ®ÏÉpfrjog. The initial situation is shown in Figure 10.12. Descriptor table (one table per process) Open file table (shared by all processes) v-node table (shared by all processes) fd 0 fd 1 fd 2 fd 3 fd 4 File size File access File type File B File pos refcnt\u00052… File A File pos refcnt\u00050…… File size File access File type… Practice Problem 10.5 (solution page 952) Assuming that the disk ﬁle ðÇÇ¾þËlÎÓÎ consists of the six ASCII characters ðÇÇ¾þË, what is the output of the following program? 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ ð®oj ð®py 6 ²³þË ²y 7 8 ð®o { ﬂÉ−Åf‘ðÇÇ¾þËlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 9 ð®p { ﬂÉ−Åf‘ðÇÇ¾þËlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 10 ‡−þ®fð®pj d²j ogy 11 ⁄ÏÉpfð®pj ð®ogy 12 ‡−þ®fð®oj d²j ogy 13 ÉË©ÅÎðf‘² { c²¿Å‘j ²gy 14 −Ó©Îfngy 15 Û Section 10.11 Putting It Together: Which I/O Functions Should I Use? 947 10.10 Standard I/O The C language deﬁnes a set of higher-level input and output functions, called the standard I/O library, that provides programmers with a higher-level alternative to Unix I/O. The library (Ä©¾²) provides functions for opening and closing ﬁles (ðÇÉ−Å and ð²ÄÇÍ−), reading and writing bytes (ðË−þ® and ðÑË©Î−), reading and writing strings (ð×−ÎÍ and ðÉÏÎÍ), and sophisticated formatted I/O (Í²þÅð and ÉË©ÅÎð). The standard I/O library models an open ﬁle as a stream. To the programmer, a stream is a pointer to a structure of type ƒ'‹¥. Every ANSI C program begins with three open streams, ÍÎ®©Å, ÍÎ®ÇÏÎ, and ÍÎ®−ËË, which correspond to standard input, standard output, and standard error, respectively: a©Å²ÄÏ®− zÍÎ®©Çl³| −ÓÎ−ËÅ ƒ'‹¥ hÍÎ®©Åy mh ·ÎþÅ®þË® ©ÅÉÏÎ f®−Í²Ë©ÉÎÇË ng hm −ÓÎ−ËÅ ƒ'‹¥ hÍÎ®ÇÏÎy mh ·ÎþÅ®þË® ÇÏÎÉÏÎ f®−Í²Ë©ÉÎÇË og hm −ÓÎ−ËÅ ƒ'‹¥ hÍÎ®−ËËy mh ·ÎþÅ®þË® −ËËÇË f®−Í²Ë©ÉÎÇË pg hm A stream of type FILE is an abstraction for a ﬁle descriptor and a stream buffer. The purpose of the stream buffer is the same as the Rio read buffer: to minimize the number of expensive Linux I/O system calls. For example, suppose we have a program that makes repeated calls to the standard I/O ×−Î² function, where each invocation returns the next character from a ﬁle. When ×−Î² is called the ﬁrst time, the library ﬁlls the stream buffer with a single call to the Ë−þ® function and then returns the ﬁrst byte in the buffer to the application. As long as there are unread bytes in the buffer, subsequent calls to ×−Î² can be served directly from the stream buffer. 10.11 Putting It Together: Which I/O Functions Should I Use? Figure 10.16 summarizes the various I/O packages that we have discussed in this chapter. C application program Standard I/O functions Rio functions Unix I/O functions (accessed via system calls) fopen fread fscanf sscanf fgets fflush fclose fdopen fwrite fprintf sprintf fputs fseek rio_readn rio_writen rio_readinitb rio_readlineb rio_readnbopen write stat read lseek close Figure 10.16 Relationship between Unix I/O, standard I/O, and Rio. 948 Chapter 10 System-Level I/O The Unix I/O model is implemented in the operating system kernel. It is avail- able to applications through functions such as ÇÉ−Å, ²ÄÇÍ−, ÄÍ−−Â, Ë−þ®, ÑË©Î−, and ÍÎþÎ. The higher-level Rio and standard I/O functions are implemented “on top of” (using) the Unix I/O functions. The Rio functions are robust wrappers for Ë−þ® and ÑË©Î− that were developed speciﬁcally for this textbook. They automati- cally deal with short counts and provide an efﬁcient buffered approach for reading text lines. The standard I/O functions provide a more complete buffered alterna- tive to the Unix I/O functions, including formatted I/O routines such as ÉË©ÅÎð and Í²þÅð. So which of these functions should you use in your programs? Here are some basic guidelines: G1: Use the standard I/O functions whenever possible. The standard I/O func- tions are the method of choice for I/O on disk and terminal devices. Most C programmers use standard I/O exclusively throughout their careers, never bothering with the lower-level Unix I/O functions (except possibly ÍÎþÎ, which has no counterpart in the standard I/O library). Whenever possible, we recommend that you do likewise. G2: Don’t use Í²þÅð or Ë©ÇˆË−þ®Ä©Å−¾ to read binary ﬁles.Functions like Í²þÅð and Ë©ÇˆË−þ®Ä©Å−¾ are designed speciﬁcally for reading text ﬁles. A common error that students make is to use these functions to read binary data, causing their programs to fail in strange and unpredictable ways. For example, binary ﬁles might be littered with many nÓþ bytes that have nothing to do with terminating text lines. G3: Use the Rio functions for I/O on network sockets. Unfortunately, standard I/O poses some nasty problems when we attempt to use it for input and output on networks. As we will see in Section 11.4, the Linux abstrac- tion for a network is a type of ﬁle called a socket. Like any Linux ﬁle, sockets are referenced by ﬁle descriptors, known in this case as socket de- scriptors. Application processes communicate with processes running on other computers by reading and writing socket descriptors. Standard I/O streams are full duplex in the sense that programs can perform input and output on the same stream. However, there are poorly documented restrictions on streams that interact badly with restrictions on sockets: Restriction 1: Input functions following output functions. An input function cannot follow an output function without an intervening call to ððÄÏÍ³, ðÍ−−Â, ðÍ−ÎÉÇÍ,or Ë−Ñ©Å®.The ððÄÏÍ³ function empties the buffer as- sociated with a stream. The latter three functions use the Unix I/O ÄÍ−−Â function to reset the current ﬁle position. Restriction 2: Output functions following input functions. An output function cannot follow an input function without an intervening call to ðÍ−−Â, ðÍ−ÎÉÇÍ,or Ë−Ñ©Å®, unless the input function encounters an end-of-ﬁle. Section 10.12 Summary 949 These restrictions pose a problem for network applications because it is illegal to use the ÄÍ−−Â function on a socket. The ﬁrst restriction on stream I/O can be worked around by adopting a discipline of ﬂushing the buffer before every input operation. However, the only way to work around the second restriction is to open two streams on the same open socket descriptor, one for reading and one for writing: ƒ'‹¥ hðÉ©Åj hðÉÇÏÎy ðÉ©Å { ð®ÇÉ−ÅfÍÇ²Âð®j ‘Ë‘gy ðÉÇÏÎ { ð®ÇÉ−ÅfÍÇ²Âð®j ‘Ñ‘gy But this approach has problems as well, because it requires the application to call ð²ÄÇÍ− on both streams in order to free the memory resources associated with each stream and avoid a memory leak: ð²ÄÇÍ−fðÉ©Ågy ð²ÄÇÍ−fðÉÇÏÎgy Each of these operations attempts to close the same underlying socket descriptor, so the second ²ÄÇÍ− operation will fail. This is not a problem for sequential programs, but closing an already closed descriptor in a threaded program is a recipe for disaster (see Section 12.7.4). Thus, we recommend that you not use the standard I/O functions for input and output on network sockets. Use the robust Rio functions instead. If you need formatted output, use the ÍÉË©ÅÎð function to format a string in memory, and then send it to the socket using Ë©ÇˆÑË©Î−Å. If you need formatted input, use Ë©Çˆ Ë−þ®Ä©Å−¾ to read an entire text line, and then use ÍÍ²þÅð to extract different ﬁelds from the text line. 10.12 Summary Linux provides a small number of system-level functions, based on the Unix I/O model, that allow applications to open, close, read, and write ﬁles, to fetch ﬁle metadata, and to perform I/O redirection. Linux read and write operations are subject to short counts that applications must anticipate and handle correctly. Instead of calling the Unix I/O functions directly, applications should use the Rio package, which deals with short counts automatically by repeatedly performing read and write operations until all of the requested data have been transferred. The Linux kernel uses three related data structures to represent open ﬁles. Entries in a descriptor table point to entries in the open ﬁle table, which point to entries in the v-node table. Each process has its own distinct descriptor table, while all processes share the same open ﬁle and v-node tables. Understanding the general organization of these structures clariﬁes our understanding of both ﬁle sharing and I/O redirection. The standard I/O library is implemented on top of Unix I/O and provides a powerful set of higher-level I/O routines. For most applications, standard I/O is the 950 Chapter 10 System-Level I/O simpler, preferred alternative to Unix I/O. However, because of some mutually incompatible restrictions on standard I/O and network ﬁles, Unix I/O, rather than standard I/O, should be used for network applications. Bibliographic Notes Kerrisk gives a comprehensive treatment of Unix I/O and the Linux ﬁle sys- tem [62]. Stevens wrote the original standard reference text for Unix I/O [111]. Kernighan and Ritchie give a clear and complete discussion of the standard I/O functions [61]. Homework Problems 10.6 ◆ What is the output of the following program? 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åfg 4 Õ 5 ©ÅÎ ð®oj ð®py 6 7 ð®o { ﬂÉ−Åf‘ðÇÇlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 8 ð®p { ﬂÉ−Åf‘¾þËlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 9 £ÄÇÍ−fð®pgy 10 ð®p { ﬂÉ−Åf‘¾þÖlÎÓÎ‘j ﬂˆ‡⁄ﬂﬁ‹»j ngy 11 ÉË©ÅÎðf‘ð®p { c®¿Å‘j ð®pgy 12 −Ó©Îfngy 13 Û 10.7 ◆ Modify the ²Éð©Ä− program in Figure 10.5 so that it uses the Rio functions to copy standard input to standard output, MAXBUF bytes at a time. 10.8 ◆◆ Write a version of the ÍÎþÎ²³−²Â program in Figure 10.10, called ðÍÎþÎ²³−²Â, that takes a descriptor number on the command line rather than a ﬁlename. 10.9 ◆◆ Consider the following invocation of the ðÍÎþÎ²³−²Â program from Problem 10.8: Ä©ÅÏÓ| fstatcheck 3 < foo.txt You might expect that this invocation of ðÍÎþÎ²³−²Â would fetch and display metadata for ﬁle ðÇÇlÎÓÎ. However, when we run it on our system, it fails with a “bad ﬁle descriptor.” Given this behavior, ﬁll in the pseudocode that the shell must be executing between the ðÇËÂ and −Ó−²Ì− calls: Solutions to Practice Problems 951 ©ð fƒÇËÂfg {{ ng Õ mh ²³©Ä® hm mh „³þÎ ²Ç®− ©Í Î³− Í³−ÄÄ −Ó−²ÏÎ©Å× Ë©×³Î ³−Ë−} hm ¥Ó−²Ì−f‘ðÍÎþÎ²³−²Â‘j þË×Ìj −ÅÌÉgy Û 10.10 ◆◆ Modify the ²Éð©Ä− program in Figure 10.5 so that it takes an optional command- line argument ©Åð©Ä−.If ©Åð©Ä− is given, then copy ©Åð©Ä− to standard output; otherwise, copy standard input to standard output as before. The twist is that your solution must use the original copy loop (lines 9–11) for both cases. You are only allowed to insert code, and you are not allowed to change any of the existing code. Solutions to Practice Problems Solution to Problem 10.1 (page 931) Unix processes begin life with open descriptors assigned to ÍÎ®©Å (descriptor 0), ÍÎ®ÇÏÎ (descriptor 1), and ÍÎ®−ËË (descriptor 2). The ÇÉ−Å function always re- turns the lowest unopened descriptor, so the ﬁrst call to ÇÉ−Å returns descriptor 3. The call to the ²ÄÇÍ− function frees up descriptor 3. The ﬁnal call to ÇÉ−Å returns descriptor 3, and thus the output of the program is ð®p{q. Solution to Problem 10.2 (page 944) The descriptors ð®o and ð®p each have their own open ﬁle table entry, so each descriptor has its own ﬁle position for ðÇÇ¾þËlÎÓÎ. Thus, the read from ð®p reads the ﬁrst byte of ðÇÇ¾þËlÎÓÎ, and the output is ²{ð and not ²{Ç as you might have thought initially. Solution to Problem 10.3 (page 944) Recall that the child inherits the parent’s descriptor table and that all processes shared the same open ﬁle table. Thus, the descriptor ð® in both the parent and child points to the same open ﬁle table entry. When the child reads the ﬁrst byte of the ﬁle, the ﬁle position increases by 1. Thus, the parent reads the second byte, and the output is ²{Ç Solution to Problem 10.4 (page 945) To redirect standard input (descriptor 0) to descriptor 5, we would call ®ÏÉpfsjng, or equivalently, ®ÏÉpfsj·¶⁄'ﬁˆƒ'‹¥ﬁﬂg. 952 Chapter 10 System-Level I/O Solution to Problem 10.5 (page 946) At ﬁrst glance, you might think the output would be ²{ð but because we are redirecting ð®o to ð®p, the output is really ²{Ç CHAPTER 11 Network Programming 11.1 The Client-Server Programming Model 954 11.2 Networks 955 11.3 The Global IP Internet 960 11.4 The Sockets Interface 968 11.5 Web Servers 984 11.6 Putting It Together: The Tiny Web Server 992 11.7 Summary 1000 Bibliographic Notes 1001 Homework Problems 1001 Solutions to Practice Problems 1002 953 954 Chapter 11 Network Programming N etwork applications are everywhere. Any time you browse the Web, send an email message, or play an online game, you are using a network application. Interestingly, all network applications are based on the same basic programming model, have similar overall logical structures, and rely on the same programming interface. Network applications rely on many of the concepts that you have already learned in our study of systems. For example, processes, signals, byte ordering, memory mapping, and dynamic storage allocation all play important roles. There are new concepts to master as well. You will need to understand the basic client- server programming model and how to write client-server programs that use the services provided by the Internet. At the end, we will tie all of these ideas together by developing a tiny but functional Web server that can serve both static and dynamic content with text and graphics to real Web browsers. 11.1 The Client-Server Programming Model Every network application is based on the client-server model. With this model, an application consists of a server process and one or more client processes. A server manages some resource, and it provides some service for its clients by manipulating that resource. For example, a Web server manages a set of disk ﬁles that it retrieves and executes on behalf of clients. An FTP server manages a set of disk ﬁles that it stores and retrieves for clients. Similarly, an email server manages a spool ﬁle that it reads and updates for clients. The fundamental operation in the client-server model is the transaction (Fig- ure 11.1). A client-server transaction consists of four steps: 1. When a client needs service, it initiates a transaction by sending a request to the server. For example, when a Web browser needs a ﬁle, it sends a request to a Web server. 2. The server receives the request, interprets it, and manipulates its resources in the appropriate way. For example, when a Web server receives a request from a browser, it reads a disk ﬁle. 3. The server sends a response to the client and then waits for the next request. For example, a Web server sends the ﬁle back to a client. 4. Client processes response 1. Client sends request 3. Server sends response 2. Server processes request Client process Server process Resource Figure 11.1 A client-server transaction. Section 11.2 Networks 955 Aside Client-server transactions versus database transactions Client-server transactions are not database transactions and do not share any of their properties, such as atomicity. In our context, a transaction is simply a sequence of steps carried out by a client and a server. 4. The client receives the response and manipulates it. For example, after a Web browser receives a page from the server, it displays it on the screen. It is important to realize that clients and servers are processes and not ma- chines, or hosts as they are often called in this context. A single host can run many different clients and servers concurrently, and a client and server transaction can be on the same or different hosts. The client-server model is the same, regardless of the mapping of clients and servers to hosts. 11.2 Networks Clients and servers often run on separate hosts and communicate using the hard- ware and software resources of a computer network. Networks are sophisticated systems, and we can only hope to scratch the surface here. Our aim is to give you a workable mental model from a programmer’s perspective. To a host, a network is just another I/O device that serves as a source and sink for data, as shown in Figure 11.2. Figure 11.2 Hardware organization of a network host. CPU chip Register file ALU Bus interface I/O bridge System bus Memory bus Main memory I/O bus Expansion slots Disk controller Network adapter Network Graphics adapter MonitorMouse Keyboard USB controller Disk 956 Chapter 11 Network Programming Figure 11.3 Ethernet segment. Host Host Host Hub 100 Mb/s 100 Mb/s An adapter plugged into an expansion slot on the I/O bus provides the physical interface to the network. Data received from the network are copied from the adapter across the I/O and memory buses into memory, typically by a DMA transfer. Similarly, data can also be copied from memory to the network. Physically, a network is a hierarchical system that is organized by geographical proximity. At the lowest level is a LAN (local area network) that spans a building or a campus. The most popular LAN technology by far is Ethernet, which was de- veloped in the mid-1970s at Xerox PARC. Ethernet has proven to be remarkably resilient, evolving from 3 Mb/s to 10 Gb/s. An Ethernet segment consists of some wires (usually twisted pairs of wires) and a small box called a hub, as shown in Figure 11.3. Ethernet segments typically span small areas, such as a room or a ﬂoor in a building. Each wire has the same maximum bit bandwidth, typically 100 Mb/s or 1 Gb/s. One end is attached to an adapter on a host, and the other end is attached to a port on the hub. A hub slavishly copies every bit that it receives on each port to every other port. Thus, every host sees every bit. Each Ethernet adapter has a globally unique 48-bit address that is stored in a nonvolatile memory on the adapter. A host can send a chunk of bits called a frame to any other host on the segment. Each frame includes some ﬁxed number of header bits that identify the source and destination of the frame and the frame length, followed by a payload of data bits. Every host adapter sees the frame, but only the destination host actually reads it. Multiple Ethernet segments can be connected into larger LANs, called bridged Ethernets, using a set of wires and small boxes called bridges, as shown in Figure 11.4. Bridged Ethernets can span entire buildings or campuses. In a bridged Ethernet, some wires connect bridges to bridges, and others connect bridges to hubs. The bandwidths of the wires can be different. In our example, the bridge–bridge wire has a 1 Gb/s bandwidth, while the four hub–bridge wires have bandwidths of 100 Mb/s. Bridges make better use of the available wire bandwidth than hubs. Using a clever distributed algorithm, they automatically learn over time which hosts are reachable from which ports and then selectively copy frames from one port to another only when it is necessary. For example, if host A sends a frame to host B, which is on the segment, then bridge X will throw away the frame when it arrives at its input port, thus saving bandwidth on the other segments. However, if host A sends a frame to host C on a different segment, then bridge X will copy the frame only to the port connected to bridge Y, which will copy the frame only to the port connected to host C’s segment. Section 11.2 Networks 957 Aside Internet versus internet We will always use lowercase internet to denote the general concept, and uppercase Internet to denote a speciﬁc implementation—namely, the global IP Internet. Host Host Host Hub Bridge Bridge Host Host 100 Mb/s 100 Mb/s 100 Mb/s 100 Mb/s 1 Gb/s Host Host Hub Host Host Hub Host Host Hub Host C X A B Y Figure 11.4 Bridged Ethernet segments. Figure 11.5 Conceptual view of a LAN. Host Host Host. . . To simplify our pictures of LANs, we will draw the hubs and bridges and the wires that connect them as a single horizontal line, as shown in Figure 11.5. At a higher level in the hierarchy, multiple incompatible LANs can be con- nected by specialized computers called routers to form an internet (interconnected network). Each router has an adapter (port) for each network that it is connected to. Routers can also connect high-speed point-to-point phone connections, which are examples of networks known as WANs (wide area networks), so called be- cause they span larger geographical areas than LANs. In general, routers can be used to build internets from arbitrary collections of LANs and WANs. For ex- ample, Figure 11.6 shows an example internet with a pair of LANs and WANs connected by three routers. 958 Chapter 11 Network Programming Host Host Host. . . LAN Host Host Host. . . LAN WAN WAN RouterRouterRouter Figure 11.6 A small internet. Two LANs and two WANs are connected by three routers. The crucial property of an internet is that it can consist of different LANs and WANs with radically different and incompatible technologies. Each host is physically connected to every other host, but how is it possible for some source host to send data bits to another destination host across all of these incompatible networks? The solution is a layer of protocol software running on each host and router that smoothes out the differences between the different networks. This software implements a protocol that governs how hosts and routers cooperate in order to transfer data. The protocol must provide two basic capabilities: Naming scheme. Different LAN technologies have different and incompatible ways of assigning addresses to hosts. The internet protocol smoothes these differences by deﬁning a uniform format for host addresses. Each host is then assigned at least one of these internet addresses that uniquely identiﬁes it. Delivery mechanism. Different networking technologies have different and incompatible ways of encoding bits on wires and of packaging these bits into frames. The internet protocol smoothes these differences by deﬁning a uniform way to bundle up data bits into discrete chunks called packets.A packet consists of a header, which contains the packet size and addresses of the source and destination hosts, and a payload, which contains data bits sent from the source host. Figure 11.7 shows an example of how hosts and routers use the internet protocol to transfer data across incompatible LANs. The example internet consists of two LANs connected by a router. A client running on host A, which is attached to LAN1, sends a sequence of data bytes to a server running on host B, which is attached to LAN2. There are eight basic steps: 1. The client on host A invokes a system call that copies the data from the client’s virtual address space into a kernel buffer. 2. The protocol software on host A creates a LAN1 frame by appending an internet header and a LAN1 frame header to the data. The internet header is addressed to internet host B. The LAN1 frame header is addressed to the router. It then passes the frame to the adapter. Notice that the payload of the LAN1 frame is an internet packet, whose payload is the actual user data. This kind of encapsulation is one of the fundamental insights of internetworking. Section 11.2 Networks 959 Host A Client Protocol software Protocol software LAN1 adapter Host B Server Protocol software Data Internet packet LAN1 frame LAN1 LAN2 (1) Data PH FH1(2) LAN2 frame Data PH FH2 (5) Data PH FH1(3) Data PH FH2(6) Data PH FH2(7) Data(8) Data PH FH1(4) LAN1 adapter LAN2 adapter Router LAN2 adapter Figure 11.7 How data travel from one host to another on an internet. PH: internet packet header; FH1: frame header for LAN1; FH2: frame header for LAN2. 3. The LAN1 adapter copies the frame to the network. 4. When the frame reaches the router, the router’s LAN1 adapter reads it from the wire and passes it to the protocol software. 5. The router fetches the destination internet address from the internet packet header and uses this as an index into a routing table to determine where to forward the packet, which in this case is LAN2. The router then strips off the old LAN1 frame header, prepends a new LAN2 frame header addressed to host B, and passes the resulting frame to the adapter. 6. The router’s LAN2 adapter copies the frame to the network. 7. When the frame reaches host B, its adapter reads the frame from the wire and passes it to the protocol software. 8. Finally, the protocol software on host B strips off the packet header and frame header. The protocol software will eventually copy the resulting data into the server’s virtual address space when the server invokes a system call that reads the data. Of course, we are glossing over many difﬁcult issues here. What if different networks have different maximum frame sizes? How do routers know where to forward frames? How are routers informed when the network topology changes? What if a packet gets lost? Nonetheless, our example captures the essence of the internet idea, and encapsulation is the key. 960 Chapter 11 Network Programming Figure 11.8 Hardware and software organization of an Internet application. Client Internet client host User code Sockets interface (system calls) Hardware interface (interrupts) TCP/IP Kernel code Network adapter Server Internet server host TCP/IP Network adapter Hardware Global IP Internet 11.3 The Global IP Internet The global IP Internet is the most famous and successful implementation of an internet. It has existed in one form or another since 1969. While the internal architecture of the Internet is complex and constantly changing, the organization of client-server applications has remained remarkably stable since the early 1980s. Figure 11.8 shows the basic hardware and software organization of an Internet client-server application. Each Internet host runs software that implements the TCP/IP protocol (Transmission Control Protocol/Internet Protocol), which is supported by almost every modern computer system. Internet clients and servers communicate using a mix of sockets interface functions and Unix I/O functions. (We will describe the sockets interface in Section 11.4.) The sockets functions are typically implemented as system calls that trap into the kernel and call various kernel-mode functions in TCP/IP. TCP/IP is actually a family of protocols, each of which contributes different capabilities. For example, IP provides the basic naming scheme and a delivery mechanism that can send packets, known as datagrams, from one Internet host to any other host. The IP mechanism is unreliable in the sense that it makes no effort to recover if datagrams are lost or duplicated in the network. UDP (Unreliable Datagram Protocol) extends IP slightly, so that datagrams can be transferred from process to process, rather than host to host. TCP is a complex protocol that builds on IP to provide reliable full duplex (bidirectional) connections between processes. To simplify our discussion, we will treat TCP/IP as a single monolithic protocol. We will not discuss its inner workings, and we will only discuss some of the basic capabilities that TCP and IP provide to application programs. We will not discuss UDP. From a programmer’s perspective, we can think of the Internet as a worldwide collection of hosts with the following properties: . The set of hosts is mapped to a set of 32-bit IP addresses. Section 11.3 The Global IP Internet 961 Aside IPv4 and IPv6 The original Internet protocol, with its 32-bit addresses, is known as Internet Protocol Version 4 (IPv4). In 1996, the Internet Engineering Task Force (IETF) proposed a new version of IP, called Internet Protocol Version 6 (IPv6), that uses 128-bit addresses and that was intended as the successor to IPv4. However, as of 2015, almost 20 years later, the vast majority of Internet trafﬁc is still carried by IPv4 networks. For example, only 4 percent of users access Google services using IPv6 [42]. Because of its low adoption rate, we will not discuss IPv6 in any detail in this book and will focus exclusively on the concepts behind IPv4. When we talk about the Internet, what we mean is the Internet based on IPv4. Nonetheless, the techniques for writing clients and servers that we will teach you later in this chapter are based on modern interfaces that are independent of any particular protocol. . The set of IP addresses is mapped to a set of identiﬁers called Internet domain names. . A process on one Internet host can communicate with a process on any other Internet host over a connection. The following sections discuss these fundamental Internet ideas in more detail. 11.3.1 IP Addresses An IP address is an unsigned 32-bit integer. Network programs store IP addresses in the IP address structure shown in Figure 11.9. Storing a scalar address in a structure is an unfortunate artifact from the early implementations of the sockets interface. It would make more sense to deﬁne a scalar type for IP addresses, but it is too late to change now because of the enormous installed base of applications. Because Internet hosts can have different host byte orders, TCP/IP deﬁnes a uniform network byte order (big-endian byte order) for any integer data item, such as an IP address, that is carried across the network in a packet header. Addresses in IP address structures are always stored in (big-endian) network byte order, even if the host byte order is little-endian. Unix provides the following functions for converting between network and host byte order. code/netp/netpfragments.c mh '– þ®®Ë−ÍÍ ÍÎËÏ²ÎÏË− hm ÍÎËÏ²Î ©Åˆþ®®Ë Õ Ï©ÅÎqpˆÎ Íˆþ®®Ëy mh ¡®®Ë−ÍÍ ©Å Å−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë f¾©×k−Å®©þÅg hm Ûy code/netp/netpfragments.c Figure 11.9 IP address structure. 962 Chapter 11 Network Programming a©Å²ÄÏ®− zþËÉþm©Å−Îl³| Ï©ÅÎqpˆÎ ³ÎÇÅÄfÏ©ÅÎqpˆÎ ³ÇÍÎÄÇÅ×gy Ï©ÅÎotˆÎ ³ÎÇÅÍfÏ©ÅÎotˆÎ ³ÇÍÎÍ³ÇËÎgy Returns: value in network byte order Ï©ÅÎqpˆÎ ÅÎÇ³ÄfÏ©ÅÎqpˆÎ Å−ÎÄÇÅ×gy Ï©ÅÎotˆÎ ÅÎÇ³ÍfÏÅ©ÎotˆÎ Å−ÎÍ³ÇËÎgy Returns: value in host byte order The ³ÎÇÅÄ function converts an unsigned 32-bit integer from host byte order to network byte order. The ÅÎÇ³Ä function converts an unsigned 32-bit integer from network byte order to host byte order. The ³ÎÇÅÍ and ÅÎÇ³Í functions perform corresponding conversions for unsigned 16-bit integers. Note that there are no equivalent functions for manipulating 64-bit values. IP addresses are typically presented to humans in a form known as dotted- decimal notation, where each byte is represented by its decimal value and sep- arated from the other bytes by a period. For example, opvlplowrlprp is the dotted-decimal representation of the address nÓvnnp²pðp. On Linux systems, you can use the hostname command to determine the dotted-decimal address of your own host: Ä©ÅÏÓ| hostname -i opvlplponlous Application programs can convert back and forth between IP addresses and dotted-decimal strings using the functions ©Å−ÎˆÉÎÇÅ and ©Å−ÎˆÅÎÇÉ. a©Å²ÄÏ®− zþËÉþm©Å−Îl³| ©ÅÎ ©Å−ÎˆÉÎÇÅf¡ƒˆ'ﬁ¥¶j ²ÇÅÍÎ ²³þË hÍË²j ÌÇ©® h®ÍÎgy Returns: 1 if OK, 0 if ÍË² is invalid dotted decimal, −1 on error ²ÇÅÍÎ ²³þË h©Å−ÎˆÅÎÇÉf¡ƒˆ'ﬁ¥¶j ²ÇÅÍÎ ÌÇ©® hÍË²j ²³þË h®ÍÎj ÍÇ²ÂÄ−ÅˆÎ Í©Ö−gy Returns: pointer to a dotted-decimal string if OK, NULL on error In these function names, the “Å” stands for network and the “É” stands for pre- sentation. They can manipulate either 32-bit IPv4 addresses (¡ƒˆ'ﬁ¥¶), as shown here, or 128-bit IPv6 addresses (¡ƒˆ'ﬁ¥¶t), which we do not cover. The ©Å−ÎˆÉÎÇÅ function converts a dotted-decimal string (ÍË²) to a binary IP address in network byte order (®ÍÎ). If ÍË² does not point to a valid dotted-decimal string, then it returns 0. Any other error returns −1 and sets −ËËÅÇ. Similarly, the ©Å−ÎˆÅÎÇÉ function converts a binary IP address in network byte order (ÍË²)to the corresponding dotted-decimal representation and copies at most Í©Ö− bytes of the resulting null-terminated string to ®ÍÎ. Section 11.3 The Global IP Internet 963 Practice Problem 11.1 (solution page 1002) Complete the following table: Dotted-decimal address Hex address onulpoplopplpns trloplorwloq onulpoplwtlpw nÓnnnnnnvn nÓƒƒƒƒƒƒnn nÓn¡nonorn Practice Problem 11.2 (solution page 1003) Write a program ³−Óp®®l² that converts its 16-bit hex argument to a 16-bit network byte order and prints the result. For example Ä©ÅÏÓ| ./hex2dd 0x400 onpr Practice Problem 11.3 (solution page 1003) Write a program ®®p³−Ól² that converts its 16-bit network byte order to a 16-bit hex number and prints the result. For example, Ä©ÅÏÓ| ./dd2hex 1024 nÓrnn 11.3.2 Internet Domain Names Internet clients and servers use IP addresses when they communicate with each other. However, large integers are difﬁcult for people to remember, so the Internet also deﬁnes a separate set of more human-friendly domain names, as well as a mechanism that maps the set of domain names to the set of IP addresses. A domain name is a sequence of words (letters, numbers, and dashes) separated by periods, such as Ñ³þÄ−Í³þËÂl©²Íl²Íl²ÀÏl−®Ï. The set of domain names forms a hierarchy, and each domain name encodes its position in the hierarchy. An example is the easiest way to understand this. Figure 11.10 shows a portion of the domain name hierarchy. The hierarchy is represented as a tree. The nodes of the tree represent domain names that are formed by the path back to the root. Subtrees are referred to as sub- domains. The ﬁrst level in the hierarchy is an unnamed root node. The next level is a collection of ﬁrst-level domain names that are deﬁned by a nonproﬁt organi- zation called ICANN (Internet Corporation for Assigned Names and Numbers). Common ﬁrst-level domains include ²ÇÀ, −®Ï, ×ÇÌ, ÇË×, and Å−Î. 964 Chapter 11 Network Programming mil edu gov com cmumit cs ece whaleshark 128.2.210.175 ics Unnamed root pdl www 128.2.131.66 amazon www 176.32.98.166 First-level domain names Second-level domain names Third-level domain names berkeley Figure 11.10 Subset of the Internet domain name hierarchy. At the next level are second-level domain names such as ²ÀÏl−®Ï, which are assigned on a ﬁrst-come ﬁrst-serve basis by various authorized agents of ICANN. Once an organization has received a second-level domain name, then it is free to create any other new domain name within its subdomain, such as ²Íl²ÀÏl−®Ï. The Internet deﬁnes a mapping between the set of domain names and the set of IP addresses. Until 1988, this mapping was maintained manually in a sin- gle text ﬁle called ¤ﬂ·¶·l¶”¶. Since then, the mapping has been maintained in a distributed worldwide database known as DNS (Domain Name System). Concep- tually, the DNS database consists of millions of host entries, each of which deﬁnes the mapping between a set of domain names and a set of IP addresses. In a math- ematical sense, think of each host entry as an equivalence class of domain names and IP addresses. We can explore some of the properties of the DNS mappings with the Linux nslookup program, which displays the IP addresses associated with a domain name.1 Each Internet host has the locally deﬁned domain name ÄÇ²þÄ³ÇÍÎ, which always maps to the loopback address opulnlnlo: Ä©ÅÏÓ| nslookup localhost ¡®®Ë−ÍÍx opulnlnlo The ÄÇ²þÄ³ÇÍÎ name provides a convenient and portable way to reference clients and servers that are running on the same machine, which can be especially useful 1. We’ve reformatted the output of nslookup to improve readability. Section 11.3 The Global IP Internet 965 for debugging. We can use hostname to determine the real domain name of our local host: Ä©ÅÏÓ| hostname Ñ³þÄ−Í³þËÂl©²Íl²Íl²ÀÏl−®Ï In the simplest case, there is a one-to-one mapping between a domain name and an IP address: Ä©ÅÏÓ| nslookup whaleshark.ics.cs.cmu.edu ¡®®Ë−ÍÍx opvlplponlous However, in some cases, multiple domain names are mapped to the same IP address: Ä©ÅÏÓ| nslookup cs.mit.edu ¡®®Ë−ÍÍx ovltplolt Ä©ÅÏÓ| nslookup eecs.mit.edu ¡®®Ë−ÍÍx ovltplolt In the most general case, multiple domain names are mapped to the same set of multiple IP addresses: Ä©ÅÏÓ| nslookup www.twitter.com ¡®®Ë−ÍÍx owwlotlostlt ¡®®Ë−ÍÍx owwlotlostlun ¡®®Ë−ÍÍx owwlotlostlonp ¡®®Ë−ÍÍx owwlotlostlpqn Ä©ÅÏÓ| nslookup twitter.com ¡®®Ë−ÍÍx owwlotlostlonp ¡®®Ë−ÍÍx owwlotlostlpqn ¡®®Ë−ÍÍx owwlotlostlt ¡®®Ë−ÍÍx owwlotlostlun Finally, we notice that some valid domain names are not mapped to any IP address: Ä©ÅÏÓ| nslookup edu hhh £þÅ’Î ð©Å® −®Ïx ﬁÇ þÅÍÑ−Ë Ä©ÅÏÓ| nslookup ics.cs.cmu.edu hhh £þÅ’Î ð©Å® ©²Íl²Íl²ÀÏl−®Ïx ﬁÇ þÅÍÑ−Ë 11.3.3 Internet Connections Internet clients and servers communicate by sending and receiving streams of bytes over connections. A connection is point-to-point in the sense that it connects a pair of processes. It is full duplex in the sense that data can ﬂow in both directions 966 Chapter 11 Network Programming Aside How many Internet hosts are there? Twice a year since 1987, the Internet Systems Consortium conducts the Internet Domain Survey.The survey, which estimates the number of Internet hosts by counting the number of IP addresses that have been assigned a domain name, reveals an amazing trend. Since 1987, when there were about 20,000 Internet hosts, the number of hosts has been increasing exponentially. By 2015, there were over 1,000,000,000 Internet hosts! at the same time. And it is reliable in the sense that—barring some catastrophic failure such as a cable cut by the proverbial careless backhoe operator—the stream of bytes sent by the source process is eventually received by the destination process in the same order it was sent. A socket is an end point of a connection. Each socket has a corresponding socket address that consists of an Internet address and a 16-bit integer port2 and is denoted by the notation þ®®Ë−ÍÍxÉÇËÎ. The port in the client’s socket address is assigned automatically by the kernel when the client makes a connection request and is known as an ephemeral port. However, the port in the server’s socket address is typically some well-known port that is permanently associated with the service. For example, Web servers typically use port 80, and email servers use port 25. Associated with each service with a well-known port is a corresponding well-known service name. For example, the well-known name for the Web service is ³ÎÎÉ, and the well-known name for email is ÍÀÎÉ. The mapping between well-known names and well-known ports is contained in a ﬁle called m−Î²mÍ−ËÌ©²−Í. A connection is uniquely identiﬁed by the socket addresses of its two end points. This pair of socket addresses is known as a socket pair and is denoted by the tuple fcliaddrxcliportj servaddrxservportg where cliaddr is the client’s IP address, cliport is the client’s port, servaddr is the server’s IP address, and servport is the server’s port. For example, Figure 11.11 shows a connection between a Web client and a Web server. In this example, the Web client’s socket address is opvlplowrlprpxsopoq where port sopoq is an ephemeral port assigned by the kernel. The Web server’s socket address is pnvlpotlovolosxvn 2. These software ports have no relation to the hardware ports in network switches and routers. Section 11.3 The Global IP Internet 967 Aside Origins of the Internet The Internet is one of the most successful examples of government, university, and industry partnership. Many factors contributed to its success, but we think two are particularly important: a sustained 30- year investment by the United States government and a commitment by passionate researchers to what Dave Clarke at MIT has dubbed “rough consensus and working code.” The seeds of the Internet were sown in 1957, when, at the height of the Cold War, the Soviet Union shocked the world by launching Sputnik, the ﬁrst artiﬁcial earth satellite. In response, the United States government created the Advanced Research Projects Agency (ARPA), whose charter was to reestablish the US lead in science and technology. In 1967, Lawrence Roberts at ARPA published plans for a new network called the ARPANET. The ﬁrst ARPANET nodes were up and running by 1969. By 1971, there were 13 ARPANET nodes, and email had emerged as the ﬁrst important network application. In 1972, Robert Kahn outlined the general principles of internetworking: a collection of intercon- nected networks, with communication between the networks handled independently on a “best-effort basis” by black boxes called “routers.” In 1974, Kahn and Vinton Cerf published the ﬁrst details of TCP/IP, which by 1982 had become the standard internetworking protocol for ARPANET. On January 1, 1983, every node on the ARPANET switched to TCP/IP, marking the birth of the global IP Internet. In 1985, Paul Mockapetris invented DNS, and there were over 1,000 Internet hosts. The next year, the National Science Foundation (NSF) built the NSFNET backbone connecting 13 sites with 56 Kb/s phone lines. It was upgraded to 1.5 Mb/s T1 links in 1988 and 45 Mb/s T3 links in 1991. By 1988, there were more than 50,000 hosts. In 1989, the original ARPANET was ofﬁcially retired. In 1995, when there were almost 10,000,000 Internet hosts, NSF retired NSFNET and replaced it with the modern Internet architecture based on private commercial backbones connected by public network access points. Figure 11.11 Anatomy of an Internet connection. Client Client host address 128.2.194.242 Connection socket pair (128.2.194.242:51213, 208.216.181.15:80) Server (port 80) Server host address 208.216.181.15 Client socket address 128.2.194.242:51213 Server socket address 208.216.181.15:80 where port vn is the well-known port associated with Web services. Given these client and server socket addresses, the connection between the client and server is uniquely identiﬁed by the socket pair fopvlplowrlprpxsopoqj pnvlpotlovolosxvng 968 Chapter 11 Network Programming Aside Origins of the sockets interface The original sockets interface was developed by researchers at University of California, Berkeley, in the early 1980s. For this reason, it is often referred to as Berkeley sockets. The Berkeley researchers developed the sockets interface to work with any underlying protocol. The ﬁrst implementation was for TCP/IP, which they included in the Unix 4.2BSD kernel and distributed to numerous universities and labs. This was an important event in Internet history. Almost overnight, thousands of people had access to TCP/IP and its source codes. It generated tremendous excitement and sparked a ﬂurry of new research in networking and internetworking. 11.4 The Sockets Interface The sockets interface is a set of functions that are used in conjunction with the Unix I/O functions to build network applications. It has been implemented on most modern systems, including all Unix variants as well as Windows and Macintosh systems. Figure 11.12 gives an overview of the sockets interface in the context of a typical client-server transaction. You should use this picture as a road map when we discuss the individual functions. Client socket open_clientfd open_listenfd connect rio_writen rio_readlineb rio_readlineb close Server Connection request Await connection request from next client EOF socket getaddrinfogetaddrinfo bind listen accept rio_writen rio_readlineb close Figure 11.12 Overview of network applications based on the sockets interface. Section 11.4 The Sockets Interface 969 Aside What does the ˆ©Å sufﬁx mean? The ˆ©Å sufﬁx is short for internet, not input. code/netp/netpfragments.c mh '– ÍÇ²Â−Î þ®®Ë−ÍÍ ÍÎËÏ²ÎÏË− hm ÍÎËÏ²Î ÍÇ²Âþ®®Ëˆ©Å Õ Ï©ÅÎotˆÎ Í©ÅˆðþÀ©ÄÔy mh –ËÇÎÇ²ÇÄ ðþÀ©ÄÔ fþÄÑþÔÍ ¡ƒˆ'ﬁ¥¶g hm Ï©ÅÎotˆÎ Í©ÅˆÉÇËÎy mh –ÇËÎ ÅÏÀ¾−Ë ©Å Å−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë hm ÍÎËÏ²Î ©Åˆþ®®Ë Í©Åˆþ®®Ëy mh '– þ®®Ë−ÍÍ ©Å Å−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë hm ÏÅÍ©×Å−® ²³þË Í©ÅˆÖ−ËÇ‰v`y mh –þ® ÎÇ Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®Ëg hm Ûy mh §−Å−Ë©² ÍÇ²Â−Î þ®®Ë−ÍÍ ÍÎËÏ²ÎÏË− fðÇË ²ÇÅÅ−²Îj ¾©Å®j þÅ® þ²²−ÉÎg hm ÍÎËÏ²Î ÍÇ²Âþ®®Ë Õ Ï©ÅÎotˆÎ ÍþˆðþÀ©ÄÔy mh –ËÇÎÇ²ÇÄ ðþÀ©ÄÔ hm ²³þË Íþˆ®þÎþ‰or`y mh ¡®®Ë−ÍÍ ®þÎþ hm Ûy code/netp/netpfragments.c Figure 11.13 Socket address structures. 11.4.1 Socket Address Structures From the perspective of the Linux kernel, a socket is an end point for communi- cation. From the perspective of a Linux program, a socket is an open ﬁle with a corresponding descriptor. Internet socket addresses are stored in 16-byte structures having the type ÍÇ²Âþ®®Ëˆ©Å, shown in Figure 11.13. For Internet applications, the Í©ÅˆðþÀ©ÄÔ ﬁeld is AF_INET, the Í©ÅˆÉÇËÎ ﬁeld is a 16-bit port number, and the Í©Åˆþ®®Ë ﬁeld contains a 32-bit IP address. The IP address and port number are always stored in network (big-endian) byte order. The ²ÇÅÅ−²Î, ¾©Å®, and þ²²−ÉÎ functions require a pointer to a protocol- speciﬁc socket address structure. The problem faced by the designers of the sockets interface was how to deﬁne these functions to accept any kind of socket address structure. Today, we would use the generic ÌÇ©® h pointer, which did not exist in C at that time. Their solution was to deﬁne sockets functions to expect a pointer to a generic ÍÇ²Âþ®®Ë structure (Figure 11.13) and then require applications to cast any pointers to protocol-speciﬁc structures to this generic structure. To simplify our code examples, we follow Stevens’s lead and deﬁne the following type: ÎÔÉ−®−ð ÍÎËÏ²Î ÍÇ²Âþ®®Ë ·¡y 970 Chapter 11 Network Programming We then use this type whenever we need to cast a ÍÇ²Âþ®®Ëˆ©Å structure to a generic ÍÇ²Âþ®®Ë structure. 11.4.2 The ÍÇ²Â−Î Function Clients and servers use the ÍÇ²Â−Î function to create a socket descriptor. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| ©ÅÎ ÍÇ²Â−Îf©ÅÎ ®ÇÀþ©Åj ©ÅÎ ÎÔÉ−j ©ÅÎ ÉËÇÎÇ²ÇÄgy Returns: nonnegative descriptor if OK, −1 on error If we wanted the socket to be the end point for a connection, then we could call ÍÇ²Â−Î with the following hardcoded arguments: ²Ä©−ÅÎð® { ·Ç²Â−Îf¡ƒˆ'ﬁ¥¶j ·ﬂ£«ˆ·¶‡¥¡›j ngy where AF_INET indicates that we are using 32-bit IP addresses and SOCK_ STREAM indicates that the socket will be an end point for a connection. However, the best practice is to use the ×−Îþ®®Ë©ÅðÇ function (Section 11.4.7) to generate these parameters automatically, so that the code is protocol-independent. We will show you how to use ×−Îþ®®Ë©ÅðÇ with the ÍÇ²Â−Î function in Section 11.4.8. The ²Ä©−ÅÎð® descriptor returned by ÍÇ²Â−Î is only partially opened and cannot yet be used for reading and writing. How we ﬁnish opening the socket depends on whether we are a client or a server. The next section describes how we ﬁnish opening the socket if we are a client. 11.4.3 The ²ÇÅÅ−²Î Function A client establishes a connection with a server by calling the ²ÇÅÅ−²Î function. a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| ©ÅÎ ²ÇÅÅ−²Îf©ÅÎ ²Ä©−ÅÎð®j ²ÇÅÍÎ ÍÎËÏ²Î ÍÇ²Âþ®®Ë hþ®®Ëj ÍÇ²ÂÄ−ÅˆÎ þ®®ËÄ−Ågy Returns: 0 if OK, −1 on error The ²ÇÅÅ−²Î function attempts to establish an Internet connection with the server at socket address þ®®Ë, where þ®®ËÄ−Å is Í©Ö−ÇðfÍÇ²Âþ®®Ëˆ©Åg.The ²ÇÅÅ−²Î function blocks until either the connection is successfully established or an error occurs. If successful, the ²Ä©−ÅÎð® descriptor is now ready for reading and writing, and the resulting connection is characterized by the socket pair fÓxÔj þ®®ËlÍ©Åˆþ®®Ëxþ®®ËlÍ©ÅˆÉÇËÎg Section 11.4 The Sockets Interface 971 where Ó is the client’s IP address and Ô is the ephemeral port that uniquely identiﬁes the client process on the client host. As with ÍÇ²Â−Î, the best practice is to use ×−Îþ®®Ë©ÅðÇ to supply the arguments to ²ÇÅÅ−²Î (see Section 11.4.8). 11.4.4 The ¾©Å® Function The remaining sockets functions—¾©Å®, Ä©ÍÎ−Å, and þ²²−ÉÎ—are used by servers to establish connections with clients. a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| ©ÅÎ ¾©Å®f©ÅÎ ÍÇ²Âð®j ²ÇÅÍÎ ÍÎËÏ²Î ÍÇ²Âþ®®Ë hþ®®Ëj ÍÇ²ÂÄ−ÅˆÎ þ®®ËÄ−Ågy Returns: 0 if OK, −1 on error The ¾©Å® function asks the kernel to associate the server’s socket address in þ®®Ë with the socket descriptor ÍÇ²Âð®.The þ®®ËÄ−Å argument is Í©Ö−ÇðfÍÇ²Âþ®®Ëˆ ©Åg. As with ÍÇ²Â−Î and ²ÇÅÅ−²Î, the best practice is to use ×−Îþ®®Ë©ÅðÇ to supply the arguments to ¾©Å® (see Section 11.4.8). 11.4.5 The Ä©ÍÎ−Å Function Clients are active entities that initiate connection requests. Servers are passive entities that wait for connection requests from clients. By default, the kernel assumes that a descriptor created by the ÍÇ²Â−Î function corresponds to an active socket that will live on the client end of a connection. A server calls the Ä©ÍÎ−Å function to tell the kernel that the descriptor will be used by a server instead of a client. a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| ©ÅÎ Ä©ÍÎ−Åf©ÅÎ ÍÇ²Âð®j ©ÅÎ ¾þ²ÂÄÇ×gy Returns: 0 if OK, −1 on error The Ä©ÍÎ−Å function converts ÍÇ²Âð® from an active socket to a listening socket that can accept connection requests from clients. The ¾þ²ÂÄÇ× argument is a hint about the number of outstanding connection requests that the kernel should queue up before it starts to refuse requests. The exact meaning of the ¾þ²ÂÄÇ× argument requires an understanding of TCP/IP that is beyond our scope. We will typically set it to a large value, such as 1,024. 972 Chapter 11 Network Programming Client Connection request clientfd Client clientfd listenfd(3) connfd(4) listenfd(3) listenfd(3) Client Server Server Server clientfd 1. Server blocks in accept, waiting for connection request on listening descriptor listenfd. 2. Client makes connection request by calling and blocking in connect. 3. Server returns connfd from accept. Client returns from connect. Connection is now established between clientfd and connfd. Figure 11.14 The roles of the listening and connected descriptors. 11.4.6 The þ²²−ÉÎ Function Servers wait for connection requests from clients by calling the þ²²−ÉÎ function. a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| ©ÅÎ þ²²−ÉÎf©ÅÎ Ä©ÍÎ−Åð®j ÍÎËÏ²Î ÍÇ²Âþ®®Ë hþ®®Ëj ©ÅÎ hþ®®ËÄ−Ågy Returns: nonnegative connected descriptor if OK, −1 on error The þ²²−ÉÎ function waits for a connection request from a client to arrive on the listening descriptor Ä©ÍÎ−Åð®, then ﬁlls in the client’s socket address in þ®®Ë, and returns a connected descriptor that can be used to communicate with the client using Unix I/O functions. The distinction between a listening descriptor and a connected descriptor confuses many students. The listening descriptor serves as an end point for client connection requests. It is typically created once and exists for the lifetime of the server. The connected descriptor is the end point of the connection that is established between the client and the server. It is created each time the server accepts a connection request and exists only as long as it takes the server to service a client. Figure 11.14 outlines the roles of the listening and connected descriptors. In step 1, the server calls þ²²−ÉÎ, which waits for a connection request to arrive on the listening descriptor, which for concreteness we will assume is descriptor 3. Recall that descriptors 0–2 are reserved for the standard ﬁles. In step 2, the client calls the ²ÇÅÅ−²Î function, which sends a connection request to Ä©ÍÎ−Åð®. In step 3, the þ²²−ÉÎ function opens a new connected de- scriptor ²ÇÅÅð® (which we will assume is descriptor 4), establishes the connection between ²Ä©−ÅÎð® and ²ÇÅÅð®, and then returns ²ÇÅÅð® to the application. The Section 11.4 The Sockets Interface 973 Aside Why the distinction between listening and connected descriptors? You might wonder why the sockets interface makes a distinction between listening and connected descriptors. At ﬁrst glance, it appears to be an unnecessary complication. However, distinguishing between the two turns out to be quite useful, because it allows us to build concurrent servers that can process many client connections simultaneously. For example, each time a connection request arrives on the listening descriptor, we might fork a new process that communicates with the client over its connected descriptor. You’ll learn more about concurrent servers in Chapter 12. client also returns from the ²ÇÅÅ−²Î, and from this point, the client and server can pass data back and forth by reading and writing ²Ä©−ÅÎð® and ²ÇÅÅð®, re- spectively. 11.4.7 Host and Service Conversion Linux provides some powerful functions, called ×−Îþ®®Ë©ÅðÇ and ×−ÎÅþÀ−©ÅðÇ, for converting back and forth between binary socket address structures and the string representations of hostnames, host addresses, service names, and port numbers. When used in conjunction with the sockets interface, they allow us to write network programs that are independent of any particular version of the IP protocol. The ×−Îþ®®Ë©ÅðÇ Function The ×−Îþ®®Ë©ÅðÇ function converts string representations of hostnames, host addresses, service names, and port numbers into socket address structures. It is the modern replacement for the obsolete ×−Î³ÇÍÎ¾ÔÅþÀ− and ×−ÎÍ−ËÌ¾ÔÅþÀ− functions. Unlike these functions, it is reentrant (see Section 12.7.2) and works with any protocol. a©Å²ÄÏ®− zÍÔÍmÎÔÉ−Íl³| a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| a©Å²ÄÏ®− zÅ−Î®¾l³| ©ÅÎ ×−Îþ®®Ë©ÅðÇf²ÇÅÍÎ ²³þË h³ÇÍÎj ²ÇÅÍÎ ²³þË hÍ−ËÌ©²−j ²ÇÅÍÎ ÍÎËÏ²Î þ®®Ë©ÅðÇ h³©ÅÎÍj ÍÎËÏ²Î þ®®Ë©ÅðÇ hhË−ÍÏÄÎgy Returns: 0 if OK, nonzero error code on error ÌÇ©® ðË−−þ®®Ë©ÅðÇfÍÎËÏ²Î þ®®Ë©ÅðÇ hË−ÍÏÄÎgy Returns: nothing ²ÇÅÍÎ ²³þË h×þ©ˆÍÎË−ËËÇËf©ÅÎ −ËË²Ç®−gy Returns: error message 974 Chapter 11 Network Programming Figure 11.15 Data structure returned by ×−Îþ®®Ë©ÅðÇ. Socket address structsai_canonname addrinfo structs ai_addr result ai_next NULL ai_addr ai_next NULL ai_addr NULL Given ³ÇÍÎ and Í−ËÌ©²− (the two components of a socket address), ×−Îþ®®Ë©ÅðÇ returns a Ë−ÍÏÄÎ that points to a linked list of þ®®Ë©ÅðÇ structures, each of which points to a socket address structure that corresponds to ³ÇÍÎ and Í−ËÌ©²− (Figure 11.15). After a client calls ×−Îþ®®Ë©ÅðÇ, it walks this list, trying each socket address in turn until the calls to ÍÇ²Â−Î and ²ÇÅÅ−²Î succeed and the connection is established. Similarly, a server tries each socket address on the list until the calls to ÍÇ²Â−Î and ¾©Å® succeed and the descriptor is bound to a valid socket address. To avoid memory leaks, the application must eventually free the list by calling ðË−−þ®®Ë©ÅðÇ.If ×−Îþ®®Ë©ÅðÇ returns a nonzero error code, the application can call ×þ©ˆÍÎË−ËËÇË to convert the code to a message string. The ³ÇÍÎ argument to ×−Îþ®®Ë©ÅðÇ can be either a domain name or a numeric address (e.g., a dotted-decimal IP address). The Í−ËÌ©²− argument can be either a service name (e.g., ³ÎÎÉ) or a decimal port number. If we are not interested in converting the hostname to an address, we can set ³ÇÍÎ to NULL. The same holds for Í−ËÌ©²−. However, at least one of them must be speciﬁed. The optional ³©ÅÎÍ argument is an þ®®Ë©ÅðÇ structure (Figure 11.16) that provides ﬁner control over the list of socket addresses that ×−Îþ®®Ë©ÅðÇ re- turns. When passed as a ³©ÅÎÍ argument, only the þ©ˆðþÀ©ÄÔ, þ©ˆÍÇ²ÂÎÔÉ−, þ©ˆÉËÇÎÇ²ÇÄ, and þ©ˆðÄþ×Í ﬁelds can be set. The other ﬁelds must be set to zero (or NULL). In practice, we use À−ÀÍ−Î to zero the entire structure and then set a few selected ﬁelds: . By default, ×−Îþ®®Ë©ÅðÇ can return both IPv4 and IPv6 socket addresses. Setting þ©ˆðþÀ©ÄÔ to AF_INET restricts the list to IPv4 addresses. Setting it to AF_INET6 restricts the list to IPv6 addresses. Section 11.4 The Sockets Interface 975 code/netp/netpfragments.c ÍÎËÏ²Î þ®®Ë©ÅðÇ Õ ©ÅÎ þ©ˆðÄþ×Íy mh ¤©ÅÎÍ þË×ÏÀ−ÅÎ ðÄþ×Í hm ©ÅÎ þ©ˆðþÀ©ÄÔy mh ƒ©ËÍÎ þË× ÎÇ ÍÇ²Â−Î ðÏÅ²Î©ÇÅ hm ©ÅÎ þ©ˆÍÇ²ÂÎÔÉ−y mh ·−²ÇÅ® þË× ÎÇ ÍÇ²Â−Î ðÏÅ²Î©ÇÅ hm ©ÅÎ þ©ˆÉËÇÎÇ²ÇÄy mh ¶³©Ë® þË× ÎÇ ÍÇ²Â−Î ðÏÅ²Î©ÇÅ hm ²³þË hþ©ˆ²þÅÇÅÅþÀ−y mh £þÅÇÅ©²þÄ ³ÇÍÎÅþÀ− hm Í©Ö−ˆÎ þ©ˆþ®®ËÄ−Åy mh ·©Ö− Çð þ©ˆþ®®Ë ÍÎËÏ²Î hm ÍÎËÏ²Î ÍÇ²Âþ®®Ë hþ©ˆþ®®Ëy mh –ÎË ÎÇ ÍÇ²Â−Î þ®®Ë−ÍÍ ÍÎËÏ²ÎÏË− hm ÍÎËÏ²Î þ®®Ë©ÅðÇ hþ©ˆÅ−ÓÎy mh –ÎË ÎÇ Å−ÓÎ ©Î−À ©Å Ä©ÅÂ−® Ä©ÍÎ hm Ûy code/netp/netpfragments.c Figure 11.16 The þ®®Ë©ÅðÇ structure used by ×−Îþ®®Ë©ÅðÇ. . By default, for each unique address associated with ³ÇÍÎ, the ×−Îþ®®Ë©ÅðÇ function can return up to three þ®®Ë©ÅðÇ structures, each with a different þ©ˆ ÍÇ²ÂÎÔÉ− ﬁeld: one for connections, one for datagrams (not covered), and one for raw sockets (not covered). Setting þ©ˆÍÇ²ÂÎÔÉ− to SOCK_STREAM restricts the list to at most one þ®®Ë©ÅðÇ structure for each unique address, one whose socket address can be used as the end point of a connection. This is the desired behavior for all of our example programs. . The þ©ˆðÄþ×Í ﬁeld is a bit mask that further modiﬁes the default behavior. You create it by oring combinations of various values. Here are some that we ﬁnd useful: AI_ADDRCONFIG. This ﬂag is recommended if you are using connec- tions [34]. It asks ×−Îþ®®Ë©ÅðÇ to return IPv4 addresses only if the local host is conﬁgured for IPv4. Similarly for IPv6. AI_CANONNAME. By default, the þ©ˆ²þÅÇÅÅþÀ− ﬁeld is NULL. If this ﬂag is set, it instructs ×−Îþ®®Ë©ÅðÇ to point the þ©ˆ²þÅÇÅÅþÀ− ﬁeld in the ﬁrst þ®®Ë©ÅðÇ structure in the list to the canonical (ofﬁcial) name of ³ÇÍÎ (see Figure 11.15). AI_NUMERICSERV. By default, the Í−ËÌ©²− argument can be a service name or a port number. This ﬂag forces the Í−ËÌ©²− argument to be a port number. AI_PASSIVE. By default, ×−Îþ®®Ë©ÅðÇ returns socket addresses that can be used by clients as active sockets in calls to ²ÇÅÅ−²Î. This ﬂag instructs it to return socket addresses that can be used by servers as listening sockets. In this case, the ³ÇÍÎ argument should be NULL. The address ﬁeld in the resulting socket address structure(s) will be the wildcard address, which tells the kernel that this server will accept requests to any of the IP addresses for this host. This is the desired behavior for all of our example servers. 976 Chapter 11 Network Programming When ×−Îþ®®Ë©ÅðÇ creates an þ®®Ë©ÅðÇ structure in the output list, it ﬁlls in each ﬁeld except for þ©ˆðÄþ×Í.The þ©ˆþ®®Ë ﬁeld points to a socket address structure, the þ©ˆþ®®ËÄ−Å ﬁeld gives the size of this socket address structure, and the þ©ˆÅ−ÓÎ ﬁeld points to the next þ®®Ë©ÅðÇ structure in the list. The other ﬁelds describe various attributes of the socket address. One of the elegant aspects of ×−Îþ®®Ë©ÅðÇ is that the ﬁelds in an þ®®Ë©ÅðÇ structure are opaque, in the sense that they can be passed directly to the functions in the sockets interface without any further manipulation by the application code. For example, þ©ˆðþÀ©ÄÔ, þ©ˆÍÇ²ÂÎÔÉ−, and þ©ˆÉËÇÎÇ²ÇÄ can be passed directly to ÍÇ²Â−Î. Similarly, þ©ˆþ®®Ë and þ©ˆþ®®ËÄ−Å can be passed directly to ²ÇÅÅ−²Î and ¾©Å®. This powerful property allows us to write clients and servers that are independent of any particular version of the IP protocol. The ×−ÎÅþÀ−©ÅðÇ Function The ×−ÎÅþÀ−©ÅðÇ function is the inverse of ×−Îþ®®Ë©ÅðÇ. It converts a socket ad- dress structure to the corresponding host and service name strings. It is the modern replacement for the obsolete ×−Î³ÇÍÎ¾Ôþ®®Ë and ×−ÎÍ−ËÌ¾ÔÉÇËÎ functions, and unlike those functions, it is reentrant and protocol-independent. a©Å²ÄÏ®− zÍÔÍmÍÇ²Â−Îl³| a©Å²ÄÏ®− zÅ−Î®¾l³| ©ÅÎ ×−ÎÅþÀ−©ÅðÇf²ÇÅÍÎ ÍÎËÏ²Î ÍÇ²Âþ®®Ë hÍþj ÍÇ²ÂÄ−ÅˆÎ ÍþÄ−Åj ²³þË h³ÇÍÎj Í©Ö−ˆÎ ³ÇÍÎÄ−Åj ²³þË hÍ−ËÌ©²−j Í©Ö−ˆÎ Í−ËÌÄ−Åj ©ÅÎ ðÄþ×Így Returns: 0 if OK, nonzero error code on error The Íþ argument points to a socket address structure of size ÍþÄ−Å bytes, ³ÇÍÎ to a buffer of size ³ÇÍÎÄ−Å bytes, and Í−ËÌ©²− to a buffer of size Í−ËÌÄ−Å bytes. The ×−ÎÅþÀ−©ÅðÇ function converts the socket address structure Íþ to the corre- sponding host and service name strings and copies them to the ³ÇÍÎ and Í−ËÌ©²− buffers. If ×−ÎÅþÀ−©ÅðÇ returns a nonzero error code, the application can convert it to a string by calling ×þ©ˆÍÎË−ËËÇË. If we don’t want the hostname, we can set ³ÇÍÎ to NULL and ³ÇÍÎÄ−Å to zero. The same holds for the service ﬁelds. However, one or the other must be set. The ðÄþ×Í argument is a bit mask that modiﬁes the default behavior. You create it by oring combinations of various values. Here are a couple of useful ones: NI_NUMERICHOST. By default, ×−ÎÅþÀ−©ÅðÇ tries to return a domain name in ³ÇÍÎ. Setting this ﬂag will cause it to return a numeric address string instead. NI_NUMERICSERV. By default, ×−ÎÅþÀ−©ÅðÇ will look in m−Î²mÍ−ËÌ©²−Í and if possible, return a service name instead of a port number. Setting this ﬂag forces it to skip the lookup and simply return the port number. Section 11.4 The Sockets Interface 977 code/netp/hostinfo.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ÍÎËÏ²Î þ®®Ë©ÅðÇ hÉj hÄ©ÍÎÉj ³©ÅÎÍy 6 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 7 ©ÅÎ Ë²j ðÄþ×Íy 8 9 ©ð fþË×² _{ pg Õ 10 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ z®ÇÀþ©Å ÅþÀ−|¿Å‘j þË×Ì‰n`gy 11 −Ó©Îfngy 12 Û 13 14 mh §−Î þ Ä©ÍÎ Çð þ®®Ë©ÅðÇ Ë−²ÇË®Í hm 15 À−ÀÍ−Îfd³©ÅÎÍj nj Í©Ö−ÇðfÍÎËÏ²Î þ®®Ë©ÅðÇggy 16 ³©ÅÎÍlþ©ˆðþÀ©ÄÔ { ¡ƒˆ'ﬁ¥¶y mh '–Ìr ÇÅÄÔ hm 17 ³©ÅÎÍlþ©ˆÍÇ²ÂÎÔÉ− { ·ﬂ£«ˆ·¶‡¥¡›y mh £ÇÅÅ−²Î©ÇÅÍ ÇÅÄÔ hm 18 ©ð ffË² { ×−Îþ®®Ë©ÅðÇfþË×Ì‰o`j ﬁ•‹‹j d³©ÅÎÍj dÄ©ÍÎÉgg _{ ng Õ 19 ðÉË©ÅÎðfÍÎ®−ËËj ‘×−Îþ®®Ë©ÅðÇ −ËËÇËx cÍ¿Å‘j ×þ©ˆÍÎË−ËËÇËfË²ggy 20 −Ó©Îfogy 21 Û 22 23 mh „þÄÂ Î³− Ä©ÍÎ þÅ® ®©ÍÉÄþÔ −þ²³ '– þ®®Ë−ÍÍ hm 24 ðÄþ×Í { ﬁ'ˆﬁ•›¥‡'£¤ﬂ·¶y mh ⁄©ÍÉÄþÔ þ®®Ë−ÍÍ ÍÎË©Å× ©ÅÍÎ−þ® Çð ®ÇÀþ©Å ÅþÀ− hm 25 ðÇË fÉ { Ä©ÍÎÉy Éy É { Ék|þ©ˆÅ−ÓÎg Õ 26 §−ÎÅþÀ−©ÅðÇfÉk|þ©ˆþ®®Ëj Ék|þ©ˆþ®®ËÄ−Åj ¾Ïðj ›¡”‹'ﬁ¥j ﬁ•‹‹j nj ðÄþ×Így 27 ÉË©ÅÎðf‘cÍ¿Å‘j ¾Ïðgy 28 Û 29 30 mh £Ä−þÅ ÏÉ hm 31 ƒË−−þ®®Ë©ÅðÇfÄ©ÍÎÉgy 32 33 −Ó©Îfngy 34 Û code/netp/hostinfo.c Figure 11.17 Hostinfo displays the mapping of a domain name to its associated IP addresses. Figure 11.17 shows a simple program, called hostinfo, that uses ×−Îþ®®Ë©ÅðÇ and ×−ÎÅþÀ−©ÅðÇ to display the mapping of a domain name to its associated IP addresses. It is similar to the nslookup program from Section 11.3.2. First, we initialize the ³©ÅÎÍ structure so that ×−Îþ®®Ë©ÅðÇ returns the ad- dresses we want. In this case, we are looking for 32-bit IP addresses (line 16) 978 Chapter 11 Network Programming that can be used as end points of connections (line 17). Since we are only asking ×−Îþ®®Ë©ÅðÇ to convert domain names, we call it with a NULL Í−ËÌ©²− argument. After the call to ×−Îþ®®Ë©ÅðÇ, we walk the list of þ®®Ë©ÅðÇ structures, using ×−ÎÅþÀ−©ÅðÇ to convert each socket address to a dotted-decimal address string. After walking the list, we are careful to free it by calling ðË−−þ®®Ë©ÅðÇ (although for this simple program it is not strictly necessary). When we run hostinfo, we see that ÎÑ©ÎÎ−Ël²ÇÀ maps to four IP addresses, which is what we saw using nslookup in Section 11.3.2. Ä©ÅÏÓ| ./hostinfo twitter.com owwlotlostlonp owwlotlostlpqn owwlotlostlt owwlotlostlun Practice Problem 11.4 (solution page 1004) The ×−Îþ®®Ë©ÅðÇ and ×−ÎÅþÀ−©ÅðÇ functions subsume the functionality of ©Å−Îˆ ÉÎÇÅ and ©Å−ÎˆÅÎÇÉ, respectively, and they provide a higher-level of abstraction that is independent of any particular address format. To convince yourself how handy this is, write a version of hostinfo (Figure 11.17) that uses ©Å−ÎˆÅÎÇÉ in- stead of ×−ÎÅþÀ−©ÅðÇ to convert each socket address to a dotted-decimal address string. 11.4.8 Helper Functions for the Sockets Interface The ×−Îþ®®Ë©ÅðÇ function and the sockets interface can seem somewhat daunting when you ﬁrst learn about them. We ﬁnd it convenient to wrap them with higher- level helper functions, called ÇÉ−Åˆ²Ä©−ÅÎð® and ÇÉ−ÅˆÄ©ÍÎ−Åð®, that clients and servers can use when they want to communicate with each other. The ÇÉ−Åˆ²Ä©−ÅÎð® Function A client establishes a connection with a server by calling ÇÉ−Åˆ²Ä©−ÅÎð®. a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ©ÅÎ ÇÉ−Åˆ²Ä©−ÅÎð®f²³þË h³ÇÍÎÅþÀ−j ²³þË hÉÇËÎgy Returns: descriptor if OK, −1 on error The ÇÉ−Åˆ²Ä©−ÅÎð® function establishes a connection with a server running on host ³ÇÍÎÅþÀ− and listening for connection requests on port number ÉÇËÎ.It returns an open socket descriptor that is ready for input and output using the Unix I/O functions. Figure 11.18 shows the code for ÇÉ−Åˆ²Ä©−ÅÎð®. We call ×−Îþ®®Ë©ÅðÇ, which returns a list of þ®®Ë©ÅðÇ structures, each of which points to a socket address structure that is suitable for establishing a con- Section 11.4 The Sockets Interface 979 code/src/csapp.c 1 ©ÅÎ ÇÉ−Åˆ²Ä©−ÅÎð®f²³þË h³ÇÍÎÅþÀ−j ²³þË hÉÇËÎg Õ 2 ©ÅÎ ²Ä©−ÅÎð®y 3 ÍÎËÏ²Î þ®®Ë©ÅðÇ ³©ÅÎÍj hÄ©ÍÎÉj hÉy 4 5 mh §−Î þ Ä©ÍÎ Çð ÉÇÎ−ÅÎ©þÄ Í−ËÌ−Ë þ®®Ë−ÍÍ−Í hm 6 À−ÀÍ−Îfd³©ÅÎÍj nj Í©Ö−ÇðfÍÎËÏ²Î þ®®Ë©ÅðÇggy 7 ³©ÅÎÍlþ©ˆÍÇ²ÂÎÔÉ− { ·ﬂ£«ˆ·¶‡¥¡›y mh ﬂÉ−Å þ ²ÇÅÅ−²Î©ÇÅ hm 8 ³©ÅÎÍlþ©ˆðÄþ×Í { ¡'ˆﬁ•›¥‡'£·¥‡‚y mh lll ÏÍ©Å× þ ÅÏÀ−Ë©² ÉÇËÎ þË×l hm 9 ³©ÅÎÍlþ©ˆðÄþ×Í Ú{ ¡'ˆ¡⁄⁄‡£ﬂﬁƒ'§y mh ‡−²ÇÀÀ−Å®−® ðÇË ²ÇÅÅ−²Î©ÇÅÍ hm 10 §−Îþ®®Ë©ÅðÇf³ÇÍÎÅþÀ−j ÉÇËÎj d³©ÅÎÍj dÄ©ÍÎÉgy 11 12 mh „þÄÂ Î³− Ä©ÍÎ ðÇË ÇÅ− Î³þÎ Ñ− ²þÅ ÍÏ²²−ÍÍðÏÄÄÔ ²ÇÅÅ−²Î ÎÇ hm 13 ðÇË fÉ { Ä©ÍÎÉy Éy É { Ék|þ©ˆÅ−ÓÎg Õ 14 mh £Ë−þÎ− þ ÍÇ²Â−Î ®−Í²Ë©ÉÎÇË hm 15 ©ð ff²Ä©−ÅÎð® { ÍÇ²Â−ÎfÉk|þ©ˆðþÀ©ÄÔj Ék|þ©ˆÍÇ²ÂÎÔÉ−j Ék|þ©ˆÉËÇÎÇ²ÇÄgg z ng 16 ²ÇÅÎ©ÅÏ−y mh ·Ç²Â−Î ðþ©Ä−®j ÎËÔ Î³− Å−ÓÎ hm 17 18 mh £ÇÅÅ−²Î ÎÇ Î³− Í−ËÌ−Ë hm 19 ©ð f²ÇÅÅ−²Îf²Ä©−ÅÎð®j Ék|þ©ˆþ®®Ëj Ék|þ©ˆþ®®ËÄ−Åg _{ kog 20 ¾Ë−þÂy mh ·Ï²²−ÍÍ hm 21 £ÄÇÍ−f²Ä©−ÅÎð®gy mh £ÇÅÅ−²Î ðþ©Ä−®j ÎËÔ þÅÇÎ³−Ë hm 22 Û 23 24 mh £Ä−þÅ ÏÉ hm 25 ƒË−−þ®®Ë©ÅðÇfÄ©ÍÎÉgy 26 ©ð f_Ég mh ¡ÄÄ ²ÇÅÅ−²ÎÍ ðþ©Ä−® hm 27 Ë−ÎÏËÅ koy 28 −ÄÍ− mh ¶³− ÄþÍÎ ²ÇÅÅ−²Î ÍÏ²²−−®−® hm 29 Ë−ÎÏËÅ ²Ä©−ÅÎð®y 30 Û code/src/csapp.c Figure 11.18 ÇÉ−Åˆ²Ä©−ÅÎð®: Helper function that establishes a connection with a server. It is reentrant and protocol-independent. nection with a server running on ³ÇÍÎÅþÀ− and listening on ÉÇËÎ. We then walk the list, trying each list entry in turn, until the calls to ÍÇ²Â−Î and ²ÇÅÅ−²Î suc- ceed. If the ²ÇÅÅ−²Î fails, we are careful to close the socket descriptor before trying the next entry. If the ²ÇÅÅ−²Î succeeds, we free the list memory and return the socket descriptor to the client, which can immediately begin using Unix I/O to communicate with the server. Notice how there is no dependence on any particular version of IP anywhere in the code. The arguments to ÍÇ²Â−Î and ²ÇÅÅ−²Î are generated for us automat- ically by ×−Îþ®®Ë©ÅðÇ, which allows our code to be clean and portable. 980 Chapter 11 Network Programming The ÇÉ−ÅˆÄ©ÍÎ−Åð® Function A server creates a listening descriptor that is ready to receive connection requests by calling the ÇÉ−ÅˆÄ©ÍÎ−Åð® function. a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ©ÅÎ ÇÉ−ÅˆÄ©ÍÎ−Åð®f²³þË hÉÇËÎgy Returns: descriptor if OK, −1 on error The ÇÉ−ÅˆÄ©ÍÎ−Åð® function returns a listening descriptor that is ready to receive connection requests on port ÉÇËÎ. Figure 11.19 shows the code for ÇÉ−ÅˆÄ©ÍÎ−Åð®. The style is similar to ÇÉ−Åˆ²Ä©−ÅÎð®. We call ×−Îþ®®Ë©ÅðÇ and then walk the resulting list until the calls to ÍÇ²Â−Î and ¾©Å® succeed. Note that in line 20 we use the Í−ÎÍÇ²ÂÇÉÎ function (not described here) to conﬁgure the server so that it can be terminated, be restarted, and begin accepting connection requests immediately. By default, a restarted server will deny connection requests from clients for approximately 30 seconds, which seriously hinders debugging. Since we have called ×−Îþ®®Ë©ÅðÇ with the AI_PASSIVE ﬂag and a NULL ³ÇÍÎ argument, the address ﬁeld in each socket address structure is set to the wildcard address, which tells the kernel that this server will accept requests to any of the IP addresses for this host. Finally, we call the Ä©ÍÎ−Å function to convert Ä©ÍÎ−Åð® to a listening descrip- tor and return it to the caller. If the Ä©ÍÎ−Å fails, we are careful to avoid a memory leak by closing the descriptor before returning. 11.4.9 Example Echo Client and Server The best way to learn the sockets interface is to study example code. Figure 11.20 shows the code for an echo client. After establishing a connection with the server, the client enters a loop that repeatedly reads a text line from standard input, sends the text line to the server, reads the echo line from the server, and prints the result to standard output. The loop terminates when ð×−ÎÍ encounters EOF on standard input, either because the user typed Ctrl+D at the keyboard or because it has exhausted the text lines in a redirected input ﬁle. After the loop terminates, the client closes the descriptor. This results in an EOF notiﬁcation being sent to the server, which it detects when it receives a return code of zero from its Ë©ÇˆË−þ®Ä©Å−¾ function. After closing its descriptor, the client terminates. Since the client’s kernel automatically closes all open descriptors when a process terminates, the ²ÄÇÍ− in line 24 is not necessary. However, it is good programming practice to explicitly close any descriptors that you have opened. Figure 11.21 shows the main routine for the echo server. After opening the listening descriptor, it enters an inﬁnite loop. Each iteration waits for a connection request from a client, prints the domain name and port of the connected client, and then calls the −²³Ç function that services the client. After the echo routine returns, Section 11.4 The Sockets Interface 981 code/src/csapp.c 1 ©ÅÎ ÇÉ−ÅˆÄ©ÍÎ−Åð®f²³þË hÉÇËÎg 2 Õ 3 ÍÎËÏ²Î þ®®Ë©ÅðÇ ³©ÅÎÍj hÄ©ÍÎÉj hÉy 4 ©ÅÎ Ä©ÍÎ−Åð®j ÇÉÎÌþÄ{oy 5 6 mh §−Î þ Ä©ÍÎ Çð ÉÇÎ−ÅÎ©þÄ Í−ËÌ−Ë þ®®Ë−ÍÍ−Í hm 7 À−ÀÍ−Îfd³©ÅÎÍj nj Í©Ö−ÇðfÍÎËÏ²Î þ®®Ë©ÅðÇggy 8 ³©ÅÎÍlþ©ˆÍÇ²ÂÎÔÉ− { ·ﬂ£«ˆ·¶‡¥¡›y mh ¡²²−ÉÎ ²ÇÅÅ−²Î©ÇÅÍ hm 9 ³©ÅÎÍlþ©ˆðÄþ×Í { ¡'ˆ–¡··'‚¥ Ú ¡'ˆ¡⁄⁄‡£ﬂﬁƒ'§y mh lll ÇÅ þÅÔ '– þ®®Ë−ÍÍ hm 10 ³©ÅÎÍlþ©ˆðÄþ×Í Ú{ ¡'ˆﬁ•›¥‡'£·¥‡‚y mh lll ÏÍ©Å× ÉÇËÎ ÅÏÀ¾−Ë hm 11 §−Îþ®®Ë©ÅðÇfﬁ•‹‹j ÉÇËÎj d³©ÅÎÍj dÄ©ÍÎÉgy 12 13 mh „þÄÂ Î³− Ä©ÍÎ ðÇË ÇÅ− Î³þÎ Ñ− ²þÅ ¾©Å® ÎÇ hm 14 ðÇË fÉ { Ä©ÍÎÉy Éy É { Ék|þ©ˆÅ−ÓÎg Õ 15 mh £Ë−þÎ− þ ÍÇ²Â−Î ®−Í²Ë©ÉÎÇË hm 16 ©ð ffÄ©ÍÎ−Åð® { ÍÇ²Â−ÎfÉk|þ©ˆðþÀ©ÄÔj Ék|þ©ˆÍÇ²ÂÎÔÉ−j Ék|þ©ˆÉËÇÎÇ²ÇÄgg z ng 17 ²ÇÅÎ©ÅÏ−y mh ·Ç²Â−Î ðþ©Ä−®j ÎËÔ Î³− Å−ÓÎ hm 18 19 mh ¥Ä©À©ÅþÎ−Í ‘¡®®Ë−ÍÍ þÄË−þ®Ô ©Å ÏÍ−‘ −ËËÇË ðËÇÀ ¾©Å® hm 20 ·−ÎÍÇ²ÂÇÉÎfÄ©ÍÎ−Åð®j ·ﬂ‹ˆ·ﬂ£«¥¶j ·ﬂˆ‡¥•·¥¡⁄⁄‡j 21 f²ÇÅÍÎ ÌÇ©® hgdÇÉÎÌþÄ j Í©Ö−Çðf©ÅÎggy 22 23 mh ¢©Å® Î³− ®−Í²Ë©ÉÎÇË ÎÇ Î³− þ®®Ë−ÍÍ hm 24 ©ð f¾©Å®fÄ©ÍÎ−Åð®j Ék|þ©ˆþ®®Ëj Ék|þ©ˆþ®®ËÄ−Åg {{ ng 25 ¾Ë−þÂy mh ·Ï²²−ÍÍ hm 26 £ÄÇÍ−fÄ©ÍÎ−Åð®gy mh ¢©Å® ðþ©Ä−®j ÎËÔ Î³− Å−ÓÎ hm 27 Û 28 29 mh £Ä−þÅ ÏÉ hm 30 ƒË−−þ®®Ë©ÅðÇfÄ©ÍÎÉgy 31 ©ð f_Ég mh ﬁÇ þ®®Ë−ÍÍ ÑÇËÂ−® hm 32 Ë−ÎÏËÅ koy 33 34 mh ›þÂ− ©Î þ Ä©ÍÎ−Å©Å× ÍÇ²Â−Î Ë−þ®Ô ÎÇ þ²²−ÉÎ ²ÇÅÅ−²Î©ÇÅ Ë−ÊÏ−ÍÎÍ hm 35 ©ð fÄ©ÍÎ−ÅfÄ©ÍÎ−Åð®j ‹'·¶¥ﬁ†g z ng Õ 36 £ÄÇÍ−fÄ©ÍÎ−Åð®gy 37 Ë−ÎÏËÅ koy 38 Û 39 Ë−ÎÏËÅ Ä©ÍÎ−Åð®y 40 Û code/src/csapp.c Figure 11.19 ÇÉ−ÅˆÄ©ÍÎ−Åð®: Helper function that opens and returns a listening descriptor. It is reentrant and protocol-independent. 982 Chapter 11 Network Programming code/netp/echoclient.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ©ÅÎ ²Ä©−ÅÎð®y 6 ²³þË h³ÇÍÎj hÉÇËÎj ¾Ïð‰›¡”‹'ﬁ¥`y 7 Ë©ÇˆÎ Ë©Çy 8 9 ©ð fþË×² _{ qg Õ 10 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ z³ÇÍÎ| zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 11 −Ó©Îfngy 12 Û 13 ³ÇÍÎ { þË×Ì‰o`y 14 ÉÇËÎ { þË×Ì‰p`y 15 16 ²Ä©−ÅÎð® { ﬂÉ−Åˆ²Ä©−ÅÎð®f³ÇÍÎj ÉÇËÎgy 17 ‡©ÇˆË−þ®©Å©Î¾fdË©Çj ²Ä©−ÅÎð®gy 18 19 Ñ³©Ä− fƒ×−ÎÍf¾Ïðj ›¡”‹'ﬁ¥j ÍÎ®©Åg _{ ﬁ•‹‹g Õ 20 ‡©ÇˆÑË©Î−Åf²Ä©−ÅÎð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 21 ‡©ÇˆË−þ®Ä©Å−¾fdË©Çj ¾Ïðj ›¡”‹'ﬁ¥gy 22 ƒÉÏÎÍf¾Ïðj ÍÎ®ÇÏÎgy 23 Û 24 £ÄÇÍ−f²Ä©−ÅÎð®gy 25 −Ó©Îfngy 26 Û code/netp/echoclient.c Figure 11.20 Echo client main routine. the main routine closes the connected descriptor. Once the client and server have closed their respective descriptors, the connection is terminated. The ²Ä©−ÅÎþ®®Ë variable in line 9 is a socket address structure that is passed to þ²²−ÉÎ. Before þ²²−ÉÎ returns, it ﬁlls in ²Ä©−ÅÎþ®®Ë with the socket address of the client on the other end of the connection. Notice how we declare ²Ä©−ÅÎþ®®Ë as type ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− rather than ÍÎËÏ²Î ÍÇ²Âþ®®Ëˆ©Å. By deﬁni- tion, the ÍÇ²Âþ®®ËˆÍÎÇËþ×− structure is large enough to hold any type of socket address, which keeps the code protocol-independent. Notice that our simple echo server can only handle one client at a time. A server of this type that iterates through clients, one at a time, is called an iterative server. In Chapter 12, we will learn how to build more sophisticated concurrent servers that can handle multiple clients simultaneously. Finally, Figure 11.22 shows the code for the −²³Ç routine, which repeatedly reads and writes lines of text until the Ë©ÇˆË−þ®Ä©Å−¾ function encounters EOF in line 10. Section 11.4 The Sockets Interface 983 code/netp/echoserveri.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® −²³Çf©ÅÎ ²ÇÅÅð®gy 4 5 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 6 Õ 7 ©ÅÎ Ä©ÍÎ−Åð®j ²ÇÅÅð®y 8 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 9 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy mh ¥ÅÇÏ×³ ÍÉþ²− ðÇË þÅÔ þ®®Ë−ÍÍ hm 10 ²³þË ²Ä©−ÅÎˆ³ÇÍÎÅþÀ−‰›¡”‹'ﬁ¥`j ²Ä©−ÅÎˆÉÇËÎ‰›¡”‹'ﬁ¥`y 11 12 ©ð fþË×² _{ pg Õ 13 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 14 −Ó©Îfngy 15 Û 16 17 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 18 Ñ³©Ä− fog Õ 19 ²Ä©−ÅÎÄ−Å { Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×−gy 20 ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hgd²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 21 §−ÎÅþÀ−©ÅðÇff·¡ hg d²Ä©−ÅÎþ®®Ëj ²Ä©−ÅÎÄ−Åj ²Ä©−ÅÎˆ³ÇÍÎÅþÀ−j ›¡”‹'ﬁ¥j 22 ²Ä©−ÅÎˆÉÇËÎj ›¡”‹'ﬁ¥j ngy 23 ÉË©ÅÎðf‘£ÇÅÅ−²Î−® ÎÇ fcÍj cÍg¿Å‘j ²Ä©−ÅÎˆ³ÇÍÎÅþÀ−j ²Ä©−ÅÎˆÉÇËÎgy 24 −²³Çf²ÇÅÅð®gy 25 £ÄÇÍ−f²ÇÅÅð®gy 26 Û 27 −Ó©Îfngy 28 Û code/netp/echoserveri.c Figure 11.21 Iterative echo server main routine. code/netp/echo.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® −²³Çf©ÅÎ ²ÇÅÅð®g 4 Õ 5 Í©Ö−ˆÎ Åy 6 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 7 Ë©ÇˆÎ Ë©Çy 8 9 ‡©ÇˆË−þ®©Å©Î¾fdË©Çj ²ÇÅÅð®gy 10 Ñ³©Ä−ffÅ { ‡©ÇˆË−þ®Ä©Å−¾fdË©Çj ¾Ïðj ›¡”‹'ﬁ¥gg _{ ng Õ 11 ÉË©ÅÎðf‘Í−ËÌ−Ë Ë−²−©Ì−® c® ¾ÔÎ−Í¿Å‘j f©ÅÎgÅgy 12 ‡©ÇˆÑË©Î−Åf²ÇÅÅð®j ¾Ïðj Ågy 13 Û 14 Û code/netp/echo.c Figure 11.22 −²³Ç function that reads and echoes text lines. 984 Chapter 11 Network Programming Aside What does EOF on a connection mean? The idea of EOF is often confusing to students, especially in the context of Internet connections. First, we need to understand that there is no such thing as an EOF character. Rather, EOF is a condition that is detected by the kernel. An application ﬁnds out about the EOF condition when it receives a zero return code from the Ë−þ® function. For disk ﬁles, EOF occurs when the current ﬁle position exceeds the ﬁle length. For Internet connections, EOF occurs when a process closes its end of the connection. The process at the other end of the connection detects the EOF when it attempts to read past the last byte in the stream. 11.5 Web Servers So far we have discussed network programming in the context of a simple echo server. In this section, we will show you how to use the basic ideas of network programming to build your own small, but quite functional, Web server. 11.5.1 Web Basics Web clients and servers interact using a text-based application-level protocol known as HTTP (hypertext transfer protocol). HTTP is a simple protocol. A Web client (known as a browser) opens an Internet connection to a server and requests some content. The server responds with the requested content and then closes the connection. The browser reads the content and displays it on the screen. What distinguishes Web services from conventional ﬁle retrieval services such as FTP? The main difference is that Web content can be written in a language known as HTML (hypertext markup language). An HTML program (page) con- tains instructions (tags) that tell the browser how to display various text and graphical objects in the page. For example, the code z¾| ›þÂ− À− ¾ÇÄ®_ zm¾| tells the browser to print the text between the z¾| and zm¾| tags in boldface type. However, the real power of HTML is that a page can contain pointers (hyperlinks) to content stored on any Internet host. For example, an HTML line of the form zþ ³Ë−ð{‘³ÎÎÉxmmÑÑÑl²ÀÏl−®Ïm©Å®−Ól³ÎÀÄ‘|£þËÅ−×©− ›−ÄÄÇÅzmþ| tells the browser to highlight the text object £þËÅ−×©− ›−ÄÄÇÅ and to create a hyperlink to an HTML ﬁle called ©Å®−Ól³ÎÀÄ that is stored on the CMU Web server. If the user clicks on the highlighted text object, the browser requests the corresponding HTML ﬁle from the CMU server and displays it. Section 11.5 Web Servers 985 Aside Origins of the World Wide Web The World Wide Web was invented by Tim Berners-Lee, a software engineer working at CERN, a Swiss physics lab. In 1989, Berners-Lee wrote an internal memo proposing a distributed hypertext system that would connect a “web of notes with links.” The intent of the proposed system was to help CERN scientists share and manage information. Over the next two years, after Berners-Lee implemented the ﬁrst Web server and Web browser, the Web developed a small following within CERN and a few other sites. A pivotal event occurred in 1993, when Marc Andreesen (who later founded Netscape and Andreessen Horowitz) and his colleagues at NCSA released a graphical browser called mosaic for all three major platforms: Linux, Windows, and Macintosh. After the release of mosaic, interest in the Web exploded, with the number of Web sites increasing at an exponential rate. By 2015, there were over 975,000,000 sites worldwide. (Source: Netcraft Web Survey) MIME type Description Î−ÓÎm³ÎÀÄ HTML page Î−ÓÎmÉÄþ©Å Unformatted text þÉÉÄ©²þÎ©ÇÅmÉÇÍÎÍ²Ë©ÉÎ Postscript document ©Àþ×−m×©ð Binary image encoded in GIF format ©Àþ×−mÉÅ× Binary image encoded in PNG format ©Àþ×−mÁÉ−× Binary image encoded in JPEG format Figure 11.23 Example MIME types. 11.5.2 Web Content To Web clients and servers, content is a sequence of bytes with an associated MIME (multipurpose internet mail extensions) type. Figure 11.23 shows some common MIME types. Web servers provide content to clients in two different ways: . Fetch a disk ﬁle and return its contents to the client. The disk ﬁle is known as static content and the process of returning the ﬁle to the client is known as serving static content. . Run an executable ﬁle and return its output to the client. The output produced by the executable at run time is known as dynamic content, and the process of running the program and returning its output to the client is known as serving dynamic content. Every piece of content returned by a Web server is associated with some ﬁle that it manages. Each of these ﬁles has a unique name known as a URL (universal resource locator). For example, the URL ³ÎÎÉxmmÑÑÑl×ÇÇ×Ä−l²ÇÀxvnm©Å®−Ól³ÎÀÄ 986 Chapter 11 Network Programming identiﬁes an HTML ﬁle called m©Å®−Ól³ÎÀÄ on Internet host ÑÑÑl×ÇÇ×Ä−l²ÇÀ that is managed by a Web server listening on port 80. The port number is op- tional and defaults to the well-known HTTP port 80. URLs for executable ﬁles can include program arguments after the ﬁlename. A ‘}’ character separates the ﬁlename from the arguments, and each argument is separated by an ‘d’ character. For example, the URL ³ÎÎÉxmm¾ÄÏ−ð©Í³l©²Íl²Íl²ÀÏl−®Ïxvnnnm²×©k¾©Åmþ®®−Ë}osnnndpoq identiﬁes an executable called m²×©k¾©Åmþ®®−Ë that will be called with two argu- ment strings: osnnn and poq. Clients and servers use different parts of the URL during a transaction. For instance, a client uses the preﬁx ³ÎÎÉxmmÑÑÑl×ÇÇ×Ä−l²ÇÀxvn to determine what kind of server to contact, where the server is, and what port it is listening on. The server uses the sufﬁx m©Å®−Ól³ÎÀÄ to ﬁnd the ﬁle on its ﬁlesystem and to determine whether the request is for static or dynamic content. There are several points to understand about how servers interpret the sufﬁx of a URL: . There are no standard rules for determining whether a URL refers to static or dynamic content. Each server has its own rules for the ﬁles it manages. A classic (old-fashioned) approach is to identify a set of directories, such as ²×©k¾©Å, where all executables must reside. . The initial ‘m’ in the sufﬁx does not denote the Linux root directory. Rather, it denotes the home directory for whatever kind of content is being requested. For example, a server might be conﬁgured so that all static content is stored in directory mÏÍËm³ÎÎÉ®m³ÎÀÄ and all dynamic content is stored in directory mÏÍËm³ÎÎÉ®m²×©k¾©Å. . The minimal URL sufﬁx is the ‘m’ character, which all servers expand to some default home page such as m©Å®−Ól³ÎÀÄ. This explains why it is possible to fetch the home page of a site by simply typing a domain name to the browser. The browser appends the missing ‘m’ to the URL and passes it to the server, which expands the ‘m’ to some default ﬁlename. 11.5.3 HTTP Transactions Since HTTP is based on text lines transmitted over Internet connections, we can use the Linux telnet program to conduct transactions with any Web server on the Internet. The telnet program has been largely supplanted by ssh as a remote login tool, but it is very handy for debugging servers that talk to clients with text lines over connections. For example, Figure 11.24 uses telnet to request the home page from the AOL Web server. Section 11.5 Web Servers 987 1 Ä©ÅÏÓ| telnet www.aol.com 80 Client: open connection to server 2 ¶ËÔ©Å× pnslovvlortlpqlll Telnet prints 3 lines to the terminal 3 £ÇÅÅ−²Î−® ÎÇ þÇÄl²ÇÀl 4 ¥Í²þÉ− ²³þËþ²Î−Ë ©Í ’´`’l 5 GET / HTTP/1.1 Client: request line 6 Host: www.aol.com Client: required HTTP/1.1 header 7 Client: empty line terminates headers 8 ¤¶¶–moln pnn ﬂ« Server: response line 9 ›'›¥k‚−ËÍ©ÇÅx oln Server: followed by five response headers 10 ⁄þÎ−x ›ÇÅj v “þÅ pnon rxswxrp §›¶ 11 ·−ËÌ−Ëx ¡Éþ²³−k£ÇÔÇÎ−molo 12 £ÇÅÎ−ÅÎk¶ÔÉ−x Î−ÓÎm³ÎÀÄ Server: expect HTML in the response body 13 £ÇÅÎ−ÅÎk‹−Å×Î³x rpnwp Server: expect 42,092 bytes in the response body 14 Server: empty line terminates response headers 15 z³ÎÀÄ| Server: first HTML line in response body 16 lll Server: 766 lines of HTML not shown 17 zm³ÎÀÄ| Server: last HTML line in response body 18 £ÇÅÅ−²Î©ÇÅ ²ÄÇÍ−® ¾Ô ðÇË−©×Å ³ÇÍÎl Server: closes connection 19 Ä©ÅÏÓ| Client: closes connection and terminates Figure 11.24 Example of an HTTP transaction that serves static content. In line 1, we run telnet from a Linux shell and ask it to open a connection to the AOL Web server. Telnet prints three lines of output to the terminal, opens the connection, and then waits for us to enter text (line 5). Each time we enter a text line and hit the −ÅÎ−Ë key, telnet reads the line, appends carriage return and line feed characters (‘¿Ë¿Å’ in C notation), and sends the line to the server. This is consistent with the HTTP standard, which requires every text line to be terminated by a carriage return and line feed pair. To initiate the transaction, we enter an HTTP request (lines 5–7). The server replies with an HTTP response (lines 8–17) and then closes the connection (line 18). HTTP Requests An HTTP request consists of a request line (line 5), followed by zero or more request headers (line 6), followed by an empty text line that terminates the list of headers (line 7). A request line has the form method URI version HTTP supports a number of different methods, including GET, POST, OPTIONS, HEAD, PUT, DELETE, and TRACE. We will only discuss the workhorse GET method, which accounts for a majority of HTTP requests. The GET method instructs the server to generate and return the content identiﬁed by the URI 988 Chapter 11 Network Programming (uniform resource identiﬁer). The URI is the sufﬁx of the corresponding URL that includes the ﬁlename and optional arguments.3 The version ﬁeld in the request line indicates the HTTP version to which the request conforms. The most recent HTTP version is HTTP/1.1 [37]. HTTP/1.0 is an earlier, much simpler version from 1996 [6]. HTTP/1.1 deﬁnes additional headers that provide support for advanced features such as caching and security, as well as a mechanism that allows a client and server to perform multiple transactions over the same persistent connection. In practice, the two versions are compatible because HTTP/1.0 clients and servers simply ignore unknown HTTP/1.1 headers. To summarize, the request line in line 5 asks the server to fetch and return the HTML ﬁle m©Å®−Ól³ÎÀÄ. It also informs the server that the remainder of the request will be in HTTP/1.1 format. Request headers provide additional information to the server, such as the brand name of the browser or the MIME types that the browser understands. Request headers have the form header-namex header-data For our purposes, the only header to be concerned with is the ¤ÇÍÎ header (line 6), which is required in HTTP/1.1 requests, but not in HTTP/1.0 requests. The ¤ÇÍÎ header is used by proxy caches, which sometimes serve as intermediaries between a browser and the origin server that manages the requested ﬁle. Multiple proxies can exist between a client and an origin server in a so-called proxy chain. The data in the ¤ÇÍÎ header, which identiﬁes the domain name of the origin server, allow a proxy in the middle of a proxy chain to determine if it might have a locally cached copy of the requested content. Continuing with our example in Figure 11.24, the empty text line in line 7 (generated by hitting −ÅÎ−Ë on our keyboard) terminates the headers and instructs the server to send the requested HTML ﬁle. HTTP Responses HTTP responses are similar to HTTP requests. An HTTP response consists of a response line (line 8), followed by zero or more response headers (lines 9–13), followed by an empty line that terminates the headers (line 14), followed by the response body (lines 15–17). A response line has the form version status-code status-message The version ﬁeld describes the HTTP version that the response conforms to. The status-code is a three-digit positive integer that indicates the disposition of the request. The status-message gives the English equivalent of the error code. Figure 11.25 lists some common status codes and their corresponding messages. 3. Actually, this is only true when a browser requests content. If a proxy server requests content, then the URI must be the complete URL. Section 11.5 Web Servers 989 Aside Passing arguments in HTTP POST requests Arguments for HTTP POST requests are passed in the request body rather than in the URI. Status code Status message Description 200 OK Request was handled without error. 301 Moved permanently Content has moved to the hostname in the Location header. 400 Bad request Request could not be understood by the server. 403 Forbidden Server lacks permission to access the requested ﬁle. 404 Not found Server could not ﬁnd the requested ﬁle. 501 Not implemented Server does not support the request method. 505 HTTP version not supported Server does not support version in request. Figure 11.25 Some HTTP status codes. The response headers in lines 9–13 provide additional information about the response. For our purposes, the two most important headers are £ÇÅÎ−ÅÎk¶ÔÉ− (line 12), which tells the client the MIME type of the content in the response body, and £ÇÅÎ−ÅÎk‹−Å×Î³ (line 13), which indicates its size in bytes. The empty text line in line 14 that terminates the response headers is followed by the response body, which contains the requested content. 11.5.4 Serving Dynamic Content If we stop to think for a moment how a server might provide dynamic content to a client, certain questions arise. For example, how does the client pass any program arguments to the server? How does the server pass these arguments to the child process that it creates? How does the server pass other information to the child that it might need to generate the content? Where does the child send its output? These questions are addressed by a de facto standard called CGI (common gateway interface). How Does the Client Pass Program Arguments to the Server? Arguments for GET requests are passed in the URI. As we have seen, a ‘}’ char- acter separates the ﬁlename from the arguments, and each argument is separated by an ‘d’ character. Spaces are not allowed in arguments and must be represented with the cpn string. Similar encodings exist for other special characters. How Does the Server Pass Arguments to the Child? After a server receives a request such as §¥¶ m²×©k¾©Åmþ®®−Ë}osnnndpoq ¤¶¶–molo 990 Chapter 11 Network Programming Environment variable Description QUERY_STRING Program arguments SERVER_PORT Port that the parent is listening on REQUEST_METHOD GET or POST REMOTE_HOST Domain name of client REMOTE_ADDR Dotted-decimal IP address of client CONTENT_TYPE POST only: MIME type of the request body CONTENT_LENGTH POST only: Size in bytes of the request body Figure 11.26 Examples of CGI environment variables. it calls ðÇËÂ to create a child process and calls −Ó−²Ì− to run the m²×©k¾©Åmþ®®−Ë program in the context of the child. Programs like the þ®®−Ë program are often referred to as CGI programs because they obey the rules of the CGI standard. Before the call to −Ó−²Ì−, the child process sets the CGI environment variable QUERY_STRING to osnnndpoq, which the þ®®−Ë program can reference at run time using the Linux ×−Î−ÅÌ function. How Does the Server Pass Other Information to the Child? CGI deﬁnes a number of other environment variables that a CGI program can expect to be set when it runs. Figure 11.26 shows a subset. Where Does the Child Send Its Output? A CGI program sends its dynamic content to the standard output. Before the child process loads and runs the CGI program, it uses the Linux ®ÏÉp function to redirect standard output to the connected descriptor that is associated with the client. Thus, anything that the CGI program writes to standard output goes directly to the client. Notice that since the parent does not know the type or size of the content that the child generates, the child is responsible for generating the £ÇÅÎ−ÅÎkÎÔÉ− and £ÇÅÎ−ÅÎkÄ−Å×Î³ response headers, as well as the empty line that terminates the headers. Figure 11.27 shows a simple CGI program that sums its two arguments and returns an HTML ﬁle with the result to the client. Figure 11.28 shows an HTTP transaction that serves dynamic content from the þ®®−Ë program. Practice Problem 11.5 (solution page 1005) Assume that a CGI program needs to send dynamic content to the client. This is typically done by making the CGI program send its content to the standard output. Explain how this content is sent to the client. Section 11.5 Web Servers 991 Aside Passing arguments in HTTP POST requests to CGI programs For POST requests, the child would also need to redirect standard input to the connected descriptor. The CGI program would then read the arguments in the request body from standard input. code/netp/tiny/cgi-bin/adder.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©ÅfÌÇ©®g Õ 4 ²³þË h¾Ïðj hÉy 5 ²³þË þË×o‰›¡”‹'ﬁ¥`j þË×p‰›¡”‹'ﬁ¥`j ²ÇÅÎ−ÅÎ‰›¡”‹'ﬁ¥`y 6 ©ÅÎ Åo{nj Åp{ny 7 8 mh ¥ÓÎËþ²Î Î³− ÎÑÇ þË×ÏÀ−ÅÎÍ hm 9 ©ð ff¾Ïð { ×−Î−ÅÌf‘†•¥‡»ˆ·¶‡'ﬁ§‘gg _{ ﬁ•‹‹g Õ 10 É { ÍÎË²³Ëf¾Ïðj ’d’gy 11 hÉ { ’¿n’y 12 ÍÎË²ÉÔfþË×oj ¾Ïðgy 13 ÍÎË²ÉÔfþË×pj Éiogy 14 Åo { þÎÇ©fþË×ogy 15 Åp { þÎÇ©fþË×pgy 16 Û 17 18 mh ›þÂ− Î³− Ë−ÍÉÇÅÍ− ¾Ç®Ô hm 19 ÍÉË©ÅÎðf²ÇÅÎ−ÅÎj ‘†•¥‡»ˆ·¶‡'ﬁ§{cÍ‘j ¾Ïðgy 20 ÍÉË©ÅÎðf²ÇÅÎ−ÅÎj ‘„−Ä²ÇÀ− ÎÇ þ®®l²ÇÀx ‘gy 21 ÍÉË©ÅÎðf²ÇÅÎ−ÅÎj ‘cÍ¶¤¥ 'ÅÎ−ËÅ−Î þ®®©Î©ÇÅ ÉÇËÎþÄl¿Ë¿ÅzÉ|‘j ²ÇÅÎ−ÅÎgy 22 ÍÉË©ÅÎðf²ÇÅÎ−ÅÎj ‘cÍ¶³− þÅÍÑ−Ë ©Íx c® i c® { c®¿Ë¿ÅzÉ|‘j 23 ²ÇÅÎ−ÅÎj Åoj Åpj Åo i Åpgy 24 ÍÉË©ÅÎðf²ÇÅÎ−ÅÎj ‘cÍ¶³þÅÂÍ ðÇË Ì©Í©Î©Å×_¿Ë¿Å‘j ²ÇÅÎ−ÅÎgy 25 26 mh §−Å−ËþÎ− Î³− ¤¶¶– Ë−ÍÉÇÅÍ− hm 27 ÉË©ÅÎðf‘£ÇÅÅ−²Î©ÇÅx ²ÄÇÍ−¿Ë¿Å‘gy 28 ÉË©ÅÎðf‘£ÇÅÎ−ÅÎkÄ−Å×Î³x c®¿Ë¿Å‘j f©ÅÎgÍÎËÄ−Åf²ÇÅÎ−ÅÎggy 29 ÉË©ÅÎðf‘£ÇÅÎ−ÅÎkÎÔÉ−x Î−ÓÎm³ÎÀÄ¿Ë¿Å¿Ë¿Å‘gy 30 ÉË©ÅÎðf‘cÍ‘j ²ÇÅÎ−ÅÎgy 31 ððÄÏÍ³fÍÎ®ÇÏÎgy 32 33 −Ó©Îfngy 34 Û code/netp/tiny/cgi-bin/adder.c Figure 11.27 CGI program that sums two integers. 992 Chapter 11 Network Programming 1 Ä©ÅÏÓ| telnet kittyhawk.cmcl.cs.cmu.edu 8000 Client: open connection 2 ¶ËÔ©Å× opvlplowrlprplll 3 £ÇÅÅ−²Î−® ÎÇ Â©ÎÎÔ³þÑÂl²À²Äl²Íl²ÀÏl−®Ïl 4 ¥Í²þÉ− ²³þËþ²Î−Ë ©Í ’´`’l 5 GET /cgi-bin/adder?15000&213 HTTP/1.0 Client: request line 6 Client: empty line terminates headers 7 ¤¶¶–moln pnn ﬂ« Server: response line 8 ·−ËÌ−Ëx ¶©ÅÔ „−¾ ·−ËÌ−Ë Server: identify server 9 £ÇÅÎ−ÅÎkÄ−Å×Î³x oos Adder: expect 115 bytes in response body 10 £ÇÅÎ−ÅÎkÎÔÉ−x Î−ÓÎm³ÎÀÄ Adder: expect HTML in response body 11 Adder: empty line terminates headers 12 „−Ä²ÇÀ− ÎÇ þ®®l²ÇÀx ¶¤¥ 'ÅÎ−ËÅ−Î þ®®©Î©ÇÅ ÉÇËÎþÄl Adder: first HTML line 13 zÉ|¶³− þÅÍÑ−Ë ©Íx osnnn i poq { ospoq Adder: second HTML line in response body 14 zÉ|¶³þÅÂÍ ðÇË Ì©Í©Î©Å×_ Adder: third HTML line in response body 15 £ÇÅÅ−²Î©ÇÅ ²ÄÇÍ−® ¾Ô ðÇË−©×Å ³ÇÍÎl Server: closes connection 16 Ä©ÅÏÓ| Client: closes connection and terminates Figure 11.28 An HTTP transaction that serves dynamic HTML content. 11.6 Putting It Together: The Tiny Web Server We conclude our discussion of network programming by developing a small but functioning Web server called Tiny. Tiny is an interesting program. It combines many of the ideas that we have learned about, such as process control, Unix I/O, the sockets interface, and HTTP, in only 250 lines of code. While it lacks the functionality, robustness, and security of a real server, it is powerful enough to serve both static and dynamic content to real Web browsers. We encourage you to study it and implement it yourself. It is quite exciting (even for the authors!) to point a real browser at your own server and watch it display a complicated Web page with text and graphics. The Tiny Àþ©Å Routine Figure 11.29 shows Tiny’s main routine. Tiny is an iterative server that listens for connection requests on the port that is passed in the command line. After opening a listening socket by calling the ÇÉ−ÅˆÄ©ÍÎ−Åð® function, Tiny executes the typical inﬁnite server loop, repeatedly accepting a connection request (line 32), performing a transaction (line 36), and closing its end of the connection (line 37). The ®Ç©Î Function The ®Ç©Î function in Figure 11.30 handles one HTTP transaction. First, we read and parse the request line (lines 11–14). Notice that we are using the Ë©Çˆ Ë−þ®Ä©Å−¾ function from Figure 10.8 to read the request line. Tiny supports only the GET method. If the client requests another method (such as POST), we send it an error message and return to the main routine Section 11.6 Putting It Together: The Tiny Web Server 993 code/netp/tiny/tiny.c 1 mh 2 h Î©ÅÔl² k ¡ Í©ÀÉÄ−j ©Î−ËþÎ©Ì− ¤¶¶–moln „−¾ Í−ËÌ−Ë Î³þÎ ÏÍ−Í Î³− 3 h §¥¶ À−Î³Ç® ÎÇ Í−ËÌ− ÍÎþÎ©² þÅ® ®ÔÅþÀ©² ²ÇÅÎ−ÅÎ 4 hm 5 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 6 7 ÌÇ©® ®Ç©Îf©ÅÎ ð®gy 8 ÌÇ©® Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍfË©ÇˆÎ hËÉgy 9 ©ÅÎ ÉþËÍ−ˆÏË©f²³þË hÏË©j ²³þË hð©Ä−ÅþÀ−j ²³þË h²×©þË×Így 10 ÌÇ©® Í−ËÌ−ˆÍÎþÎ©²f©ÅÎ ð®j ²³þË hð©Ä−ÅþÀ−j ©ÅÎ ð©Ä−Í©Ö−gy 11 ÌÇ©® ×−Îˆð©Ä−ÎÔÉ−f²³þË hð©Ä−ÅþÀ−j ²³þË hð©Ä−ÎÔÉ−gy 12 ÌÇ©® Í−ËÌ−ˆ®ÔÅþÀ©²f©ÅÎ ð®j ²³þË hð©Ä−ÅþÀ−j ²³þË h²×©þË×Így 13 ÌÇ©® ²Ä©−ÅÎ−ËËÇËf©ÅÎ ð®j ²³þË h²þÏÍ−j ²³þË h−ËËÅÏÀj 14 ²³þË hÍ³ÇËÎÀÍ×j ²³þË hÄÇÅ×ÀÍ×gy 15 16 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 17 Õ 18 ©ÅÎ Ä©ÍÎ−Åð®j ²ÇÅÅð®y 19 ²³þË ³ÇÍÎÅþÀ−‰›¡”‹'ﬁ¥`j ÉÇËÎ‰›¡”‹'ﬁ¥`y 20 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 21 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy 22 23 mh £³−²Â ²ÇÀÀþÅ®kÄ©Å− þË×Í hm 24 ©ð fþË×² _{ pg Õ 25 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 26 −Ó©Îfogy 27 Û 28 29 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 30 Ñ³©Ä− fog Õ 31 ²Ä©−ÅÎÄ−Å { Í©Ö−Çðf²Ä©−ÅÎþ®®Ëgy 32 ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hgd²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 33 §−ÎÅþÀ−©ÅðÇff·¡ hg d²Ä©−ÅÎþ®®Ëj ²Ä©−ÅÎÄ−Åj ³ÇÍÎÅþÀ−j ›¡”‹'ﬁ¥j 34 ÉÇËÎj ›¡”‹'ﬁ¥j ngy 35 ÉË©ÅÎðf‘¡²²−ÉÎ−® ²ÇÅÅ−²Î©ÇÅ ðËÇÀ fcÍj cÍg¿Å‘j ³ÇÍÎÅþÀ−j ÉÇËÎgy 36 ®Ç©Îf²ÇÅÅð®gy 37 £ÄÇÍ−f²ÇÅÅð®gy 38 Û 39 Û code/netp/tiny/tiny.c Figure 11.29 The Tiny Web server. 994 Chapter 11 Network Programming code/netp/tiny/tiny.c 1 ÌÇ©® ®Ç©Îf©ÅÎ ð®g 2 Õ 3 ©ÅÎ ©ÍˆÍÎþÎ©²y 4 ÍÎËÏ²Î ÍÎþÎ Í¾Ïðy 5 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`j À−Î³Ç®‰›¡”‹'ﬁ¥`j ÏË©‰›¡”‹'ﬁ¥`j Ì−ËÍ©ÇÅ‰›¡”‹'ﬁ¥`y 6 ²³þË ð©Ä−ÅþÀ−‰›¡”‹'ﬁ¥`j ²×©þË×Í‰›¡”‹'ﬁ¥`y 7 Ë©ÇˆÎ Ë©Çy 8 9 mh ‡−þ® Ë−ÊÏ−ÍÎ Ä©Å− þÅ® ³−þ®−ËÍ hm 10 ‡©ÇˆË−þ®©Å©Î¾fdË©Çj ð®gy 11 ‡©ÇˆË−þ®Ä©Å−¾fdË©Çj ¾Ïðj ›¡”‹'ﬁ¥gy 12 ÉË©ÅÎðf‘‡−ÊÏ−ÍÎ ³−þ®−ËÍx¿Å‘gy 13 ÉË©ÅÎðf‘cÍ‘j ¾Ïðgy 14 ÍÍ²þÅðf¾Ïðj ‘cÍ cÍ cÍ‘j À−Î³Ç®j ÏË©j Ì−ËÍ©ÇÅgy 15 ©ð fÍÎË²þÍ−²ÀÉfÀ−Î³Ç®j ‘§¥¶‘gg Õ 16 ²Ä©−ÅÎ−ËËÇËfð®j À−Î³Ç®j ‘sno‘j ‘ﬁÇÎ ©ÀÉÄ−À−ÅÎ−®‘j 17 ‘¶©ÅÔ ®Ç−Í ÅÇÎ ©ÀÉÄ−À−ÅÎ Î³©Í À−Î³Ç®‘gy 18 Ë−ÎÏËÅy 19 Û 20 Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍfdË©Çgy 21 22 mh –þËÍ− •‡' ðËÇÀ §¥¶ Ë−ÊÏ−ÍÎ hm 23 ©ÍˆÍÎþÎ©² { ÉþËÍ−ˆÏË©fÏË©j ð©Ä−ÅþÀ−j ²×©þË×Így 24 ©ð fÍÎþÎfð©Ä−ÅþÀ−j dÍ¾Ïðg z ng Õ 25 ²Ä©−ÅÎ−ËËÇËfð®j ð©Ä−ÅþÀ−j ‘rnr‘j ‘ﬁÇÎ ðÇÏÅ®‘j 26 ‘¶©ÅÔ ²ÇÏÄ®Å’Î ð©Å® Î³©Í ð©Ä−‘gy 27 Ë−ÎÏËÅy 28 Û 29 30 ©ð f©ÍˆÍÎþÎ©²g Õ mh ·−ËÌ− ÍÎþÎ©² ²ÇÅÎ−ÅÎ hm 31 ©ð f_f·ˆ'·‡¥§fÍ¾ÏðlÍÎˆÀÇ®−gg ÚÚ _f·ˆ'‡•·‡ d Í¾ÏðlÍÎˆÀÇ®−gg Õ 32 ²Ä©−ÅÎ−ËËÇËfð®j ð©Ä−ÅþÀ−j ‘rnq‘j ‘ƒÇË¾©®®−Å‘j 33 ‘¶©ÅÔ ²ÇÏÄ®Å’Î Ë−þ® Î³− ð©Ä−‘gy 34 Ë−ÎÏËÅy 35 Û 36 Í−ËÌ−ˆÍÎþÎ©²fð®j ð©Ä−ÅþÀ−j Í¾ÏðlÍÎˆÍ©Ö−gy 37 Û 38 −ÄÍ− Õ mh ·−ËÌ− ®ÔÅþÀ©² ²ÇÅÎ−ÅÎ hm 39 ©ð f_f·ˆ'·‡¥§fÍ¾ÏðlÍÎˆÀÇ®−gg ÚÚ _f·ˆ'”•·‡ d Í¾ÏðlÍÎˆÀÇ®−gg Õ 40 ²Ä©−ÅÎ−ËËÇËfð®j ð©Ä−ÅþÀ−j ‘rnq‘j ‘ƒÇË¾©®®−Å‘j 41 ‘¶©ÅÔ ²ÇÏÄ®Å’Î ËÏÅ Î³− £§' ÉËÇ×ËþÀ‘gy 42 Ë−ÎÏËÅy 43 Û 44 Í−ËÌ−ˆ®ÔÅþÀ©²fð®j ð©Ä−ÅþÀ−j ²×©þË×Így 45 Û 46 Û code/netp/tiny/tiny.c Figure 11.30 Tiny ®Ç©Î handles one HTTP transaction. Section 11.6 Putting It Together: The Tiny Web Server 995 (lines 15–19), which then closes the connection and awaits the next connection request. Otherwise, we read and (as we shall see) ignore any request headers (line 20). Next, we parse the URI into a ﬁlename and a possibly empty CGI argument string, and we set a ﬂag that indicates whether the request is for static or dynamic content (line 23). If the ﬁle does not exist on disk, we immediately send an error message to the client and return. Finally, if the request is for static content, we verify that the ﬁle is a regular ﬁle and that we have read permission (line 31). If so, we serve the static content (line 36) to the client. Similarly, if the request is for dynamic content, we verify that the ﬁle is executable (line 39), and, if so, we go ahead and serve the dynamic content (line 44). The ²Ä©−ÅÎ−ËËÇË Function Tiny lacks many of the error-handling features of a real server. However, it does check for some obvious errors and reports them to the client. The ²Ä©−ÅÎ−ËËÇË function in Figure 11.31 sends an HTTP response to the client with the appropriate code/netp/tiny/tiny.c 1 ÌÇ©® ²Ä©−ÅÎ−ËËÇËf©ÅÎ ð®j ²³þË h²þÏÍ−j ²³þË h−ËËÅÏÀj 2 ²³þË hÍ³ÇËÎÀÍ×j ²³þË hÄÇÅ×ÀÍ×g 3 Õ 4 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`j ¾Ç®Ô‰›¡”¢•ƒ`y 5 6 mh ¢Ï©Ä® Î³− ¤¶¶– Ë−ÍÉÇÅÍ− ¾Ç®Ô hm 7 ÍÉË©ÅÎðf¾Ç®Ôj ‘z³ÎÀÄ|zÎ©ÎÄ−|¶©ÅÔ ¥ËËÇËzmÎ©ÎÄ−|‘gy 8 ÍÉË©ÅÎðf¾Ç®Ôj ‘cÍz¾Ç®Ô ¾×²ÇÄÇË{‘‘ðððððð‘‘|¿Ë¿Å‘j ¾Ç®Ôgy 9 ÍÉË©ÅÎðf¾Ç®Ôj ‘cÍcÍx cÍ¿Ë¿Å‘j ¾Ç®Ôj −ËËÅÏÀj Í³ÇËÎÀÍ×gy 10 ÍÉË©ÅÎðf¾Ç®Ôj ‘cÍzÉ|cÍx cÍ¿Ë¿Å‘j ¾Ç®Ôj ÄÇÅ×ÀÍ×j ²þÏÍ−gy 11 ÍÉË©ÅÎðf¾Ç®Ôj ‘cÍz³Ë|z−À|¶³− ¶©ÅÔ „−¾ Í−ËÌ−Ëzm−À|¿Ë¿Å‘j ¾Ç®Ôgy 12 13 mh –Ë©ÅÎ Î³− ¤¶¶– Ë−ÍÉÇÅÍ− hm 14 ÍÉË©ÅÎðf¾Ïðj ‘¤¶¶–moln cÍ cÍ¿Ë¿Å‘j −ËËÅÏÀj Í³ÇËÎÀÍ×gy 15 ‡©ÇˆÑË©Î−Åfð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 16 ÍÉË©ÅÎðf¾Ïðj ‘£ÇÅÎ−ÅÎkÎÔÉ−x Î−ÓÎm³ÎÀÄ¿Ë¿Å‘gy 17 ‡©ÇˆÑË©Î−Åfð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 18 ÍÉË©ÅÎðf¾Ïðj ‘£ÇÅÎ−ÅÎkÄ−Å×Î³x c®¿Ë¿Å¿Ë¿Å‘j f©ÅÎgÍÎËÄ−Åf¾Ç®Ôggy 19 ‡©ÇˆÑË©Î−Åfð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 20 ‡©ÇˆÑË©Î−Åfð®j ¾Ç®Ôj ÍÎËÄ−Åf¾Ç®Ôggy 21 Û code/netp/tiny/tiny.c Figure 11.31 Tiny ²Ä©−ÅÎ−ËËÇË sends an error message to the client. 996 Chapter 11 Network Programming code/netp/tiny/tiny.c 1 ÌÇ©® Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍfË©ÇˆÎ hËÉg 2 Õ 3 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 4 5 ‡©ÇˆË−þ®Ä©Å−¾fËÉj ¾Ïðj ›¡”‹'ﬁ¥gy 6 Ñ³©Ä−fÍÎË²ÀÉf¾Ïðj ‘¿Ë¿Å‘gg Õ 7 ‡©ÇˆË−þ®Ä©Å−¾fËÉj ¾Ïðj ›¡”‹'ﬁ¥gy 8 ÉË©ÅÎðf‘cÍ‘j ¾Ïðgy 9 Û 10 Ë−ÎÏËÅy 11 Û code/netp/tiny/tiny.c Figure 11.32 Tiny Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍ reads and ignores request headers. status code and status message in the response line, along with an HTML ﬁle in the response body that explains the error to the browser’s user. Recall that an HTML response should indicate the size and type of the content in the body. Thus, we have opted to build the HTML content as a single string so that we can easily determine its size. Also, notice that we are using the robust Ë©ÇˆÑË©Î−Å function from Figure 10.4 for all output. The Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍ Function Tiny does not use any of the information in the request headers. It simply reads and ignores them by calling the Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍ function in Figure 11.32. Notice that the empty text line that terminates the request headers consists of a carriage return and line feed pair, which we check for in line 6. The ÉþËÍ−ˆÏË© Function Tiny assumes that the home directory for static content is its current directory and that the home directory for executables is lm²×©k¾©Å. Any URI that contains the string ²×©k¾©Å is assumed to denote a request for dynamic content. The default ﬁlename is lm³ÇÀ−l³ÎÀÄ. The ÉþËÍ−ˆÏË© function in Figure 11.33 implements these policies. It parses the URI into a ﬁlename and an optional CGI argument string. If the request is for static content (line 5), we clear the CGI argument string (line 6) and then convert the URI into a relative Linux pathname such as lm©Å®−Ól³ÎÀÄ (lines 7–8). If the URI ends with a ‘m’ character (line 9), then we append the default ﬁlename (line 10). On the other hand, if the request is for dynamic content (line 13), we extract any CGI arguments (lines 14–20) and convert the remaining portion of the URI to a relative Linux ﬁlename (lines 21–22). Section 11.6 Putting It Together: The Tiny Web Server 997 code/netp/tiny/tiny.c 1 ©ÅÎ ÉþËÍ−ˆÏË©f²³þË hÏË©j ²³þË hð©Ä−ÅþÀ−j ²³þË h²×©þË×Íg 2 Õ 3 ²³þË hÉÎËy 4 5 ©ð f_ÍÎËÍÎËfÏË©j ‘²×©k¾©Å‘gg Õ mh ·ÎþÎ©² ²ÇÅÎ−ÅÎ hm 6 ÍÎË²ÉÔf²×©þË×Íj ‘‘gy 7 ÍÎË²ÉÔfð©Ä−ÅþÀ−j ‘l‘gy 8 ÍÎË²þÎfð©Ä−ÅþÀ−j ÏË©gy 9 ©ð fÏË©‰ÍÎËÄ−ÅfÏË©gko` {{ ’m’g 10 ÍÎË²þÎfð©Ä−ÅþÀ−j ‘³ÇÀ−l³ÎÀÄ‘gy 11 Ë−ÎÏËÅ oy 12 Û 13 −ÄÍ− Õ mh ⁄ÔÅþÀ©² ²ÇÅÎ−ÅÎ hm 14 ÉÎË { ©Å®−ÓfÏË©j ’}’gy 15 ©ð fÉÎËg Õ 16 ÍÎË²ÉÔf²×©þË×Íj ÉÎËiogy 17 hÉÎË { ’¿n’y 18 Û 19 −ÄÍ− 20 ÍÎË²ÉÔf²×©þË×Íj ‘‘gy 21 ÍÎË²ÉÔfð©Ä−ÅþÀ−j ‘l‘gy 22 ÍÎË²þÎfð©Ä−ÅþÀ−j ÏË©gy 23 Ë−ÎÏËÅ ny 24 Û 25 Û code/netp/tiny/tiny.c Figure 11.33 Tiny ÉþËÍ−ˆÏË© parses an HTTP URI. The Í−ËÌ−ˆÍÎþÎ©² Function Tiny serves ﬁve common types of static content: HTML ﬁles, unformatted text ﬁles, and images encoded in GIF, PNG, and JPEG formats. The Í−ËÌ−ˆÍÎþÎ©² function in Figure 11.34 sends an HTTP response whose body contains the contents of a local ﬁle. First, we determine the ﬁle type by inspecting the sufﬁx in the ﬁlename (line 7) and then send the response line and response headers to the client (lines 8–13). Notice that a blank line terminates the headers. Next, we send the response body by copying the contents of the requested ﬁle to the connected descriptor ð®. The code here is somewhat subtle and needs to be studied carefully. Line 18 opens ð©Ä−ÅþÀ− for reading and gets its descriptor. In line 19, the Linux ÀÀþÉ function maps the requested ﬁle to a virtual memory area. Recall from our discussion of ÀÀþÉ in Section 9.8 that the call to ÀÀþÉ maps the 998 Chapter 11 Network Programming code/netp/tiny/tiny.c 1 ÌÇ©® Í−ËÌ−ˆÍÎþÎ©²f©ÅÎ ð®j ²³þË hð©Ä−ÅþÀ−j ©ÅÎ ð©Ä−Í©Ö−g 2 Õ 3 ©ÅÎ ÍË²ð®y 4 ²³þË hÍË²Éj ð©Ä−ÎÔÉ−‰›¡”‹'ﬁ¥`j ¾Ïð‰›¡”¢•ƒ`y 5 6 mh ·−Å® Ë−ÍÉÇÅÍ− ³−þ®−ËÍ ÎÇ ²Ä©−ÅÎ hm 7 ×−Îˆð©Ä−ÎÔÉ−fð©Ä−ÅþÀ−j ð©Ä−ÎÔÉ−gy 8 ÍÉË©ÅÎðf¾Ïðj ‘¤¶¶–moln pnn ﬂ«¿Ë¿Å‘gy 9 ÍÉË©ÅÎðf¾Ïðj ‘cÍ·−ËÌ−Ëx ¶©ÅÔ „−¾ ·−ËÌ−Ë¿Ë¿Å‘j ¾Ïðgy 10 ÍÉË©ÅÎðf¾Ïðj ‘cÍ£ÇÅÅ−²Î©ÇÅx ²ÄÇÍ−¿Ë¿Å‘j ¾Ïðgy 11 ÍÉË©ÅÎðf¾Ïðj ‘cÍ£ÇÅÎ−ÅÎkÄ−Å×Î³x c®¿Ë¿Å‘j ¾Ïðj ð©Ä−Í©Ö−gy 12 ÍÉË©ÅÎðf¾Ïðj ‘cÍ£ÇÅÎ−ÅÎkÎÔÉ−x cÍ¿Ë¿Å¿Ë¿Å‘j ¾Ïðj ð©Ä−ÎÔÉ−gy 13 ‡©ÇˆÑË©Î−Åfð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 14 ÉË©ÅÎðf‘‡−ÍÉÇÅÍ− ³−þ®−ËÍx¿Å‘gy 15 ÉË©ÅÎðf‘cÍ‘j ¾Ïðgy 16 17 mh ·−Å® Ë−ÍÉÇÅÍ− ¾Ç®Ô ÎÇ ²Ä©−ÅÎ hm 18 ÍË²ð® { ﬂÉ−Åfð©Ä−ÅþÀ−j ﬂˆ‡⁄ﬂﬁ‹»j ngy 19 ÍË²É { ›ÀþÉfnj ð©Ä−Í©Ö−j –‡ﬂ¶ˆ‡¥¡⁄j ›¡–ˆ–‡'‚¡¶¥j ÍË²ð®j ngy 20 £ÄÇÍ−fÍË²ð®gy 21 ‡©ÇˆÑË©Î−Åfð®j ÍË²Éj ð©Ä−Í©Ö−gy 22 ›ÏÅÀþÉfÍË²Éj ð©Ä−Í©Ö−gy 23 Û 24 25 mh 26 h ×−Îˆð©Ä−ÎÔÉ− k ⁄−Ë©Ì− ð©Ä− ÎÔÉ− ðËÇÀ ð©Ä−ÅþÀ− 27 hm 28 ÌÇ©® ×−Îˆð©Ä−ÎÔÉ−f²³þË hð©Ä−ÅþÀ−j ²³þË hð©Ä−ÎÔÉ−g 29 Õ 30 ©ð fÍÎËÍÎËfð©Ä−ÅþÀ−j ‘l³ÎÀÄ‘gg 31 ÍÎË²ÉÔfð©Ä−ÎÔÉ−j ‘Î−ÓÎm³ÎÀÄ‘gy 32 −ÄÍ− ©ð fÍÎËÍÎËfð©Ä−ÅþÀ−j ‘l×©ð‘gg 33 ÍÎË²ÉÔfð©Ä−ÎÔÉ−j ‘©Àþ×−m×©ð‘gy 34 −ÄÍ− ©ð fÍÎËÍÎËfð©Ä−ÅþÀ−j ‘lÉÅ×‘gg 35 ÍÎË²ÉÔfð©Ä−ÎÔÉ−j ‘©Àþ×−mÉÅ×‘gy 36 −ÄÍ− ©ð fÍÎËÍÎËfð©Ä−ÅþÀ−j ‘lÁÉ×‘gg 37 ÍÎË²ÉÔfð©Ä−ÎÔÉ−j ‘©Àþ×−mÁÉ−×‘gy 38 −ÄÍ− 39 ÍÎË²ÉÔfð©Ä−ÎÔÉ−j ‘Î−ÓÎmÉÄþ©Å‘gy 40 Û code/netp/tiny/tiny.c Figure 11.34 Tiny Í−ËÌ−ˆÍÎþÎ©² serves static content to a client. Section 11.6 Putting It Together: The Tiny Web Server 999 ﬁrst ð©Ä−Í©Ö− bytes of ﬁle ÍË²ð® to a private read-only area of virtual memory that starts at address ÍË²É. Once we have mapped the ﬁle to memory, we no longer need its descriptor, so we close the ﬁle (line 20). Failing to do this would introduce a potentially fatal memory leak. Line 21 performs the actual transfer of the ﬁle to the client. The Ë©ÇˆÑË©Î−Å function copies the ð©Ä−Í©Ö− bytes starting at location ÍË²É (which of course is mapped to the requested ﬁle) to the client’s connected descriptor. Finally, line 22 frees the mapped virtual memory area. This is important to avoid a potentially fatal memory leak. The Í−ËÌ−ˆ®ÔÅþÀ©² Function Tiny serves any type of dynamic content by forking a child process and then running a CGI program in the context of the child. The Í−ËÌ−ˆ®ÔÅþÀ©² function in Figure 11.35 begins by sending a response line indicating success to the client, along with an informational ·−ËÌ−Ë header. The CGI program is responsible for sending the rest of the response. Notice that this is not as robust as we might wish, since it doesn’t allow for the possibility that the CGI program might encounter some error. After sending the ﬁrst part of the response, we fork a new child process (line 11). The child initializes the QUERY_STRING environment variable with the CGI arguments from the request URI (line 13). Notice that a real server would code/netp/tiny/tiny.c 1 ÌÇ©® Í−ËÌ−ˆ®ÔÅþÀ©²f©ÅÎ ð®j ²³þË hð©Ä−ÅþÀ−j ²³þË h²×©þË×Íg 2 Õ 3 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`j h−ÀÉÎÔÄ©ÍÎ‰` { Õ ﬁ•‹‹ Ûy 4 5 mh ‡−ÎÏËÅ ð©ËÍÎ ÉþËÎ Çð ¤¶¶– Ë−ÍÉÇÅÍ− hm 6 ÍÉË©ÅÎðf¾Ïðj ‘¤¶¶–moln pnn ﬂ«¿Ë¿Å‘gy 7 ‡©ÇˆÑË©Î−Åfð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 8 ÍÉË©ÅÎðf¾Ïðj ‘·−ËÌ−Ëx ¶©ÅÔ „−¾ ·−ËÌ−Ë¿Ë¿Å‘gy 9 ‡©ÇˆÑË©Î−Åfð®j ¾Ïðj ÍÎËÄ−Åf¾Ïðggy 10 11 ©ð fƒÇËÂfg {{ ng Õ mh £³©Ä® hm 12 mh ‡−þÄ Í−ËÌ−Ë ÑÇÏÄ® Í−Î þÄÄ £§' ÌþËÍ ³−Ë− hm 13 Í−Î−ÅÌf‘†•¥‡»ˆ·¶‡'ﬁ§‘j ²×©þË×Íj ogy 14 ⁄ÏÉpfð®j ·¶⁄ﬂ•¶ˆƒ'‹¥ﬁﬂgy mh ‡−®©Ë−²Î ÍÎ®ÇÏÎ ÎÇ ²Ä©−ÅÎ hm 15 ¥Ó−²Ì−fð©Ä−ÅþÀ−j −ÀÉÎÔÄ©ÍÎj −ÅÌ©ËÇÅgy mh ‡ÏÅ £§' ÉËÇ×ËþÀ hm 16 Û 17 „þ©Îfﬁ•‹‹gy mh –þË−ÅÎ Ñþ©ÎÍ ðÇË þÅ® Ë−þÉÍ ²³©Ä® hm 18 Û code/netp/tiny/tiny.c Figure 11.35 Tiny Í−ËÌ−ˆ®ÔÅþÀ©² serves dynamic content to a client. 1000 Chapter 11 Network Programming Aside Dealing with prematurely closed connections Although the basic functions of a Web server are quite simple, we don’t want to give you the false impression that writing a real Web server is easy. Building a robust Web server that runs for extended periods without crashing is a difﬁcult task that requires a deeper understanding of Linux systems programming than we’ve learned here. For example, if a server writes to a connection that has already been closed by the client (say, because you clicked the “Stop” button on your browser), then the ﬁrst such write returns normally, but the second write causes the delivery of a SIGPIPE signal whose default behavior is to terminate the process. If the SIGPIPE signal is caught or ignored, then the second write operation returns −1 with −ËËÅÇ set to EPIPE. The ÍÎË−ËË and É−ËËÇË functions report the EPIPE error as a “Broken pipe,” a nonintuitive message that has confused generations of students. The bottom line is that a robust server must catch these SIGPIPE signals and check ÑË©Î− function calls for EPIPE errors. set the other CGI environment variables here as well. For brevity, we have omitted this step. Next, the child redirects the child’s standard output to the connected ﬁle descriptor (line 14) and then loads and runs the CGI program (line 15). Since the CGI program runs in the context of the child, it has access to the same open ﬁles and environment variables that existed before the call to the −Ó−²Ì− function. Thus, everything that the CGI program writes to standard output goes directly to the client process, without any intervention from the parent process. Meanwhile, the parent blocks in a call to Ñþ©Î, waiting to reap the child when it terminates (line 17). 11.7 Summary Every network application is based on the client-server model. With this model, an application consists of a server and one or more clients. The server manages resources, providing a service for its clients by manipulating the resources in some way. The basic operation in the client-server model is a client-server transaction, which consists of a request from a client, followed by a response from the server. Clients and servers communicate over a global network known as the Internet. From a programmer’s point of view, we can think of the Internet as a worldwide collection of hosts with the following properties: (1) Each Internet host has a unique 32-bit name called its IP address. (2) The set of IP addresses is mapped to a set of Internet domain names. (3) Processes on different Internet hosts can communicate with each other over connections. Clients and servers establish connections by using the sockets interface. A socket is an end point of a connection that is presented to applications in the form of a ﬁle descriptor. The sockets interface provides functions for opening and closing socket descriptors. Clients and servers communicate with each other by reading and writing these descriptors. Homework Problems 1001 Web servers and their clients (such as browsers) communicate with each other using the HTTP protocol. A browser requests either static or dynamic content from the server. A request for static content is served by fetching a ﬁle from the server’s disk and returning it to the client. A request for dynamic content is served by running a program in the context of a child process on the server and returning its output to the client. The CGI standard provides a set of rules that govern how the client passes program arguments to the server, how the server passes these arguments and other information to the child process, and how the child sends its output back to the client. A simple but functioning Web server that serves both static and dynamic content can be implemented in a few hundred lines of C code. Bibliographic Notes The ofﬁcial source of information for the Internet is contained in a set of freely available numbered documents known as RFCs (requests for comments).A searchable index of RFCs is available on the Web at http://rfc-editor.org RFCs are typically written for developers of Internet infrastructure, and thus they are usually too detailed for the casual reader. However, for authoritative information, there is no better source. The HTTP/1.1 protocol is documented in RFC 2616. The authoritative list of MIME types is maintained at http://www.iana.org/assignments/media-types Kerrisk is the bible for all aspects of Linux programming and provides a de- tailed discussion of modern network programming [62]. There are a number of good general texts on computer networking [65, 84, 114]. The great technical writer W. Richard Stevens developed a series of classic texts on such topics as ad- vanced Unix programming [111], the Internet protocols [109, 120, 107], and Unix network programming [108, 110]. Serious students of Unix systems programming will want to study all of them. Tragically, Stevens died on September 1, 1999. His contributions are greatly missed. Homework Problems 11.6 ◆◆ A. Modify Tiny so that it echoes every request line and request header. B. Use your favorite browser to make a request to Tiny for static content. Capture the output from Tiny in a ﬁle. C. Inspect the output from Tiny to determine the version of HTTP your browser uses. 1002 Chapter 11 Network Programming D. Consult the HTTP/1.1 standard in RFC 2616 to determine the meaning of each header in the HTTP request from your browser. You can obtain RFC 2616 from www.rfc-editor.org/rfc.html. 11.7 ◆◆ Extend Tiny so that it serves MPG video ﬁles. Check your work using a real browser. 11.8 ◆◆ Modify Tiny so that it reaps CGI children inside a SIGCHLD handler instead of explicitly waiting for them to terminate. 11.9 ◆◆ Modify Tiny so that when it serves static content, it copies the requested ﬁle to the connected descriptor using ÀþÄÄÇ², Ë©ÇˆË−þ®Å, and Ë©ÇˆÑË©Î−Å, instead of ÀÀþÉ and Ë©ÇˆÑË©Î−Å. 11.10 ◆◆ A. Write an HTML form for the CGI þ®®−Ë function in Figure 11.27. Your form should include two text boxes that users ﬁll in with the two numbers to be added together. Your form should request content using the GET method. B. Check your work by using a real browser to request the form from Tiny, submit the ﬁlled-in form to Tiny, and then display the dynamic content generated by þ®®−Ë. 11.11 ◆◆ Extend Tiny to support the HTTP HEAD method. Check your work using telnet as a Web client. 11.12 ◆◆◆ Extend Tiny so that it serves dynamic content requested by the HTTP POST method. Check your work using your favorite Web browser. 11.13 ◆◆◆ Modify Tiny so that it deals cleanly (without terminating) with the SIGPIPE signals and EPIPE errors that occur when the ÑË©Î− function attempts to write to a prematurely closed connection. Solutions to Practice Problems Solution to Problem 11.1 (page 963) Dotted-decimal address Hex address onulpoplopplpns nÓt¢⁄ru¡£⁄ trloplorwloq nÓrnn£wsn⁄ onulpoplwtlpw nÓt¢⁄rtno⁄ ‰nln`l‰nlopv` nÓnnnnnnvn Solutions to Practice Problems 1003 Dotted-decimal address Hex address ‰psslpss`l‰pssln` nÓƒƒƒƒƒƒnn ‰onlo`l‰oltr` nÓn¡nonorn Solution to Problem 11.2 (page 963) code/netp/global-hex2dd.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ÍÎËÏ²Î ©Åˆþ®®Ë ©Åþ®®Ëy mh ¡®®Ë−ÍÍ ©Å Å−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë hm 6 Ï©ÅÎotˆÎ þ®®Ëy mh ¡®®Ë−ÍÍ ©Å ³ÇÍÎ ¾ÔÎ− ÇË®−Ë hm 7 ²³þË ¾Ïð‰›¡”¢•ƒ`y mh ¢Ïðð−Ë ðÇË ®ÇÎÎ−®k®−²©ÀþÄ ÍÎË©Å× hm 8 9 ©ð fþË×² _{ pg Õ 10 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ z³−Ó ÅÏÀ¾−Ë|¿Å‘j þË×Ì‰n`gy 11 −Ó©Îfngy 12 Û 13 ÍÍ²þÅðfþË×Ì‰o`j ‘cÓ‘j dþ®®Ëgy 14 ©Åþ®®ËlÍˆþ®®Ë { ³ÎÇÅÍfþ®®Ëgy 15 16 ©ð f_©Å−ÎˆÅÎÇÉf¡ƒˆ'ﬁ¥¶j d©Åþ®®Ëj ¾Ïðj ›¡”¢•ƒgg 17 ÏÅ©Óˆ−ËËÇËf‘©Å−ÎˆÅÎÇÉ‘gy 18 ÉË©ÅÎðf‘cÍ¿Å‘j ¾Ïðgy 19 20 −Ó©Îfngy 21 Û code/net/global-hex2dd.c Solution to Problem 11.3 (page 963) code/netp/global-dd2hex.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ÍÎËÏ²Î ©Åˆþ®®Ë ©Åþ®®Ëy mh ¡®®Ë−ÍÍ ©Å Å−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë hm 6 ©ÅÎ Ë²y 7 8 ©ð fþË×² _{ pg Õ 9 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÅ−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë|¿Å‘j þË×Ì‰n`gy 10 −Ó©Îfngy 11 Û 12 13 Ë² { ©Å−ÎˆÉÎÇÅf¡ƒˆ'ﬁ¥¶j þË×Ì‰o`j d©Åþ®®Ëgy 14 ©ð fË² {{ ng 15 þÉÉˆ−ËËÇËf‘©Å−ÎˆÉÎÇÅ −ËËÇËx ©ÅÌþÄ©® Å−ÎÑÇËÂ ¾ÔÎ− ÇË®−Ë‘gy 1004 Chapter 11 Network Programming 16 −ÄÍ− ©ð fË² z ng 17 ÏÅ©Óˆ−ËËÇËf‘©Å−ÎˆÉÎÇÅ −ËËÇË‘gy 18 19 ÉË©ÅÎðf‘nÓcÓ¿Å‘j ÅÎÇ³Íf©Åþ®®ËlÍˆþ®®Ëggy 20 −Ó©Îfngy 21 Û code/netp/global-dd2hex.c Solution to Problem 11.4 (page 978) Here’s a solution. Notice how much more difﬁcult it is to use ©Å−ÎˆÅÎÇÉ, which requires messy casting and deep structure references. The ×−ÎÅþÀ−©ÅðÇ function is much simpler because it does all of that work for us. code/netp/hostinfo-ntop.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 4 Õ 5 ÍÎËÏ²Î þ®®Ë©ÅðÇ hÉj hÄ©ÍÎÉj ³©ÅÎÍy 6 ÍÎËÏ²Î ÍÇ²Âþ®®Ëˆ©Å hÍÇ²ÂÉy 7 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 8 ©ÅÎ Ë²y 9 10 ©ð fþË×² _{ pg Õ 11 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ z®ÇÀþ©Å ÅþÀ−|¿Å‘j þË×Ì‰n`gy 12 −Ó©Îfngy 13 Û 14 15 mh §−Î þ Ä©ÍÎ Çð þ®®Ë©ÅðÇ Ë−²ÇË®Í hm 16 À−ÀÍ−Îfd³©ÅÎÍj nj Í©Ö−ÇðfÍÎËÏ²Î þ®®Ë©ÅðÇggy 17 ³©ÅÎÍlþ©ˆðþÀ©ÄÔ { ¡ƒˆ'ﬁ¥¶y mh '–Ìr ÇÅÄÔ hm 18 ³©ÅÎÍlþ©ˆÍÇ²ÂÎÔÉ− { ·ﬂ£«ˆ·¶‡¥¡›y mh £ÇÅÅ−²Î©ÇÅÍ ÇÅÄÔ hm 19 ©ð ffË² { ×−Îþ®®Ë©ÅðÇfþË×Ì‰o`j ﬁ•‹‹j d³©ÅÎÍj dÄ©ÍÎÉgg _{ ng Õ 20 ðÉË©ÅÎðfÍÎ®−ËËj ‘×−Îþ®®Ë©ÅðÇ −ËËÇËx cÍ¿Å‘j ×þ©ˆÍÎË−ËËÇËfË²ggy 21 −Ó©Îfogy 22 Û 23 24 mh „þÄÂ Î³− Ä©ÍÎ þÅ® ®©ÍÉÄþÔ −þ²³ þÍÍÇ²©þÎ−® '– þ®®Ë−ÍÍ hm 25 ðÇË fÉ { Ä©ÍÎÉy Éy É { Ék|þ©ˆÅ−ÓÎg Õ 26 ÍÇ²ÂÉ { fÍÎËÏ²Î ÍÇ²Âþ®®Ëˆ©Å hgÉk|þ©ˆþ®®Ëy 27 'Å−ÎˆÅÎÇÉf¡ƒˆ'ﬁ¥¶j dfÍÇ²ÂÉk|Í©Åˆþ®®Ëgj ¾Ïðj ›¡”‹'ﬁ¥gy 28 ÉË©ÅÎðf‘cÍ¿Å‘j ¾Ïðgy 29 Û 30 Solutions to Practice Problems 1005 31 mh £Ä−þÅ ÏÉ hm 32 ƒË−−þ®®Ë©ÅðÇfÄ©ÍÎÉgy 33 34 −Ó©Îfngy 35 Û code/netp/hostinfo-ntop.c Solution to Problem 11.5 (page 990) Before the process that runs the CGI program is loaded, a Linux ®ÏÉp function is used to redirect standard output to the connected descriptor that is associated with the client. Thus, anything that the CGI program writes to standard output goes directly to the client. This page is intentionally left blank. CHAPTER 12 Concurrent Programming 12.1 Concurrent Programming with Processes 1009 12.2 Concurrent Programming with I/O Multiplexing 1013 12.3 Concurrent Programming with Threads 1021 12.4 Shared Variables in Threaded Programs 1028 12.5 Synchronizing Threads with Semaphores 1031 12.6 Using Threads for Parallelism 1049 12.7 Other Concurrency Issues 1056 12.8 Summary 1066 Bibliographic Notes 1066 Homework Problems 1067 Solutions to Practice Problems 1072 1007 1008 Chapter 12 Concurrent Programming A s we learned in Chapter 8, logical control ﬂows are concurrent if they overlap in time. This general phenomenon, known as concurrency, shows up at many different levels of a computer system. Hardware exception handlers, processes, and Linux signal handlers are all familiar examples. Thus far, we have treated concurrency mainly as a mechanism that the oper- ating system kernel uses to run multiple application programs. But concurrency is not just limited to the kernel. It can play an important role in application programs as well. For example, we have seen how Linux signal handlers allow applications to respond to asynchronous events such as the user typing Ctrl+C or the program accessing an undeﬁned area of virtual memory. Application-level concurrency is useful in other ways as well: . Accessing slow I/O devices. When an application is waiting for data to arrive from a slow I/O device such as a disk, the kernel keeps the CPU busy by running other processes. Individual applications can exploit concurrency in a similar way by overlapping useful work with I/O requests. . Interacting with humans.People who interact with computers demand the abil- ity to perform multiple tasks at the same time. For example, they might want to resize a window while they are printing a document. Modern windowing systems use concurrency to provide this capability. Each time the user requests some action (say, by clicking the mouse), a separate concurrent logical ﬂow is created to perform the action. . Reducing latency by deferring work. Sometimes, applications can use concur- rency to reduce the latency of certain operations by deferring other operations and performing them concurrently. For example, a dynamic storage allocator might reduce the latency of individual ðË−− operations by deferring coalesc- ing to a concurrent “coalescing” ﬂow that runs at a lower priority, soaking up spare CPU cycles as they become available. . Servicing multiple network clients. The iterative network servers that we stud- ied in Chapter 11 are unrealistic because they can only service one client at a time. Thus, a single slow client can deny service to every other client. For a real server that might be expected to service hundreds or thousands of clients per second, it is not acceptable to allow one slow client to deny service to the others. A better approach is to build a concurrent server that creates a separate logical ﬂow for each client. This allows the server to service multiple clients concurrently and precludes slow clients from monopolizing the server. . Computing in parallel on multi-core machines. Many modern systems are equipped with multi-core processors that contain multiple CPUs. Applica- tions that are partitioned into concurrent ﬂows often run faster on multi-core machines than on uniprocessor machines because the ﬂows execute in parallel rather than being interleaved. Applications that use application-level concurrency are known as concurrent programs. Modern operating systems provide three basic approaches for building concurrent programs: Section 12.1 Concurrent Programming with Processes 1009 . Processes. With this approach, each logical control ﬂow is a process that is scheduled and maintained by the kernel. Since processes have separate virtual address spaces, ﬂows that want to communicate with each other must use some kind of explicit interprocess communication (IPC) mechanism. . I/O multiplexing.This is a form of concurrent programming where applications explicitly schedule their own logical ﬂows in the context of a single process. Logical ﬂows are modeled as state machines that the main program explicitly transitions from state to state as a result of data arriving on ﬁle descriptors. Since the program is a single process, all ﬂows share the same address space. . Threads. Threads are logical ﬂows that run in the context of a single process and are scheduled by the kernel. You can think of threads as a hybrid of the other two approaches, scheduled by the kernel like process ﬂows and sharing the same virtual address space like I/O multiplexing ﬂows. This chapter investigates these three different concurrent programming tech- niques. To keep our discussion concrete, we will work with the same motivating application throughout—a concurrent version of the iterative echo server from Section 11.4.9. 12.1 Concurrent Programming with Processes The simplest way to build a concurrent program is with processes, using familiar functions such as ðÇËÂ, −Ó−², and Ñþ©ÎÉ©®. For example, a natural approach for building a concurrent server is to accept client connection requests in the parent and then create a new child process to service each new client. To see how this might work, suppose we have two clients and a server that is listening for connection requests on a listening descriptor (say, 3). Now suppose that the server accepts a connection request from client 1 and returns a connected descriptor (say, 4), as shown in Figure 12.1. After accepting the connection request, the server forks a child, which gets a complete copy of the server’s descriptor table. The child closes its copy of listening descriptor 3, and the parent closes its copy of connected descriptor 4, since they are no longer needed. This gives us the situation shown in Figure 12.2, where the child process is busy servicing the client. Since the connected descriptors in the parent and child each point to the same ﬁle table entry, it is crucial for the parent to close its copy of the connected Figure 12.1 Step 1: Server accepts connection request from client. Client 1 clientfd Client 2 clientfd connfd(4) listenfd(3) Server Connection request 1010 Chapter 12 Concurrent Programming Figure 12.2 Step 2: Server forks a child process to service the client. Client 1 clientfd Client 2 clientfd connfd(4) Child 1 listenfd(3) Server Data transfers Figure 12.3 Step 3: Server accepts another connection request. Client 1 clientfd Client 2 clientfd connfd(4) connfd(5) Child 1 listenfd(3) Server Data transfers Connection request descriptor. Otherwise, the ﬁle table entry for connected descriptor 4 will never be released, and the resulting memory leak will eventually consume the available memory and crash the system. Now suppose that after the parent creates the child for client 1, it accepts a new connection request from client 2 and returns a new connected descriptor (say, 5), as shown in Figure 12.3. The parent then forks another child, which begins servicing its client using connected descriptor 5, as shown in Figure 12.4. At this point, the parent is waiting for the next connection request and the two children are servicing their respective clients concurrently. 12.1.1 A Concurrent Server Based on Processes Figure 12.5 shows the code for a concurrent echo server based on processes. The −²³Ç function called in line 29 comes from Figure 11.22. There are several important points to make about this server: . First, servers typically run for long periods of time, so we must include a SIGCHLD handler that reaps zombie children (lines 4–9). Since SIGCHLD signals are blocked while the SIGCHLD handler is executing, and since Linux signals are not queued, the SIGCHLD handler must be prepared to reap multiple zombie children. . Second, the parent and the child must close their respective copies of ²ÇÅÅð® (lines 33 and 30, respectively). As we have mentioned, this is especially im- Section 12.1 Concurrent Programming with Processes 1011 Figure 12.4 Step 4: Server forks another child to service the new client. Client 1 clientfd Client 2 clientfd connfd(4) Child 1 connfd(5) Child 2 listenfd(3) Server Data transfers Data transfers portant for the parent, which must close its copy of the connected descriptor to avoid a memory leak. . Finally, because of the reference count in the socket’s ﬁle table entry, the connection to the client will not be terminated until both the parent’s and child’s copies of ²ÇÅÅð® are closed. 12.1.2 Pros and Cons of Processes Processes have a clean model for sharing state information between parents and children: ﬁle tables are shared and user address spaces are not. Having separate address spaces for processes is both an advantage and a disadvantage. It is im- possible for one process to accidentally overwrite the virtual memory of another process, which eliminates a lot of confusing failures—an obvious advantage. On the other hand, separate address spaces make it more difﬁcult for pro- cesses to share state information. To share information, they must use explicit IPC (interprocess communications) mechanisms. (See the Aside on page 1013.) An- other disadvantage of process-based designs is that they tend to be slower because the overhead for process control and IPC is high. Practice Problem 12.1 (solution page 1072) Figure 12.5 demonstrates a concurrent server in which the parent process creates a child process to handle each new connection request. Trace the value of the reference counter for the associated ﬁle table for Figure 12.5. Practice Problem 12.2 (solution page 1072) If we were to delete line 33 of Figure12.5, which closes the connected descriptor, the code would still be correct, in the sense that there would be no memory leak. Why? 1012 Chapter 12 Concurrent Programming code/conc/echoserverp.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 ÌÇ©® −²³Çf©ÅÎ ²ÇÅÅð®gy 3 4 ÌÇ©® Í©×²³Ä®ˆ³þÅ®Ä−Ëf©ÅÎ Í©×g 5 Õ 6 Ñ³©Ä− fÑþ©ÎÉ©®fkoj nj „ﬁﬂ¤¡ﬁ§g | ng 7 y 8 Ë−ÎÏËÅy 9 Û 10 11 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 12 Õ 13 ©ÅÎ Ä©ÍÎ−Åð®j ²ÇÅÅð®y 14 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 15 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy 16 17 ©ð fþË×² _{ pg Õ 18 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 19 −Ó©Îfngy 20 Û 21 22 ·©×ÅþÄf·'§£¤‹⁄j Í©×²³Ä®ˆ³þÅ®Ä−Ëgy 23 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 24 Ñ³©Ä− fog Õ 25 ²Ä©−ÅÎÄ−Å { Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×−gy 26 ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hg d²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 27 ©ð fƒÇËÂfg {{ ng Õ 28 £ÄÇÍ−fÄ©ÍÎ−Åð®gy mh £³©Ä® ²ÄÇÍ−Í ©ÎÍ Ä©ÍÎ−Å©Å× ÍÇ²Â−Î hm 29 −²³Çf²ÇÅÅð®gy mh £³©Ä® Í−ËÌ©²−Í ²Ä©−ÅÎ hm 30 £ÄÇÍ−f²ÇÅÅð®gy mh £³©Ä® ²ÄÇÍ−Í ²ÇÅÅ−²Î©ÇÅ Ñ©Î³ ²Ä©−ÅÎ hm 31 −Ó©Îfngy mh £³©Ä® −Ó©ÎÍ hm 32 Û 33 £ÄÇÍ−f²ÇÅÅð®gy mh –þË−ÅÎ ²ÄÇÍ−Í ²ÇÅÅ−²Î−® ÍÇ²Â−Î f©ÀÉÇËÎþÅÎ_g hm 34 Û 35 Û code/conc/echoserverp.c Figure 12.5 Concurrent echo server based on processes. The parent forks a child to handle each new connection request. Section 12.2 Concurrent Programming with I/O Multiplexing 1013 Aside Unix IPC You have already encountered several examples of IPC in this text. The Ñþ©ÎÉ©® function and signals from Chapter 8 are primitive IPC mechanisms that allow processes to send tiny messages to processes running on the same host. The sockets interface from Chapter 11 is an important form of IPC that allows processes on different hosts to exchange arbitrary byte streams. However, the term Unix IPC is typically reserved for a hodgepodge of techniques that allow processes to communicate with other processes that are running on the same host. Examples include pipes, FIFOs, System V shared memory, and System V semaphores. These mechanisms are beyond our scope. The book by Kerrisk [62] is an excellent reference. 12.2 Concurrent Programming with I/O Multiplexing Suppose you are asked to write an echo server that can also respond to interactive commands that the user types to standard input. In this case, the server must respond to two independent I/O events: (1) a network client making a connection request, and (2) a user typing a command line at the keyboard. Which event do we wait for ﬁrst? Neither option is ideal. If we are waiting for a connection request in þ²²−ÉÎ, then we cannot respond to input commands. Similarly, if we are waiting for an input command in Ë−þ®, then we cannot respond to any connection requests. One solution to this dilemma is a technique called I/O multiplexing. The basic idea is to use the Í−Ä−²Î function to ask the kernel to suspend the process, return- ing control to the application only after one or more I/O events have occurred, as in the following examples: . Return when any descriptor in the set {0, 4} is ready for reading. . Return when any descriptor in the set {1, 2, 7} is ready for writing. . Time out if 152.13 seconds have elapsed waiting for an I/O event to occur. ·−Ä−²Î is a complicated function with many different usage scenarios. We will only discuss the ﬁrst scenario: waiting for a set of descriptors to be ready for reading. See [62, 110] for a complete discussion. a©Å²ÄÏ®− zÍÔÍmÍ−Ä−²Îl³| ©ÅÎ Í−Ä−²Îf©ÅÎ Åj ð®ˆÍ−Î hð®Í−Îj ﬁ•‹‹j ﬁ•‹‹j ﬁ•‹‹gy Returns: nonzero count of ready descriptors, −1 on error ƒ⁄ˆ…¥‡ﬂfð®ˆÍ−Î hð®Í−Îgy mh £Ä−þË þÄÄ ¾©ÎÍ ©Å ð®Í−Î hm ƒ⁄ˆ£‹‡f©ÅÎ ð®j ð®ˆÍ−Î hð®Í−Îgy mh £Ä−þË ¾©Î ð® ©Å ð®Í−Î hm ƒ⁄ˆ·¥¶f©ÅÎ ð®j ð®ˆÍ−Î hð®Í−Îgy mh ¶ÏËÅ ÇÅ ¾©Î ð® ©Å ð®Í−Î hm ƒ⁄ˆ'··¥¶f©ÅÎ ð®j ð®ˆÍ−Î hð®Í−Îgy mh 'Í ¾©Î ð® ©Å ð®Í−Î ÇÅ} hm Macros for manipulating descriptor sets 1014 Chapter 12 Concurrent Programming The Í−Ä−²Î function manipulates sets of type ð®ˆÍ−Î, which are known as de- scriptor sets. Logically, we think of a descriptor set as a bit vector (introduced in Section 2.1) of size n: bn−1,...,b1,b0 Each bit bk corresponds to descriptor k. Descriptor k is a member of the descriptor set if and only if bk = 1. You are only allowed to do three things with descriptor sets: (1) allocate them, (2) assign one variable of this type to another, and (3) modify and inspect them using the FD_ZERO, FD_SET, FD_CLR, and FD_ ISSET macros. For our purposes, the Í−Ä−²Î function takes two inputs: a descriptor set (ð®Í−Î) called the read set, and the cardinality (Å) of the read set (actually the maximum cardinality of any descriptor set). The Í−Ä−²Î function blocks until at least one descriptor in the read set is ready for reading. A descriptor k is ready for reading if and only if a request to read 1 byte from that descriptor would not block. As a side effect, Í−Ä−²Î modiﬁes the ð®ˆÍ−Î pointed to by argument ð®Í−Î to indicate a subset of the read set called the ready set, consisting of the descriptors in the read set that are ready for reading. The value returned by the function indicates the cardinality of the ready set. Note that because of the side effect, we must update the read set every time Í−Ä−²Î is called. The best way to understand Í−Ä−²Î is to study a concrete example. Figure 12.6 shows how we might use Í−Ä−²Î to implement an iterative echo server that also accepts user commands on the standard input. We begin by using the ÇÉ−Åˆ Ä©ÍÎ−Åð® function from Figure 11.19 to open a listening descriptor (line 16), and then using FD_ZERO to create an empty read set (line 18): listenfd stdin read_set (∅): 0 3 0 2 0 1 0 0 Next, in lines 19 and 20, we deﬁne the read set to consist of descriptor 0 (standard input) and descriptor 3 (the listening descriptor), respectively: listenfd stdin read_set ({0,3}): 1 3 0 2 0 1 1 0 At this point, we begin the typical server loop. But instead of waiting for a connection request by calling the þ²²−ÉÎ function, we call the Í−Ä−²Î function, which blocks until either the listening descriptor or standard input is ready for reading (line 24). For example, here is the value of Ë−þ®ÔˆÍ−Î that Í−Ä−²Î would return if the user hit the enter key, thus causing the standard input descriptor to Section 12.2 Concurrent Programming with I/O Multiplexing 1015 code/conc/select.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 ÌÇ©® −²³Çf©ÅÎ ²ÇÅÅð®gy 3 ÌÇ©® ²ÇÀÀþÅ®fÌÇ©®gy 4 5 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 6 Õ 7 ©ÅÎ Ä©ÍÎ−Åð®j ²ÇÅÅð®y 8 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 9 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy 10 ð®ˆÍ−Î Ë−þ®ˆÍ−Îj Ë−þ®ÔˆÍ−Îy 11 12 ©ð fþË×² _{ pg Õ 13 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 14 −Ó©Îfngy 15 Û 16 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 17 18 ƒ⁄ˆ…¥‡ﬂfdË−þ®ˆÍ−Îgy mh £Ä−þË Ë−þ® Í−Î hm 19 ƒ⁄ˆ·¥¶f·¶⁄'ﬁˆƒ'‹¥ﬁﬂj dË−þ®ˆÍ−Îgy mh ¡®® ÍÎ®©Å ÎÇ Ë−þ® Í−Î hm 20 ƒ⁄ˆ·¥¶fÄ©ÍÎ−Åð®j dË−þ®ˆÍ−Îgy mh ¡®® Ä©ÍÎ−Åð® ÎÇ Ë−þ® Í−Î hm 21 22 Ñ³©Ä− fog Õ 23 Ë−þ®ÔˆÍ−Î { Ë−þ®ˆÍ−Îy 24 ·−Ä−²ÎfÄ©ÍÎ−Åð®ioj dË−þ®ÔˆÍ−Îj ﬁ•‹‹j ﬁ•‹‹j ﬁ•‹‹gy 25 ©ð fƒ⁄ˆ'··¥¶f·¶⁄'ﬁˆƒ'‹¥ﬁﬂj dË−þ®ÔˆÍ−Îgg 26 ²ÇÀÀþÅ®fgy mh ‡−þ® ²ÇÀÀþÅ® Ä©Å− ðËÇÀ ÍÎ®©Å hm 27 ©ð fƒ⁄ˆ'··¥¶fÄ©ÍÎ−Åð®j dË−þ®ÔˆÍ−Îgg Õ 28 ²Ä©−ÅÎÄ−Å { Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×−gy 29 ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hgd²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 30 −²³Çf²ÇÅÅð®gy mh ¥²³Ç ²Ä©−ÅÎ ©ÅÉÏÎ ÏÅÎ©Ä ¥ﬂƒ hm 31 £ÄÇÍ−f²ÇÅÅð®gy 32 Û 33 Û 34 Û 35 36 ÌÇ©® ²ÇÀÀþÅ®fÌÇ©®g Õ 37 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 38 ©ð f_ƒ×−ÎÍf¾Ïðj ›¡”‹'ﬁ¥j ÍÎ®©Ågg 39 −Ó©Îfngy mh ¥ﬂƒ hm 40 ÉË©ÅÎðf‘cÍ‘j ¾Ïðgy mh –ËÇ²−ÍÍ Î³− ©ÅÉÏÎ ²ÇÀÀþÅ® hm 41 Û code/conc/select.c Figure 12.6 An iterative echo server that uses I/O multiplexing. The server uses Í−Ä−²Î to wait for connection requests on a listening descriptor and commands on standard input. 1016 Chapter 12 Concurrent Programming become ready for reading: listenfd stdin ready_set ({0}): 0 3 0 2 0 1 1 0 Once Í−Ä−²Î returns, we use the FD_ISSET macro to determine which de- scriptors are ready for reading. If standard input is ready (line 25), we call the ²ÇÀÀþÅ® function, which reads, parses, and responds to the command before re- turning to the main routine. If the listening descriptor is ready (line 27), we call þ²²−ÉÎ to get a connected descriptor and then call the −²³Ç function from Fig- ure 11.22, which echoes each line from the client until the client closes its end of the connection. While this program is a good example of using Í−Ä−²Î, it still leaves something to be desired. The problem is that once it connects to a client, it continues echoing input lines until the client closes its end of the connection. Thus, if you type a command to standard input, you will not get a response until the server is ﬁnished with the client. A better approach would be to multiplex at a ﬁner granularity, echoing (at most) one text line each time through the server loop. Practice Problem 12.3 (solution page 1072) In Linux systems, typing Ctrl+D indicates EOF on standard input. What happens if you type Ctrl+D to the program in Figure 12.6 while it is echoing each line of the client? 12.2.1 A Concurrent Event-Driven Server Based on I/O Multiplexing I/O multiplexing can be used as the basis for concurrent event-driven programs, where ﬂows make progress as a result of certain events. The general idea is to model logical ﬂows as state machines. Informally, a state machine is a collection of states, input events, and transitions that map states and input events to states. Each transition maps an (input state, input event) pair to an output state. A self-loop is a transition between the same input and output state. State machines are typically drawn as directed graphs, where nodes represent states, directed arcs represent transitions, and arc labels represent input events. A state machine begins execution in some initial state. Each input event triggers a transition from the current state to the next state. For each new client k, a concurrent server based on I/O multiplexing creates a new state machine sk and associates it with connected descriptor dk. As shown in Figure 12.7, each state machine sk has one state (“waiting for descriptor dk to be ready for reading”), one input event (“descriptor dk is ready for reading”), and one transition (“read a text line from descriptor dk”). Section 12.2 Concurrent Programming with I/O Multiplexing 1017 Figure 12.7 State machine for a logical ﬂow in a concurrent event-driven echo server. Input event: “descriptor dk is ready for reading” Transition: “read a text line from descriptor dk” State: “waiting for descriptor dk to be ready for reading” The server uses the I/O multiplexing, courtesy of the Í−Ä−²Î function, to detect the occurrence of input events. As each connected descriptor becomes ready for reading, the server executes the transition for the corresponding state machine—in this case, reading and echoing a text line from the descriptor. Figure 12.8 shows the complete example code for a concurrent event-driven server based on I/O multiplexing. The set of active clients is maintained in a ÉÇÇÄ structure (lines 3–11). After initializing the pool by calling ©Å©ÎˆÉÇÇÄ (line 27), the server enters an inﬁnite loop. During each iteration of this loop, the server calls the Í−Ä−²Î function to detect two different kinds of input events: (1) a connection request arriving from a new client, and (2) a connected descriptor for an existing client being ready for reading. When a connection request arrives (line 35), the server opens the connection (line 37) and calls the þ®®ˆ²Ä©−ÅÎ function to add the client to the pool (line 38). Finally, the server calls the ²³−²Âˆ²Ä©−ÅÎÍ function to echo a single text line from each ready connected descriptor (line 42). The ©Å©ÎˆÉÇÇÄ function (Figure 12.9) initializes the client pool. The ²Ä©−ÅÎð® array represents a set of connected descriptors, with the integer −1 denoting an available slot. Initially, the set of connected descriptors is empty (lines 5–7), and the listening descriptor is the only descriptor in the Í−Ä−²Î read set (lines 10–12). The þ®®ˆ²Ä©−ÅÎ function (Figure 12.10) adds a new client to the pool of active clients. After ﬁnding an empty slot in the ²Ä©−ÅÎð® array, the server adds the connected descriptor to the array and initializes a corresponding Rio read buffer so that we can call Ë©ÇˆË−þ®Ä©Å−¾ on the descriptor (lines 8–9). We then add the connected descriptor to the Í−Ä−²Î read set (line 12), and we update some global properties of the pool. The ÀþÓð® variable (lines 15–16) keeps track of the largest ﬁle descriptor for Í−Ä−²Î.The ÀþÓ© variable (lines 17–18) keeps track of the largest index into the ²Ä©−ÅÎð® array so that the ²³−²Âˆ²Ä©−ÅÎÍ function does not have to search the entire array. The ²³−²Âˆ²Ä©−ÅÎÍ function in Figure 12.11 echoes a text line from each ready connected descriptor. If we are successful in reading a text line from the descriptor, then we echo that line back to the client (lines 15–18). Notice that in line 15, we are maintaining a cumulative count of total bytes received from all clients. If we detect EOF because the client has closed its end of the connection, then we close our end of the connection (line 23) and remove the descriptor from the pool (lines 24–25). 1018 Chapter 12 Concurrent Programming code/conc/echoservers.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÎÔÉ−®−ð ÍÎËÏ²Î Õ mh ‡−ÉË−Í−ÅÎÍ þ ÉÇÇÄ Çð ²ÇÅÅ−²Î−® ®−Í²Ë©ÉÎÇËÍ hm 4 ©ÅÎ ÀþÓð®y mh ‹þË×−ÍÎ ®−Í²Ë©ÉÎÇË ©Å Ë−þ®ˆÍ−Î hm 5 ð®ˆÍ−Î Ë−þ®ˆÍ−Îy mh ·−Î Çð þÄÄ þ²Î©Ì− ®−Í²Ë©ÉÎÇËÍ hm 6 ð®ˆÍ−Î Ë−þ®ÔˆÍ−Îy mh ·Ï¾Í−Î Çð ®−Í²Ë©ÉÎÇËÍ Ë−þ®Ô ðÇË Ë−þ®©Å× hm 7 ©ÅÎ ÅË−þ®Ôy mh ﬁÏÀ¾−Ë Çð Ë−þ®Ô ®−Í²Ë©ÉÎÇËÍ ðËÇÀ Í−Ä−²Î hm 8 ©ÅÎ ÀþÓ©y mh ¤©×³ ÑþÎ−Ë ©Å®−Ó ©ÅÎÇ ²Ä©−ÅÎ þËËþÔ hm 9 ©ÅÎ ²Ä©−ÅÎð®‰ƒ⁄ˆ·¥¶·'…¥`y mh ·−Î Çð þ²Î©Ì− ®−Í²Ë©ÉÎÇËÍ hm 10 Ë©ÇˆÎ ²Ä©−ÅÎË©Ç‰ƒ⁄ˆ·¥¶·'…¥`y mh ·−Î Çð þ²Î©Ì− Ë−þ® ¾Ïðð−ËÍ hm 11 Û ÉÇÇÄy 12 13 ©ÅÎ ¾ÔÎ−ˆ²ÅÎ { ny mh £ÇÏÅÎÍ ÎÇÎþÄ ¾ÔÎ−Í Ë−²−©Ì−® ¾Ô Í−ËÌ−Ë hm 14 15 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 16 Õ 17 ©ÅÎ Ä©ÍÎ−Åð®j ²ÇÅÅð®y 18 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 19 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy 20 ÍÎþÎ©² ÉÇÇÄ ÉÇÇÄy 21 22 ©ð fþË×² _{ pg Õ 23 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 24 −Ó©Îfngy 25 Û 26 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 27 ©Å©ÎˆÉÇÇÄfÄ©ÍÎ−Åð®j dÉÇÇÄgy 28 29 Ñ³©Ä− fog Õ 30 mh „þ©Î ðÇË Ä©ÍÎ−Å©Å×m²ÇÅÅ−²Î−® ®−Í²Ë©ÉÎÇËfÍg ÎÇ ¾−²ÇÀ− Ë−þ®Ô hm 31 ÉÇÇÄlË−þ®ÔˆÍ−Î { ÉÇÇÄlË−þ®ˆÍ−Îy 32 ÉÇÇÄlÅË−þ®Ô { ·−Ä−²ÎfÉÇÇÄlÀþÓð®ioj dÉÇÇÄlË−þ®ÔˆÍ−Îj ﬁ•‹‹j ﬁ•‹‹j ﬁ•‹‹gy 33 34 mh 'ð Ä©ÍÎ−Å©Å× ®−Í²Ë©ÉÎÇË Ë−þ®Ôj þ®® Å−Ñ ²Ä©−ÅÎ ÎÇ ÉÇÇÄ hm 35 ©ð fƒ⁄ˆ'··¥¶fÄ©ÍÎ−Åð®j dÉÇÇÄlË−þ®ÔˆÍ−Îgg Õ 36 ²Ä©−ÅÎÄ−Å { Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×−gy 37 ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hgd²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 38 þ®®ˆ²Ä©−ÅÎf²ÇÅÅð®j dÉÇÇÄgy 39 Û 40 41 mh ¥²³Ç þ Î−ÓÎ Ä©Å− ðËÇÀ −þ²³ Ë−þ®Ô ²ÇÅÅ−²Î−® ®−Í²Ë©ÉÎÇË hm 42 ²³−²Âˆ²Ä©−ÅÎÍfdÉÇÇÄgy 43 Û 44 Û code/conc/echoservers.c Figure 12.8 Concurrent echo server based on I/O multiplexing. Each server iteration echoes a text line from each ready descriptor. Section 12.2 Concurrent Programming with I/O Multiplexing 1019 code/conc/echoservers.c 1 ÌÇ©® ©Å©ÎˆÉÇÇÄf©ÅÎ Ä©ÍÎ−Åð®j ÉÇÇÄ hÉg 2 Õ 3 mh 'Å©Î©þÄÄÔj Î³−Ë− þË− ÅÇ ²ÇÅÅ−²Î−® ®−Í²Ë©ÉÎÇËÍ hm 4 ©ÅÎ ©y 5 Ék|ÀþÓ© { koy 6 ðÇË f©{ny ©z ƒ⁄ˆ·¥¶·'…¥y ©iig 7 Ék|²Ä©−ÅÎð®‰©` { koy 8 9 mh 'Å©Î©þÄÄÔj Ä©ÍÎ−Åð® ©Í ÇÅÄÔ À−À¾−Ë Çð Í−Ä−²Î Ë−þ® Í−Î hm 10 Ék|ÀþÓð® { Ä©ÍÎ−Åð®y 11 ƒ⁄ˆ…¥‡ﬂfdÉk|Ë−þ®ˆÍ−Îgy 12 ƒ⁄ˆ·¥¶fÄ©ÍÎ−Åð®j dÉk|Ë−þ®ˆÍ−Îgy 13 Û code/conc/echoservers.c Figure 12.9 ©Å©ÎˆÉÇÇÄ initializes the pool of active clients. code/conc/echoservers.c 1 ÌÇ©® þ®®ˆ²Ä©−ÅÎf©ÅÎ ²ÇÅÅð®j ÉÇÇÄ hÉg 2 Õ 3 ©ÅÎ ©y 4 Ék|ÅË−þ®Ôkky 5 ðÇË f© { ny © z ƒ⁄ˆ·¥¶·'…¥y ©iig mh ƒ©Å® þÅ þÌþ©Äþ¾Ä− ÍÄÇÎ hm 6 ©ð fÉk|²Ä©−ÅÎð®‰©` z ng Õ 7 mh ¡®® ²ÇÅÅ−²Î−® ®−Í²Ë©ÉÎÇË ÎÇ Î³− ÉÇÇÄ hm 8 Ék|²Ä©−ÅÎð®‰©` { ²ÇÅÅð®y 9 ‡©ÇˆË−þ®©Å©Î¾fdÉk|²Ä©−ÅÎË©Ç‰©`j ²ÇÅÅð®gy 10 11 mh ¡®® Î³− ®−Í²Ë©ÉÎÇË ÎÇ ®−Í²Ë©ÉÎÇË Í−Î hm 12 ƒ⁄ˆ·¥¶f²ÇÅÅð®j dÉk|Ë−þ®ˆÍ−Îgy 13 14 mh •É®þÎ− ÀþÓ ®−Í²Ë©ÉÎÇË þÅ® ÉÇÇÄ ³©×³ ÑþÎ−Ë ÀþËÂ hm 15 ©ð f²ÇÅÅð® | Ék|ÀþÓð®g 16 Ék|ÀþÓð® { ²ÇÅÅð®y 17 ©ð f© | Ék|ÀþÓ©g 18 Ék|ÀþÓ© { ©y 19 ¾Ë−þÂy 20 Û 21 ©ð f© {{ ƒ⁄ˆ·¥¶·'…¥g mh £ÇÏÄ®Å’Î ð©Å® þÅ −ÀÉÎÔ ÍÄÇÎ hm 22 þÉÉˆ−ËËÇËf‘þ®®ˆ²Ä©−ÅÎ −ËËÇËx ¶ÇÇ ÀþÅÔ ²Ä©−ÅÎÍ‘gy 23 Û code/conc/echoservers.c Figure 12.10 þ®®ˆ²Ä©−ÅÎ adds a new client connection to the pool. 1020 Chapter 12 Concurrent Programming code/conc/echoservers.c 1 ÌÇ©® ²³−²Âˆ²Ä©−ÅÎÍfÉÇÇÄ hÉg 2 Õ 3 ©ÅÎ ©j ²ÇÅÅð®j Åy 4 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 5 Ë©ÇˆÎ Ë©Çy 6 7 ðÇË f© { ny f© z{ Ék|ÀþÓ©g dd fÉk|ÅË−þ®Ô | ngy ©iig Õ 8 ²ÇÅÅð® { Ék|²Ä©−ÅÎð®‰©`y 9 Ë©Ç { Ék|²Ä©−ÅÎË©Ç‰©`y 10 11 mh 'ð Î³− ®−Í²Ë©ÉÎÇË ©Í Ë−þ®Ôj −²³Ç þ Î−ÓÎ Ä©Å− ðËÇÀ ©Î hm 12 ©ð ff²ÇÅÅð® | ng dd fƒ⁄ˆ'··¥¶f²ÇÅÅð®j dÉk|Ë−þ®ÔˆÍ−Îggg Õ 13 Ék|ÅË−þ®Ôkky 14 ©ð ffÅ { ‡©ÇˆË−þ®Ä©Å−¾fdË©Çj ¾Ïðj ›¡”‹'ﬁ¥gg _{ ng Õ 15 ¾ÔÎ−ˆ²ÅÎ i{ Åy 16 ÉË©ÅÎðf‘·−ËÌ−Ë Ë−²−©Ì−® c® fc® ÎÇÎþÄg ¾ÔÎ−Í ÇÅ ð® c®¿Å‘j 17 Åj ¾ÔÎ−ˆ²ÅÎj ²ÇÅÅð®gy 18 ‡©ÇˆÑË©Î−Åf²ÇÅÅð®j ¾Ïðj Ågy 19 Û 20 21 mh ¥ﬂƒ ®−Î−²Î−®j Ë−ÀÇÌ− ®−Í²Ë©ÉÎÇË ðËÇÀ ÉÇÇÄ hm 22 −ÄÍ− Õ 23 £ÄÇÍ−f²ÇÅÅð®gy 24 ƒ⁄ˆ£‹‡f²ÇÅÅð®j dÉk|Ë−þ®ˆÍ−Îgy 25 Ék|²Ä©−ÅÎð®‰©` { koy 26 Û 27 Û 28 Û 29 Û code/conc/echoservers.c Figure 12.11 ²³−²Âˆ²Ä©−ÅÎÍ services ready client connections. In terms of the ﬁnite state model in Figure 12.7, the Í−Ä−²Î function detects input events, and the þ®®ˆ²Ä©−ÅÎ function creates a new logical ﬂow (state ma- chine). The ²³−²Âˆ²Ä©−ÅÎÍ function performs state transitions by echoing input lines, and it also deletes the state machine when the client has ﬁnished sending text lines. Practice Problem 12.4 (solution page 1072) In the server in Figure 12.8, ÉÇÇÄlÅË−þ®Ô is reinitialized with the value obtained from the call to Í−Ä−²Î. Why? Section 12.3 Concurrent Programming with Threads 1021 Aside Event-driven Web servers Despite the disadvantages outlined in Section 12.2.2, modern high-performance servers such as Node.js, nginx, and Tornado use event-driven programming based on I/O multiplexing, mainly because of the signiﬁcant performance advantage compared to processes and threads. 12.2.2 Pros and Cons of I/O Multiplexing The server in Figure 12.8 provides a nice example of the advantages and disad- vantages of event-driven programming based on I/O multiplexing. One advantage is that event-driven designs give programmers more control over the behavior of their programs than process-based designs. For example, we can imagine writ- ing an event-driven concurrent server that gives preferred service to some clients, which would be difﬁcult for a concurrent server based on processes. Another advantage is that an event-driven server based on I/O multiplexing runs in the context of a single process, and thus every logical ﬂow has access to the entire address space of the process. This makes it easy to share data between ﬂows. A related advantage of running as a single process is that you can debug your concurrent server as you would any sequential program, using a familiar debugging tool such as gdb. Finally, event-driven designs are often signiﬁcantly more efﬁcient than process-based designs because they do not require a process context switch to schedule a new ﬂow. A signiﬁcant disadvantage of event-driven designs is coding complexity. Our event-driven concurrent echo server requires three times more code than the process-based server. Unfortunately, the complexity increases as the granularity of the concurrency decreases. By granularity, we mean the number of instructions that each logical ﬂow executes per time slice. For instance, in our example concur- rent server, the granularity of concurrency is the number of instructions required to read an entire text line. As long as some logical ﬂow is busy reading a text line, no other logical ﬂow can make progress. This is ﬁne for our example, but it makes our event-driven server vulnerable to a malicious client that sends only a partial text line and then halts. Modifying an event-driven server to handle partial text lines is a nontrivial task, but it is handled cleanly and automatically by a process- based design. Another signiﬁcant disadvantage of event-based designs is that they cannot fully utilize multi-core processors. 12.3 Concurrent Programming with Threads To this point, we have looked at two approaches for creating concurrent logical ﬂows. With the ﬁrst approach, we use a separate process for each ﬂow. The kernel schedules each process automatically, and each process has its own private address space, which makes it difﬁcult for ﬂows to share data. With the second approach, we create our own logical ﬂows and use I/O multiplexing to explicitly schedule the ﬂows. Because there is only one process, ﬂows share the entire address space. 1022 Chapter 12 Concurrent Programming This section introduces a third approach—based on threads—that is a hybrid of these two. A thread is a logical ﬂow that runs in the context of a process. Thus far in this book, our programs have consisted of a single thread per process. But modern systems also allow us to write programs that have multiple threads running concurrently in a single process. The threads are scheduled automatically by the kernel. Each thread has its own thread context, including a unique integer thread ID (TID), stack, stack pointer, program counter, general-purpose registers, and condition codes. All threads running in a process share the entire virtual address space of that process. Logical ﬂows based on threads combine qualities of ﬂows based on processes and I/O multiplexing. Like processes, threads are scheduled automatically by the kernel and are known to the kernel by an integer ID. Like ﬂows based on I/O multiplexing, multiple threads run in the context of a single process, and thus they share the entire contents of the process virtual address space, including its code, data, heap, shared libraries, and open ﬁles. 12.3.1 Thread Execution Model The execution model for multiple threads is similar in some ways to the execution model for multiple processes. Consider the example in Figure 12.12. Each process begins life as a single thread called the main thread. At some point, the main thread creates a peer thread, and from this point in time the two threads run concurrently. Eventually, control passes to the peer thread via a context switch, either because the main thread executes a slow system call such as Ë−þ® or ÍÄ−−É or because it is interrupted by the system’s interval timer. The peer thread executes for a while before control passes back to the main thread, and so on. Thread execution differs from processes in some important ways. Because a thread context is much smaller than a process context, a thread context switch is faster than a process context switch. Another difference is that threads, unlike pro- cesses, are not organized in a rigid parent-child hierarchy. The threads associated Figure 12.12 Concurrent thread execution. Thread 1 (main thread) Thread 2 (peer thread) Time Thread context switch Thread context switch Thread context switch Section 12.3 Concurrent Programming with Threads 1023 with a process form a pool of peers, independent of which threads were created by which other threads. The main thread is distinguished from other threads only in the sense that it is always the ﬁrst thread to run in the process. The main impact of this notion of a pool of peers is that a thread can kill any of its peers or wait for any of its peers to terminate. Further, each peer can read and write the same shared data. 12.3.2 Posix Threads Posix threads (Pthreads) is a standard interface for manipulating threads from C programs. It was adopted in 1995 and is available on all Linux systems. Pthreads deﬁnes about 60 functions that allow programs to create, kill, and reap threads, to share data safely with peer threads, and to notify peers about changes in the system state. Figure 12.13 shows a simple Pthreads program. The main thread creates a peer thread and then waits for it to terminate. The peer thread prints ¤−ÄÄÇj ÑÇËÄ®_¿Å and terminates. When the main thread detects that the peer thread has terminated, it terminates the process by calling −Ó©Î. This is the ﬁrst threaded program we have seen, so let us dissect it carefully. The code and local data for a thread are encapsulated in a thread routine. As shown by the prototype in line 2, each thread routine takes as input a single generic pointer and returns a generic pointer. If you want to pass multiple arguments to a thread routine, then you should put the arguments into a structure and pass a pointer to the structure. Similarly, if you code/conc/hello.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 3 4 ©ÅÎ Àþ©Åfg 5 Õ 6 ÉÎ³Ë−þ®ˆÎ Î©®y 7 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j ﬁ•‹‹gy 8 –Î³Ë−þ®ˆÁÇ©ÅfÎ©®j ﬁ•‹‹gy 9 −Ó©Îfngy 10 Û 11 12 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég mh ¶³Ë−þ® ËÇÏÎ©Å− hm 13 Õ 14 ÉË©ÅÎðf‘¤−ÄÄÇj ÑÇËÄ®_¿Å‘gy 15 Ë−ÎÏËÅ ﬁ•‹‹y 16 Û code/conc/hello.c Figure 12.13 ³−ÄÄÇl²: The Pthreads “Hello, world!” program. 1024 Chapter 12 Concurrent Programming want the thread routine to return multiple arguments, you can return a pointer to a structure. Line 4 marks the beginning of the code for the main thread. The main thread declares a single local variable Î©®, which will be used to store the thread ID of the peer thread (line 6). The main thread creates a new peer thread by calling the ÉÎ³Ë−þ®ˆ²Ë−þÎ− function (line 7). When the call to ÉÎ³Ë−þ®ˆ²Ë−þÎ− returns, the main thread and the newly created peer thread are running concurrently, and Î©® contains the ID of the new thread. The main thread waits for the peer thread to terminate with the call to ÉÎ³Ë−þ®ˆÁÇ©Å in line 8. Finally, the main thread calls −Ó©Î (line 9), which terminates all threads (in this case, just the main thread) currently running in the process. Lines 12–16 deﬁne the thread routine for the peer thread. It simply prints a string and then terminates the peer thread by executing the Ë−ÎÏËÅ statement in line 15. 12.3.3 Creating Threads Threads create other threads by calling the ÉÎ³Ë−þ®ˆ²Ë−þÎ− function. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ÎÔÉ−®−ð ÌÇ©® hfðÏÅ²gfÌÇ©® hgy ©ÅÎ ÉÎ³Ë−þ®ˆ²Ë−þÎ−fÉÎ³Ë−þ®ˆÎ hÎ©®j ÉÎ³Ë−þ®ˆþÎÎËˆÎ hþÎÎËj ðÏÅ² hðj ÌÇ©® hþË×gy Returns: 0 if OK, nonzero on error The ÉÎ³Ë−þ®ˆ²Ë−þÎ− function creates a new thread and runs the thread routine ð in the context of the new thread and with an input argument of þË×.The þÎÎË argument can be used to change the default attributes of the newly created thread. Changing these attributes is beyond our scope, and in our examples, we will always call ÉÎ³Ë−þ®ˆ²Ë−þÎ− with a NULL þÎÎË argument. When ÉÎ³Ë−þ®ˆ²Ë−þÎ− returns, argument Î©® contains the ID of the newly created thread. The new thread can determine its own thread ID by calling the ÉÎ³Ë−þ®ˆÍ−Äð function. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ÉÎ³Ë−þ®ˆÎ ÉÎ³Ë−þ®ˆÍ−ÄðfÌÇ©®gy Returns: thread ID of caller 12.3.4 Terminating Threads A thread terminates in one of the following ways: . The thread terminates implicitly when its top-level thread routine returns. Section 12.3 Concurrent Programming with Threads 1025 . The thread terminates explicitly by calling the ÉÎ³Ë−þ®ˆ−Ó©Î function. If the main thread calls ÉÎ³Ë−þ®ˆ−Ó©Î, it waits for all other peer threads to terminate and then terminates the main thread and the entire process with a return value of Î³Ë−þ®ˆË−ÎÏËÅ. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ÌÇ©® ÉÎ³Ë−þ®ˆ−Ó©ÎfÌÇ©® hÎ³Ë−þ®ˆË−ÎÏËÅgy Never returns . Some peer thread calls the Linux −Ó©Î function, which terminates the process and all threads associated with the process. . Another peer thread terminates the current thread by calling the ÉÎ³Ë−þ®ˆ ²þÅ²−Ä function with the ID of the current thread. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ©ÅÎ ÉÎ³Ë−þ®ˆ²þÅ²−ÄfÉÎ³Ë−þ®ˆÎ Î©®gy Returns: 0 if OK, nonzero on error 12.3.5 Reaping Terminated Threads Threads wait for other threads to terminate by calling the ÉÎ³Ë−þ®ˆÁÇ©Å function. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ©ÅÎ ÉÎ³Ë−þ®ˆÁÇ©ÅfÉÎ³Ë−þ®ˆÎ Î©®j ÌÇ©® hhÎ³Ë−þ®ˆË−ÎÏËÅgy Returns: 0 if OK, nonzero on error The ÉÎ³Ë−þ®ˆÁÇ©Å function blocks until thread Î©® terminates, assigns the generic fÌÇ©® hg pointer returned by the thread routine to the location pointed to by Î³Ë−þ®ˆË−ÎÏËÅ, and then reaps any memory resources held by the terminated thread. Notice that, unlike the Linux Ñþ©Î function, the ÉÎ³Ë−þ®ˆÁÇ©Å function can only wait for a speciﬁc thread to terminate. There is no way to instruct ÉÎ³Ë−þ®ˆ ÁÇ©Å to wait for an arbitrary thread to terminate. This can complicate our code by forcing us to use other, less intuitive mechanisms to detect process termination. Indeed, Stevens argues convincingly that this is a bug in the speciﬁcation [110]. 12.3.6 Detaching Threads At any point in time, a thread is joinable or detached. A joinable thread can be reaped and killed by other threads. Its memory resources (such as the stack) are not freed until it is reaped by another thread. In contrast, a detached thread cannot 1026 Chapter 12 Concurrent Programming be reaped or killed by other threads. Its memory resources are freed automatically by the system when it terminates. By default, threads are created joinable. In order to avoid memory leaks, each joinable thread should be either explicitly reaped by another thread or detached by a call to the ÉÎ³Ë−þ®ˆ®−Îþ²³ function. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ©ÅÎ ÉÎ³Ë−þ®ˆ®−Îþ²³fÉÎ³Ë−þ®ˆÎ Î©®gy Returns: 0 if OK, nonzero on error The ÉÎ³Ë−þ®ˆ®−Îþ²³ function detaches the joinable thread Î©®. Threads can detach themselves by calling ÉÎ³Ë−þ®ˆ®−Îþ²³ with an argument of ÉÎ³Ë−þ®ˆ Í−Äðfg. Although some of our examples will use joinable threads, there are good rea- sons to use detached threads in real programs. For example, a high-performance Web server might create a new peer thread each time it receives a connection re- quest from a Web browser. Since each connection is handled independently by a separate thread, it is unnecessary—and indeed undesirable—for the server to ex- plicitly wait for each peer thread to terminate. In this case, each peer thread should detach itself before it begins processing the request so that its memory resources can be reclaimed after it terminates. 12.3.7 Initializing Threads The ÉÎ³Ë−þ®ˆÇÅ²− function allows you to initialize the state associated with a thread routine. a©Å²ÄÏ®− zÉÎ³Ë−þ®l³| ÉÎ³Ë−þ®ˆÇÅ²−ˆÎ ÇÅ²−ˆ²ÇÅÎËÇÄ { –¶¤‡¥¡⁄ˆﬂﬁ£¥ˆ'ﬁ'¶y ©ÅÎ ÉÎ³Ë−þ®ˆÇÅ²−fÉÎ³Ë−þ®ˆÇÅ²−ˆÎ hÇÅ²−ˆ²ÇÅÎËÇÄj ÌÇ©® fh©Å©ÎˆËÇÏÎ©Å−gfÌÇ©®ggy Always returns 0 The ÇÅ²−ˆ²ÇÅÎËÇÄ variable is a global or static variable that is always initialized to PTHREAD_ONCE_INIT. The ﬁrst time you call ÉÎ³Ë−þ®ˆÇÅ²− with an ar- gument of ÇÅ²−ˆ²ÇÅÎËÇÄ, it invokes ©Å©ÎˆËÇÏÎ©Å−, which is a function with no input arguments that returns nothing. Subsequent calls to ÉÎ³Ë−þ®ˆÇÅ²− with the same ÇÅ²−ˆ²ÇÅÎËÇÄ variable do nothing. The ÉÎ³Ë−þ®ˆÇÅ²− function is useful whenever you need to dynamically initialize global variables that are shared by multiple threads. We will look at an example in Section 12.5.5. Section 12.3 Concurrent Programming with Threads 1027 12.3.8 A Concurrent Server Based on Threads Figure 12.14 shows the code for a concurrent echo server based on threads. The overall structure is similar to the process-based design. The main thread repeat- edly waits for a connection request and then creates a peer thread to handle the request. While the code looks simple, there are a couple of general and some- what subtle issues we need to look at more closely. The ﬁrst issue is how to pass code/conc/echoservert.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÌÇ©® −²³Çf©ÅÎ ²ÇÅÅð®gy 4 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 5 6 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 7 Õ 8 ©ÅÎ Ä©ÍÎ−Åð®j h²ÇÅÅð®Éy 9 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 10 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy 11 ÉÎ³Ë−þ®ˆÎ Î©®y 12 13 ©ð fþË×² _{ pg Õ 14 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 15 −Ó©Îfngy 16 Û 17 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 18 19 Ñ³©Ä− fog Õ 20 ²Ä©−ÅÎÄ−Å{Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×−gy 21 ²ÇÅÅð®É { ›þÄÄÇ²fÍ©Ö−Çðf©ÅÎggy 22 h²ÇÅÅð®É { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hg d²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 23 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j ²ÇÅÅð®Égy 24 Û 25 Û 26 27 mh ¶³Ë−þ® ËÇÏÎ©Å− hm 28 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 29 Õ 30 ©ÅÎ ²ÇÅÅð® { hff©ÅÎ hgÌþË×Égy 31 –Î³Ë−þ®ˆ®−Îþ²³fÉÎ³Ë−þ®ˆÍ−Äðfggy 32 ƒË−−fÌþË×Égy 33 −²³Çf²ÇÅÅð®gy 34 £ÄÇÍ−f²ÇÅÅð®gy 35 Ë−ÎÏËÅ ﬁ•‹‹y 36 Û code/conc/echoservert.c Figure 12.14 Concurrent echo server based on threads. 1028 Chapter 12 Concurrent Programming the connected descriptor to the peer thread when we call ÉÎ³Ë−þ®ˆ²Ë−þÎ−.The obvious approach is to pass a pointer to the descriptor, as in the following: ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hg d²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j d²ÇÅÅð®gy Then we have the peer thread dereference the pointer and assign it to a local variable, as follows: ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég Õ ©ÅÎ ²ÇÅÅð® { hff©ÅÎ hgÌþË×Égy l l l Û This would be wrong, however, because it introduces a race between the assign- ment statement in the peer thread and the þ²²−ÉÎ statement in the main thread. If the assignment statement completes before the next þ²²−ÉÎ, then the local ²ÇÅÅð® variable in the peer thread gets the correct descriptor value. However, if the as- signment completes after the þ²²−ÉÎ, then the local ²ÇÅÅð® variable in the peer thread gets the descriptor number of the next connection. The unhappy result is that two threads are now performing input and output on the same descriptor. In order to avoid the potentially deadly race, we must assign each connected descrip- tor returned by þ²²−ÉÎ to its own dynamically allocated memory block, as shown in lines 21–22. We will return to the issue of races in Section 12.7.4. Another issue is avoiding memory leaks in the thread routine. Since we are not explicitly reaping threads, we must detach each thread so that its memory resources will be reclaimed when it terminates (line 31). Further, we must be careful to free the memory block that was allocated by the main thread (line 32). Practice Problem 12.5 (solution page 1072) In the process-based server in Figure 12.5, we observed that there is no memory leak and the code remains correct even when line 33 is deleted. In the threads- based server in Figure 12.14, are there any chances of memory leak if lines 31 or 32 are deleted. Why? 12.4 Shared Variables in Threaded Programs From a programmer’s perspective, one of the attractive aspects of threads is the ease with which multiple threads can share the same program variables. However, this sharing can be tricky. In order to write correctly threaded programs, we must have a clear understanding of what we mean by sharing and how it works. There are some basic questions to work through in order to understand whether a variable in a C program is shared or not: (1) What is the underlying memory model for threads? (2) Given this model, how are instances of the vari- able mapped to memory? (3) Finally, how many threads reference each of these Section 12.4 Shared Variables in Threaded Programs 1029 code/conc/sharing.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ﬁ p 3 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 4 5 ²³þË hhÉÎËy mh §ÄÇ¾þÄ ÌþË©þ¾Ä− hm 6 7 ©ÅÎ Àþ©Åfg 8 Õ 9 ©ÅÎ ©y 10 ÉÎ³Ë−þ®ˆÎ Î©®y 11 ²³þË hÀÍ×Í‰ﬁ` { Õ 12 ‘¤−ÄÄÇ ðËÇÀ ðÇÇ‘j 13 ‘¤−ÄÄÇ ðËÇÀ ¾þË‘ 14 Ûy 15 16 ÉÎË { ÀÍ×Íy 17 ðÇËf©{ny©zﬁy ©iig 18 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j fÌÇ©® hg©gy 19 –Î³Ë−þ®ˆ−Ó©Îfﬁ•‹‹gy 20 Û 21 22 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 23 Õ 24 ©ÅÎ ÀÔ©® { f©ÅÎgÌþË×Éy 25 ÍÎþÎ©² ©ÅÎ ²ÅÎ { ny 26 ÉË©ÅÎðf‘‰c®`x cÍ f²ÅÎ{c®g¿Å‘j ÀÔ©®j ÉÎË‰ÀÔ©®`j ii²ÅÎgy 27 Ë−ÎÏËÅ ﬁ•‹‹y 28 Û code/conc/sharing.c Figure 12.15 Example program that illustrates different aspects of sharing. instances? The variable is shared if and only if multiple threads reference some instance of the variable. To keep our discussion of sharing concrete, we will use the program in Fig- ure 12.15 as a running example. Although somewhat contrived, it is nonetheless useful to study because it illustrates a number of subtle points about sharing. The example program consists of a main thread that creates two peer threads. The main thread passes a unique ID to each peer thread, which uses the ID to print a personalized message along with a count of the total number of times that the thread routine has been invoked. 12.4.1 Threads Memory Model A pool of concurrent threads runs in the context of a process. Each thread has its own separate thread context, which includes a thread ID, stack, stack pointer, 1030 Chapter 12 Concurrent Programming program counter, condition codes, and general-purpose register values. Each thread shares the rest of the process context with the other threads. This includes the entire user virtual address space, which consists of read-only text (code), read/write data, the heap, and any shared library code and data areas. The threads also share the same set of open ﬁles. In an operational sense, it is impossible for one thread to read or write the register values of another thread. On the other hand, any thread can access any location in the shared virtual memory. If some thread modiﬁes a memory location, then every other thread will eventually see the change if it reads that location. Thus, registers are never shared, whereas virtual memory is always shared. The memory model for the separate thread stacks is not as clean. These stacks are contained in the stack area of the virtual address space and are usually accessed independently by their respective threads. We say usually rather than always, because different thread stacks are not protected from other threads. So if a thread somehow manages to acquire a pointer to another thread’s stack, then it can read and write any part of that stack. Our example program shows this in line 26, where the peer threads reference the contents of the main thread’s stack indirectly through the global ÉÎË variable. 12.4.2 Mapping Variables to Memory Variables in threaded C programs are mapped to virtual memory according to their storage classes: Global variables.A global variable is any variable declared outside of a func- tion. At run time, the read/write area of virtual memory contains exactly one instance of each global variable that can be referenced by any thread. For example, the global ÉÎË variable declared in line 5 has one run-time instance in the read/write area of virtual memory. When there is only one instance of a variable, we will denote the instance by simply using the variable name—in this case, ÉÎË. Local automatic variables.A local automatic variable is one that is declared inside a function without the ÍÎþÎ©² attribute. At run time, each thread’s stack contains its own instances of any local automatic variables. This is true even if multiple threads execute the same thread routine. For example, there is one instance of the local variable Î©®, and it resides on the stack of the main thread. We will denote this instance as Î©®lÀ. As another example, there are two instances of the local variable ÀÔ©®, one instance on the stack of peer thread 0 and the other on the stack of peer thread 1. We will denote these instances as ÀÔ©®lÉn and ÀÔ©®lÉo, respectively. Local static variables.A local static variable is one that is declared inside a func- tion with the ÍÎþÎ©² attribute. As with global variables, the read/write area of virtual memory contains exactly one instance of each local static Section 12.5 Synchronizing Threads with Semaphores 1031 variable declared in a program. For example, even though each peer thread in our example program declares ²ÅÎ in line 25, at run time there is only one instance of ²ÅÎ residing in the read/write area of virtual memory. Each peer thread reads and writes this instance. 12.4.3 Shared Variables We say that a variable v is shared if and only if one of its instances is referenced by more than one thread. For example, variable ²ÅÎ in our example program is shared because it has only one run-time instance and this instance is referenced by both peer threads. On the other hand, ÀÔ©® is not shared, because each of its two instances is referenced by exactly one thread. However, it is important to realize that local automatic variables such as ÀÍ×Í can also be shared. Practice Problem 12.6 (solution page 1072) A. Using the analysis from Section 12.4, ﬁll each entry in the following table with “Yes” or “No” for the example program in Figure 12.15. In the ﬁrst column, the notation v.t denotes an instance of variable v residing on the local stack for thread t, where t is either À (main thread), Én (peer thread 0), or Éo (peer thread 1). Referenced byVariable instance main thread? peer thread 0? peer thread 1? ÉÎË ²ÅÎ ©lÀ ÀÍ×ÍlÀ ÀÔ©®lÉn ÀÔ©®lÉo B. Given the analysis in part A, which of the variables ÉÎË, ²ÅÎ, ©, ÀÍ×Í, and ÀÔ©® are shared? 12.5 Synchronizing Threads with Semaphores Shared variables can be convenient, but they introduce the possibility of nasty synchronization errors. Consider the ¾þ®²ÅÎl² program in Figure 12.16, which creates two threads, each of which increments a global shared counter variable called ²ÅÎ. Since each thread increments the counter Å©Î−ËÍ times, we expect its ﬁnal value to be 2 × Å©Î−ËÍ. This seems quite simple and straightforward. However, when we run ¾þ®²ÅÎl² on our Linux system, we not only get wrong answers, we get different answers each time! 1032 Chapter 12 Concurrent Programming code/conc/badcnt.c 1 mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ô_ hm 2 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 3 4 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy mh ¶³Ë−þ® ËÇÏÎ©Å− ÉËÇÎÇÎÔÉ− hm 5 6 mh §ÄÇ¾þÄ Í³þË−® ÌþË©þ¾Ä− hm 7 ÌÇÄþÎ©Ä− ÄÇÅ× ²ÅÎ { ny mh £ÇÏÅÎ−Ë hm 8 9 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 10 Õ 11 ÄÇÅ× Å©Î−ËÍy 12 ÉÎ³Ë−þ®ˆÎ Î©®oj Î©®py 13 14 mh £³−²Â ©ÅÉÏÎ þË×ÏÀ−ÅÎ hm 15 ©ð fþË×² _{ pg Õ 16 ÉË©ÅÎðf‘ÏÍþ×−x cÍ zÅ©Î−ËÍ|¿Å‘j þË×Ì‰n`gy 17 −Ó©Îfngy 18 Û 19 Å©Î−ËÍ { þÎÇ©fþË×Ì‰o`gy 20 21 mh £Ë−þÎ− Î³Ë−þ®Í þÅ® Ñþ©Î ðÇË Î³−À ÎÇ ð©Å©Í³ hm 22 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®oj ﬁ•‹‹j Î³Ë−þ®j dÅ©Î−ËÍgy 23 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®pj ﬁ•‹‹j Î³Ë−þ®j dÅ©Î−ËÍgy 24 –Î³Ë−þ®ˆÁÇ©ÅfÎ©®oj ﬁ•‹‹gy 25 –Î³Ë−þ®ˆÁÇ©ÅfÎ©®pj ﬁ•‹‹gy 26 27 mh £³−²Â Ë−ÍÏÄÎ hm 28 ©ð f²ÅÎ _{ fp h Å©Î−ËÍgg 29 ÉË©ÅÎðf‘¢ﬂﬂ›_ ²ÅÎ{cÄ®¿Å‘j ²ÅÎgy 30 −ÄÍ− 31 ÉË©ÅÎðf‘ﬂ« ²ÅÎ{cÄ®¿Å‘j ²ÅÎgy 32 −Ó©Îfngy 33 Û 34 35 mh ¶³Ë−þ® ËÇÏÎ©Å− hm 36 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 37 Õ 38 ÄÇÅ× ©j Å©Î−ËÍ { hffÄÇÅ× hgÌþË×Égy 39 40 ðÇË f© { ny © z Å©Î−ËÍy ©iig 41 ²ÅÎiiy 42 43 Ë−ÎÏËÅ ﬁ•‹‹y 44 Û code/conc/badcnt.c Figure 12.16 ¾þ®²ÅÎl²: An improperly synchronized counter program. Section 12.5 Synchronizing Threads with Semaphores 1033 Ä©ÅÏÓ| ./badcnt 1000000 ¢ﬂﬂ›_ ²ÅÎ{orrsnvs Ä©ÅÏÓ| ./badcnt 1000000 ¢ﬂﬂ›_ ²ÅÎ{owosppn Ä©ÅÏÓ| ./badcnt 1000000 ¢ﬂﬂ›_ ²ÅÎ{ornrurt So what went wrong? To understand the problem clearly, we need to study the assembly code for the counter loop (lines 40–41), as shown in Figure 12.17. We will ﬁnd it helpful to partition the loop code for thread i into ﬁve parts: Hi: The block of instructions at the head of the loop Li: The instruction that loads the shared variable ²ÅÎ into the accumulator register cË®Ói, where cË®Ói denotes the value of register cË®Ó in thread i Ui: The instruction that updates (increments) cË®Ói Si: The instruction that stores the updated value of cË®Ói back to the shared variable ²ÅÎ Ti: The block of instructions at the tail of the loop Notice that the head and tail manipulate only local stack variables, while Li, Ui, and Si manipulate the contents of the shared counter variable. When the two peer threads in ¾þ®²ÅÎl² run concurrently on a uniprocessor, the machine instructions are completed one after the other in some order. Thus, each concurrent execution deﬁnes some total ordering (or interleaving) of the in- structions in the two threads. Unfortunately, some of these orderings will produce correct results, but others will not. C code for thread i Asm code for thread i for (i = 0; i < niters; i++) cnt++; movq (%rdi), %rcx testq %rcx, %rcx jle .L2 movl $0, %eax .L3: movq cnt(%rip),%rdx addq %eax movq %eax,cnt(%rip) addq $1, %rax cmpq %rcx, %rax jne .L3 .L2: Hi : Head Ti : Tail Li : Load cnt Ui : Update cnt Si : Store cnt Figure 12.17 Assembly code for the counter loop (lines 40–41) in ¾þ®²ÅÎl². 1034 Chapter 12 Concurrent Programming (a) Correct ordering Step Thread Instr. cË®Ó1 cË®Ó2 ²ÅÎ 11 H1 —— 0 21 L1 0— 0 31 U1 1— 0 41 S1 1— 1 52 H2 —— 1 62 L2 —1 1 72 U2 —2 1 82 S2 —2 2 92 T2 —2 2 10 1 T1 1— 2 (b) Incorrect ordering Step Thread Instr. cË®Ó1 cË®Ó2 ²ÅÎ 11 H1 —— 0 21 L1 0— 0 31 U1 1— 0 42 H2 —— 0 52 L2 —0 0 61 S1 1— 1 71 T1 1— 1 82 U2 —1 1 92 S2 —1 1 10 2 T2 —1 1 Figure 12.18 Instruction orderings for the ﬁrst loop iteration in ¾þ®²ÅÎl². Here is the crucial point: In general, there is no way for you to predict whether the operating system will choose a correct ordering for your threads. For example, Figure 12.18(a) shows the step-by-step operation of a correct instruction ordering. After each thread has updated the shared variable ²ÅÎ, its value in memory is 2, which is the expected result. On the other hand, the ordering in Figure 12.18(b) produces an incorrect value for ²ÅÎ. The problem occurs because thread 2 loads ²ÅÎ in step 5, after thread 1 loads ²ÅÎ in step 2 but before thread 1 stores its updated value in step 6. Thus, each thread ends up storing an updated counter value of 1. We can clarify these notions of correct and incorrect instruction orderings with the help of a device known as a progress graph, which we introduce in the next section. Practice Problem 12.7 (solution page 1073) Complete the table for the following instruction ordering of ¾þ®²ÅÎl²: Step Thread Instr. cË®Ó1 cË®Ó2 ²ÅÎ 11 H1 —— 0 21 L1 32 H2 42 L2 52 U2 62 S2 71 U1 Step Thread Instr. cË®Ó1 cË®Ó2 ²ÅÎ 81 S1 91 T1 Section 12.5 Synchronizing Threads with Semaphores 1035 10 2 T2 Does this ordering result in a correct value for ²ÅÎ? 12.5.1 Progress Graphs A progress graph models the execution of n concurrent threads as a trajectory through an n-dimensional Cartesian space. Each axis k corresponds to the progress of thread k. Each point (I1,I2,. . .,In) represents the state where thread k (k = 1,. . .,n) has completed instruction Ik. The origin of the graph corresponds to the initial state where none of the threads has yet completed an instruction. Figure 12.19 shows the two-dimensional progress graph for the ﬁrst loop iteration of the ¾þ®²ÅÎl² program. The horizontal axis corresponds to thread 1, the vertical axis to thread 2. Point (L1,S2) corresponds to the state where thread 1 has completed L1 and thread 2 has completed S2. A progress graph models instruction execution as a transition from one state to another. A transition is represented as a directed edge from one point to an adjacent point. Legal transitions move to the right (an instruction in thread 1 completes) or up (an instruction in thread 2 completes). Two instructions cannot complete at the same time—diagonal transitions are not allowed. Programs never run backward so transitions that move down or to the left are not legal either. Figure 12.19 Progress graph for the ﬁrst loop iteration of ¾þ®²ÅÎl². Thread 2 Thread 1 T2 S2 U2 L2 H2 H1 L1 U1 S1 T1 (L1, S2) 1036 Chapter 12 Concurrent Programming Figure 12.20 An example trajectory. Thread 2 Thread 1 T2 S2 U2 L2 H2 H1 L1 U1 S1 T1 The execution history of a program is modeled as a trajectory through the state space. Figure 12.20 shows the trajectory that corresponds to the following instruction ordering: H1,L1,U1,H2,L2,S1,T1,U2,S2,T2 For thread i, the instructions (Li,Ui,Si) that manipulate the contents of the shared variable ²ÅÎ constitute a critical section (with respect to shared variable ²ÅÎ) that should not be interleaved with the critical section of the other thread. In other words, we want to ensure that each thread has mutually exclusive access to the shared variable while it is executing the instructions in its critical section. The phenomenon in general is known as mutual exclusion. On the progress graph, the intersection of the two critical sections deﬁnes a region of the state space known as an unsafe region. Figure 12.21 shows the unsafe region for the variable ²ÅÎ. Notice that the unsafe region abuts, but does not include, the states along its perimeter. For example, states (H1,H2) and (S1,U2) abut the unsafe region, but they are not part of it. A trajectory that skirts the unsafe region is known as a safe trajectory. Conversely, a trajectory that touches any part of the unsafe region is an unsafe trajectory. Figure 12.21 shows examples of safe and unsafe trajectories through the state space of our example ¾þ®²ÅÎl² program. The upper trajectory skirts the unsafe region along its left and top sides, and thus is safe. The lower trajectory crosses the unsafe region, and thus is unsafe. Any safe trajectory will correctly update the shared counter. In order to guarantee correct execution of our example threaded program—and indeed any concurrent program that shares global data structures—we must somehow syn- chronize the threads so that they always have a safe trajectory. A classic approach is based on the idea of a semaphore, which we introduce next. Section 12.5 Synchronizing Threads with Semaphores 1037 Figure 12.21 Safe and unsafe trajectories. The intersection of the critical regions forms an unsafe region. Trajectories that skirt the unsafe region correctly update the counter variable. Thread 2 Critical section wrt cnt Critical section wrt cnt Thread 1 T2 S2 U2 L2 H2 H1 L1 U1 S1 T1 Unsafe region Unsafe trajectory Safe trajectory Practice Problem 12.8 (solution page 1074) Using the progress graph in Figure 12.21, classify the following trajectories as either safe or unsafe. A. H1,L1,U1,S1,H2,L2,U2,S2,T2,T1 B. H2,L2,H1,L1,U1,S1,T1,U2,S2,T2 C. H1,H2,L2,U2,S2,L1,U1,S1,T1,T2 12.5.2 Semaphores Edsger Dijkstra, a pioneer of concurrent programming, proposed a classic solution to the problem of synchronizing different execution threads based on a special type of variable called a semaphore. A semaphore, s, is a global variable with a nonnegative integer value that can only be manipulated by two special operations, called P and V : P(s):If s is nonzero, then P decrements s and returns immediately. If s is zero, then suspend the thread until s becomes nonzero and the thread is restarted by a V operation. After restarting, the P operation decrements s and returns control to the caller. V(s):The V operation increments s by 1. If there are any threads blocked at a P operation waiting for s to become nonzero, then the V operation restarts exactly one of these threads, which then completes its P operation by decrementing s. 1038 Chapter 12 Concurrent Programming Aside Origin of the names P and V Edsger Dijkstra (1930–2002) was originally from the Netherlands. The names P and V come from the Dutch words proberen (to test) and verhogen (to increment). The test and decrement operations in P occur indivisibly, in the sense that once the semaphore s becomes nonzero, the decrement of s occurs without in- terruption. The increment operation in V also occurs indivisibly, in that it loads, increments, and stores the semaphore without interruption. Notice that the deﬁ- nition of V does not deﬁne the order in which waiting threads are restarted. The only requirement is that the V must restart exactly one waiting thread. Thus, when several threads are waiting at a semaphore, you cannot predict which one will be restarted as a result of the V . The deﬁnitions of P and V ensure that a running program can never enter a state where a properly initialized semaphore has a negative value. This property, known as the semaphore invariant, provides a powerful tool for controlling the trajectories of concurrent programs, as we shall see in the next section. The Posix standard deﬁnes a variety of functions for manipulating sema- phores. a©Å²ÄÏ®− zÍ−ÀþÉ³ÇË−l³| ©ÅÎ Í−Àˆ©Å©ÎfÍ−ÀˆÎ hÍ−Àj nj ÏÅÍ©×Å−® ©ÅÎ ÌþÄÏ−gy ©ÅÎ Í−ÀˆÑþ©ÎfÍ−ÀˆÎ hÍgy mh –fÍg hm ©ÅÎ Í−ÀˆÉÇÍÎfÍ−ÀˆÎ hÍgy mh ‚fÍg hm Returns: 0 if OK, −1 on error The Í−Àˆ©Å©Î function initializes semaphore Í−À to ÌþÄÏ−. Each semaphore must be initialized before it can be used. For our purposes, the middle argument is always 0. Programs perform P and V operations by calling the Í−ÀˆÑþ©Î and Í−ÀˆÉÇÍÎ functions, respectively. For conciseness, we prefer to use the following equivalent P and V wrapper functions instead: a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ÌÇ©® –fÍ−ÀˆÎ hÍgy mh „ËþÉÉ−Ë ðÏÅ²Î©ÇÅ ðÇË Í−ÀˆÑþ©Î hm ÌÇ©® ‚fÍ−ÀˆÎ hÍgy mh „ËþÉÉ−Ë ðÏÅ²Î©ÇÅ ðÇË Í−ÀˆÉÇÍÎ hm Returns: nothing 12.5.3 Using Semaphores for Mutual Exclusion Semaphores provide a convenient way to ensure mutually exclusive access to shared variables. The basic idea is to associate a semaphore s, initially 1, with Section 12.5 Synchronizing Threads with Semaphores 1039 Thread 2 Thread 1 S2 T2 U2 L2 P(s) H2 H1 P(s) L1 U1 S1 V(s) V(s) T1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 –1 –1 –1 –1 0 0 –1 –1 –1 –1 0 0 –1 –1 –1 –1 0 0 –1 –1 –1 –1 1 1 0 0 0 0 1 1 0 0 0 0 110000 1 1 110000 1 1 Unsafe region Forbidden region Initially s\u00051 Figure 12.22 Using semaphores for mutual exclusion. The infeasible states where s< 0 deﬁne a forbidden region that surrounds the unsafe region and prevents any feasible trajectory from touching the unsafe region. each shared variable (or related set of shared variables) and then surround the corresponding critical section with P(s) and V(s) operations. A semaphore that is used in this way to protect shared variables is called a binary semaphore because its value is always 0 or 1. Binary semaphores whose purpose is to provide mutual exclusion are often called mutexes. Performing a P operation on a mutex is called locking the mutex. Similarly, performing the V operation is called unlocking the mutex. A thread that has locked but not yet unlocked a mutex is said to be holding the mutex. A semaphore that is used as a counter for a set of available resources is called a counting semaphore. The progress graph in Figure 12.22 shows how we would use binary sema- phores to properly synchronize our example counter program. Each state is labeled with the value of semaphore s in that state. The crucial idea is that this combination of P and V operations creates a collection of states, called a forbidden region, where s< 0. Because of the semaphore invariant, no feasible trajectory can include one of the states in the forbidden region. And since the forbidden region completely encloses the unsafe region, no feasible trajectory can touch any part of the unsafe region. Thus, every feasible trajectory is safe, and regardless of the ordering of the instructions at run time, the program correctly increments the counter. 1040 Chapter 12 Concurrent Programming Aside Limitations of progress graphs Progress graphs give us a nice way to visualize concurrent program execution on uniprocessors and to understand why we need synchronization. However, they do have limitations, particularly with respect to concurrent execution on multiprocessors, where a set of CPU/cache pairs share the same main memory. Multiprocessors behave in ways that cannot be explained by progress graphs. In particular, a multiprocessor memory system can be in a state that does not correspond to any trajectory in a progress graph. Regardless, the message remains the same: always synchronize accesses to your shared variables, regardless if you’re running on a uniprocessor or a multiprocessor. In an operational sense, the forbidden region created by the P and V op- erations makes it impossible for multiple threads to be executing instructions in the enclosed critical region at any point in time. In other words, the semaphore operations ensure mutually exclusive access to the critical region. Putting it all together, to properly synchronize the example counter program in Figure 12.16 using semaphores, we ﬁrst declare a semaphore called ÀÏÎ−Ó: ÌÇÄþÎ©Ä− ÄÇÅ× ²ÅÎ { ny mh £ÇÏÅÎ−Ë hm Í−ÀˆÎ ÀÏÎ−Óy mh ·−ÀþÉ³ÇË− Î³þÎ ÉËÇÎ−²ÎÍ ²ÇÏÅÎ−Ë hm and then we initialize it to unity in the main routine: ·−Àˆ©Å©ÎfdÀÏÎ−Ój nj ogy mh ÀÏÎ−Ó{ohm Finally, we protect the update of the shared ²ÅÎ variable in the thread routine by surrounding it with P and V operations: ðÇË f© { ny © z Å©Î−ËÍy ©iig Õ –fdÀÏÎ−Ógy ²ÅÎiiy ‚fdÀÏÎ−Ógy Û When we run the properly synchronized program, it now produces the correct answer each time. Ä©ÅÏÓ| ./goodcnt 1000000 ﬂ« ²ÅÎ{pnnnnnn Ä©ÅÏÓ| ./goodcnt 1000000 ﬂ« ²ÅÎ{pnnnnnn 12.5.4 Using Semaphores to Schedule Shared Resources Another important use of semaphores, besides providing mutual exclusion, is to schedule accesses to shared resources. In this scenario, a thread uses a semaphore Section 12.5 Synchronizing Threads with Semaphores 1041 Producer thread Consumer thread Bounded buffer Figure 12.23 Producer-consumer problem. The producer generates items and inserts them into a bounded buffer. The consumer removes items from the buffer and then consumes them. operation to notify another thread that some condition in the program state has become true. Two classical and useful examples are the producer-consumer and readers-writers problems. Producer-Consumer Problem The producer-consumer problem is shown in Figure 12.23. A producer and con- sumer thread share a bounded buffer with n slots. The producer thread repeatedly produces new items and inserts them in the buffer. The consumer thread repeat- edly removes items from the buffer and then consumes (uses) them. Variants with multiple producers and consumers are also possible. Since inserting and removing items involves updating shared variables, we must guarantee mutually exclusive access to the buffer. But guaranteeing mutual exclusion is not sufﬁcient. We also need to schedule accesses to the buffer. If the buffer is full (there are no empty slots), then the producer must wait until a slot becomes available. Similarly, if the buffer is empty (there are no available items), then the consumer must wait until an item becomes available. Producer-consumer interactions occur frequently in real systems. For exam- ple, in a multimedia system, the producer might encode video frames while the consumer decodes and renders them on the screen. The purpose of the buffer is to reduce jitter in the video stream caused by data-dependent differences in the encoding and decoding times for individual frames. The buffer provides a reser- voir of slots to the producer and a reservoir of encoded frames to the consumer. Another common example is the design of graphical user interfaces. The producer detects mouse and keyboard events and inserts them in the buffer. The consumer removes the events from the buffer in some priority-based manner and paints the screen. In this section, we will develop a simple package, called Sbuf, for building producer-consumer programs. In the next section, we look at how to use it to build an interesting concurrent server based on prethreading. Sbuf manipulates bounded buffers of type Í¾ÏðˆÎ (Figure 12.24). Items are stored in a dynamically allocated integer array (¾Ïð) with Å items. The ðËÇÅÎ and Ë−þË indices keep track of the ﬁrst and last items in the array. Three semaphores synchronize access to the buffer. The ÀÏÎ−Ó semaphore provides mutually exclusive buffer access. Semaphores ÍÄÇÎÍ and ©Î−ÀÍ are counting semaphores that count the number of empty slots and available items, respectively. 1042 Chapter 12 Concurrent Programming code/conc/sbuf.h 1 ÎÔÉ−®−ð ÍÎËÏ²Î Õ 2 ©ÅÎ h¾Ïðy mh ¢Ïðð−Ë þËËþÔ hm 3 ©ÅÎ Åy mh ›þÓ©ÀÏÀ ÅÏÀ¾−Ë Çð ÍÄÇÎÍ hm 4 ©ÅÎ ðËÇÅÎy mh ¾Ïð‰fðËÇÅÎiogcÅ` ©Í ð©ËÍÎ ©Î−À hm 5 ©ÅÎ Ë−þËy mh ¾Ïð‰Ë−þËcÅ` ©Í ÄþÍÎ ©Î−À hm 6 Í−ÀˆÎ ÀÏÎ−Óy mh –ËÇÎ−²ÎÍ þ²²−ÍÍ−Í ÎÇ ¾Ïð hm 7 Í−ÀˆÎ ÍÄÇÎÍy mh £ÇÏÅÎÍ þÌþ©Äþ¾Ä− ÍÄÇÎÍ hm 8 Í−ÀˆÎ ©Î−ÀÍy mh £ÇÏÅÎÍ þÌþ©Äþ¾Ä− ©Î−ÀÍ hm 9 Û Í¾ÏðˆÎy code/conc/sbuf.h Figure 12.24 Í¾ÏðˆÎ: Bounded buffer used by the Sbuf package. Figure 12.25 shows the implementation of the Sbuf package. The Í¾Ïðˆ©Å©Î function allocates heap memory for the buffer, sets ðËÇÅÎ and Ë−þË to indicate an empty buffer, and assigns initial values to the three semaphores. This function is called once, before calls to any of the other three functions. The Í¾Ïðˆ®−©Å©Î function frees the buffer storage when the application is through using it. The Í¾Ïðˆ©ÅÍ−ËÎ function waits for an available slot, locks the mutex, adds the item, unlocks the mutex, and then announces the availability of a new item. The Í¾Ïðˆ Ë−ÀÇÌ− function is symmetric. After waiting for an available buffer item, it locks the mutex, removes the item from the front of the buffer, unlocks the mutex, and then signals the availability of a new slot. Practice Problem 12.9 (solution page 1074) Let p denote the number of producers, c the number of consumers, and n the buffer size in units of items. For each of the following scenarios, indicate whether the mutex semaphore in Í¾Ïðˆ©ÅÍ−ËÎ and Í¾ÏðˆË−ÀÇÌ− is necessary or not. A. p = 1, c = 1, n> 1 B. p = 1, c = 1, n = 1 C. p> 1, c> 1, n = 1 Readers-Writers Problem The readers-writers problem is a generalization of the mutual exclusion problem. A collection of concurrent threads is accessing a shared object such as a data structure in main memory or a database on disk. Some threads only read the object, while others modify it. Threads that modify the object are called writers. Threads that only read it are called readers. Writers must have exclusive access to the object, but readers may share the object with an unlimited number of other readers. In general, there are an unbounded number of concurrent readers and writers. Section 12.5 Synchronizing Threads with Semaphores 1043 code/conc/sbuf.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a©Å²ÄÏ®− ‘Í¾Ïðl³‘ 3 4 mh £Ë−þÎ− þÅ −ÀÉÎÔj ¾ÇÏÅ®−®j Í³þË−® ƒ'ƒﬂ ¾Ïðð−Ë Ñ©Î³ Å ÍÄÇÎÍ hm 5 ÌÇ©® Í¾Ïðˆ©Å©ÎfÍ¾ÏðˆÎ hÍÉj ©ÅÎ Åg 6 Õ 7 ÍÉk|¾Ïð { £þÄÄÇ²fÅj Í©Ö−Çðf©ÅÎggy 8 ÍÉk|Å { Åy mh ¢Ïðð−Ë ³ÇÄ®Í ÀþÓ Çð Å ©Î−ÀÍ hm 9 ÍÉk|ðËÇÅÎ { ÍÉk|Ë−þË { ny mh ¥ÀÉÎÔ ¾Ïðð−Ë ©ðð ðËÇÅÎ {{ Ë−þË hm 10 ·−Àˆ©Å©ÎfdÍÉk|ÀÏÎ−Ój nj ogy mh ¢©ÅþËÔ Í−ÀþÉ³ÇË− ðÇË ÄÇ²Â©Å× hm 11 ·−Àˆ©Å©ÎfdÍÉk|ÍÄÇÎÍj nj Ågy mh 'Å©Î©þÄÄÔj ¾Ïð ³þÍ Å −ÀÉÎÔ ÍÄÇÎÍ hm 12 ·−Àˆ©Å©ÎfdÍÉk|©Î−ÀÍj nj ngy mh 'Å©Î©þÄÄÔj ¾Ïð ³þÍ Ö−ËÇ ®þÎþ ©Î−ÀÍ hm 13 Û 14 15 mh £Ä−þÅ ÏÉ ¾Ïðð−Ë ÍÉ hm 16 ÌÇ©® Í¾Ïðˆ®−©Å©ÎfÍ¾ÏðˆÎ hÍÉg 17 Õ 18 ƒË−−fÍÉk|¾Ïðgy 19 Û 20 21 mh 'ÅÍ−ËÎ ©Î−À ÇÅÎÇ Î³− Ë−þË Çð Í³þË−® ¾Ïðð−Ë ÍÉ hm 22 ÌÇ©® Í¾Ïðˆ©ÅÍ−ËÎfÍ¾ÏðˆÎ hÍÉj ©ÅÎ ©Î−Àg 23 Õ 24 –fdÍÉk|ÍÄÇÎÍgy mh „þ©Î ðÇË þÌþ©Äþ¾Ä− ÍÄÇÎ hm 25 –fdÍÉk|ÀÏÎ−Ógy mh ‹Ç²Â Î³− ¾Ïðð−Ë hm 26 ÍÉk|¾Ïð‰fiiÍÉk|Ë−þËgcfÍÉk|Åg` { ©Î−Ày mh 'ÅÍ−ËÎ Î³− ©Î−À hm 27 ‚fdÍÉk|ÀÏÎ−Ógy mh •ÅÄÇ²Â Î³− ¾Ïðð−Ë hm 28 ‚fdÍÉk|©Î−ÀÍgy mh ¡ÅÅÇÏÅ²− þÌþ©Äþ¾Ä− ©Î−À hm 29 Û 30 31 mh ‡−ÀÇÌ− þÅ® Ë−ÎÏËÅ Î³− ð©ËÍÎ ©Î−À ðËÇÀ ¾Ïðð−Ë ÍÉ hm 32 ©ÅÎ Í¾ÏðˆË−ÀÇÌ−fÍ¾ÏðˆÎ hÍÉg 33 Õ 34 ©ÅÎ ©Î−Ày 35 –fdÍÉk|©Î−ÀÍgy mh „þ©Î ðÇË þÌþ©Äþ¾Ä− ©Î−À hm 36 –fdÍÉk|ÀÏÎ−Ógy mh ‹Ç²Â Î³− ¾Ïðð−Ë hm 37 ©Î−À { ÍÉk|¾Ïð‰fiiÍÉk|ðËÇÅÎgcfÍÉk|Åg`y mh ‡−ÀÇÌ− Î³− ©Î−À hm 38 ‚fdÍÉk|ÀÏÎ−Ógy mh •ÅÄÇ²Â Î³− ¾Ïðð−Ë hm 39 ‚fdÍÉk|ÍÄÇÎÍgy mh ¡ÅÅÇÏÅ²− þÌþ©Äþ¾Ä− ÍÄÇÎ hm 40 Ë−ÎÏËÅ ©Î−Ày 41 Û code/conc/sbuf.c Figure 12.25 Sbuf: A package for synchronizing concurrent access to bounded buffers. 1044 Chapter 12 Concurrent Programming Readers-writers interactions occur frequently in real systems. For example, in an online airline reservation system, an unlimited number of customers are al- lowed to concurrently inspect the seat assignments, but a customer who is booking a seat must have exclusive access to the database. As another example, in a multi- threaded caching Web proxy, an unlimited number of threads can fetch existing pages from the shared page cache, but any thread that writes a new page to the cache must have exclusive access. The readers-writers problem has several variations, each based on the priori- ties of readers and writers. The ﬁrst readers-writers problem, which favors readers, requires that no reader be kept waiting unless a writer has already been granted permission to use the object. In other words, no reader should wait simply because a writer is waiting. The second readers-writers problem, which favors writers, re- quires that once a writer is ready to write, it performs its write as soon as possible. Unlike the ﬁrst problem, a reader that arrives after a writer must wait, even if the writer is also waiting. Figure 12.26 shows a solution to the ﬁrst readers-writers problem. Like the solutions to many synchronization problems, it is subtle and deceptively simple. The Ñ semaphore controls access to the critical sections that access the shared object. The ÀÏÎ−Ó semaphore protects access to the shared Ë−þ®²ÅÎ variable, which counts the number of readers currently in the critical section. A writer locks the Ñ mutex each time it enters the critical section and unlocks it each time it leaves. This guarantees that there is at most one writer in the critical section at any point in time. On the other hand, only the ﬁrst reader to enter the critical section locks Ñ, and only the last reader to leave the critical section unlocks it. The Ñ mutex is ignored by readers who enter and leave while other readers are present. This means that as long as a single reader holds the Ñ mutex, an unbounded number of readers can enter the critical section unimpeded. A correct solution to either of the readers-writers problems can result in starvation, where a thread blocks indeﬁnitely and fails to make progress. For example, in the solution in Figure 12.26, a writer could wait indeﬁnitely while a stream of readers arrived. Practice Problem 12.10 (solution page 1074) The solution to the ﬁrst readers-writers problem in Figure 12.26 gives priority to readers, but this priority is weak in the sense that a writer leaving its critical section might restart a waiting writer instead of a waiting reader. Describe a scenario where this weak priority would allow a collection of writers to starve a reader. 12.5.5 Putting It Together: A Concurrent Server Based on Prethreading We have seen how semaphores can be used to access shared variables and to schedule accesses to shared resources. To help you understand these ideas more clearly, let us apply them to a concurrent server based on a technique called prethreading. Section 12.5 Synchronizing Threads with Semaphores 1045 mh §ÄÇ¾þÄ ÌþË©þ¾Ä−Í hm ©ÅÎ Ë−þ®²ÅÎy mh 'Å©Î©þÄÄÔ{nhm Í−ÀˆÎ ÀÏÎ−Ój Ñy mh ¢ÇÎ³ ©Å©Î©þÄÄÔ{ohm ÌÇ©® Ë−þ®−ËfÌÇ©®g Õ Ñ³©Ä− fog Õ –fdÀÏÎ−Ógy Ë−þ®²ÅÎiiy ©ð fË−þ®²ÅÎ {{ og mh ƒ©ËÍÎ ©Å hm –fdÑgy ‚fdÀÏÎ−Ógy mh £Ë©Î©²þÄ Í−²Î©ÇÅ hm mh ‡−þ®©Å× ³þÉÉ−ÅÍ hm –fdÀÏÎ−Ógy Ë−þ®²ÅÎkky ©ð fË−þ®²ÅÎ {{ ng mh ‹þÍÎ ÇÏÎ hm ‚fdÑgy ‚fdÀÏÎ−Ógy Û Û ÌÇ©® ÑË©Î−ËfÌÇ©®g Õ Ñ³©Ä− fog Õ –fdÑgy mh £Ë©Î©²þÄ Í−²Î©ÇÅ hm mh „Ë©Î©Å× ³þÉÉ−ÅÍ hm ‚fdÑgy Û Û Figure 12.26 Solution to the ﬁrst readers-writers problem. Favors readers over writers. In the concurrent server in Figure 12.14, we created a new thread for each new client. A disadvantage of this approach is that we incur the nontrivial cost of creating a new thread for each new client. A server based on prethreading tries to reduce this overhead by using the producer-consumer model shown in Figure 12.27. The server consists of a main thread and a set of worker threads. The main thread repeatedly accepts connection requests from clients and places 1046 Chapter 12 Concurrent Programming Aside Other synchronization mechanisms We have shown you how to synchronize threads using semaphores, mainly because they are simple, clas- sical, and have a clean semantic model. But you should know that other synchronization techniques exist as well. For example, Java threads are synchronized with a mechanism called a Java monitor [48], which provides a higher-level abstraction of the mutual exclusion and scheduling capabilities of sema- phores; in fact, monitors can be implemented with semaphores. As another example, the Pthreads interface deﬁnes a set of synchronization operations on mutex and condition variables. Pthreads mu- texes are used for mutual exclusion. Condition variables are used for scheduling accesses to shared resources, such as the bounded buffer in a producer-consumer program. Client Client Master thread Worker thread Pool of worker threads Worker thread Buffer Remove descriptors Accept connections Insert descriptors Service client Service client. . .. . . Figure 12.27 Organization of a prethreaded concurrent server. A set of existing threads repeatedly remove and process connected descriptors from a bounded buffer. the resulting connected descriptors in a bounded buffer. Each worker thread repeatedly removes a descriptor from the buffer, services the client, and then waits for the next descriptor. Figure 12.28 shows how we would use the Sbuf package to implement a prethreaded concurrent echo server. After initializing buffer Í¾Ïð (line 24), the main thread creates the set of worker threads (lines 25–26). Then it enters the inﬁnite server loop, accepting connection requests and inserting the resulting connected descriptors in Í¾Ïð. Each worker thread has a very simple behavior. It waits until it is able to remove a connected descriptor from the buffer (line 39) and then calls the −²³Çˆ²ÅÎ function to echo client input. The −²³Çˆ²ÅÎ function in Figure 12.29 is a version of the −²³Ç function from Figure 11.22 that records the cumulative number of bytes received from all clients in a global variable called ¾ÔÎ−ˆ²ÅÎ. This is interesting code to study because it shows you a general technique for initializing packages that are called from thread routines. In our case, we need to initialize the ¾ÔÎ−ˆ²ÅÎ counter and the ÀÏÎ−Ó semaphore. One approach, which we used for the Sbuf and Rio packages, is to require the main thread to explicitly call an initialization function. Another approach, shown here, uses the ÉÎ³Ë−þ®ˆÇÅ²− function (line 19) to call Section 12.5 Synchronizing Threads with Semaphores 1047 code/conc/echoservert-pre.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a©Å²ÄÏ®− ‘Í¾Ïðl³‘ 3 a®−ð©Å− ﬁ¶¤‡¥¡⁄· r 4 a®−ð©Å− ·¢•ƒ·'…¥ ot 5 6 ÌÇ©® −²³Çˆ²ÅÎf©ÅÎ ²ÇÅÅð®gy 7 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 8 9 Í¾ÏðˆÎ Í¾Ïðy mh ·³þË−® ¾Ïðð−Ë Çð ²ÇÅÅ−²Î−® ®−Í²Ë©ÉÎÇËÍ hm 10 11 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 12 Õ 13 ©ÅÎ ©j Ä©ÍÎ−Åð®j ²ÇÅÅð®y 14 ÍÇ²ÂÄ−ÅˆÎ ²Ä©−ÅÎÄ−Åy 15 ÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×− ²Ä©−ÅÎþ®®Ëy 16 ÉÎ³Ë−þ®ˆÎ Î©®y 17 18 ©ð fþË×² _{ pg Õ 19 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÏÍþ×−x cÍ zÉÇËÎ|¿Å‘j þË×Ì‰n`gy 20 −Ó©Îfngy 21 Û 22 Ä©ÍÎ−Åð® { ﬂÉ−ÅˆÄ©ÍÎ−Åð®fþË×Ì‰o`gy 23 24 Í¾Ïðˆ©Å©ÎfdÍ¾Ïðj ·¢•ƒ·'…¥gy 25 ðÇË f© { ny © z ﬁ¶¤‡¥¡⁄·y ©iig mh £Ë−þÎ− ÑÇËÂ−Ë Î³Ë−þ®Í hm 26 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j ﬁ•‹‹gy 27 28 Ñ³©Ä− fog Õ 29 ²Ä©−ÅÎÄ−Å { Í©Ö−ÇðfÍÎËÏ²Î ÍÇ²Âþ®®ËˆÍÎÇËþ×−gy 30 ²ÇÅÅð® { ¡²²−ÉÎfÄ©ÍÎ−Åð®j f·¡ hg d²Ä©−ÅÎþ®®Ëj d²Ä©−ÅÎÄ−Ågy 31 Í¾Ïðˆ©ÅÍ−ËÎfdÍ¾Ïðj ²ÇÅÅð®gy mh 'ÅÍ−ËÎ ²ÇÅÅð® ©Å ¾Ïðð−Ë hm 32 Û 33 Û 34 35 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 36 Õ 37 –Î³Ë−þ®ˆ®−Îþ²³fÉÎ³Ë−þ®ˆÍ−Äðfggy 38 Ñ³©Ä− fog Õ 39 ©ÅÎ ²ÇÅÅð® { Í¾ÏðˆË−ÀÇÌ−fdÍ¾Ïðgy mh ‡−ÀÇÌ− ²ÇÅÅð® ðËÇÀ ¾Ïðð−Ë hm 40 −²³Çˆ²ÅÎf²ÇÅÅð®gy mh ·−ËÌ©²− ²Ä©−ÅÎ hm 41 £ÄÇÍ−f²ÇÅÅð®gy 42 Û 43 Û code/conc/echoservert-pre.c Figure 12.28 A prethreaded concurrent echo server. The server uses a producer-consumer model with one producer and multiple consumers. 1048 Chapter 12 Concurrent Programming code/conc/echo-cnt.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ÍÎþÎ©² ©ÅÎ ¾ÔÎ−ˆ²ÅÎy mh ¢ÔÎ− ²ÇÏÅÎ−Ë hm 4 ÍÎþÎ©² Í−ÀˆÎ ÀÏÎ−Óy mh þÅ® Î³− ÀÏÎ−Ó Î³þÎ ÉËÇÎ−²ÎÍ ©Î hm 5 6 ÍÎþÎ©² ÌÇ©® ©Å©Îˆ−²³Çˆ²ÅÎfÌÇ©®g 7 Õ 8 ·−Àˆ©Å©ÎfdÀÏÎ−Ój nj ogy 9 ¾ÔÎ−ˆ²ÅÎ { ny 10 Û 11 12 ÌÇ©® −²³Çˆ²ÅÎf©ÅÎ ²ÇÅÅð®g 13 Õ 14 ©ÅÎ Åy 15 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 16 Ë©ÇˆÎ Ë©Çy 17 ÍÎþÎ©² ÉÎ³Ë−þ®ˆÇÅ²−ˆÎ ÇÅ²− { –¶¤‡¥¡⁄ˆﬂﬁ£¥ˆ'ﬁ'¶y 18 19 –Î³Ë−þ®ˆÇÅ²−fdÇÅ²−j ©Å©Îˆ−²³Çˆ²ÅÎgy 20 ‡©ÇˆË−þ®©Å©Î¾fdË©Çj ²ÇÅÅð®gy 21 Ñ³©Ä−ffÅ { ‡©ÇˆË−þ®Ä©Å−¾fdË©Çj ¾Ïðj ›¡”‹'ﬁ¥gg _{ ng Õ 22 –fdÀÏÎ−Ógy 23 ¾ÔÎ−ˆ²ÅÎ i{ Åy 24 ÉË©ÅÎðf‘Í−ËÌ−Ë Ë−²−©Ì−® c® fc® ÎÇÎþÄg ¾ÔÎ−Í ÇÅ ð® c®¿Å‘j 25 Åj ¾ÔÎ−ˆ²ÅÎj ²ÇÅÅð®gy 26 ‚fdÀÏÎ−Ógy 27 ‡©ÇˆÑË©Î−Åf²ÇÅÅð®j ¾Ïðj Ågy 28 Û 29 Û code/conc/echo-cnt.c Figure 12.29 −²³Çˆ²ÅÎ: A version of −²³Ç that counts all bytes received from clients. the initialization function the ﬁrst time some thread calls the −²³Çˆ²ÅÎ function. The advantage of this approach is that it makes the package easier to use. The disadvantage is that every call to −²³Çˆ²ÅÎ makes a call to ÉÎ³Ë−þ®ˆÇÅ²−, which most times does nothing useful. Once the package is initialized, the −²³Çˆ²ÅÎ function initializes the Rio buffered I/O package (line 20) and then echoes each text line that is received from the client. Notice that the accesses to the shared ¾ÔÎ−ˆ²ÅÎ variable in lines 23–25 are protected by P and V operations. Section 12.6 Using Threads for Parallelism 1049 Aside Event-driven programs based on threads I/O multiplexing is not the only way to write an event-driven program. For example, you might have noticed that the concurrent prethreaded server that we just developed is really an event-driven server with simple state machines for the main and worker threads. The main thread has two states (“waiting for connection request” and “waiting for available buffer slot”), two I/O events (“connection request arrives” and “buffer slot becomes available”), and two transitions (“accept connection request” and “insert buffer item”). Similarly, each worker thread has one state (“waiting for available buffer item”), one I/O event (“buffer item becomes available”), and one transition (“remove buffer item”). Figure 12.30 Relationships between the sets of sequential, concurrent, and parallel programs. All programs Concurrent programs Sequential programsParallel programs 12.6 Using Threads for Parallelism Thus far in our study of concurrency, we have assumed concurrent threads exe- cuting on uniprocessor systems. However, most modern machines have multi-core processors. Concurrent programs often run faster on such machines because the operating system kernel schedules the concurrent threads in parallel on multi- ple cores, rather than sequentially on a single core. Exploiting such parallelism is critically important in applications such as busy Web servers, database servers, and large scientiﬁc codes, and it is becoming increasingly useful in mainstream applications such as Web browsers, spreadsheets, and document processors. Figure 12.30 shows the set relationships between sequential, concurrent, and parallel programs. The set of all programs can be partitioned into the disjoint sets of sequential and concurrent programs. A sequential program is written as a single logical ﬂow. A concurrent program is written as multiple concurrent ﬂows. A parallel program is a concurrent program running on multiple processors. Thus, the set of parallel programs is a proper subset of the set of concurrent programs. A detailed treatment of parallel programs is beyond our scope, but studying a few simple example programs will help you understand some important aspects of parallel programming. For example, consider how we might sum the sequence of integers 0,...,n − 1 in parallel. Of course, there is a closed-form solution for this particular problem, but nonetheless it is a concise and easy-to-understand ex- emplar that will allow us to make some interesting points about parallel programs. The most straightforward approach for assigning work to different threads is to partition the sequence into t disjoint regions and then assign each of t different 1050 Chapter 12 Concurrent Programming threads to work on its own region. For simplicity, assume that n is a multiple of t, such that each region has n/t elements. Let’s look at some of the different ways that multiple threads might work on their assigned regions in parallel. The simplest and most straightforward option is to have the threads sum into a shared global variable that is protected by a mutex. Figure 12.31 shows how we might implement this. In lines 28–33, the main thread creates the peer threads and then waits for them to terminate. Notice that the main thread passes a small integer to each peer thread that serves as a unique thread ID. Each peer thread will use its thread ID to determine which portion of the sequence it should work on. This idea of passing a small unique thread ID to the peer threads is a general technique that is used in many parallel applications. After the peer threads have terminated, the global variable ×ÍÏÀ contains the ﬁnal sum. The main thread then uses the closed-form solution to verify the result (lines 36–37). Figure 12.32 shows the function that each peer thread executes. In line 4, the thread extracts the thread ID from the thread argument and then uses this ID to determine the region of the sequence it should work on (lines 5–6). In lines 9–13, the thread iterates over its portion of the sequence, updating the shared global variable ×ÍÏÀ on each iteration. Notice that we are careful to protect each update with P and V mutex operations. When we run ÉÍÏÀkÀÏÎ−Ó on a system with four cores on a sequence of size n = 231 and measure its running time (in seconds) as a function of the number of threads, we get a nasty surprise: Number of threads Version 1 2 4 8 16 ÉÍÏÀkÀÏÎ−Ó 68 432 719 552 599 Not only is the program extremely slow when it runs sequentially as a single thread, it is nearly an order of magnitude slower when it runs in parallel as multiple threads. And the performance gets worse as we add more cores. The reason for this poor performance is that the synchronization operations (P and V ) are very expensive relative to the cost of a single memory update. This highlights an important lesson about parallel programming: Synchronization overhead is expensive and should be avoided if possible. If it cannot be avoided, the overhead should be amortized by as much useful computation as possible. One way to avoid synchronization in our example program is to have each peer thread compute its partial sum in a private variable that is not shared with any other thread, as shown in Figure 12.33. The main thread (not shown) deﬁnes a global array called ÉÍÏÀ, and each peer thread i accumulates its partial sum in ÉÍÏÀ‰©`. Since we are careful to give each peer thread a unique memory location to update, it is not necessary to protect these updates with mutexes. The only necessary synchronization is that the main thread must wait for all of the children to ﬁnish. After the peer threads have terminated, the main thread sums up the elements of the ÉÍÏÀ vector to arrive at the ﬁnal result. Section 12.6 Using Threads for Parallelism 1051 code/conc/psum-mutex.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ›¡”¶¤‡¥¡⁄· qp 3 4 ÌÇ©® hÍÏÀˆÀÏÎ−ÓfÌÇ©® hÌþË×Égy mh ¶³Ë−þ® ËÇÏÎ©Å− hm 5 6 mh §ÄÇ¾þÄ Í³þË−® ÌþË©þ¾Ä−Í hm 7 ÄÇÅ× ×ÍÏÀ { ny mh §ÄÇ¾þÄ ÍÏÀ hm 8 ÄÇÅ× Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ﬁÏÀ¾−Ë Çð −Ä−À−ÅÎÍ ÎÇ ÍÏÀ hm 9 Í−ÀˆÎ ÀÏÎ−Óy mh ›ÏÎ−Ó ÎÇ ÉËÇÎ−²Î ×ÄÇ¾þÄ ÍÏÀ hm 10 11 ©ÅÎ Àþ©Åf©ÅÎ þË×²j ²³þË hhþË×Ìg 12 Õ 13 ÄÇÅ× ©j Å−Ä−ÀÍj ÄÇ×ˆÅ−Ä−ÀÍj ÅÎ³Ë−þ®Íj ÀÔ©®‰›¡”¶¤‡¥¡⁄·`y 14 ÉÎ³Ë−þ®ˆÎ Î©®‰›¡”¶¤‡¥¡⁄·`y 15 16 mh §−Î ©ÅÉÏÎ þË×ÏÀ−ÅÎÍ hm 17 ©ð fþË×² _{ qg Õ 18 ÉË©ÅÎðf‘•Íþ×−x cÍ zÅÎ³Ë−þ®Í| zÄÇ×ˆÅ−Ä−ÀÍ|¿Å‘j þË×Ì‰n`gy 19 −Ó©Îfngy 20 Û 21 ÅÎ³Ë−þ®Í { þÎÇ©fþË×Ì‰o`gy 22 ÄÇ×ˆÅ−Ä−ÀÍ { þÎÇ©fþË×Ì‰p`gy 23 Å−Ä−ÀÍ { fo‹ zz ÄÇ×ˆÅ−Ä−ÀÍgy 24 Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ® { Å−Ä−ÀÍ m ÅÎ³Ë−þ®Íy 25 Í−Àˆ©Å©ÎfdÀÏÎ−Ój nj ogy 26 27 mh £Ë−þÎ− É−−Ë Î³Ë−þ®Í þÅ® Ñþ©Î ðÇË Î³−À ÎÇ ð©Å©Í³ hm 28 ðÇË f© { ny © z ÅÎ³Ë−þ®Íy ©iig Õ 29 ÀÔ©®‰©` { ©y 30 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®‰©`j ﬁ•‹‹j ÍÏÀˆÀÏÎ−Ój dÀÔ©®‰©`gy 31 Û 32 ðÇË f© { ny © z ÅÎ³Ë−þ®Íy ©iig 33 –Î³Ë−þ®ˆÁÇ©ÅfÎ©®‰©`j ﬁ•‹‹gy 34 35 mh £³−²Â ð©ÅþÄ þÅÍÑ−Ë hm 36 ©ð f×ÍÏÀ _{ fÅ−Ä−ÀÍ h fÅ−Ä−ÀÍkoggmpg 37 ÉË©ÅÎðf‘¥ËËÇËx Ë−ÍÏÄÎ{cÄ®¿Å‘j ×ÍÏÀgy 38 39 −Ó©Îfngy 40 Û code/conc/psum-mutex.c Figure 12.31 Main routine for ÉÍÏÀkÀÏÎ−Ó. Uses multiple threads to sum the elements of a sequence into a shared global variable protected by a mutex. 1052 Chapter 12 Concurrent Programming code/conc/psum-mutex.c 1 mh ¶³Ë−þ® ËÇÏÎ©Å− ðÇË ÉÍÏÀkÀÏÎ−Ól² hm 2 ÌÇ©® hÍÏÀˆÀÏÎ−ÓfÌÇ©® hÌþË×Ég 3 Õ 4 ÄÇÅ× ÀÔ©® { hffÄÇÅ× hgÌþË×Égy mh ¥ÓÎËþ²Î Î³− Î³Ë−þ® '⁄ hm 5 ÄÇÅ× ÍÎþËÎ { ÀÔ©® h Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ·ÎþËÎ −Ä−À−ÅÎ ©Å®−Ó hm 6 ÄÇÅ× −Å® { ÍÎþËÎ i Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ¥Å® −Ä−À−ÅÎ ©Å®−Ó hm 7 ÄÇÅ× ©y 8 9 ðÇË f© { ÍÎþËÎy © z −Å®y ©iig Õ 10 –fdÀÏÎ−Ógy 11 ×ÍÏÀ i{ ©y 12 ‚fdÀÏÎ−Ógy 13 Û 14 Ë−ÎÏËÅ ﬁ•‹‹y 15 Û code/conc/psum-mutex.c Figure 12.32 Thread routine for ÉÍÏÀkÀÏÎ−Ó. Each peer thread sums into a shared global variable protected by a mutex. code/conc/psum-array.c 1 mh ¶³Ë−þ® ËÇÏÎ©Å− ðÇË ÉÍÏÀkþËËþÔl² hm 2 ÌÇ©® hÍÏÀˆþËËþÔfÌÇ©® hÌþË×Ég 3 Õ 4 ÄÇÅ× ÀÔ©® { hffÄÇÅ× hgÌþË×Égy mh ¥ÓÎËþ²Î Î³− Î³Ë−þ® '⁄ hm 5 ÄÇÅ× ÍÎþËÎ { ÀÔ©® h Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ·ÎþËÎ −Ä−À−ÅÎ ©Å®−Ó hm 6 ÄÇÅ× −Å® { ÍÎþËÎ i Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ¥Å® −Ä−À−ÅÎ ©Å®−Ó hm 7 ÄÇÅ× ©y 8 9 ðÇË f© { ÍÎþËÎy © z −Å®y ©iig Õ 10 ÉÍÏÀ‰ÀÔ©®` i{ ©y 11 Û 12 Ë−ÎÏËÅ ﬁ•‹‹y 13 Û code/conc/psum-array.c Figure 12.33 Thread routine for ÉÍÏÀkþËËþÔ. Each peer thread accumulates its partial sum in a private array element that is not shared with any other peer thread. Section 12.6 Using Threads for Parallelism 1053 When we run ÉÍÏÀkþËËþÔ on our four-core system, we see that it runs orders of magnitude faster than ÉÍÏÀkÀÏÎ−Ó: Number of threads Version 1248 16 ÉÍÏÀkÀÏÎ−Ó 68.00 432.00 719.00 552.00 599.00 ÉÍÏÀkþËËþÔ 7.26 3.64 1.91 1.85 1.84 In Chapter 5, we learned how to use local variables to eliminate unnecessary memory references. Figure 12.34 shows how we can apply this principle by having each peer thread accumulate its partial sum into a local variable rather than a global variable. When we run ÉÍÏÀkÄÇ²þÄ on our four-core machine, we get another order-of-magnitude decrease in running time: Number of threads Version 1248 16 ÉÍÏÀkÀÏÎ−Ó 68.00 432.00 719.00 552.00 599.00 ÉÍÏÀkþËËþÔ 7.26 3.64 1.91 1.85 1.84 ÉÍÏÀkÄÇ²þÄ 1.06 0.54 0.28 0.29 0.30 code/conc/psum-local.c 1 mh ¶³Ë−þ® ËÇÏÎ©Å− ðÇË ÉÍÏÀkÄÇ²þÄl² hm 2 ÌÇ©® hÍÏÀˆÄÇ²þÄfÌÇ©® hÌþË×Ég 3 Õ 4 ÄÇÅ× ÀÔ©® { hffÄÇÅ× hgÌþË×Égy mh ¥ÓÎËþ²Î Î³− Î³Ë−þ® '⁄ hm 5 ÄÇÅ× ÍÎþËÎ { ÀÔ©® h Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ·ÎþËÎ −Ä−À−ÅÎ ©Å®−Ó hm 6 ÄÇÅ× −Å® { ÍÎþËÎ i Å−Ä−ÀÍˆÉ−ËˆÎ³Ë−þ®y mh ¥Å® −Ä−À−ÅÎ ©Å®−Ó hm 7 ÄÇÅ× ©j ÍÏÀ { ny 8 9 ðÇË f© { ÍÎþËÎy © z −Å®y ©iig Õ 10 ÍÏÀ i{ ©y 11 Û 12 ÉÍÏÀ‰ÀÔ©®` { ÍÏÀy 13 Ë−ÎÏËÅ ﬁ•‹‹y 14 Û code/conc/psum-local.c Figure 12.34 Thread routine for ÉÍÏÀkÄÇ²þÄ. Each peer thread accumulates its partial sum in a local variable. 1054 Chapter 12 Concurrent Programming Figure 12.35 Performance of ÉÍÏÀk ÄÇ²þÄ (Figure 12.34). Summing a sequence of 231 elements using four processor cores. 1.2 1.0 0.8 0.6 0.4 0.2 0 1 1.06 ThreadsElapsed time (s) 2 0.54 4 0.28 0.29 816 0.3 An important lesson to take away from this exercise is that writing parallel programs is tricky. Seemingly small changes to the code have a signiﬁcant impact on performance. Characterizing the Performance of Parallel Programs Figure 12.35 plots the total elapsed running time of the ÉÍÏÀkÄÇ²þÄ program in Figure 12.34 as a function of the number of threads. In each case, the program runs on a system with four processor cores and sums a sequence of n = 231 ele- ments. We see that running time decreases as we increase the number of threads, up to four threads, at which point it levels off and even starts to increase a little. In the ideal case, we would expect the running time to decrease linearly with the number of cores. That is, we would expect running time to drop by half each time we double the number of threads. This is indeed the case until we reach the point (t> 4) where each of the four cores is busy running at least one thread. Running time actually increases a bit as we increase the number of threads because of the overhead of context switching multiple threads on the same core. For this reason, parallel programs are often written so that each core runs exactly one thread. Although absolute running time is the ultimate measure of any program’s performance, there are some useful relative measures that can provide insight into how well a parallel program is exploiting potential parallelism. The speedup of a parallel program is typically deﬁned as Sp = T1 Tp where p is the number of processor cores and Tk is the running time on k cores. This formulation is sometimes referred to as strong scaling. When T1 is the execution Section 12.6 Using Threads for Parallelism 1055 Threads (t) 1 248 16 Cores (p) 1 2444 Running time (Tp) 1.06 0.54 0.28 0.29 0.30 Speedup (Sp) 1 1.9 3.8 3.7 3.5 Efﬁciency (Ep) 100% 98% 95% 91% 88% Figure 12.36 Speedup and parallel efﬁciency for the execution times in Figure 12.35. time of a sequential version of the program, then Sp is called the absolute speedup. When T1 is the execution time of the parallel version of the program running on one core, then Sp is called the relative speedup. Absolute speedup is a truer mea- sure of the beneﬁts of parallelism than relative speedup. Parallel programs often suffer from synchronization overheads, even when they run on one processor, and these overheads can artiﬁcially inﬂate the relative speedup numbers because they increase the size of the numerator. On the other hand, absolute speedup is more difﬁcult to measure than relative speedup because measuring absolute speedup requires two different versions of the program. For complex parallel codes, creat- ing a separate sequential version might not be feasible, either because the code is too complex or because the source code is not available. A related measure, known as efﬁciency, is deﬁned as Ep = Sp p = T1 pTp and is typically reported as a percentage in the range (0, 100]. Efﬁciency is a mea- sure of the overhead due to parallelization. Programs with high efﬁciency are spending more time doing useful work and less time synchronizing and commu- nicating than programs with low efﬁciency. Figure 12.36 shows the different speedup and efﬁciency measures for our example parallel sum program. Efﬁciencies over 90 percent such as these are very good, but do not be fooled. We were able to achieve high efﬁciency because our problem was trivially easy to parallelize. In practice, this is not usually the case. Parallel programming has been an active area of research for decades. With the advent of commodity multi-core machines whose core count is doubling every few years, parallel programming continues to be a deep, difﬁcult, and active area of research. There is another view of speedup, known as weak scaling, which increases the problem size along with the number of processors, such that the amount of work performed on each processor is held constant as the number of processors increases. With this formulation, speedup and efﬁciency are expressed in terms of the total amount of work accomplished per unit time. For example, if we can double the number of processors and do twice the amount of work per hour, then we are enjoying linear speedup and 100 percent efﬁciency. 1056 Chapter 12 Concurrent Programming Weak scaling is often a truer measure than strong scaling because it more accurately reﬂects our desire to use bigger machines to do more work. This is particularly true for scientiﬁc codes, where the problem size can be easily increased and where bigger problem sizes translate directly to better predictions of nature. However, there exist applications whose sizes are not so easily increased, and for these applications strong scaling is more appropriate. For example, the amount of work performed by real-time signal-processing applications is often determined by the properties of the physical sensors that are generating the signals. Changing the total amount of work requires using different physical sensors, which might not be feasible or necessary. For these applications, we typically want to use parallelism to accomplish a ﬁxed amount of work as quickly as possible. Practice Problem 12.11 (solution page 1074) Fill in the blanks for the parallel program in the following table. Assume strong scaling. Threads (t)1 4 8 Cores (p)1 4 8 Running time (Tp)16 8 4 Speedup (Sp)1 Efﬁciency (Ep) 100% 12.7 Other Concurrency Issues You probably noticed that life got much more complicated once we were asked to synchronize accesses to shared data. So far, we have looked at techniques for mutual exclusion and producer-consumer synchronization, but this is only the tip of the iceberg. Synchronization is a fundamentally difﬁcult problem that raises issues that simply do not arise in ordinary sequential programs. This section is a survey (by no means complete) of some of the issues you need to be aware of when you write concurrent programs. To keep things concrete, we will couch our discussion in terms of threads. Keep in mind, however, that these are typical of the issues that arise when concurrent ﬂows of any kind manipulate shared resources. 12.7.1 Thread Safety When we program with threads, we must be careful to write functions that have a property called thread safety. A function is said to be thread-safe if and only if it will always produce correct results when called repeatedly from multiple concurrent threads. If a function is not thread-safe, then we say it is thread-unsafe. We can identify four (nondisjoint) classes of thread-unsafe functions: Class 1: Functions that do not protect shared variables. We have already en- countered this problem with the Î³Ë−þ® function in Figure 12.16, which Section 12.7 Other Concurrency Issues 1057 code/conc/rand.c 1 ÏÅÍ©×Å−® Å−ÓÎˆÍ−−® { oy 2 3 mh ËþÅ® k Ë−ÎÏËÅ ÉÍ−Ï®ÇËþÅ®ÇÀ ©ÅÎ−×−Ë ©Å Î³− ËþÅ×− nllqputu hm 4 ÏÅÍ©×Å−® ËþÅ®fÌÇ©®g 5 Õ 6 Å−ÓÎˆÍ−−® { Å−ÓÎˆÍ−−®hoonqsosprs i opsrqy 7 Ë−ÎÏËÅ fÏÅÍ©×Å−®gfÅ−ÓÎˆÍ−−®||otg c qputvy 8 Û 9 10 mh ÍËþÅ® k Í−Î Î³− ©Å©Î©þÄ Í−−® ðÇË ËþÅ®fg hm 11 ÌÇ©® ÍËþÅ®fÏÅÍ©×Å−® Å−ÑˆÍ−−®g 12 Õ 13 Å−ÓÎˆÍ−−® { Å−ÑˆÍ−−®y 14 Û code/conc/rand.c Figure 12.37 A thread-unsafe pseudorandom number generator. (Based on [61]) increments an unprotected global counter variable. This class of thread- unsafe functions is relatively easy to make thread-safe: protect the shared variables with synchronization operations such as P and V . An advantage is that it does not require any changes in the calling program. A disadvan- tage is that the synchronization operations slow down the function. Class 2: Functions that keep state across multiple invocations. A pseudorandom number generator is a simple example of this class of thread-unsafe func- tions. Consider the pseudorandom number generator package in Fig- ure 12.37. The ËþÅ® function is thread-unsafe because the result of the current invocation depends on an intermediate result from the previous iteration. When we call ËþÅ® repeatedly from a single thread after seeding it with a call to ÍËþÅ®, we can expect a repeatable sequence of numbers. However, this assumption no longer holds if multiple threads are calling ËþÅ®. The only way to make a function such as ËþÅ® thread-safe is to rewrite it so that it does not use any ÍÎþÎ©² data, relying instead on the caller to pass the state information in arguments. The disadvantage is that the programmer is now forced to change the code in the calling routine as well. In a large program where there are potentially hundreds of different call sites, making such modiﬁcations could be nontrivial and prone to error. Class 3: Functions that return a pointer to a static variable. Some functions, such as ²Î©À− and ×−Î³ÇÍÎ¾ÔÅþÀ−, compute a result in a ÍÎþÎ©² variable and then return a pointer to that variable. If we call such functions from 1058 Chapter 12 Concurrent Programming code/conc/ctime-ts.c 1 ²³þË h²Î©À−ˆÎÍf²ÇÅÍÎ Î©À−ˆÎ hÎ©À−Éj ²³þË hÉË©ÌþÎ−Ég 2 Õ 3 ²³þË hÍ³þË−®Éy 4 5 –fdÀÏÎ−Ógy 6 Í³þË−®É { ²Î©À−fÎ©À−Égy 7 ÍÎË²ÉÔfÉË©ÌþÎ−Éj Í³þË−®Égy mh £ÇÉÔ ÍÎË©Å× ðËÇÀ Í³þË−® ÎÇ ÉË©ÌþÎ− hm 8 ‚fdÀÏÎ−Ógy 9 Ë−ÎÏËÅ ÉË©ÌþÎ−Éy 10 Û code/conc/ctime-ts.c Figure 12.38 Thread-safe wrapper function for the C standard library ²Î©À− function. This example uses the lock-and-copy technique to call a class 3 thread-unsafe function. concurrent threads, then disaster is likely, as results being used by one thread are silently overwritten by another thread. There are two ways to deal with this class of thread-unsafe func- tions. One option is to rewrite the function so that the caller passes the address of the variable in which to store the results. This eliminates all shared data, but it requires the programmer to have access to the function source code. If the thread-unsafe function is difﬁcult or impossible to modify (e.g., the code is very complex or there is no source code available), then an- other option is to use the lock-and-copy technique. The basic idea is to associate a mutex with the thread-unsafe function. At each call site, lock the mutex, call the thread-unsafe function, copy the result returned by the function to a private memory location, and then unlock the mutex. To minimize changes to the caller, you should deﬁne a thread-safe wrap- per function that performs the lock-and-copy and then replace all calls to the thread-unsafe function with calls to the wrapper. For example, Figure 12.38 shows a thread-safe wrapper for ²Î©À− that uses the lock- and-copy technique. Class 4: Functions that call thread-unsafe functions.If a function f calls a thread- unsafe function g,is f thread-unsafe? It depends. If g is a class 2 function that relies on state across multiple invocations, then f is also thread- unsafe and there is no recourse short of rewriting g. However, if g is a class 1 or class 3 function, then f can still be thread-safe if you protect the call site and any resulting shared data with a mutex. We see a good example of this in Figure 12.38, where we use lock-and-copy to write a thread-safe function that calls a thread-unsafe function. Section 12.7 Other Concurrency Issues 1059 Figure 12.39 Relationships between the sets of reentrant, thread-safe, and thread- unsafe functions. All functions Thread-safe functions Thread-unsafe functionsReentrant functions code/conc/rand-r.c 1 mh ËþÅ®ˆË k Ë−ÎÏËÅ þ ÉÍ−Ï®ÇËþÅ®ÇÀ ©ÅÎ−×−Ë ÇÅ nllqputu hm 2 ©ÅÎ ËþÅ®ˆËfÏÅÍ©×Å−® ©ÅÎ hÅ−ÓÎÉg 3 Õ 4 hÅ−ÓÎÉ { hÅ−ÓÎÉ h oonqsosprs i opqrsy 5 Ë−ÎÏËÅ fÏÅÍ©×Å−® ©ÅÎgfhÅ−ÓÎÉ m tssqtg c qputvy 6 Û code/conc/rand-r.c Figure 12.40 ËþÅ®ˆË: A reentrant version of the ËþÅ® function from Figure 12.37. 12.7.2 Reentrancy There is an important class of thread-safe functions, known as reentrant functions, that are characterized by the property that they do not reference any shared data when they are called by multiple threads. Although the terms thread-safe and reentrant are sometimes used (incorrectly) as synonyms, there is a clear technical distinction that is worth preserving. Figure 12.39 shows the set relationships be- tween reentrant, thread-safe, and thread-unsafe functions. The set of all functions is partitioned into the disjoint sets of thread-safe and thread-unsafe functions. The set of reentrant functions is a proper subset of the thread-safe functions. Reentrant functions are typically more efﬁcient than non-reentrant thread- safe functions because they require no synchronization operations. Furthermore, the only way to convert a class 2 thread-unsafe function into a thread-safe one is to rewrite it so that it is reentrant. For example, Figure 12.40 shows a reentrant version of the ËþÅ® function from Figure 12.37. The key idea is that we have replaced the static Å−ÓÎ variable with a pointer that is passed in by the caller. Is it possible to inspect the code of some function and declare a priori that it is reentrant? Unfortunately, it depends. If all function arguments are passed by value (i.e., no pointers) and all data references are to local automatic stack variables (i.e., no references to static or global variables), then the function is explicitly reentrant, in the sense that we can assert its reentrancy regardless of how it is called. However, if we loosen our assumptions a bit and allow some parameters in our otherwise explicitly reentrant function to be passed by reference (i.e., we allow them to pass pointers), then we have an implicitly reentrant function, in the sense that it is only reentrant if the calling threads are careful to pass pointers 1060 Chapter 12 Concurrent Programming to nonshared data. For example, the ËþÅ®ˆË function in Figure 12.40 is implicitly reentrant. We always use the term reentrant to include both explicit and implicit re- entrant functions. However, it is important to realize that reentrancy is sometimes a property of both the caller and the callee, and not just the callee alone. Practice Problem 12.12 (solution page 1074) The ËþÅ®ˆË function in Figure 12.40 is implicitly reentrant. Explain. 12.7.3 Using Existing Library Functions in Threaded Programs Most Linux functions, including the functions deﬁned in the standard C library (such as ÀþÄÄÇ², ðË−−, Ë−þÄÄÇ², ÉË©ÅÎð, and Í²þÅð), are thread-safe, with only a few exceptions. Figure 12.41 lists some common exceptions. (See [110] for a complete list.) The ÍÎËÎÇÂ function is a deprecated function (one whose use is discouraged) for parsing strings. The þÍ²Î©À−, ²Î©À−, and ÄÇ²þÄÎ©À− functions are popular functions for converting back and forth between different time and date formats. The ×−Î³ÇÍÎ¾Ôþ®®Ë, ×−Î³ÇÍÎ¾ÔÅþÀ−, and ©Å−ÎˆÅÎÇþ functions are obsolete network programming functions that have been replaced by the reentrant ×−Îþ®®Ë©ÅðÇ, ×−ÎÅþÀ−©ÅðÇ, and ©Å−ÎˆÅÎÇÉ functions, respectively (see Chapter 11). With the exceptions of ËþÅ® and ÍÎËÎÇÂ, they are of the class 3 variety that return a pointer to a static variable. If we need to call one of these functions in a threaded program, the least disruptive approach to the caller is to lock and copy. However, the lock-and-copy approach has a number of disadvantages. First, the additional synchronization slows down the program. Second, functions that return pointers to complex structures of structures require a deep copy of the structures in order to copy the entire structure hierarchy. Third, the lock-and-copy approach will not work for a class 2 thread-unsafe function such as ËþÅ® that relies on static state across calls. Thread-unsafe function Thread-unsafe class Linux thread-safe version ËþÅ® 2 ËþÅ®ˆË ÍÎËÎÇÂ 2 ÍÎËÎÇÂˆË þÍ²Î©À− 3 þÍ²Î©À−ˆË ²Î©À− 3 ²Î©À−ˆË ×−Î³ÇÍÎ¾Ôþ®®Ë 3 ×−Î³ÇÍÎ¾Ôþ®®ËˆË ×−Î³ÇÍÎ¾ÔÅþÀ− 3 ×−Î³ÇÍÎ¾ÔÅþÀ−ˆË ©Å−ÎˆÅÎÇþ 3 (none) ÄÇ²þÄÎ©À− 3 ÄÇ²þÄÎ©À−ˆË Figure 12.41 Common thread-unsafe library functions. Section 12.7 Other Concurrency Issues 1061 Therefore, Linux systems provide reentrant versions of most thread-unsafe functions. The names of the reentrant versions always end with the ˆË sufﬁx. For example, the reentrant version of þÍ²Î©À− is called þÍ²Î©À−ˆË. We recommend using these functions whenever possible. 12.7.4 Races A race occurs when the correctness of a program depends on one thread reaching point x in its control ﬂow before another thread reaches point y. Races usually occur because programmers assume that threads will take some particular trajec- tory through the execution state space, forgetting the golden rule that threaded programs must work correctly for any feasible trajectory. An example is the easiest way to understand the nature of races. Consider the simple program in Figure 12.42. The main thread creates four peer threads and passes a pointer to a unique integer ID to each one. Each peer thread copies the code/conc/race.c 1 mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ô_ hm 2 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 3 a®−ð©Å− ﬁ r 4 5 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 6 7 ©ÅÎ Àþ©Åfg 8 Õ 9 ÉÎ³Ë−þ®ˆÎ Î©®‰ﬁ`y 10 ©ÅÎ ©y 11 12 ðÇËf©{ny©zﬁy ©iig 13 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®‰©`j ﬁ•‹‹j Î³Ë−þ®j d©gy 14 ðÇËf©{ny©zﬁy ©iig 15 –Î³Ë−þ®ˆÁÇ©ÅfÎ©®‰©`j ﬁ•‹‹gy 16 −Ó©Îfngy 17 Û 18 19 mh ¶³Ë−þ® ËÇÏÎ©Å− hm 20 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 21 Õ 22 ©ÅÎ ÀÔ©® { hff©ÅÎ hgÌþË×Égy 23 ÉË©ÅÎðf‘¤−ÄÄÇ ðËÇÀ Î³Ë−þ® c®¿Å‘j ÀÔ©®gy 24 Ë−ÎÏËÅ ﬁ•‹‹y 25 Û code/conc/race.c Figure 12.42 A program with a race. 1062 Chapter 12 Concurrent Programming ID passed in its argument to a local variable (line 22) and then prints a message containing the ID. It looks simple enough, but when we run this program on our system, we get the following incorrect result: Ä©ÅÏÓ| ./race ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® o ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® q ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® p ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® q The problem is caused by a race between each peer thread and the main thread. Can you spot the race? Here is what happens. When the main thread creates a peer thread in line 13, it passes a pointer to the local stack variable i. At this point, the race is on between the next increment of © in line 12 and the dereferencing and assignment of the argument in line 22. If the peer thread executes line 22 before the main thread increments © in line 12, then the ÀÔ©® variable gets the correct ID. Otherwise, it will contain the ID of some other thread. The scary thing is that whether we get the correct answer depends on how the kernel schedules the execution of the threads. On our system it fails, but on other systems it might work correctly, leaving the programmer blissfully unaware of a serious bug. To eliminate the race, we can dynamically allocate a separate block for each integer ID and pass the thread routine a pointer to this block, as shown in Fig- ure 12.43 (lines 12–14). Notice that the thread routine must free the block in order to avoid a memory leak. When we run this program on our system, we now get the correct result: Ä©ÅÏÓ| ./norace ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® n ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® o ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® p ¤−ÄÄÇ ðËÇÀ Î³Ë−þ® q Practice Problem 12.13 (solution page 1075) In Figure 12.43, we might be tempted to free the allocated memory block immedi- ately after line 14 in the main thread, instead of freeing it in the peer thread. But this would be a bad idea. Why? Practice Problem 12.14 (solution page 1075) A. In Figure 12.43, we eliminated the race by allocating a separate block for each integer ID. Outline a different approach that does not call the ÀþÄÄÇ² or ðË−− functions. B. What are the advantages and disadvantages of this approach? Section 12.7 Other Concurrency Issues 1063 code/conc/norace.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 a®−ð©Å− ﬁ r 3 4 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 5 6 ©ÅÎ Àþ©Åfg 7 Õ 8 ÉÎ³Ë−þ®ˆÎ Î©®‰ﬁ`y 9 ©ÅÎ ©j hÉÎËy 10 11 ðÇËf©{ny©zﬁy ©iig Õ 12 ÉÎË { ›þÄÄÇ²fÍ©Ö−Çðf©ÅÎggy 13 hÉÎË { ©y 14 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®‰©`j ﬁ•‹‹j Î³Ë−þ®j ÉÎËgy 15 Û 16 ðÇËf©{ny©zﬁy ©iig 17 –Î³Ë−þ®ˆÁÇ©ÅfÎ©®‰©`j ﬁ•‹‹gy 18 −Ó©Îfngy 19 Û 20 21 mh ¶³Ë−þ® ËÇÏÎ©Å− hm 22 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 23 Õ 24 ©ÅÎ ÀÔ©® { hff©ÅÎ hgÌþË×Égy 25 ƒË−−fÌþË×Égy 26 ÉË©ÅÎðf‘¤−ÄÄÇ ðËÇÀ Î³Ë−þ® c®¿Å‘j ÀÔ©®gy 27 Ë−ÎÏËÅ ﬁ•‹‹y 28 Û code/conc/norace.c Figure 12.43 A correct version of the program in Figure 12.42 without a race. 12.7.5 Deadlocks Semaphores introduce the potential for a nasty kind of run-time error, called deadlock, where a collection of threads is blocked, waiting for a condition that will never be true. The progress graph is an invaluable tool for understanding deadlock. For example, Figure 12.44 shows the progress graph for a pair of threads that use two semaphores for mutual exclusion. From this graph, we can glean some important insights about deadlock: . The programmer has incorrectly ordered the P and V operations such that the forbidden regions for the two semaphores overlap. If some execution trajectory happens to reach the deadlock state d, then no further progress is 1064 Chapter 12 Concurrent Programming. . . . . . . . . . . . . . . . . .. . .. . .. . . Thread 2 Thread 1 A trajectory that deadlocks A trajectory that does not deadlock P(s) P(t ) P(s) P(t ) V(s) V(t ) V(t ) V(s) Initially s\u00051 t\u00051 Forbidden region for s Forbidden region for t Deadlock state d Deadlock region Figure 12.44 Progress graph for a program that can deadlock. possible because the overlapping forbidden regions block progress in every legal direction. In other words, the program is deadlocked because each thread is waiting for the other to do a V operation that will never occur. . The overlapping forbidden regions induce a set of states called the deadlock region. If a trajectory happens to touch a state in the deadlock region, then deadlock is inevitable. Trajectories can enter deadlock regions, but they can never leave. . Deadlock is an especially difﬁcult issue because it is not always predictable. Some lucky execution trajectories will skirt the deadlock region, while others will be trapped by it. Figure 12.44 shows an example of each. The implications for a programmer are scary. You might run the same program a thousand times without any problem, but then the next time it deadlocks. Or the program might work ﬁne on one machine but deadlock on another. Worst of all, the error is often not repeatable because different executions have different trajectories. Programs deadlock for many reasons, and preventing them is a difﬁcult prob- lem in general. However, when binary semaphores are used for mutual exclusion, as in Figure 12.44, then you can apply the following simple and effective rule to prevent deadlocks: Section 12.7 Other Concurrency Issues 1065. . . . . . . . . . . . . . .. . .. . .. . . Thread 2 Thread 1 P(t ) P(s) P(s) P(t ) V(s) V(t ) V(t ) V(s) Initially s\u00051 t\u00051 Forbidden region for s Forbidden region for t Figure 12.45 Progress graph for a deadlock-free program. Mutex lock ordering rule: Given a total ordering of all mutexes, a program is deadlock-free if each thread acquires its mutexes in order and releases them in reverse order. For example, we can ﬁx the deadlock in Figure 12.44 by locking s ﬁrst, then t, in each thread. Figure 12.45 shows the resulting progress graph. Practice Problem 12.15 (solution page 1075) Consider the following program, which attempts to use a pair of semaphores for mutual exclusion. 'Å©Î©þÄÄÔxÍ{ojÎ{nl ¶³Ë−þ® ox ¶³Ë−þ® px –fÍgy –fÍgy ‚fÍgy ‚fÍgy –fÎgy –fÎgy ‚fÎgy ‚fÎgy A. Draw the progress graph for this program. B. Does it always deadlock? 1066 Chapter 12 Concurrent Programming C. If so, what simple change to the initial semaphore values will eliminate the potential for deadlock? D. Draw the progress graph for the resulting deadlock-free program. 12.8 Summary A concurrent program consists of a collection of logical ﬂows that overlap in time. In this chapter, we have studied three different mechanisms for building concur- rent programs: processes, I/O multiplexing, and threads. We used a concurrent network server as the motivating application throughout. Processes are scheduled automatically by the kernel, and because of their separate virtual address spaces, they require explicit IPC mechanisms in order to share data. Event-driven programs create their own concurrent logical ﬂows, which are modeled as state machines, and use I/O multiplexing to explicitly sched- ule the ﬂows. Because the program runs in a single process, sharing data between ﬂows is fast and easy. Threads are a hybrid of these approaches. Like ﬂows based on processes, threads are scheduled automatically by the kernel. Like ﬂows based on I/O multiplexing, threads run in the context of a single process, and thus can share data quickly and easily. Regardless of the concurrency mechanism, synchronizing concurrent accesses to shared data is a difﬁcult problem. The P and V operations on semaphores have been developed to help deal with this problem. Semaphore operations can be used to provide mutually exclusive access to shared data, as well as to schedule access to resources such as the bounded buffers in producer-consumer systems and shared objects in readers-writers systems. A concurrent prethreaded echo server provides a compelling example of these usage scenarios for semaphores. Concurrency introduces other difﬁcult issues as well. Functions that are called by threads must have a property known as thread safety. We have identiﬁed four classes of thread-unsafe functions, along with suggestions for making them thread-safe. Reentrant functions are the proper subset of thread-safe functions that do not access any shared data. Reentrant functions are often more efﬁcient than non-reentrant functions because they do not require any synchronization primitives. Some other difﬁcult issues that arise in concurrent programs are races and deadlocks. Races occur when programmers make incorrect assumptions about how logical ﬂows are scheduled. Deadlocks occur when a ﬂow is waiting for an event that will never happen. Bibliographic Notes Semaphore operations were introduced by Dijkstra [31]. The progress graph concept was introduced by Coffman [23] and later formalized by Carson and Reynolds [16]. The readers-writers problem was introduced by Courtois et al [25]. Operating systems texts describe classical synchronization problems such as the dining philosophers, sleeping barber, and cigarette smokers problems in more de- Homework Problems 1067 tail [102, 106, 113]. The book by Butenhof [15] is a comprehensive description of the Posix threads interface. The paper by Birrell [7] is an excellent introduction to threads programming and its pitfalls. The book by Reinders [90] describes a C/C++ library that simpliﬁes the design and implementation of threaded programs. Sev- eral texts cover the fundamentals of parallel programming on multi-core sys- tems [47, 71]. Pugh identiﬁes weaknesses with the way that Java threads interact through memory and proposes replacement memory models [88]. Gustafson pro- posed the weak-scaling speedup model [43] as an alternative to strong scaling. Homework Problems 12.16 ◆ Write a version of ³−ÄÄÇl² (Figure 12.13) that creates and reaps n joinable peer threads, where n is a command-line argument. 12.17 ◆ A. The program in Figure 12.46 has a bug. The thread is supposed to sleep for 1 second and then print a string. However, when we run it on our system, nothing prints. Why? B. You can ﬁx this bug by replacing the −Ó©Î function in line 10 with one of two different Pthreads function calls. Which ones? code/conc/hellobug.c 1 mh „¡‡ﬁ'ﬁ§x ¶³©Í ²Ç®− ©Í ¾Ï××Ô_ hm 2 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 3 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Égy 4 5 ©ÅÎ Àþ©Åfg 6 Õ 7 ÉÎ³Ë−þ®ˆÎ Î©®y 8 9 –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j ﬁ•‹‹gy 10 −Ó©Îfngy 11 Û 12 13 mh ¶³Ë−þ® ËÇÏÎ©Å− hm 14 ÌÇ©® hÎ³Ë−þ®fÌÇ©® hÌþË×Ég 15 Õ 16 ·Ä−−Éfogy 17 ÉË©ÅÎðf‘¤−ÄÄÇj ÑÇËÄ®_¿Å‘gy 18 Ë−ÎÏËÅ ﬁ•‹‹y 19 Û code/conc/hellobug.c Figure 12.46 Buggy program for Problem 12.17. 1068 Chapter 12 Concurrent Programming 12.18 ◆ Using the progress graph in Figure 12.21, classify the following trajectories as either safe or unsafe. A. H2,L2,U2,H1,L1,S2,U1,S1,T1,T2 B. H2,H1,L1,U1,S1,L2,T1,U2,S2,T2 C. H1,L1,H2,L2,U2,S2,U1,S1,T1,T2 12.19 ◆◆ The solution to the ﬁrst readers-writers problem in Figure 12.26 gives a somewhat weak priority to readers because a writer leaving its critical section might restart a waiting writer instead of a waiting reader. Derive a solution that gives stronger priority to readers, where a writer leaving its critical section will always restart a waiting reader if one exists. 12.20 ◆◆◆ Consider a simpler variant of the readers-writers problem where there are at most N readers. Derive a solution that gives equal priority to readers and writers, in the sense that pending readers and writers have an equal chance of being granted access to the resource. Hint: You can solve this problem using a single counting semaphore and a single mutex. 12.21 ◆◆◆◆ Derive a solution to the second readers-writers problem, which favors writers instead of readers. 12.22 ◆◆ Test your understanding of the Í−Ä−²Î function by modifying the server in Fig- ure 12.6 so that it echoes at most one text line per iteration of the main server loop. 12.23 ◆◆ The event-driven concurrent echo server in Figure 12.8 is ﬂawed because a mali- cious client can deny service to other clients by sending a partial text line. Write an improved version of the server that can handle these partial text lines without blocking. 12.24 ◆ The functions in the Rio I/O package (Section 10.5) are thread-safe. Are they reentrant as well? 12.25 ◆ In the prethreaded concurrent echo server in Figure 12.28, each thread calls the −²³Çˆ²ÅÎ function (Figure 12.29). Is −²³Çˆ²ÅÎ thread-safe? Is it reentrant? Why or why not? Homework Problems 1069 12.26 ◆◆◆ Use the lock-and-copy technique to implement a thread-safe non-reentrant ver- sion of ×−Î³ÇÍÎ¾ÔÅþÀ− called ×−Î³ÇÍÎ¾ÔÅþÀ−ˆÎÍ. A correct solution will use a deep copy of the ³ÇÍÎ−ÅÎ structure protected by a mutex. 12.27 ◆◆ Some network programming texts suggest the following approach for reading and writing sockets: Before interacting with the client, open two standard I/O streams on the same open connected socket descriptor, one for reading and one for writing: ƒ'‹¥ hðÉ©Åj hðÉÇÏÎy ðÉ©Å { ð®ÇÉ−ÅfÍÇ²Âð®j ‘Ë‘gy ðÉÇÏÎ { ð®ÇÉ−ÅfÍÇ²Âð®j ‘Ñ‘gy When the server ﬁnishes interacting with the client, close both streams as follows: ð²ÄÇÍ−fðÉ©Ågy ð²ÄÇÍ−fðÉÇÏÎgy However, if you try this approach in a concurrent server based on threads, you will create a deadly race condition. Explain. 12.28 ◆ In Figure 12.45, does swapping the order of the two V operations have any effect on whether or not the program deadlocks? Justify your answer by drawing the progress graphs for the four possible cases: Case 1 Case 2 Case 3 Case 4 Thread 1 Thread 2 Thread 1 Thread 2 Thread 1 Thread 2 Thread 1 Thread 2 –fÍg –fÍg –fÍg –fÍg –fÍg –fÍg –fÍg –fÍg –fÎg –fÎg –fÎg –fÎg –fÎg –fÎg –fÎg –fÎg ‚fÍg ‚fÍg ‚fÍg ‚fÎg ‚fÎg ‚fÍg ‚fÎg ‚fÎg ‚fÎg ‚fÎg ‚fÎg ‚fÍg ‚fÍg ‚fÎg ‚fÍg ‚fÍg 12.29 ◆ Can the following program deadlock? Why or why not? 'Å©Î©þÄÄÔxþ{oj¾{oj²{ol ¶³Ë−þ® ox ¶³Ë−þ® px –fþgy –f²gy –f¾gy –f¾gy ‚f¾gy ‚f¾gy –f²gy ‚f²gy ‚f²gy ‚fþgy 1070 Chapter 12 Concurrent Programming 12.30 ◆ Consider the following program that deadlocks. 'Å©Î©þÄÄÔxþ{oj¾{oj²{ol ¶³Ë−þ® ox ¶³Ë−þ® px ¶³Ë−þ® qx –fþgy –f²gy –f²gy –f¾gy –f¾gy ‚f²gy ‚f¾gy ‚f¾gy –f¾gy –f²gy ‚f²gy –fþgy ‚f²gy –fþgy ‚fþgy ‚fþgy ‚fþgy ‚f¾gy A. For each thread, list the pairs of mutexes that it holds simultaneously. B. If a< b < c, which threads violate the mutex lock ordering rule? C. For these threads, show a new lock ordering that guarantees freedom from deadlock. 12.31 ◆◆◆ Implement a version of the standard I/O ð×−ÎÍ function, called Îð×−ÎÍ, that times out and returns ﬁ•‹‹ if it does not receive an input line on standard input within 5 seconds. Your function should be implemented in a package called Îð×−ÎÍk ÉËÇ²l² using processes, signals, and nonlocal jumps. It should not use the Linux þÄþËÀ function. Test your solution using the driver program in Figure 12.47. code/conc/tfgets-main.c 1 a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ 2 3 ²³þË hÎð×−ÎÍf²³þË hÍj ©ÅÎ Í©Ö−j ƒ'‹¥ hÍÎË−þÀgy 4 5 ©ÅÎ Àþ©Åfg 6 Õ 7 ²³þË ¾Ïð‰›¡”‹'ﬁ¥`y 8 9 ©ð fÎð×−ÎÍf¾Ïðj ›¡”‹'ﬁ¥j ÍÎ®©Åg {{ ﬁ•‹‹g 10 ÉË©ÅÎðf‘¢ﬂﬂ›_¿Å‘gy 11 −ÄÍ− 12 ÉË©ÅÎðf‘cÍ‘j ¾Ïðgy 13 14 −Ó©Îfngy 15 Û code/conc/tfgets-main.c Figure 12.47 Driver program for Problems 12.31–12.33. Homework Problems 1071 12.32 ◆◆◆ Implement a version of the Îð×−ÎÍ function from Problem 12.31 that uses the Í−Ä−²Î function. Your function should be implemented in a package called Îð×−ÎÍkÍ−Ä−²Îl². Test your solution using the driver program from Problem 12.31. You may assume that standard input is assigned to descriptor 0. 12.33 ◆◆◆ Implement a threaded version of the Îð×−ÎÍ function from Problem 12.31. Your function should be implemented in a package called Îð×−ÎÍkÎ³Ë−þ®l². Test your solution using the driver program from Problem 12.31. 12.34 ◆◆◆ Write a parallel threaded version of an N × M matrix multiplication kernel. Com- pare the performance to the sequential case. 12.35 ◆◆◆ Implement a concurrent version of the Tiny Web server based on processes. Your solution should create a new child process for each new connection request. Test your solution using a real Web browser. 12.36 ◆◆◆ Implement a concurrent version of the Tiny Web server based on I/O multiplexing. Test your solution using a real Web browser. 12.37 ◆◆◆ Implement a concurrent version of the Tiny Web server based on threads. Your solution should create a new thread for each new connection request. Test your solution using a real Web browser. 12.38 ◆◆◆◆ Implement a concurrent prethreaded version of the Tiny Web server. Your solu- tion should dynamically increase or decrease the number of threads in response to the current load. One strategy is to double the number of threads when the buffer becomes full, and halve the number of threads when the buffer becomes empty. Test your solution using a real Web browser. 12.39 ◆◆◆◆ A Web proxy is a program that acts as a middleman between a Web server and browser. Instead of contacting the server directly to get a Web page, the browser contacts the proxy, which forwards the request to the server. When the server replies to the proxy, the proxy sends the reply to the browser. For this lab, you will write a simple Web proxy that ﬁlters and logs requests: A. In the ﬁrst part of the lab, you will set up the proxy to accept requests, parse the HTTP, forward the requests to the server, and return the results to the browser. Your proxy should log the URLs of all requests in a log ﬁle on disk, and it should also block requests to any URL contained in a ﬁlter ﬁle on disk. 1072 Chapter 12 Concurrent Programming B. In the second part of the lab, you will upgrade your proxy to deal with multiple open connections at once by spawning a separate thread to handle each request. While your proxy is waiting for a remote server to respond to a request so that it can serve one browser, it should be working on a pending request from another browser. Check your proxy solution using a real Web browser. Solutions to Practice Problems Solution to Problem 12.1 (page 1011) When the parent process on the concurrent server starts executing, the reference counter increments from 0 to 1 for the associated ﬁle table. When this parent process forks the child process, the reference counter is incremented from 1 to 2. When the parent closes its copy of the descriptor, the reference count is decremented from 2 to 1. Similarly, when the child’s end of connection closes, the reference counter is decremented from 1 to 0. Solution to Problem 12.2 (page 1011) When a process terminates for any reason, the kernel closes all open descriptors. Thus, the parent’s copy of the connected ﬁle descriptor will be closed automatically when the parent exits. Solution to Problem 12.3 (page 1016) Recall that the −²³Ç function from Figure 11.22 echoes each line from the client until the client loses its end of the connection. If Ctrl+D is typed when the −²³Ç function is under execution, the server would consider it to be the EOF and may assume that the client has closed its end of connection and hence, may stop echoing back to the client. Solution to Problem 12.4 (page 1020) ÉÇÇÄlÅË−þ®Ô is an integer variable. We reinitialize the ÉÇÇÄlÅË−þ®Ô variable with the value obtained from the call to Í−Ä−²Î so as to store the total number of ready descriptors returned by Í−Ä−²Î. Solution to Problem 12.5 (page 1028) Yes, there are chances of memory leak if lines 31 or 32 are deleted from Fig- ure 12.14. Since the threads are not explicitly reaped, each thread must be de- tached so that its memory resource will be reclaimed when it terminates. Similarly, it is important to free the memory block that was allocated by the main thread. Solution to Problem 12.6 (page 1031) The main idea here is that stack variables are private, whereas global and static variables are shared. Static variables such as ²ÅÎ are a little tricky because the sharing is limited to the functions within their scope—in this case, the thread routine. Solutions to Practice Problems 1073 A. Here is the table: Referenced byVariable instance main thread? peer thread 0? peer thread 1? ÉÎË yes yes yes ²ÅÎ no yes yes ©lÀ yes no no ÀÍ×ÍlÀ yes yes yes ÀÔ©®lÉn no yes no ÀÔ©®lÉo no no yes Notes: ÉÎË A global variable that is written by the main thread and read by the peer threads. ²ÅÎ A static variable with only one instance in memory that is read and written by the two peer threads. ©lÀ A local automatic variable stored on the stack of the main thread. Even though its value is passed to the peer threads, the peer threads never reference it on the stack, and thus it is not shared. ÀÍ×ÍlÀ A local automatic variable stored on the main thread’s stack and referenced indirectly through ÉÎË by both peer threads. ÀÔ©®lÉn and ÀÔ©®lÉo Instances of a local automatic variable residing on the stacks of peer threads 0 and 1, respectively. B. Variables ÉÎË, ²ÅÎ, and ÀÍ×Í are referenced by more than one thread and thus are shared. Solution to Problem 12.7 (page 1034) The important idea here is that you cannot make any assumptions about the ordering that the kernel chooses when it schedules your threads. Step Thread Instr. cË®Ó1 cË®Ó2 ²ÅÎ 11 H1 —— 0 21 L1 0— 0 32 H2 —— 0 42 L2 —0 0 52 U2 —1 0 62 S2 —1 1 71 U1 1— 1 81 S1 1— 1 91 T1 1— 1 10 2 T2 —1 1 Variable ²ÅÎ has a ﬁnal incorrect value of 1. 1074 Chapter 12 Concurrent Programming Solution to Problem 12.8 (page 1037) This problem is a simple test of your understanding of safe and unsafe trajectories in progress graphs. Trajectories such as A and C that skirt the critical region are safe and will produce correct results. A. H1,L1,U1,S1,H2,L2,U2,S2,T2,T1: safe B. H2,L2,H1,L1,U1,S1,T1,U2,S2,T2: unsafe C. H1,H2,L2,U2,S2,L1,U1,S1,T1,T2: safe Solution to Problem 12.9 (page 1042) A. p = 1, c = 1, n> 1: Yes, the mutex semaphore is necessary because the producer and consumer can concurrently access the buffer. B. p = 1, c = 1, n = 1: No, the mutex semaphore is not necessary in this case, because a nonempty buffer is equivalent to a full buffer. When the buffer contains an item, the producer is blocked. When the buffer is empty, the consumer is blocked. So at any point in time, only a single thread can access the buffer, and thus mutual exclusion is guaranteed without using the mutex. C. p> 1, c> 1, n = 1: No, the mutex semaphore is not necessary in this case either, by the same argument as the previous case. Solution to Problem 12.10 (page 1044) Suppose that a particular semaphore implementation uses a LIFO stack of threads for each semaphore. When a thread blocks on a semaphore in a P operation, its ID is pushed onto the stack. Similarly, the V operation pops the top thread ID from the stack and restarts that thread. Given this stack implementation, an adversarial writer in its critical section could simply wait until another writer blocks on the semaphore before releasing the semaphore. In this scenario, a waiting reader might wait forever as two writers passed control back and forth. Notice that although it might seem more intuitive to use a FIFO queue rather than a LIFO stack, using such a stack is not incorrect and does not violate the semantics of the P and V operations. Solution to Problem 12.11 (page 1056) This problem is a simple sanity check of your understanding of speedup and parallel efﬁciency: Threads (t)1 4 8 Cores (p)1 4 8 Running time (Tp)16 8 4 Speedup (Sp)1 2 4 Efﬁciency (Ep) 100% 50% 25% Solution to Problem 12.12 (page 1060) The ËþÅ®ˆË function is implicitly reentrant function, because it passes the param- eter by reference; i.e., the parameter hÅ−ÓÎÉ and not by value. Explicit reentrant Solutions to Practice Problems 1075 functions pass arguments only by value and all data references are to local auto- matic stack variables. Solution to Problem 12.13 (page 1062) If we free the block immediately after the call to ÉÎ³Ë−þ®ˆ²Ë−þÎ− in line 14, then we will introduce a new race, this time between the call to ðË−− in the main thread and the assignment statement in line 24 of the thread routine. Solution to Problem 12.14 (page 1062) A. Another approach is to pass the integer © directly, rather than passing a pointer to ©: ðÇËf©{ny©zﬁy ©iig –Î³Ë−þ®ˆ²Ë−þÎ−fdÎ©®‰©`j ﬁ•‹‹j Î³Ë−þ®j fÌÇ©® hg©gy In the thread routine, we cast the argument back to an ©ÅÎ and assign it to ÀÔ©®: ©ÅÎ ÀÔ©® { f©ÅÎg ÌþË×Éy B. The advantage is that it reduces overhead by eliminating the calls to ÀþÄÄÇ² and ðË−−. A signiﬁcant disadvantage is that it assumes that pointers are at least as large as ©ÅÎs. While this assumption is true for all modern systems, it might not be true for legacy or future systems. Solution to Problem 12.15 (page 1065) A. The progress graph for the original program is shown in Figure 12.48 on the next page. B. The program always deadlocks, since any feasible trajectory is eventually trapped in a deadlock state. C. To eliminate the deadlock potential, initialize the binary semaphore Î to 1 instead of 0. D. The progress graph for the corrected program is shown in Figure 12.49. 1076 Chapter 12 Concurrent Programming. . . . . . . . . . . . . . . . . .. . .. . .. . .. . . Thread 2 Thread 1 V(s) P(s) P(s) V(s) P(t) V(t) P(t) V(t) Initially s\u00051 t\u00050 Forbidden region for t Forbidden region for s Forbidden region for t Figure 12.48 Progress graph for a program that deadlocks.. . . . . . . . . . . . . . .. . .. . .. . . Thread 2 Thread 1 V(s) P(s) P(s) V(s) P(t) V(t) P(t) V(t) Initially s\u00051 t\u00051 Forbidden region for s Forbidden region for t Figure 12.49 Progress graph for the corrected deadlock-free program. APPENDIX A Error Handling Programmers should always check the error codes returned by system-level func- tions. There are many subtle ways that things can go wrong, and it only makes sense to use the status information that the kernel is able to provide us. Unfortu- nately, programmers are often reluctant to do error checking because it clutters their code, turning a single line of code into a multi-line conditional statement. Error checking is also confusing because different functions indicate errors in dif- ferent ways. We were faced with a similar problem when writing this text. On the one hand, we would like our code examples to be concise and simple to read. On the other hand, we do not want to give students the wrong impression that it is OK to skip error checking. To resolve these issues, we have adopted an approach based on error-handling wrappers that was pioneered by W. Richard Stevens in his network programming text [110]. The idea is that given some base system-level function ðÇÇ, we deﬁne a wrapper function ƒÇÇ with identical arguments, but with the ﬁrst letter capitalized. The wrapper calls the base function and checks for errors. If it detects an error, the wrapper prints an informative message and terminates the process. Otherwise, it returns to the caller. Notice that if there are no errors, the wrapper behaves exactly like the base function. Put another way, if a program runs correctly with wrappers, it will run correctly if we render the ﬁrst letter of each wrapper in lowercase and recompile. The wrappers are packaged in a single source ﬁle (²ÍþÉÉl²) that is compiled and linked into each program. A separate header ﬁle (²ÍþÉÉl³) contains the function prototypes for the wrappers. This appendix gives a tutorial on the different kinds of error handling in Unix systems and gives examples of the different styles of error-handling wrappers. Copies of the ²ÍþÉÉl³ and ²ÍþÉÉl² ﬁles are available at the CS:APP Web site. 1077 1078 Appendix A Error Handling A.1 Error Handling in Unix Systems The systems-level function calls that we will encounter in this book use three different styles for returning errors: Unix-style, Posix-style, and GAI-style. Unix-Style Error Handling Functions such as ðÇËÂ and Ñþ©Î that were developed in the early days of Unix (as well as some older Posix functions) overload the function return value with both error codes and useful results. For example, when the Unix-style Ñþ©Î function encounters an error (e.g., there is no child process to reap), it returns −1 and sets the global variable −ËËÅÇ to an error code that indicates the cause of the error. If Ñþ©Î completes successfully, then it returns the useful result, which is the PID of the reaped child. Unix-style error-handling code is typically of the following form: 1 ©ð ffÉ©® { Ñþ©Îfﬁ•‹‹gg z ng Õ 2 ðÉË©ÅÎðfÍÎ®−ËËj ‘Ñþ©Î −ËËÇËx cÍ¿Å‘j ÍÎË−ËËÇËf−ËËÅÇggy 3 −Ó©Îfngy 4 Û The ÍÎË−ËËÇË function returns a text description for a particular value of −ËËÅÇ. Posix-Style Error Handling Many of the newer Posix functions such as Pthreads use the return value only to indicate success (zero) or failure (nonzero). Any useful results are returned in function arguments that are passed by reference. We refer to this approach as Posix-style error handling. For example, the Posix-style ÉÎ³Ë−þ®ˆ²Ë−þÎ− function indicates success or failure with its return value and returns the ID of the newly created thread (the useful result) by reference in its ﬁrst argument. Posix-style error-handling code is typically of the following form: 1 ©ð ffË−Î²Ç®− { ÉÎ³Ë−þ®ˆ²Ë−þÎ−fdÎ©®j ﬁ•‹‹j Î³Ë−þ®j ﬁ•‹‹gg _{ ng Õ 2 ðÉË©ÅÎðfÍÎ®−ËËj ‘ÉÎ³Ë−þ®ˆ²Ë−þÎ− −ËËÇËx cÍ¿Å‘j ÍÎË−ËËÇËfË−Î²Ç®−ggy 3 −Ó©Îfngy 4 Û The ÍÎË−ËËÇË function returns a text description for a particular value of Ë−Î²Ç®−. GAI-Style Error Handling The ×−Îþ®®Ë©ÅðÇ (GAI) and ×−ÎÅþÀ−©ÅðÇ functions return zero on success and a nonzero value on failure. GAI error-handling code is typically of the follow- ing form: 1 ©ð ffË−Î²Ç®− { ×−Îþ®®Ë©ÅðÇf³ÇÍÎj Í−ËÌ©²−j d³©ÅÎÍj dË−ÍÏÄÎgg _{ ng Õ 2 ðÉË©ÅÎðfÍÎ®−ËËj ‘×−Îþ®®Ë©ÅðÇ −ËËÇËx cÍ¿Å‘j ×þ©ˆÍÎË−ËËÇËfË−Î²Ç®−ggy 3 −Ó©Îfngy 4 Û Section A.2 Error-Handling Wrappers 1079 The ×þ©ˆÍÎË−ËËÇË function returns a text description for a particular value of Ë−Î²Ç®−. Summary of Error-Reporting Functions Thoughout this book, we use the following error-reporting functions to accommo- date different error-handling styles. a©Å²ÄÏ®− ‘²ÍþÉÉl³‘ ÌÇ©® ÏÅ©Óˆ−ËËÇËf²³þË hÀÍ×gy ÌÇ©® ÉÇÍ©Óˆ−ËËÇËf©ÅÎ ²Ç®−j ²³þË hÀÍ×gy ÌÇ©® ×þ©ˆ−ËËÇËf©ÅÎ ²Ç®−j ²³þË hÀÍ×gy ÌÇ©® þÉÉˆ−ËËÇËf²³þË hÀÍ×gy Returns: nothing As their names suggest, the ÏÅ©Óˆ−ËËÇË, ÉÇÍ©Óˆ−ËËÇË, and ×þ©ˆ−ËËÇË functions report Unix-style, Posix-style, and GAI-style errors and then terminate. The þÉÉˆ −ËËÇË function is included as a convenience for application errors. It simply prints its input and then terminates. Figure A.1 shows the code for the error-reporting functions. A.2 Error-Handling Wrappers Here are some examples of the different error-handling wrappers. Unix-style error-handling wrappers. Figure A.2 shows the wrapper for the Unix- style Ñþ©Î function. If the Ñþ©Î returns with an error, the wrapper prints an informative message and then exits. Otherwise, it returns a PID to the caller. Figure A.3 shows the wrapper for the Unix-style Â©ÄÄ function. Notice that this function, unlike Ñþ©Î, returns ÌÇ©® on success. Posix-style error-handling wrappers. Figure A.4 shows the wrapper for the Posix-style ÉÎ³Ë−þ®ˆ®−Îþ²³ function. Like most Posix-style functions, it does not overload useful results with error-return codes, so the wrapper returns ÌÇ©® on success. GAI-style error-handling wrappers. Figure A.5 shows the error-handling wrap- per for the GAI-style ×−Îþ®®Ë©ÅðÇ function. 1080 Appendix A Error Handling code/src/csapp.c 1 ÌÇ©® ÏÅ©Óˆ−ËËÇËf²³þË hÀÍ×g mh •Å©ÓkÍÎÔÄ− −ËËÇË hm 2 Õ 3 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍx cÍ¿Å‘j ÀÍ×j ÍÎË−ËËÇËf−ËËÅÇggy 4 −Ó©Îfngy 5 Û 6 7 ÌÇ©® ÉÇÍ©Óˆ−ËËÇËf©ÅÎ ²Ç®−j ²³þË hÀÍ×g mh –ÇÍ©ÓkÍÎÔÄ− −ËËÇË hm 8 Õ 9 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍx cÍ¿Å‘j ÀÍ×j ÍÎË−ËËÇËf²Ç®−ggy 10 −Ó©Îfngy 11 Û 12 13 ÌÇ©® ×þ©ˆ−ËËÇËf©ÅÎ ²Ç®−j ²³þË hÀÍ×g mh §−Îþ®®Ë©ÅðÇkÍÎÔÄ− −ËËÇË hm 14 Õ 15 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍx cÍ¿Å‘j ÀÍ×j ×þ©ˆÍÎË−ËËÇËf²Ç®−ggy 16 −Ó©Îfngy 17 Û 18 19 ÌÇ©® þÉÉˆ−ËËÇËf²³þË hÀÍ×g mh ¡ÉÉÄ©²þÎ©ÇÅ −ËËÇË hm 20 Õ 21 ðÉË©ÅÎðfÍÎ®−ËËj ‘cÍ¿Å‘j ÀÍ×gy 22 −Ó©Îfngy 23 Û code/src/csapp.c Figure A.1 Error-reporting functions. code/src/csapp.c 1 É©®ˆÎ „þ©Îf©ÅÎ hÍÎþÎÏÍg 2 Õ 3 É©®ˆÎ É©®y 4 5 ©ð ffÉ©® { Ñþ©ÎfÍÎþÎÏÍgg z ng 6 ÏÅ©Óˆ−ËËÇËf‘„þ©Î −ËËÇË‘gy 7 Ë−ÎÏËÅ É©®y 8 Û code/src/csapp.c Figure A.2 Wrapper for Unix-style Ñþ©Î function. Section A.2 Error-Handling Wrappers 1081 code/src/csapp.c 1 ÌÇ©® «©ÄÄfÉ©®ˆÎ É©®j ©ÅÎ Í©×ÅÏÀg 2 Õ 3 ©ÅÎ Ë²y 4 5 ©ð ffË² { Â©ÄÄfÉ©®j Í©×ÅÏÀgg z ng 6 ÏÅ©Óˆ−ËËÇËf‘«©ÄÄ −ËËÇË‘gy 7 Û code/src/csapp.c Figure A.3 Wrapper for Unix-style Â©ÄÄ function. code/src/csapp.c 1 ÌÇ©® –Î³Ë−þ®ˆ®−Îþ²³fÉÎ³Ë−þ®ˆÎ Î©®g Õ 2 ©ÅÎ Ë²y 3 4 ©ð ffË² { ÉÎ³Ë−þ®ˆ®−Îþ²³fÎ©®gg _{ ng 5 ÉÇÍ©Óˆ−ËËÇËfË²j ‘–Î³Ë−þ®ˆ®−Îþ²³ −ËËÇË‘gy 6 Û code/src/csapp.c Figure A.4 Wrapper for Posix-style ÉÎ³Ë−þ®ˆ®−Îþ²³ function. code/src/csapp.c 1 ÌÇ©® §−Îþ®®Ë©ÅðÇf²ÇÅÍÎ ²³þË hÅÇ®−j ²ÇÅÍÎ ²³þË hÍ−ËÌ©²−j 2 ²ÇÅÍÎ ÍÎËÏ²Î þ®®Ë©ÅðÇ h³©ÅÎÍj ÍÎËÏ²Î þ®®Ë©ÅðÇ hhË−Íg 3 Õ 4 ©ÅÎ Ë²y 5 6 ©ð ffË² { ×−Îþ®®Ë©ÅðÇfÅÇ®−j Í−ËÌ©²−j ³©ÅÎÍj Ë−Ígg _{ ng 7 ×þ©ˆ−ËËÇËfË²j ‘§−Îþ®®Ë©ÅðÇ −ËËÇË‘gy 8 Û code/src/csapp.c Figure A.5 Wrapper for GAI-style ×−Îþ®®Ë©ÅðÇ function. This page is intentionally left blank. References [1] Advanced Micro Devices, Inc. Software Optimization Guide for AMD64 Processors, 2005. Publication Number 25112. [2] Advanced Micro Devices, Inc. AMD64 Architecture Programmer’s Manual, Volume 1: Application Programming, 2013. Publication Number 24592. [3] Advanced Micro Devices, Inc. AMD64 Architecture Programmer’s Manual, Volume 3: General-Purpose and System Instructions, 2013. Publication Number 24594. [4] Advanced Micro Devices, Inc. AMD64 Architecture Programmer’s Manual, Volume 4: 128-Bit and 256-Bit Media Instructions, 2013. Publication Number 26568. [5] K. Arnold, J. Gosling, and D. Holmes. The Java Programming Language, Fourth Edition. Prentice Hall, 2005. [6] T. Berners-Lee, R. Fielding, and H. Frystyk. Hypertext transfer protocol - HTTP/1.0. RFC 1945, 1996. [7] A. Birrell. An introduction to programming with threads. Technical Report 35, Digital Systems Research Center, 1989. [8] A. Birrell, M. Isard, C. Thacker, and T. Wobber. A design for high-performance ﬂash disks. SIGOPS Operating Systems Review 41(2):88– 93, 2007. [9] G. E. Blelloch, J. T. Fineman, P. B. Gibbons, and H. V. Simhadri. Scheduling irregular parallel computations on hierarchical caches. In Proceedings of the 23rd Symposium on Parallelism in Algorithms and Architectures (SPAA), pages 355–366. ACM, June 2011. [10] S. Borkar. Thousand core chips: A technology perspective. In Proceedings of the 44th Design Automation Conference, pages 746–749. ACM, 2007. [11] D. Bovet and M. Cesati. Understanding the Linux Kernel, Third Edition. O’Reilly Media, Inc., 2005. [12] A. Demke Brown and T. Mowry. Taming the memory hogs: Using compiler-inserted releases to manage physical memory intelligently. In Proceedings of the 4th Symposium on Operating Systems Design and Implementation (OSDI), pages 31–44. Usenix, October 2000. [13] R. E. Bryant. Term-level veriﬁcation of a pipelined CISC microprocessor. Technical Report CMU-CS-05-195, Carnegie Mellon University, School of Computer Science, 2005. [14] R. E. Bryant and D. R. O’Hallaron. Introducing computer systems from a programmer’s perspective. In Proceedings of the Technical Symposium on Computer Science Education (SIGCSE), pages 90–94. ACM, February 2001. [15] D. Butenhof. Programming with Posix Threads. Addison-Wesley, 1997. [16] S. Carson and P. Reynolds. The geometry of semaphore programs. ACM Transactions on Programming Languages and Systems 9(1):25– 53, 1987. [17] J. B. Carter, W. C. Hsieh, L. B. Stoller, M. R. Swanson, L. Zhang, E. L. Brunvand, A. Davis, C.-C. Kuo, R. Kuramkote, M. A. Parker, L. Schaelicke, and T. Tateyama. Impulse: Building a smarter memory controller. In Proceedings of the 5th International Symposium on High Performance Computer Architecture (HPCA), pages 70–79. ACM, January 1999. [18] K. Chang, D. Lee, Z. Chishti, A. Alameldeen, C. Wilkerson, Y. Kim, and O. Mutlu. Improving DRAM performance by parallelizing refreshes with accesses. In Proceedings of the 20th International Symposium on High-Performance Computer Architecture (HPCA). ACM, February 2014. 1083 1084 References [19] S. Chellappa, F. Franchetti, and M. P ¨uschel. How to write fast numerical code: A small in- troduction. In Generative and Transformational Techniques in Software Engineering II, volume 5235 of Lecture Notes in Computer Science, pages 196–259. Springer-Verlag, 2008. [20] P. Chen, E. Lee, G. Gibson, R. Katz, and D. Patterson. RAID: High-performance, reliable secondary storage. ACM Computing Surveys 26(2):145–185, June 1994. [21] S. Chen, P. Gibbons, and T. Mowry. Improving index performance through prefetching. In Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data, pages 235–246. ACM, May 2001. [22] T. Chilimbi, M. Hill, and J. Larus. Cache- conscious structure layout. In Proceedings of the 1999 ACM Conference on Programming Language Design and Implementation (PLDI), pages 1–12. ACM, May 1999. [23] E. Coffman, M. Elphick, and A. Shoshani. System deadlocks. ACM Computing Surveys 3(2):67–78, June 1971. [24] D. Cohen. On holy wars and a plea for peace. IEEE Computer 14(10):48–54, October 1981. [25] P. J. Courtois, F. Heymans, and D. L. Parnas. Concurrent control with “readers” and “writers.” Communications of the ACM 14(10):667–668, 1971. [26] C. Cowan, P. Wagle, C. Pu, S. Beattie, and J. Walpole. Buffer overﬂows: Attacks and defenses for the vulnerability of the decade. In DARPA Information Survivability Conference and Expo (DISCEX), volume 2, pages 119–129, March 2000. [27] J. H. Crawford. The i486 CPU: Executing instructions in one clock cycle. IEEE Micro 10(1):27–36, February 1990. [28] V. Cuppu, B. Jacob, B. Davis, and T. Mudge. A performance comparison of contemporary DRAM architectures. In Proceedings of the 26th International Symposium on Computer Architecture (ISCA), pages 222–233, ACM, 1999. [29] B. Davis, B. Jacob, and T. Mudge. The new DRAM interfaces: SDRAM, RDRAM, and variants. In Proceedings of the 3rd International Symposium on High Performance Computing (ISHPC), volume 1940 of Lecture Notes in Computer Science, pages 26–31. Springer- Verlag, October 2000. [30] E. Demaine. Cache-oblivious algorithms and data structures. In Lecture Notes from the EEF Summer School on Massive Data Sets. BRICS, University of Aarhus, Denmark, 2002. [31] E. W. Dijkstra. Cooperating sequential processes. Technical Report EWD-123, Technological University, Eindhoven, the Netherlands, 1965. [32] C. Ding and K. Kennedy. Improving cache performance of dynamic applications through data and computation reorganizations at run time. In Proceedings of the 1999 ACM Conference on Programming Language Design and Implementation (PLDI), pages 229–241. ACM, May 1999. [33] M. Dowson. The Ariane 5 software failure. SIGSOFT Software Engineering Notes 22(2):84, 1997. [34] U. Drepper. User-level IPv6 programming introduction. Available at http://www.akkadia .org/drepper/userapi-ipv6.html, 2008. [35] M. W. Eichen and J. A. Rochlis. With micro- scope and tweezers: An analysis of the Internet virus of November, 1988. In Proceedings of the IEEE Symposium on Research in Security and Privacy, pages 326–343. IEEE, 1989. [36] ELF-64 Object File Format, Version 1.5 Draft 2, 1998. Available at http://www.uclibc.org/docs/ elf-64-gen.pdf. [37] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, and T. Berners-Lee. Hypertext transfer protocol - HTTP/1.1. RFC 2616, 1999. [38] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In Proceedings of the 40th IEEE Symposium on Foundations of Computer Science (FOCS), pages 285–297. IEEE, August 1999. [39] M. Frigo and V. Strumpen. The cache complex- ity of multithreaded cache oblivious algorithms. In Proceedings of the 18th Symposium on Paral- References 1085 lelism in Algorithms and Architectures (SPAA), pages 271–280. ACM, 2006. [40] G. Gibson, D. Nagle, K. Amiri, J. Butler, F. Chang, H. Gobioff, C. Hardin, E. Riedel, D. Rochberg, and J. Zelenka. A cost-effective, high-bandwidth storage architecture. In Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 92–103. ACM, October 1998. [41] G. Gibson and R. Van Meter. Network attached storage architecture. Communications of the ACM 43(11):37–45, November 2000. [42] Google. IPv6 Adoption. Available at http:// www.google.com/intl/en/ipv6/statistics.html. [43] J. Gustafson. Reevaluating Amdahl’s law. Communications of the ACM 31(5):532–533, August 1988. [44] L. Gwennap. New algorithm improves branch prediction. Microprocessor Report 9(4), March 1995. [45] S. P. Harbison and G. L. Steele, Jr. C, A Reference Manual, Fifth Edition. Prentice Hall, 2002. [46] J. L. Hennessy and D. A. Patterson. Computer Architecture: A Quantitative Approach, Fifth Edition. Morgan Kaufmann, 2011. [47] M. Herlihy and N. Shavit. The Art of Multi- processor Programming. Morgan Kaufmann, 2008. [48] C. A. R. Hoare. Monitors: An operating system structuring concept. Communications of the ACM 17(10):549–557, October 1974. [49] Intel Corporation. Intel 64 and IA-32 Ar- chitectures Optimization Reference Manual. Available at http://www.intel.com/content/ www/us/en/processors/architectures-software- developer-manuals.html. [50] Intel Corporation. Intel 64 and IA-32 Ar- chitectures Software Developer’s Manual, Volume 1: Basic Architecture. Available at http://www.intel.com/content/www/us/en/ processors/architectures-software-developer- manuals.html. [51] Intel Corporation. Intel 64 and IA-32 Ar- chitectures Software Developer’s Manual, Volume 2: Instruction Set Reference. Available at http://www.intel.com/content/www/us/en/ processors/architectures-software-developer- manuals.html. [52] Intel Corporation. Intel 64 and IA-32 Architec- tures Software Developer’s Manual, Volume 3a: System Programming Guide, Part 1. Available at http://www.intel.com/content/www/us/en/ processors/architectures-software-developer- manuals.html. [53] Intel Corporation. Intel Solid-State Drive 730 Series: Product Speciﬁcation. Available at http://www.intel.com/content/www/us/en/solid- state-drives/ssd-730-series-spec.html. [54] Intel Corporation. Tool Interface Standards Portable Formats Speciﬁcation, Version 1.1, 1993. Order number 241597. [55] F. Jones, B. Prince, R. Norwood, J. Hartigan, W. Vogley, C. Hart, and D. Bondurant. Memory—a new era of fast dynamic RAMs (for video applications). IEEE Spectrum, pages 43–45, October 1992. [56] R. Jones and R. Lins. Garbage Collection: Algorithms for Automatic Dynamic Memory Management. Wiley, 1996. [57] M. Kaashoek, D. Engler, G. Ganger, H. Briceo, R. Hunt, D. Maziers, T. Pinckney, R. Grimm, J. Jannotti, and K. MacKenzie. Application performance and ﬂexibility on Exokernel systems. In Proceedings of the 16th ACM Symposium on Operating System Principles (SOSP), pages 52–65. ACM, October 1997. [58] R. Katz and G. Borriello. Contemporary Logic Design, Second Edition. Prentice Hall, 2005. [59] B. W. Kernighan and R. Pike. The Practice of Programming. Addison-Wesley, 1999. [60] B. Kernighan and D. Ritchie. The C Program- ming Language, First Edition. Prentice Hall, 1978. [61] B. Kernighan and D. Ritchie. The C Program- ming Language, Second Edition. Prentice Hall, 1988. [62] Michael Kerrisk. The Linux Programming Interface. No Starch Press, 2010. [63] T. Kilburn, B. Edwards, M. Lanigan, and F. Sumner. One-level storage system. IRE 1086 References Transactions on Electronic Computers EC- 11:223–235, April 1962. [64] D. Knuth. The Art of Computer Programming, Volume 1: Fundamental Algorithms, Third Edition. Addison-Wesley, 1997. [65] J. Kurose and K. Ross. Computer Networking: A Top-Down Approach, Sixth Edition. Addison- Wesley, 2012. [66] M. Lam, E. Rothberg, and M. Wolf. The cache performance and optimizations of blocked algorithms. In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 63–74. ACM, April 1991. [67] D. Lea. A memory allocator. Available at http://gee.cs.oswego.edu/dl/html/malloc.html, 1996. [68] C. E. Leiserson and J. B. Saxe. Retiming synchronous circuitry. Algorithmica 6(1–6), June 1991. [69] J. R. Levine. Linkers and Loaders. Morgan Kaufmann, 1999. [70] David Levinthal. Performance Analysis Guide for Intel Core i7 Processor and Intel Xeon 5500 Processors. Available at https://software .intel.com/sites/products/collateral/hpc/vtune/ performance_analysis_guide.pdf. [71] C. Lin and L. Snyder. Principles of Parallel Programming. Addison Wesley, 2008. [72] Y. Lin and D. Padua. Compiler analysis of irregular memory accesses. In Proceedings of the 2000 ACM Conference on Programming Language Design and Implementation (PLDI), pages 157–168. ACM, June 2000. [73] J. L. Lions. Ariane 5 Flight 501 failure. Technical Report, European Space Agency, July 1996. [74] S. Macguire. Writing Solid Code. Microsoft Press, 1993. [75] S. A. Mahlke, W. Y. Chen, J. C. Gyllenhal, and W. W. Hwu. Compiler code transformations for superscalar-based high-performance systems. In Proceedings of the 1992 ACM/IEEE Conference on Supercomputing, pages 808–817. ACM, 1992. [76] E. Marshall. Fatal error: How Patriot over- looked a Scud. Science, page 1347, March 13, 1992. [77] M. Matz, J. Hubiˇcka, A. Jaeger, and M. Mitchell. System V application binary interface AMD64 architecture processor supplement. Technical Report, x86-64.org, 2013. Available at http:// www.x86-64.org/documentation_folder/abi-0 .99.pdf. [78] J. Morris, M. Satyanarayanan, M. Conner, J. Howard, D. Rosenthal, and F. Smith. Andrew: A distributed personal computing environment. Communications of the ACM, pages 184–201, March 1986. [79] T. Mowry, M. Lam, and A. Gupta. Design and evaluation of a compiler algorithm for prefetching. In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 62–73. ACM, October 1992. [80] S. S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann, 1997. [81] S. Nath and P. Gibbons. Online maintenance of very large random samples on ﬂash storage. In Proceedings of VLDB, pages 970–983. VLDB Endowment, August 2008. [82] M. Overton. Numerical Computing with IEEE Floating Point Arithmetic. SIAM, 2001. [83] D. Patterson, G. Gibson, and R. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data, pages 109–116. ACM, June 1988. [84] L. Peterson and B. Davie. Computer Networks: A Systems Approach, Fifth Edition. Morgan Kaufmann, 2011. [85] J. Pincus and B. Baker. Beyond stack smashing: Recent advances in exploiting buffer overruns. IEEE Security and Privacy 2(4):20–27, 2004. [86] S. Przybylski. Cache and Memory Hierarchy Design: A Performance-Directed Approach. Morgan Kaufmann, 1990. [87] W. Pugh. The Omega test: A fast and practical integer programming algorithm for depen- References 1087 dence analysis. Communications of the ACM 35(8):102–114, August 1992. [88] W. Pugh. Fixing the Java memory model. In Proceedings of the ACM Conference on Java Grande, pages 89–98. ACM, June 1999. [89] J. Rabaey, A. Chandrakasan, and B. Nikolic. Digital Integrated Circuits: A Design Perspec- tive, Second Edition. Prentice Hall, 2003. [90] J. Reinders. Intel Threading Building Blocks. O’Reilly, 2007. [91] D. Ritchie. The evolution of the Unix time- sharing system. AT&T Bell Laboratories Technical Journal 63(6 Part 2):1577–1593, October 1984. [92] D. Ritchie. The development of the C language. In Proceedings of the 2nd ACM SIGPLAN Conference on History of Programming Languages, pages 201–208. ACM, April 1993. [93] D. Ritchie and K. Thompson. The Unix time- sharing system. Communications of the ACM 17(7):365–367, July 1974. [94] M. Satyanarayanan, J. Kistler, P. Kumar, M. Okasaki, E. Siegel, and D. Steere. Coda: A highly available ﬁle system for a distributed workstation environment. IEEE Transactions on Computers 39(4):447–459, April 1990. [95] J. Schindler and G. Ganger. Automated disk drive characterization. Technical Report CMU- CS-99-176, School of Computer Science, Carnegie Mellon University, 1999. [96] F. B. Schneider and K. P. Birman. The monoculture risk put into context. IEEE Security and Privacy 7(1):14–17, January 2009. [97] R. C. Seacord. Secure Coding in C and C++, Second Edition. Addison-Wesley, 2013. [98] R. Sedgewick and K. Wayne. Algorithms, Fourth Edition. Addison-Wesley, 2011. [99] H. Shacham, M. Page, B. Pfaff, E.-J. Goh, N. Modadugu, and D. Boneh. On the effec- tiveness of address-space randomization. In Proceedings of the 11th ACM Conference on Computer and Communications Security (CCS), pages 298–307. ACM, 2004. [100] J. P. Shen and M. Lipasti. Modern Processor De- sign: Fundamentals of Superscalar Processors. McGraw Hill, 2005. [101] B. Shriver and B. Smith. The Anatomy of a High-Performance Microprocessor: A Systems Perspective. IEEE Computer Society, 1998. [102] A. Silberschatz, P. Galvin, and G. Gagne. Operating Systems Concepts, Ninth Edition. Wiley, 2014. [103] R. Skeel. Roundoff error and the Patriot missile. SIAM News 25(4):11, July 1992. [104] A. Smith. Cache memories. ACM Computing Surveys 14(3), September 1982. [105] E. H. Spafford. The Internet worm program: An analysis. Technical Report CSD-TR-823, Department of Computer Science, Purdue University, 1988. [106] W. Stallings. Operating Systems: Internals and Design Principles, Eighth Edition. Prentice Hall, 2014. [107] W. R. Stevens. TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP and the Unix Domain Protocols. Addison-Wesley, 1996. [108] W. R. Stevens. Unix Network Programming: Interprocess Communications, Second Edition, volume 2. Prentice Hall, 1998. [109] W. R. Stevens and K. R. Fall. TCP/IP Illustrated, Volume 1: The Protocols, Second Edition. Addison-Wesley, 2011. [110] W. R. Stevens, B. Fenner, and A. M. Rudoff. Unix Network Programming: The Sockets Networking API, Third Edition, volume 1. Prentice Hall, 2003. [111] W. R. Stevens and S. A. Rago. Advanced Programming in the Unix Environment, Third Edition. Addison-Wesley, 2013. [112] T. Stricker and T. Gross. Global address space, non-uniform bandwidth: A memory system performance characterization of parallel systems. In Proceedings of the 3rd International Symposium on High Performance Computer Architecture (HPCA), pages 168–179. IEEE, February 1997. 1088 References [113] A. S. Tanenbaum and H. Bos. Modern Operating Systems, Fourth Edition. Prentice Hall, 2015. [114] A. S. Tanenbaum and D. Wetherall. Computer Networks, Fifth Edition. Prentice Hall, 2010. [115] K. P. Wadleigh and I. L. Crawford. Software Optimization for High-Performance Comput- ing: Creating Faster Applications. Prentice Hall, 2000. [116] J. F. Wakerly. Digital Design Principles and Practices, Fourth Edition. Prentice Hall, 2005. [117] M. V. Wilkes. Slave memories and dynamic storage allocation. IEEE Transactions on Electronic Computers, EC-14(2), April 1965. [118] P. Wilson, M. Johnstone, M. Neely, and D. Boles. Dynamic storage allocation: A survey and critical review. In International Workshop on Memory Management, volume 986 of Lecture Notes in Computer Science, pages 1–116. Springer-Verlag, 1995. [119] M. Wolf and M. Lam. A data locality algorithm. In Proceedings of the 1991 ACM Conference on Programming Language Design and Implementation (PLDI), pages 30–44, June 1991. [120] G. R. Wright and W. R. Stevens. TCP/IP Illustrated, Volume 2: The Implementation. Addison-Wesley, 1995. [121] J. Wylie, M. Bigrigg, J. Strunk, G. Ganger, H. Kiliccote, and P. Khosla. Survivable information storage systems. IEEE Computer 33:61–68, August 2000. [122] T.-Y. Yeh and Y. N. Patt. Alternative implemen- tation of two-level adaptive branch prediction. In Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA), pages 451–461. ACM, 1998. Index Page numbers of deﬁning references are italicized. Entries that belong to a hard- ware or software system are followed by a tag in brackets that identiﬁes the system, along with a brief description to jog your memory. Here is the list of tags and their meanings. [C] C language construct [C Stdlib] C standard library function [CS:APP] Program or function developed in this text [HCL] HCL language construct [Unix] Unix program, function, variable, or constant [x86-64] x86-64 machine-language instruction [Y86-64] Y86-64 machine-language instruction _ [HCL] not operation, 409 b for immediate operands, 217 d [C] address of operation local variables, 284 logic gates, 409 pointers, 84, 224, 293, 313 h [C] dereference pointer operation, 224 k| [C] dereference and select ﬁeld operation, 302 . (periods) in dotted-decimal notation, 962 ÚÚ [HCL] or operation, 409 z operator for left hoinkies, 945 zz “put to” operator (C++), 926 | operator for right hoinkies, 945 || “get from” operator (C++), 926 i t w (two’s-complement addition), 96, 126 h t w (two’s-complement multiplica- tion), 96, 133 k t w (two’s-complement negation), 96, 131 i u w (unsigned addition), 96, 121, 125 h u w (unsigned multiplication), 96, 132 k u w (unsigned negation), 96, 125 8086 microprocessor, 203 8087 ﬂoating-point coprocessor, 145, 173, 203 80286 microprocessor, 203 lþ archive ﬁles, 722 þlÇÏÎ object ﬁle, 709 Abel, Niels Henrik, 125 abelian group, 125 ABI (application binary interface), 346 abort exception class, 762 aborts, 764 absolute addressing relocation type, 727, 729–730 absolute pathnames, 929 absolute speedup of parallel programs, 1055 abstract operation model for Core i7, 561–567 abstractions, 63 þ²²−ÉÎ [Unix] wait for client connection request, 969, 972, 972–973 access disks, 633–636 IA32 registers, 215–216 main memory, 623–625 x86-64 registers data movement, 218–225 operand speciﬁers, 216–218 access permission bits, 930 access time for disks, 629, 629–631 accumulator variable expansion, 606 accumulators, multiple, 572–577 Acorn RISC machine (ARM) ISAs, 388 processor architecture, 399 actions, signal, 798 active sockets, 971 actuator arms, 628 acyclic networks, 410 adapters, 45, 633 add [instruction class] add, 228 þ®®ˆ²Ä©−ÅÎ function, 1017, 1019 add every signal to signal set instruction, 801 add instruction, 228 add operation in execute stage, 444 add signal to signal set instruction, 801 þ®®−Ë [CS:APP] CGI adder, 991 addition ﬂoating point, 158–160, 338 two’s complement, 126, 126–131 unsigned, 120–126, 121 Y86-64, 392 additive inverse, 88 1089 1090 Index þ®®Ê [Y86-64] add, 392, 438 address exceptions, status code for, 440 address of operator (&) [C] local variables, 284 logic gates, 409 pointers, 84, 224, 293, 313 address order of free lists, 899 address partitioning in caches, 651, 651–652 address-space layout randomization (ASLR), 321, 321–322 address spaces, 840 child processes, 777 linear, 840 private, 770 virtual, 840–841 address translation, 840 caches and VM integration, 853 Core i7, 862–864 end-to-end, 857–861 multi-level page tables, 855–857 optimizing, 866 overview, 849–852 TLBs for, 853–855 addresses and addressing byte ordering, 78–85 effective, 726 ﬂat, 203 internet, 958 invalid address status code, 400 I/O devices, 634 IP, 960, 961–963 machine-level programming, 206– 207 operands, 217 out of bounds. See buffer overﬂow physical vs. virtual, 839–840 pointers, 293, 313 procedure return, 276 segmented, 323–324 sockets, 966, 969–970 structures, 301–303 symbol relocation, 726–727 virtual, 840 virtual memory, 70 Y86-64, 392, 395 addressing modes, 217 adjacency matrices, 696 ¡⁄‡ [Y86-64] status code indicating invalid address, 400 Advanced Micro Devices (AMD), 201, 204 Intel compatibility, 204 x86-64. See x86-64 microprocessors Advanced Research Projects Administration (ARPA), 967 advanced vector extensions (AVX) instructions, 330, 582–583 AFS (Andrew File System), 646 aggregate data types, 207 aggregate payloads, 881 cþÄ [x86-64] low order 8 of register cËþÓ, 216 þÄþËÀ [Unix] schedule alarm to self, 798, 799 algebra, Boolean, 86–89, 88 aliasing memory, 535, 536 .align directive, 402 alignment data, 309, 309–312 memory blocks, 880 þÄÄÇ²þ [Unix] stack storage allocation function, 321, 326, 360 allocate and initialize bounded buffer function, 1043 allocate heap block function, 896, 897 allocate heap storage function, 876 allocated bit, 884 allocated blocks vs. free, 875 placement, 885 allocation blocks, 896 dynamic memory. See dynamic memory allocation pages, 846 allocators block allocation, 896 block freeing and coalescing, 896 free list creation, 893–895 free list manipulation, 892–893 general design, 890–892 practice problems, 897–898 requirements and goals, 880–881 styles, 875–876 Alpha (Compaq Computer Corp.) RISC processors, 399 alternate representations of signed integers, 104 ¡‹•¡⁄⁄ [Y86-64] function code for þ®®Ê instruction, 440 ALUs (arithmetic/logic units), 46 combinational circuits, 416 in execute stage, 421 sequential Y86-64 implementation, 444–445 always taken branch prediction strategy, 464 AMD (Advanced Micro Devices), 201, 204 Intel compatibility, 204 microprocessor data alignment, 312 x86-64. See x86-64 microprocessors Amdahl, Gene, 58 Amdahl’s law, 58, 58–60, 598, 604 American National Standards Institute (ANSI), 40,71 ampersands (&) address operator, 284 local addresses, 284 logic gates, 409 pointers, 84, 224, 293, 313 and [instruction class] and, 228 and instruction, 228 and operations Boolean, 87–88 execute stage, 444 HCL expressions, 410–411 logic gates, 409 logical, 92–93 and packed double precision instruction, 341 and packed single precision instruction, 341 þÅ®Ê [Y86-64] and, 392 Andreesen, Marc, 985 Andrew File System (AFS), 646 anonymous ﬁles, 869 ANSI (American National Standards Institute), 40,71 ¡ﬂ« [Y86-64] status code for normal operation, 399 þÉÉˆ−ËËÇË [CS:APP] reports application errors, 1079 application binary interface (ABI), 346 applications, loading and linking shared libraries from, 737–739 ar Linux archiver, 722, 749 arbitrary size arithmetic, 121 Archimedes, 176 architecture ﬂoating-point, 329, 329–332 Y86. See Y86-64 instruction set architecture archives, 722 areal density of disks, 627 areas shared, 870 swap, 869 virtual memory, 866 arguments −Ó−²Ì− function, 786 Web servers, 989–990 Index 1091 arithmetic, 69, 227 discussion, 232–233 ﬂoating-point code, 338–340 integer. See integer arithmetic latency and issue time, 559 load effective address, 227–229 pointers, 293–294, 909 saturating, 170 shift operations, 94, 140–142, 228, 230–232 special, 233–236 unary and binary, 230–232 arithmetic/logic units (ALUs), 46 combinational circuits, 416 in execute stage, 421 sequential Y86-64 implementation, 444–445 ARM (Acorn RISC machine), 79 ISAs, 388 processor architecture, 399 ARM A7 microprocessor, 389 arms, actuator, 628 ARPA (Advanced Research Projects Administration), 967 ARPANET, 967 arrays, 291 basic principles, 291–293 declarations, 291–292, 299 DRAM, 618 ﬁxed-size, 296–298 machine-code representation, 207 nested, 294–296 pointer arithmetic, 293–294 pointer relationships, 84, 313 stride, 642 variable-size, 298–301 ASCII standard, 39 character codes, 85 limitations, 86 þÍ²Î©À− function, 1060 ASLR (address-space layout randomization), 321, 321–322 þÍÀ directive, 214 assembler directives, 402 assemblers, 41, 41, 200, 206 assembly code, 41, 200 with C programs, 325–326 formatting, 211–213 Y86-64, 395 assembly phase, 41 associate socket address with descriptor function, 971, 971 associative caches, 660–662 associative memory, 661 associativity caches, 669 ﬂoating-point addition, 159–160 asterisks (h) dereference pointer operation, 224, 293, 313 asymmetric ranges in two’s- complement representation, 102, 113 async-signal-safe function, 802 async-signal safety, 802 asynchronous interrupts, 762 atomic reads and writes, 806 ATT assembly code format, 213, 330, 347 argument listing, 342 condition codes, 237–238 ²ÊÇ instruction, 235 vs. Intel, 213 operands, 217, 228 Y86-64, 392 automatic variables, 1030 AVX (advanced vector extensions) instructions, 312, 330, 582–583 cþÓ [x86-64] low order 16 bits of register cËþÓ, 216 B2T (binary to two’s-complement conversion), 96, 100, 108, 133 B2U (binary to unsigned conversion), 96, 98, 108, 118, 133 background processes, 789, 789–792 backlogs for listening sockets, 971 backups for disks, 647 backward compatibility, 71 backward taken, forward not taken (BTFNT) branch prediction strategy, 464 bad pointers and virtual memory, 906–907 ¾þ®²ÅÎl² [CS:APP] improperly synchronized program, 1031– 1035, 1032 bandwidth, read, 675 Barracuda 7400 drives, 636 base pointers, 326 base registers, 217 ¾þÍ³ [Unix] Unix shell program, 789 basic blocks, 605 Bell Laboratories, 71 Berkeley sockets, 968 Berners-Lee, Tim, 985 best-ﬁt block placement policy, 885, 885 bi-endian ordering convention, 79 biased number encoding, 149, 149–153 biasing in division, 142 big-endian ordering convention, 78, 78–80 bigrams statistics, 601 bijections, 100, 100 m¾©ÅmÂ©ÄÄ program, 796 binary ﬁles, 39, 927 binary notation, 68 binary points, 146, 146–147 binary representations conversions with hexadecimal, 72–73 signed and unsigned, 106–112 to two’s complement, 100, 108–109, 133 to unsigned, 98–99 fractional, 145–148 machine language, 230 binary semaphores, 1039 binary tree structure, 306–307 ¾©Å® [Unix] associate socket address with descriptor, 969, 971, 971 binding, lazy, 742 binutils package, 749 bistable memory cells, 617 bit-level operations, 90–92 bit representation expansion, 112–116 bit vectors, 87, 87–88 bits, 39 overview, 68 union access to, 307–308 bitwise operations, 341–342 c¾Ä [x86-64] low order 8 of register cË¾Ó, 216 block and unblock signals instruction, 801 block devices, 928 block offset bits, 652 block pointers, 892 block size caches, 669 minimum, 884 ¾ÄÇ²Â−® bit vectors, 795 blocked signals, 794, 795, 800–801 blocking signals, 800–801 for temporal locality, 683 blocks aligning, 880 allocated, 875, 885 vs. cache lines, 670 caches, 647, 647–648, 651, 669 coalescing, 886–887, 896 epilogue, 891 free lists, 883–885 freeing, 896 heap, 875 logical disk, 631, 631–632, 637 prologue, 891 1092 Index blocks (continued) referencing data in, 910–911 splitting, 885–886 bodies, response, 988 ¾ÇÇÄ [HCL] bit-level signal, 410 Boole, George, 86 Boolean algebra and functions, 86 HCL, 410–411 logic gates, 409 properties, 88 working with, 86–89 Boolean rings, 88 bottlenecks, 598 proﬁlers, 601–604 program proﬁling, 598–600 bottom of stack, 226 boundary tags, 887, 887–890, 895 bounded buffers, 1040, 1041– 1042 bounds latency, 554, 560 throughput, 554, 560 c¾É [x86-64] low order 16 bits of register cË¾É, 216 c¾ÉÄ [x86-64] low order 8 of register cË¾É, 216 branch prediction, 555, 555 misprediction handling, 479–480 performance, 585–589 Y86-64 pipelining, 464 branch prediction logic, 251 branches, conditional, 208, 245 assembly form, 247 condition codes, 237–238 condition control, 245–249 moves, 250–256, 586–589 ÍÑ©Î²³, 268–274 ¾Ë−þÂ command in gdb, 316 with ÍÑ©Î²³, 269 ¾Ë−þÂ ÀÏÄÎÍÎÇË− command in gdb, 316 breakpoints, 315–316 bridged Ethernet, 956, 957 bridges Ethernet, 956 I/O, 623 browsers, 984, 985 l¾ÍÍ section, 710 BTFNT (backward taken, forward not taken) branch prediction strategy, 464 bubbles, pipeline, 470, 470–471, 495–496 buddies, 901 buddy systems, 901, 901 buffer overﬂow, 315 execution code regions limits for, 325–326 memory-related bugs, 907 overview, 315–320 stack corruption detection for, 322–325 stack randomization for, 320–322 vulnerabilities, 43 buffered I/O functions, 934–938 buffers bounded, 1040, 1041–1042 read, 934, 936–937 store, 593–594 streams, 947 bus transactions, 623 buses, 44, 623 designs, 624, 634 I/O, 632 memory, 623 bypassing for data hazards, 472–475 byte data connections in hardware diagrams, 434 byte order, 78–85 disassembled code, 245 network, 961 unions, 308 bytes, 39, 70 copying, 169 range, 72 register operations, 217 Y86 encoding, 395–396 c¾Ó [x86-64] low order 16 bits of register cË¾Ó, 216 C language bit-level operations, 90–92 ﬂoating-point representation, 160–162 history, 71 logical operations, 92–93 origins, 40 shift operations, 93–95 static libraries, 720–724 C++ language, 713 linker symbols, 716 objects, 302–303 software exceptions, 759–760, 822 l² source ﬁles, 707 C standard library, 40–41, 42 C11 standard, 71 C90 standard, 71 C99 standard, 71 ﬁxed data sizes, 77 integral data types, 103 cache block offset (CO), 859 cache blocks, 651 cache-friendly code, 669–675, 670 cache lines cache sets, 651 vs. sets and blocks, 670 cache-oblivious algorithms, 685 cache set index (CI), 859 cache tags (CT), 859 cached pages, 842 caches and cache memory, 646, 651 address translation, 859 anatomy, 667 associativity, 669 cache-friendly code, 669–675, 670 data, 556, 667, 667 direct-mapped. See direct-mapped caches DRAM, 842 fully associative, 663–664 hits, 648 importance, 47–50 instruction, 554, 667, 667 locality in, 641, 679–683, 846 managing, 649 memory mountains, 675–679 misses, 506, 648, 648–649 organization, 651–653 overview, 646–648 page allocation, 846 page faults, 844, 844–845 page hits, 844 page tables, 842–844, 843 performance, 569, 667–669, 675–683 practice problems, 664–666 proxy, 988 purpose, 616 set associative, 660, 660–662 size, 668 SRAM, 842 symbols, 653 virtual memory with, 841–847, 853 write issues, 666–667 write strategies, 669 Y86-64 pipelining, 505–506 ²þÄÄ [x86-64] procedure call, 277–278, 393 ²þÄÄ [Y86-64] instruction, 440, 464 callee procedures, 287 callee-save registers, 287, 287–288 caller procedures, 287 caller-save registers, 287, 287–288 calling environments, 819 ²þÄÄÇ² function [C Stdlib] memory allocation declaration, 170 dynamic memory allocation, 877 Index 1093 security vulnerability, 136–137 ²þÄÄÊ [x86-64] procedure call, 277 calls, 53, 763–764 error handling, 773–774 Linux/x86-64 systems, 766–767 in performance, 548–549 canary values, 322–323 canceling mispredicted branch handling, 480 capacity caches, 651 disks, 627, 627–628 functional units, 559 capacity misses, 649 cards, graphics, 633 carriage return (CR) characters, 928 carry ﬂag condition code, 237, 342 CAS (column access strobe) requests, 619 case expressions in HCL, 414, 414 casting, 80 explicit, 111 ﬂoating-point values, 161 pointers, 314, 890 signed values, 106–107 catching signals, 794, 797, 799 cells DRAM, 618, 619 SRAM, 617 central processing units (CPUs), 45, 45–46 Core i7. See Core i7 microproces- sors early instruction sets, 397 effective cycle time, 638 embedded, 399 Intel. See Intel microprocessors logic design. See logic design many-core, 507 multi-core, 52, 60–61, 204, 641, 1008 overview, 388–390 pipelining. See pipelining RAM, 420 sequential Y86 implementation. See sequential Y86-64 implementation superscalar, 62, 507, 554 trends, 638–639 Y86. See Y86-64 instruction set architecture Cerf, Vinton, 967 CERT (Computer Emergency Response Team), 136 £ƒ [x86-64] carry ﬂag condition code, 237, 342 CGI (common gateway interface) program, 989, 989–991 CGI adder function, 991 chains, proxy, 988 ²³þË [C] data types, 76, 97 character codes, 85 character devices, 928 ²³−²Âˆ²Ä©−ÅÎÍ function, 1017, 1020 child processes, 776 creating, 777–779 default behavior, 780 error conditions, 781–782 exit status, 781 reaping, 779, 779–785 Ñþ©ÎÉ©® function, 782–785 CI (cache set index), 859 circuits combinational, 410, 410–416 retiming, 457 sequential, 417 CISC (complex instruction set computers), 397, 397–399 c²Ä [x86-64] low order 8 of register cË²Ó, 216 Clarke, Dave, 967 classes data hazards, 471 exceptions, 762–764 instructions, 218 size, 899 storage, 1030–1031 clear bit in descriptor set macro, 1014 clear descriptor set macro, 1014 clear signal set instruction, 801 client-server model, 954, 954–955 ²Ä©−ÅÎ−ËËÇË [CS:APP] Tiny helper function, 995–996 clients client-server model, 954 telnet, 57 clock signals, 417 clocked registers, 437–438 clocking in logic design, 417–420 ²ÄÇÍ− [Unix] close ﬁle, 930, 930–931 close operations for ﬁles, 927, 930–931 close shared library function, 738 ²ÄÇÍ−®©Ë functions, 941 ²ÄÎÊ [x86-64] Sign extend c−þÓ to cËþÓ, 221 ²ÀÇÌþ [x86-64] move if unsigned greater, 253 ²ÀÇÌþ− [x86-64] move if unsigned greater or equal, 253 ²ÀÇÌ¾ [x86-64] move if unsigned less, 253 ²ÀÇÌ¾− [x86-64] move if unsigned less or equal, 253 ²ÀÇÌ− [Y86-64] move when equal, 393 ²ÀÇÌ× [x86-64] move if greater, 253, 393 ²ÀÇÌ×− [x86-64] move if greater or equal, 253, 393 ²ÀÇÌÄ [x86-64] move if less, 253, 393 ²ÀÇÌÄ− [x86-64] move if less or equal, 253, 393 ²ÀÇÌÅþ [x86-64] move if not unsigned greater, 253 ²ÀÇÌÅþ− [x86-64] move if unsigned greater or equal, 253 ²ÀÇÌÅ¾ [x86-64] move if not unsigned less, 253 ²ÀÇÌÅ¾− [x86-64] move if not unsigned less or equal, 253 ²ÀÇÌÅ− [x86-64] move if not equal, 253, 393 ²ÀÇÌÅ× [x86-64] move if not greater, 253 ²ÀÇÌÅ×− [x86-64] move if not greater or equal, 253 ²ÀÇÌÅÄ [x86-64] move if not less, 253 ²ÀÇÌÅÄ− [x86-64] move if not less or equal, 253 ²ÀÇÌÅÍ [x86-64] move if nonnegative, 253 ²ÀÇÌÅÖ [x86-64] move if not zero, 253 ²ÀÇÌÉ [x86-64] move if even parity, 360 ²ÀÇÌÍ [x86-64] move if negative, 253 ²ÀÇÌÖ [x86-64] move if zero, 253 cmp [instruction class] Compare, 238 ²ÀÉ¾ [x86-64] compare byte, 238 ²ÀÉÄ [x86-64] compare double word, 238 ²ÀÉÊ [x86-64] compare double word, 238 ²ÀÉÑ [x86-64] compare word, 238 cmtest script, 501 CO (cache block offset), 859 coalescing blocks, 896 with boundary tags, 887–890 free, 886 memory, 883 Cocke, John, 397 code performance strategies, 597–598 proﬁlers, 598–600 representing, 85–86 self-modifying, 471 Y86 instructions, 394, 395–396 code motion, 544 1094 Index code segments, 732, 733–734 Cohen, Danny, 79 cold caches, 648 cold misses, 648 Cold War, 967 collectors, garbage, 875, 902 basics, 902–903 conservative, 903, 905–906 Mark&Sweep, 903–906 column access strobe (CAS) requests, 619 column-major sum function, 672 combinational circuits, 410, 410–416 combinational pipelines, 448–450, 496–498 common gateway interface (CGI) program, 989, 989–991 Compaq Computer Corp. RISC processors, 399 compare byte instruction, 238 compare double precision, 342 compare double word instruction, 238 compare instructions, 238 compare single precision, 342 compare word instruction, 238 comparison operations for ﬂoating- point code, 342–345 compilation phase, 41 compilation systems, 42, 42–43 compile time, 706 compile-time interpositioning, 744– 745 compiler drivers, 40, 707–708 compilers, 42, 200 optimizing capabilities and limitations, 534–538 process, 205–206 purpose, 207 complement instruction, 228 complex instruction set computers (CISC), 397, 397–399 compulsory misses, 648 computation stages in pipelining, 457–458 computed ×ÇÎÇ, 269 Computer Emergency Response Team (CERT), 136 computer systems, 38 concurrency, 1008 ECF for, 759 ﬂow synchronizing, 812–814 and parallelism, 60 run, 769 thread-level, 60–62 concurrent execution, 769 concurrent ﬂow, 769, 769–770 concurrent processes, 51,52 concurrent programming, 1008–1009 deadlocks, 1063–1066 with I/O multiplexing, 1014–1021 library functions in, 1060–1061 with processes, 1009–1013 races, 1061–1063 reentrancy issues, 1059–1060 shared variables, 1028–1031 summary, 1066 threads, 1021–1028 for parallelism, 1049–1054 safety issues, 1056–1058 concurrent programs, 1008 concurrent servers, 1008 based on prethreading, 1041–1049 based on processes, 1010–1011 based on threads, 1027–1028 condition code registers, 207 hazards, 471 SEQ timing, 437–438 condition codes, 237, 237–238 accessing, 238–241 x86-64, 237 Y86-64, 391–393 condition variables, 1046 conditional branches, 208, 245 assembly form, 247 condition codes, 237–238 condition control, 245–249 moves, 250–256, 586–589 ÍÑ©Î²³, 268–274 conﬂict misses, 649, 658–660 ²ÇÅÅ−²Î [Unix] establish connection with server, 970, 970–971 connected descriptors, 972, 972–973 connections EOF on, 984 Internet, 961, 965–967 I/O devices, 632–633 persistent, 988 conservative garbage collectors, 903, 905–906 constant words in Y86-64, 395 constants ﬂoating-point code, 340–341 free lists, 892–893 maximum and minimum values, 104 multiplication, 137–139 for ranges, 103–104 Unix, 782 content dynamic, 989–990 serving, 985 Web, 984, 985–986 context switches, 52, 772–773 contexts, 772 processes, 52, 768 thread, 1022, 1029 ²ÇÅÎ©ÅÏ− command, 316 Control Data Corporation 6600 processor, 558 control dependencies in pipelining, 455, 465 control ﬂow, 758 exceptional. See exceptional control ﬂow (ECF) logical, 768, 768–769 machine-language procedures, 275 control hazards, 465 control logic blocks, 434, 434, 441, 462 control logic in pipelining, 491 control mechanism combinations, 496–498 control mechanisms, 495–496 design testing and verifying, 501 implementation, 498–500 special cases, 491–493 special conditions, 493–495 control structures, 236–237 condition codes, 236–241 conditional branches, 245–249 conditional move instructions, 250–256 jumps, 241–245 loops. See loops ÍÑ©Î²³ statements, 268–274 control transfer, 277–281, 758 controllers disk, 631, 631–632 I/O devices, 45 memory, 619, 620 conventional DRAMs, 618–620 conversions binary with hexadecimal, 72–73 signed and unsigned, 106–112 to two’s complement, 100, 108–109, 133 to unsigned, 98–99 ﬂoating point, 161, 332–337 lowercase, 545–547 number systems, 72–75 convert active socket to listening socket function, 971 convert application-to-network function, 962 convert double precision to integer instruction, 333 Index 1095 convert double precision to quad-word integer instruction, 333 convert double to single precision instruction, 335 convert host and service names function, 973, 973–976 convert host-to-network long function, 961 convert host-to-network short function, 961 convert integer to double precision instruction, 333 convert integer to single precision instruction, 333 convert network-to-application function, 962 convert network-to-host long function, 961 convert network-to-host short function, 961 convert packed single to packed double precision instruction, 334 convert quad-word integer to double precision instruction, 333 convert quad-word integer to single precision instruction, 333 convert quad word to oct word instruction, 234 convert single precision to integer instruction, 333 convert single precision to quad-word integer instruction, 333 convert single to double precision instruction, 334 convert socket address to host and service names function, 976, 976–978 ²ÇÉÔˆ−Ä−À−ÅÎÍ function, 136 copy ﬁle descriptor function, 945 ²ÇÉÔˆðËÇÀˆÂ−ËÅ−Ä function, 122–123 copy-on-write technique, 871, 871–872 copying bytes in memory, 169 descriptor tables, 945 text ﬁles, 936 Core 2 microprocessors, 204, 624 Core i7 microprocessors, 61 abstract operation model, 561–567 address translation, 862–864 caches, 667 Haswell, 543 memory mountain, 677 Nehalem, 204 page table entries, 862–864 QuickPath interconnect, 624 virtual memory, 861–864 core memory, 793 cores in multi-core processors, 204, 641, 1008 correct signal handling, 806–810 counting semaphores, 1039 CPE (cycles per element) metric, 538, 540, 543–544 ²Éð©Ä− [CS:APP] text ﬁle copy, 936 CPI (cycles per instruction) ﬁve-stage pipelines, 507 in performance analysis, 500–504 CPUs. See central processing units (CPUs) ²ÊÎÇ [x86-64] convert quad word to oct word, 234, 235 CR (carriage return) characters, 928 CR3 register, 862 Cray 1 supercomputer, 389 create/change environment variable function, 788 create child process function, 776, 777–779 create thread function, 1024 critical path analysis, 534 critical paths, 561, 565 critical sections in progress graphs, 1036 CS:APP header ﬁles, 782 wrapper functions, 774, 1077 ²ÍþÉÉl² [CS:APP] CS:APP wrapper functions, 774, 1077 ²ÍþÉÉl³ [CS:APP] CS:APP header ﬁle, 774, 782, 1077 ²Í³ [Unix] Unix shell program, 789 CT (cache tags), 859 ctest script, 501 ²Î©À− function, 1060 ²Î©À−ˆÎÍ [CS:APP] thread-safe non- reentrant wrapper for ²Î©À−, 1058 Ctrl+C key nonlocal jumps, 821 signals, 794, 797, 831 Ctrl+Z key, 797, 831 current working directory, 928 ²ÌÎÍ®pÍÍ [x86-64] convert double to single precision, 335 ²ÌÎÍÍpÍ® [x86-64] convert single to double precision, 334 cycles per element (CPE) metric, 538, 540, 543–544 cycles per instruction (CPI) ﬁve-stage pipelines, 507 in performance analysis, 500–504 cylinders disk, 627 spare, 632 c²Ó [x86-64] low order 16 bits of register cË²Ó, 216 d-caches (data caches), 556, 667 data conditional transfers, 250–256 forwarding, 472–475, 473 sizes, 75–78 data alignment, 309, 309–312 data caches (d-caches), 556, 667 data dependencies in pipelining, 455, 465–467 data-ﬂow graphs, 561–566 data formats in machine-level programming, 213–215 data hazards, 465 avoiding, 477–480 classes, 471 forwarding for, 472–475 load/use, 475–477 stalling, 469–472 Y86-64 pipelining, 465–469 data memory in SEQ timing, 437 data movement instructions, 218–225 data references locality, 642–643 PIC, 740–741 l®þÎþ section, 710 data segments, 732 data structures, 301 data alignment, 309–312 structures, 301–305 unions, 305–309 data transfer, procedures, 281–284 data types. See types database transactions, 955 datagrams, 960 ddd debugger with graphical user interface, 315 DDR SDRAM (double data-rate synchronous DRAM), 622 deadlocks, 1063, 1063–1066 deallocate heap storage function, 877 l®−¾Ï× section, 711 debugging, 315–316 dec [instruction class] decrement, 228 decimal notation, 68 decimal system conversions, 73–75 declarations arrays, 291–292, 299 pointers, 77 1096 Index declarations (continued) public and private, 713 structures, 301–305 unions, 305–309 decode stage instruction processing, 421, 423–433 PIPE processor, 485–489 sequential processing, 436 Y86-64 implementation, 442–444 Y86-64 pipelining, 459 decoding instructions, 555 decrement instruction, 228, 230 deep copies, 1060 deep pipelining, 454–455 default actions with signal, 798 default behavior for child processes, 780 default function code, 440 deferred coalescing, 886 a®−ð©Å− [C] preprocessor directive ®−Ä−Î− command, 316 delete environment variable function, 788 DELETE method in HTTP, 987 delete signal from signal set instruction, 801 delivering signals, 794 delivery mechanisms for protocols, 958 demand paging, 846 demand-zero pages, 869 demangling process (C++ and Java), 716, 716 denormalized ﬂoating-point value, 150, 150–152 dependencies control in pipelining systems, 455, 465 data in pipelining systems, 455, 465–467 reassociation transformations, 578 write/read, 593–595 dereferencing pointers, 84, 224, 293, 313, 906–907 descriptor sets, 1013, 1014 descriptor tables, 943, 945 descriptors, 927 connected and listening, 972, 972–973 socket, 970 destination hosts, 958 detach thread function, 1026 detached threads, 1025 detaching threads, 1025–1026 c®© [x86-64] low order 16 bits of register cË®©, 216 diagrams hardware, 434 pipeline, 449 Digital Equipment Corporation, 92 Dijkstra, Edsger, 1037–1038 c®©Ä [x86-64] low order 8 of register cË®©, 216 DIMM (dual inline memory module), 620 direct jumps, 242 direct-mapped caches, 653 conﬂict misses, 658–660 example, 655–657 line matching, 654 line replacement, 655 set selection, 654 word selection, 655 direct memory access (DMA), 47, 634 directives, assembler, 212, 402 directories description, 927, 927–928 reading contents, 941–942 directory streams, 941 dirty bits in cache, 666 Core i7, 863 dirty pages, 863 ®©ÍþÍ command, 316 disassemblers, 80, 105, 209, 209–210 disks, 625 accessing, 633–636 anatomy, 636 backups, 647 capacity, 627, 627–628 connecting, 632–633 controllers, 631, 631–632 geometry, 626–627 logical blocks, 631–632 operation, 628–631 trends, 638 distributing software, 737 division ﬂoating-point, 338 instructions, 234–236 Linux/x86-64 system errors, 765 by powers of 2, 139–143 ®©ÌÊ [x86-64] unsigned divide, 234, 236 c®Ä [x86-64] low order 8 of register cË®Ó, 216 ®Ä²ÄÇÍ− [Unix] close shared library, 738 ®Ä−ËËÇË [Unix] report shared library error, 738 DLL (dynamic link library), 735 ®ÄÇÉ−Å [Unix] open shared libary, 737 ®ÄÍÔÀ [Unix] get address of shared library symbol, 738 DMA (direct memory access), 47, 634 DMA transfer, 634 DNS (domain name system), 964 ®Ç [C] variant of Ñ³©Ä− loop, 256–259 ®ÇkÑ³©Ä− statement, 256 ®Ç©Î [CS:APP] Tiny helper function, 992, 994, 994–995 dollar signs (b) for immediate operands, 217 domain names, 961, 963–965 domain name system (DNS), 964 ®ÇÎÉËÇ® [CS:APP] vector dot product, 658 dots (.) in dotted-decimal notation, 962 dotted-decimal notation, 962, 962 ®ÇÏ¾Ä− [C] double-precision ﬂoating point, 160, 161 ®ÇÏ¾Ä− [C] integer data type, 77 double data-rate synchronous DRAM (DDR SDRAM), 622 ®ÇÏ¾Ä− ﬂoating-point declaration, 214 double-precision addition instruction, 338 double-precision division instruction, 338 double-precision maximum instruction, 338 double-precision minimum instruction, 338 double-precision multiplication instruction, 338 double-precision representation C, 77, 160–162 IEEE, 149, 149 machine-level data, 214 double-precision square root instruction, 338 double-precision subtraction instruction, 338 double word to quad word instruction, 235 double words, 213 DRAM. See dynamic RAM (DRAM) DRAM arrays, 618 DRAM cells, 618, 619 drivers, compiler, 40, 707–708 dual inline memory module (DIMM), 620 ®ÏÉp [Unix] copy ﬁle descriptor, 945 duplicate symbol names, 716–720 dynamic code, 326 dynamic content, 737, 989–990 dynamic link libraries (DLLs), 735 Index 1097 dynamic linkers, 735 dynamic linking, 735, 735–737 dynamic memory allocation allocated block placement, 885 allocator design, 890–892 allocator requirements and goals, 880–881 coalescing free blocks, 886–887 coalescing with boundary tags, 887–890 explicit free lists, 898–899 fragmentation, 882 heap memory requests, 886 implementation issues, 882–883 implicit free lists, 883–885 ÀþÄÄÇ² and ðË−− functions, 876– 879 overview, 875–876 purpose, 879–880 segregated free lists, 899–901 splitting free blocks, 885–886 dynamic memory allocators, 875–876 dynamic RAM (DRAM), 45, 618 caches, 842, 844, 844–845 conventional, 618–620 enhanced, 621–622 historical popularity, 622 modules, 620, 621 vs. SRAM, 618 trends, 638–639 dynamic Web content, 985 c®Ó [x86-64] low order 16 bits of register cË®Ó, 216 E-way set associative caches, 660–661 c−þÓ [x86-64] low order 32 bits of register cËþÓ, 216 c−¾É [x86-64] low order 32 bits of register cË¾É, 216 c−¾Ó [x86-64] low order 32 bits of register cË¾Ó, 216 ECF. See exceptional control ﬂow (ECF) ECHILD return code, 782–783 −²³Ç [CS:APP] read and echo input lines, 983 −²³Ç function, 317–318, 323 −²³Çˆ²ÅÎ [CS:APP] counting version of −²³Ç, 1048 −²³Ç²Ä©−ÅÎl² [CS:APP] echo client, 980–981 −²³ÇÍ−ËÌ−Ë©l² [CS:APP] iterative echo server, 972–973, 983 −²³ÇÍ−ËÌ−ËÎl² [CS:APP] concurrent echo server based on threads, 1027 −²³ÇÍ−ËÌ−ËÎˆÉË−l² [CS:APP] prethreaded concurrent echo server, 1047 c−²Ó [x86-64] low order 32 bits of register cË²Ó, 216 c−®© [x86-64] low order 32 bits of register cË®©, 216 EDO DRAM (extended data out DRAM), 622 c−®Ó [x86-64] low order 32 bits of register cË®Ó, 216 EEPROMs (electrically erasable programmable ROMs), 623 effective addresses, 217, 726 effective cycle time, 638 efﬁciency of parallel programs, 1055, 1055 EINTR return code, 782 electrically erasable programmable ROMs (EEPROMs), 623 ELF. See executable and linkable format (ELF) EM64T processors, 204 embedded processors, 399 encapsulation, 958 encodings in machine-level programming, 205–206 code examples, 208–211 code overview, 206–207 formatting, 211–213 Y86-64 instructions, 394–396 end-of-ﬁle (EOF) condition, 927, 984 end of line (EOL) indicators, 928 entry points, 732, 733–734 environment variables lists, 787–788 EOF (end-of-ﬁle) condition, 927, 984 EOL (end of line) indicators, 928 ephemeral ports, 966 epilogue blocks, 891 EPIPE error return code, 1000 erasable programmable ROMs (EPROMs), 623 −ËËÅÇ [Unix] Unix error variable, 1078 error-correcting codes for memory, 618 error handling system calls, 773–774 Unix systems, 1078–1079 wrappers, 774, 1077, 1079–1081 error-reporting functions, 773 errors child processes, 781–782 link-time, 43 off-by-one, 908 race, 812, 812–814 reporting, 1079 synchronization, 1031 c−Í© [x86-64] low order 32 bits of register cËÍ©, 216 c−ÍÉ [x86-64] low order 32 bits of stack pointer register cËÍÉ, 216 establish connection with server functions, 970, 970–971, 978–980 establish listening socket function, 980, 980 etest script, 501 Ethernet segments, 956, 956 Ethernet technology, 956 EUs (execution units), 554, 556 −ÌþÄ [CS:APP] shell helper routine, 790, 791 event-driven programs, 1016 based on I/O multiplexing, 1016–1021 based on threads, 1049 events, 759 scheduling, 799 state machines, 1016 evicting blocks, 648 exabytes, 75 excepting instructions, 481 exception handlers, 760, 760 exception handling in instruction processing, 421 Y86-64, 399–400, 480–483 exception numbers, 761 exception table base registers, 761 exception tables, 761, 761 exceptional control ﬂow (ECF), 758 exceptions, 759–767 importance, 758–759 nonlocal jumps, 817–822 process control. See processes signals. See signals summary, 823 system call error handling, 773–774 exceptions, 759 anatomy, 759–760 asynchronous, 762 classes, 762–764 data alignment, 312 handling, 760–762 Linux/x86-64 systems, 765–767 status code for, 440 synchronous, 763 Y86, 392 exclamation points _ for not operation, 409 exclusive-or Boolean operation, 87 exclusive-or instruction x86-64, 228 Y86-64, 392 1098 Index exclusive-or operation in execute stage, 444 exclusive-or packed double precision instruction, 341 exclusive-or packed single precision instruction, 341 executable and linkable format (ELF), 709 executable object ﬁles, 731–732 header tables, 710, 732 headers, 710–711 relocation, 726 symbol tables, 711–715 executable code, 206 executable object ﬁles, 40 creating, 708 description, 708 fully linked, 732 loading, 733–734 running, 43–44 executable object programs, 40 execute access, 325 execute disable bit, 863 execute stage instruction processing, 421, 423–433 PIPE processor, 489–490 sequential processing, 436 sequential Y86-64 implementation, 444–445 Y86-64 pipelining, 459 execution concurrent, 769 parallel, 770 speculative, 555, 555, 585–586 tracing, 423, 430–431, 439 execution code regions, 325–326 execution units (EUs), 554, 556 −Ó−²Ì− [Unix] load program, 786 arguments and environment variables, 786–788 child processes, 735, 737 loading programs, 733 running programs, 789–792 virtual memory, 872–873 −Ó©Î [C Stdlib] terminate process, 775 exit status, 775, 781 expanding bit representation, 112–116 expansion slots, 633 explicit allocator requirements and goals, 880–881 explicit dynamic memory allocators, 875–876 explicit free lists, 898–899 explicit thread termination, 1024 explicit waiting for, signals, 814–817 explicitly reentrant functions, 1059 exploit code, 320 exponents in ﬂoating-point representation, 148 −ÓÎ−Å®ˆ³−þÉ [CS:APP] allocator: extend heap, 894 extended data out DRAM (EDO DRAM), 622 extended precision ﬂoating-point representation, 173, 173 external exceptions in pipelining, 480 external fragmentation, 882, 882 fall through in ÍÑ©Î²³ statements, 269 false fragmentation, 886 fast page mode DRAM (FPM DRAM), 621 fault exception class, 762 faulting instructions, 763 faults, 764 Linux/x86-64 systems, 765, 868–869 Y86-64 pipelining caches, 506 ƒ⁄ˆ£‹‡ [Unix] clear bit in descriptor set, 1013, 1014 ƒ⁄ˆ'··¥¶ [Unix] bit turned on in descriptor set, 1013, 1014, 1016 ƒ⁄ˆ·¥¶ [Unix] set bit in descriptor set, 1013, 1014 ƒ⁄ˆ…¥‡ﬂ [Unix] clear descriptor set, 1013, 1014 feedback in pipelining, 455–457, 461 feedback paths, 432, 455 fetch ﬁle metadata function, 939 fetch stage instruction processing, 420, 423–433 PIPE processor, 483–485 SEQ, 440–442 sequential processing, 436 Y86-64 pipelining, 459 fetches, locality, 643–644 ð×−ÎÍ function, 318 Fibonacci (Pisano), 68 ﬁeld-programmable gate arrays (FPGAs), 503 FIFOs, 1013 ﬁle descriptors, 927 ﬁle position, 927 ﬁle tables, 772, 942 ﬁle type, 947 ﬁlenames, 927 ﬁles, 55 as abstraction, 63 anonymous, 869 binary, 39 metadata, 939–940 object. See object ﬁles register, 46, 207, 394–395, 418–419, 437, 557 regular, 869 sharing, 942–944 system-level I/O. See system-level I/O types, 927–929 Unix, 926, 926–927 ﬁnger command, 320 ð©Å×−Ë® daemon, 320 ð©Å©Í³ command, 316 ﬁrmware, 623 ﬁrst-ﬁt block placement policy, 885, 885 ﬁrst-level domain names, 963 ﬁrst readers-writers problem, 1044 ﬁts, segregated, 899, 900–901 ﬁve-stage pipelines, 507 ﬁxed-size arithmetic, 121 ﬁxed-size arrays, 296–298 ﬁxed-size integer types, 77, 103 ﬂash memory, 623 ﬂash translation layers, 636–637 ﬂat addressing, 203 ðÄÇþÎ [C] single-precision ﬂoating point, 160 ðÄÇþÎ ﬂoating-point declaration, 214 ﬂoating-point code architecture, 329, 329–332 arithmetic operations, 338–340 bitwise operations, 341–342 comparison operations, 342–345 constants, 340–341 movement and conversion operations, 332–337 observations, 345 in procedures, 337–338 ﬂoating-point representation and programs, 144–145 arithmetic, 69 C, 160–162 denormalized values, 150, 150–152 encodings, 68 extended precision, 173, 173 fractional binary numbers, 145–148 IEEE, 148–150 normalized value, 149–150 operations, 158–160 overﬂow, 163 pi, 176 rounding, 156, 156–158 special values, 151 support, 76 x87 processors, 203 ﬂows concurrent, 769, 769–770 control, 758 logical, 768, 768–769 parallel, 770 synchronizing, 812–814 Index 1099 ﬂushed instructions, 558 ƒﬁﬂﬁ¥ [Y86-64] default function code, 440 footers of blocks, 887 ðÇË [C] general loop statement, 264–268 guarded-do translation, 261 jump-to-middle translation, 259 forbidden regions, 1039 foreground processes, 789 ðÇËÂ [Unix] create child process, 776 child processes, 737 example, 777–779 running programs, 789–792 virtual memory, 872 ðÇËÂl² [CS:APP] ðÇËÂ example, 777 formal veriﬁcation in pipelining, 502 format strings, 83 formats for machine-level data, 213–215 formatted disk capacity, 632 formatted printing, 83 formatting disks, 632 machine-level code, 211–213 forwarding for data hazards, 472–475 load, 513 forwarding priority, 487–488 FPGAs (ﬁeld-programmable gate arrays), 503 FPM DRAM (fast page mode DRAM), 621 ðÉË©ÅÎð [C Stdlib] function, 83 fractional binary numbers, 145–148 fractional ﬂoating-point representa- tion, 148–156, 173 fragmentation, 882 dynamic memory allocation, 882 false, 886 frame pointers, 326 frames Ethernet, 956 stack, 276, 276–277, 312, 326–329 free blocks, 875 coalescing, 886–887 splitting, 885–886 free bounded buffer function, 1043 ðË−− [C Stdlib] deallocate heap storage, 877, 877–879 interpositioning libraries, 744 wrappers for, 747 free heap block function, 896 free heap blocks, referencing data in, 910–911 free lists creating, 893–895 dynamic memory allocation, 883– 885 explicit, 898–899 implicit, 884 manipulating, 892–893 segregated, 899–901 free software, 42 free up getaddrinfo resources function, 973 ðË−−þ®®Ë©ÅðÇ [Unix] free up getaddrinfo resources, 973, 974 FreeBSD open-source operating system, 122–123 freeing blocks, 896 Freescale processor family, 388 RISC design, 397 front side bus (FSB), 624 ðÍÎþÎ [Unix] fetch ﬁle metadata, 939 full duplex connections, 965 full duplex streams, 948 fully associative caches, 662 line matching and word selection, 663–664 set selection, 663 fully linked executable object ﬁles, 732 fully pipelined functional units, 559 function calls performance strategies, 597 PIC, 741–743 function part in Y86-64 instruction speciﬁer, 394 functional units, 556–557, 559–560 functions pointers to, 314 reentrant, 802, 1059 static libraries, 720–724 system-level, 766 thread-safe and thread-unsafe, 1056, 1056–1058 wrapper, 747 in Y86 instructions, 395 ×þ©ˆ−ËËÇË [CS:APP] reports GAI- style errors, 1079 ×þ©ˆÍÎË−ËËÇË [Unix] print getaddrinfo error message, 974 GAI-style error handling, 1078, 1078–1079 gaps between disk sectors, 626, 632 garbage, 902 garbage collection, 876, 902 garbage collectors, 876, 902 basics, 902–903 conservative, 903, 905– 906 Mark&Sweep, 903–906 overview, 901–902 gates, logic, 409 gcc (GNU compiler collection) compiler code formatting, 211–212 inline assembly, 214 options, 71 working with, 204–205 gdb GNU debugger, 209, 315, 315– 316 general protection faults, 765 general-purpose registers, 215, 215– 216 geometry of disks, 626–627 get address of shared library symbol function, 738 “get from” operator (C++), 926 GET method in HTTP, 987 get parent process ID function, 775 get process group ID function, 795 get process ID function, 775 get thread ID function, 1024 ×−Îþ®®Ë©ÅðÇ [Unix] convert host and service names, 973, 973–976 ×−Î−ÅÌ [C Stdlib] read environment variable, 787 ×−Î³ÇÍÎ¾Ôþ®®Ë [Unix] get DNS host entry, 1060 ×−Î³ÇÍÎ¾ÔÅþÀ− [Unix] get DNS host entry, 1060 ×−ÎÅþÀ−©ÅðÇ [Unix] convert socket address to host and service names, 976, 976–978 ×−ÎÉ−−ËÅþÀ− function [C Stdlib] security vulnerability, 122–123 ×−ÎÉ×ËÉ [Unix] get process group ID, 795 ×−ÎÉ©® [Unix] get process ID, 775 ×−ÎÉÉ©® [Unix] get parent process ID, 775 ×−ÎËÏÍþ×− [Unix] function, 847 ×−ÎÍ function, 315, 317–318 GHz (gigahertz), 538 giga-instructions per second (GIPS), 449 gigabytes, 628 gigahertz (GHz), 538 GIPS (giga-instructions per second), 449 global IP Internet. See Internet Global Offset Table (GOT), 741, 741–743 global symbols, 711 global variable mapping, 1030–1031 1100 Index GNU compiler collection. See gcc (GNU compiler collection) compiler GNU project, 42 GOT (global offset table), 741, 741–743 ×ÇÎÇ [C] control transfer statement, 246, 269 goto code, 246 gprof Unix proﬁler, 598, 598–599 gradual underﬂow, 151 granularity of concurrency, 1021 graphic user interfaces for debuggers, 315 graphics adapters, 632 graphs data-ﬂow, 561–566 process, 777, 778 progress. See progress graphs reachability, 902 greater than signs | deferencing operation, 302 “get from” operator, 926 right hoinkies, 945 groups abelian, 125 process, 795 guard values, 322 guarded-do translation, 261 l³ header ﬁles, 722 half-precision ﬂoating-point representation, 173, 173 ³þÄÎ [Y86-64] halt instruction execution, 393 code for, 440–441 exceptions, 400, 480–483 in pipelining, 498 handlers exception, 760, 760 interrupt, 762 signal, 794, 799 handling signals blocking and unblocking, 800–801 portable, 810–811 hardware caches. See caches and cache memory hardware control language (HCL), 408 Boolean expressions, 410–411 integer expressions, 412–416 logic gates, 409 hardware description languages (HDLs), 409, 503 hardware exceptions, 760 hardware interrupts, 762 hardware management, 50–51 hardware organization, 44 buses, 44 I/O devices, 45 main memory, 45 processors, 45–46 hardware registers, 417–420 hardware structure for Y86-64, 432–436 hardware units, 432–434, 437 hash tables, 603–604 Haswell microarchitecture, 861 Haswell microprocessors, 204, 251, 330, 543, 557, 559 hazards in pipelining, 390, 465 avoiding, 477–480 classes, 471 forwarding for, 472–475 load/use, 475–477 overview, 465–469 stalling for, 469–472 HCL (hardware control language), 408 Boolean expressions, 410–411 integer expressions, 412–416 logic gates, 409 HDLs (hardware description languages), 409, 503 head crashes, 629 HEAD method in HTTP, 987 header ﬁles static libraries, 723 system, 782 header tables in ELF, 710, 732 headers blocks, 883 Ethernet, 956 request, 987 response, 988 heap, 54, 54–55, 875 dynamic memory allocation, 875– 876 Linux systems, 733 referencing data in, 910–911 requests, 886 ³−ÄÄÇ [CS:APP] C hello program, 38, 46–48 ³−ÄÉ command, 316 helper functions, sockets interface, 978–980 Hennessy, John, 397, 507 heterogeneous data structures, 301 data alignment, 309–312 structures, 301–305 unions, 305–309 hexadecimal (hex) notation, 72, 72–75 hierarchies domain name, 963 storage devices, 50, 50, 645–650 high-level design performance strategies, 597 hit rates, 667 hit time, 667 hits cache, 648, 667 write, 666 ³ÄÎ [x86-64] halt instruction execution, 393 ¤‹¶ [Y86-64] status code indicating ³þÄÎ instruction, 400 hoinkies, 945, 946 holding mutexes, 1039 Horner, William, 566 Horner’s method, 566 host bus adapters, 633 host bus interfaces, 633 host entries, 964 host information program command, 962 hostname command, 962 hosts client-server model, 955 network, 958 number of, 966 sockets interface, 973–978 htest script, 501 HTML (hypertext markup language), 984, 984–985 ³ÎÇÅÄ [Unix] convert host-to-network long, 961 ³ÎÇÅÍ [Unix] convert host-to-network short, 961 HTTP. See hypertext transfer protocol (HTTP) hubs, 956 hyperlinks, 984 hypertext markup language (HTML), 984, 984–985 hypertext transfer protocol (HTTP), 984 dynamic content, 989–990 methods, 987–988 requests, 987, 987–988 responses, 988, 988–989 transactions, 986–987 hyperthreading, 60, 204 HyperTransport interconnect, 624 i-caches (instruction caches), 554, 667 l© source ﬁles, 707 i386 microprocessor, 203 i486 microprocessor, 203 Index 1101 IA32 (Intel Architecture 32-bit) microprocessors, 81, 204 machine language, 201–202 registers, 215–216 ©þ®®Ê [Y86-64] immediate add, 405 IBM Freescale microprocessors, 388, 397 out-of-order processing, 558 RISC design, 397–399 '£¡‹‹ [Y86-64] instruction code for ²þÄÄ instruction, 440 ICANN (Internet Corporation for Assigned Names and Numbers), 963 icode (instruction code), 420, 441 ICUs (instruction control units), 554 identiﬁers, register, 394 ©®©ÌÄ [x86-64] signed divide, 235 ©®©ÌÊ [x86-64] signed divide, 234 IDs (identiﬁers) processes, 775–776 register, 394–395 IEEE. See Institute for Electrical and Electronics Engineers (IEEE) ©ð [C] conditional statement, 247–249 ifun (instruction function), 420, 441 '¤¡‹¶ [Y86-64] instruction code for ³þÄÎ instruction, 440 ''‡›ﬂ‚† [Y86-64] instruction code for ©ËÀÇÌÊ instruction, 440 ijk matrix multiplication, 680–682, 681 '“”” [Y86-64] instruction code for jump instructions, 440 ikj matrix multiplication, 680–682, 681 illegal instruction exceptions, 440 ©À−Àˆ−ËËÇË signal, 441 immediate add instruction, 405 immediate coalescing, 886 immediate offset, 217 immediate operands, 217 immediate to register move instruction, 392 implicit dynamic memory allocators, 876 implicit free lists, 883–885, 884 implicit thread termination, 1024 implicitly reentrant functions, 1059 implied leading 1 representation, 150 '›‡›ﬂ‚† [Y86-64] instruction code for ÀËÀÇÌÊ instruction, 440 imul [instruction class] multiply, 228 ©ÀÏÄÊ [x86-64] signed multiply, 234, 234 ©Å [HCL] set membership test, 417 ©Åˆþ®®Ë [Unix] IP address structure, 961 inc [instruction class] increment, 228 include ﬁles, 722 a©Å²ÄÏ®− [C] preprocessor directive, 206 ©Å²Ê instruction, 230 increment instruction, 228, 230 indeﬁnite integer values, 161 ©Å®−Ól³ÎÀÄ ﬁle, 986 index registers, 217 indexes for direct-mapped caches, 658–660 indirect jumps, 242, 270 inefﬁciencies in loops, 544–548 ©Å−ÎˆÅÎÇþ [Unix] convert network- to-application, 1060 ©Å−ÎˆÅÎÇÉ [Unix] convert network- to-application, 962 ©Å−ÎˆÉÎÇÅ [Unix] convert application-to-network, 962 inﬁnity constants, 160 representation, 150–151 ©ÅðÇ ðËþÀ− command, 316 ©ÅðÇ Ë−×©ÍÎ−ËÍ command, 316 information, 38–40 information access with x86-64 registers, 215–216 data movement, 218–225 operand speciﬁers, 216–218 information storage, 70 addressing and byte ordering, 78–85 bit-level operations, 90–92 Boolean algebra, 86–89 code, 85–86 data sizes, 75–78 disks. See disks ﬂoating point. See ﬂoating-point representation and programs hexadecimal, 72–75 integers. See integers locality. See locality logical operations, 92–93 memory. See memory segregated, 899 shift operations, 93–95 strings, 85 summary, 684 ©Å©Î function, 779 ©Å©ÎˆÉÇÇÄ function, 1017, 1019 initial state in progress graphs, 1035 initialize nonlocal handler jump function, 819 initialize nonlocal jump functions, 819 initialize read buffer function, 934, 936 initialize semaphore function, 1038 initialize thread function, 1026 initializing threads, 1026 inline assembly, 214 inline substitution, 537 inlining, 537 'ﬁﬂ– [Y86-64] instruction code for ÅÇÉ instruction, 440 input events, 1016 input/output. See I/O (input/output) insert item in bounded buffer function, 1043 install portable handler function, 811 installing signal handlers, 799 Institute for Electrical and Electronics Engineers (IEEE) description, 145 ﬂoating-point representation and programs, 148–150 denormalized, 150 normalized, 149–150 special values, 151 Standard 754, 145 standards, 145 Posix standards, 52 ©ÅÍÎËˆÌþÄ©® signal, 441–442 instruction caches (i-caches), 554, 667 instruction code (icode), 420, 441 instruction control units (ICUs), 554 instruction function (ifun), 420, 441 instruction-level parallelism, 62, 533, 554, 598 instruction memory in SEQ timing, 437 instruction set architectures (ISAs), 46, 63, 206, 388 instruction set simulators, 402 instructions classes, 218 decoding, 554 excepting, 481 fetch locality, 643–644 issuing, 463–464 jump, 46, 241–245 load, 46 low-level. See machine-level programming move, 250–256, 586–589 operate, 46 pipelining, 504–505, 585 privileged, 771 store, 46 update, 45–46 Y86-64. See Y86-64 instruction set architecture instructions per cycle (IPC), 507 ©ÅÎ [C] integer data type, 76 1102 Index ©ÅÎ [HCL] integer signal, 412 ©ÅÎ data types, integral, 97 'ﬁ¶ˆ›¡” constant, maximum signed integer, 104 'ﬁ¶ˆ›'ﬁ constant, minimum signed integer, 104 ©ÅÎqpˆÎ [Unix] ﬁxed-size, 77 integer arithmetic, 120, 228 division by powers of 2, 139–143 multiplication by constants, 137– 139 overview, 143–144 two’s complement addition, 126–131 two’s complement multiplication, 133–137 two’s complement negation, 131 unsigned addition, 120–126 integer bits in ﬂoating-point representation, 173 integer expressions in HCL, 412–416 integer indeﬁnite values, 161 integer operation instruction, 440 integer registers in x86-64, 215–216 integers, 68, 95–96 arithmetic operations. See integer arithmetic bit-level operations, 90–92 bit representation expansion, 112–116 byte order, 79–80 data types, 96–98 shift operations, 93–95 signed and unsigned conversions, 106–112 signed vs. unsigned guidelines, 119–120 truncating, 117–118 two’s complement representation, 100–106 unsigned encoding, 98–100 integral data types, 96, 96–98 integration of caches and VM, 853 Intel assembly-code format, 213, 330, 347 Intel Corporation, 201 Intel microprocessors 8086, 62, 203 80286, 203 Core 2, 204, 624 Core i7. See Core i7 microproces- sors data alignment, 312 evolution, 203–204 ﬂoating-point representation, 173 Haswell, 204, 251, 330, 559 i386, 203 i486, 203 northbridge and southbridge chipsets, 624 out-of-order processing, 558 Pentium, 203 Pentium II, 203 Pentium III, 203–204 Pentium 4, 204 Pentium 4E, 204 PentiumPro, 203, 558 Sandy Bridge, 204 x86-64. See x86-64 microprocessors Y86-64. See Y86-64 instruction set architecture interconnected networks (internets), 957, 957–958 interfaces bus, 624 host bus, 633 interlocks, load, 477 internal exceptions in pipelining, 480 internal fragmentation, 882 internal read function, 937 International Standards Organization (ISO), 40, 71 Internet, 957 connections, 965–967 domain names, 963–965 IP addresses, 961–963 organization, 960–961 origins, 967 internet addresses, 958 Internet Corporation for Assigned Names and Numbers (ICANN), 963 Internet domain names, 961 Internet Domain Survey, 966 Internet hosts, number of, 966 Internet Protocol (IP), 960 Internet Software Consortium, 966 Internet worms, 320 internets (interconnected networks), 957, 957–958 interpositioning libraries, 743, 743– 744 compile-time, 744–745 link-time, 744, 746 run-time, 746–748 interpretation of bit patterns, 68 interprocess communication (IPC), 1013 interrupt handlers, 762 interruptions, 800 interrupts, 762, 762–763 interval counting schemes, 600 'ﬁ¶Nˆ›¡” [C] maximum value of N -bit signed data type, 103 'ﬁ¶Nˆ›'ﬁ [C] minimum value of N -bit signed data type, 103 ©ÅÎNˆÎ [C] N -bit signed integer data type, 103 z©ÅÎÎÔÉ−Íl³| ﬁxed-size integer types, 234 invalid address status code, 400 invariants, semaphore, 1038 I/O (input/output), 45, 926 memory-mapped, 634 ports, 634 redirection, 945, 945–946 system-level. See system-level I/O Unix, 55, 926, 926–927 I/O bridges, 623 I/O buses, 624, 632, 634 I/O devices, 45 addressing, 634 connecting, 632–633 I/O multiplexing, 1009 concurrent programming with, 1014–1021 event-driven servers based on, 1016–1021 pros and cons, 1021 'ﬂ–‹ [Y86-64] instruction code for integer operation instruction, 440 IP (Internet Protocol), 960 IP address structure, 961, 962 IP addresses, 960, 961–963 IPC (instructions per cycle), 507 IPC (interprocess communication), 1013 iPhone 5S, 389 '–ﬂ–† [Y86-64] instruction code for ÉÇÉÊ instruction, 440 '–•·¤† [Y86-64] instruction code for ÉÏÍ³Ê instruction, 440 IPv6, 961 '‡¥¶ [Y86-64] instruction code for Ë−Î instruction, 440 '‡››ﬂ‚† [Y86-64] instruction code for ËÀÀÇÌÊ instruction, 440 ©ËÀÇÌÊ [Y86-64] immediate to register move, 392, 440 '‡‡›ﬂ‚† [Y86-64] instruction code for ËËÀÇÌÊ instruction, 440 ISAs (instruction set architectures), 46, 63, 206, 388 ISO (International Standards Organization), 40, 71 ISO C11 C standard, 71 ISO C90 C standard, 71 ISO C99 C standard, 71, 77, 360 integral data types, 103 Index 1103 static libraries, 720–724 ©Í–ÎË function, 905 issue time for arithmetic operations, 559 issuing instructions, 463–464 iterative servers, 982 iterative sorting routines, 603 Áþ [x86-64] jump if unsigned greater, 242 Áþ− [x86-64] jump if unsigned greater or equal, 242 Java language, 713 byte code, 346 linker symbols, 716 numeric ranges, 104 objects, 302–303 software exceptions, 759–760, 822 threads, 1066 Java monitors, 1046 Java Native Interface (JNI), 740 Á¾ [x86-64] jump if unsigned less, 242 Á¾− [x86-64] jump if unsigned less or equal, 242 Á− [Y86-64] jump when equal, 393, 430 Á× [x86-64] jump if greater, 242, 393 Á×− [x86-64] jump if greater or equal, 242, 393 jik matrix multiplication, 680–682, 681 jki matrix multiplication, 680–682, 681 ÁÄ [x86-64] jump if less, 242, 393 ÁÄ− [x86-64] jump if less or equal, 242, 393 ÁÀÉ [x86-64] jump unconditionally, 242, 393 ÁÅþ [x86-64] jump if not unsigned greater, 242 ÁÅþ− [x86-64] jump if unsigned greater or equal, 242 ÁÅ¾ [x86-64] jump if not unsigned less, 242 ÁÅ¾− [x86-64] jump if not unsigned less or equal, 242 ÁÅ− [x86-64] jump if not equal, 242, 393 ÁÅ× [x86-64] jump if not greater, 242 ÁÅ×− [x86-64] jump if not greater or equal, 242 JNI (Java Native Interface), 740 ÁÅÄ [x86-64] jump if not less, 242 ÁÅÄ− [x86-64] jump if not less or equal, 242 ÁÅÍ [x86-64] jump if nonnegative, 242 ÁÅÖ [x86-64] jump if not zero, 242 jobs, 796 joinable threads, 1025 ÁÉ [x86-64] jump when parity ﬂag set, 342 ÁÍ [x86-64] jump if negative, 242 jtest script, 501 jump if greater instruction, 242, 393 jump if greater or equal instruction, 242, 393 jump if less instruction, 242, 393 jump if less or equal instruction, 242, 393 jump if negative instruction, 242 jump if nonnegative instruction, 242 jump if not equal instruction, 242, 393 jump if not greater instruction, 242 jump if not greater or equal instruction, 242 jump if not less instruction, 242 jump if not less or equal instruction, 242 jump if not unsigned greater instruction, 242 jump if not unsigned less instruction, 242 jump if not unsigned less or equal instruction, 242 jump if not zero instruction, 242 jump if unsigned greater instruction, 242 jump if unsigned greater or equal instruction, 242 jump if unsigned less instruction, 242 jump if unsigned less or equal instruction, 242 jump if zero instruction, 242 jump instructions, 46, 241–245, 440 direct, 242 indirect, 242, 270 instruction code for, 440 nonlocal, 759, 817, 817–822 targets, 242 jump tables, 269, 270–271, 761 jump-to-middle translation, 259 jump unconditionally instruction, 242, 242 jump when equal instruction, 393 jump when parity ﬂag set instruction, 342 just-in-time compilation, 326, 346 ÁÖ [x86-64] jump if zero, 242 k × 1 loop unrolling, 567 k × 1a loop unrolling, 580 k × k loop unrolling, 575–576 K&R (C book), 40 Kahan, William, 145 Kahn, Robert, 967 kernel mode exception handlers, 762 processes, 770–772, 771 system calls, 764 kernels, 53, 55, 734 exception numbers, 761 virtual memory, 866–867 Kernighan, Brian, 38, 40, 52, 71, 314, 950 Kerrisk, Michael, 950 keyboard, signals from, 796–797 kij matrix multiplication, 680–682, 681 Â©ÄÄ [Unix] send signal, 797 Â©ÄÄ command in gdb debugger, 316 Â©ÄÄl² [CS:APP] Â©ÄÄ example, 797 kji matrix multiplication, 680–682, 681 Knuth, Donald, 885, 887 ÂÍ³ [Unix] Unix shell program, 789 l sufﬁx, 215 L1 cache, 49, 651 L2 cache, 49, 651 L3 cache, 651 labels for jump instructions, 241 LANs (local area networks), 956, 956–958 last-in, ﬁrst out discipline, 225 last-in ﬁrst-out (LIFO) free list order, 899 latency arithmetic operations, 559, 560 disks, 630 instruction, 449 load operations, 590–591 pipelining, 448 latency bounds, 554, 560 lazy binding, 742 ld Unix static linker, 708 ld-linux.so linker, 735 ‹⁄ˆ–‡¥‹ﬂ¡⁄ environment variable, 746–748 ldd tool, 749 ‹¥¡ instruction, 138 leaf procedures, 277 leaks, memory, 911, 1028 Ä−þÊ [x86-64] load effective address, 227, 227–228, 313 least-frequently-used (LFU) replacement policies, 662 least-recently-used (LRU) replace- ment policies, 648, 662 least squares ﬁt, 538, 540 Ä−þÌ− [x86-64] prepare stack for return instruction, 328 left hoinkies (z), 946 length of strings, 119 1104 Index less than signs z left hoinkies, 945 “put to” operator, 926 levels optimization, 534 storage, 645–646 LF (line feed) characters, 928 LFU (least-frequently-used) replacement policies, 662 Ä©¾² library, 947 ˆˆÄ©¾²ˆÍÎþËÎˆÀþ©Å, 734 libraries in concurrent programming, 1060– 1061 header ﬁles, 119 interpositioning, 743, 743–748 shared, 55, 735, 735–737 standard I/O, 947 static, 720, 720–724 LIFO (last-in ﬁrst-out) free list order, 899 zÄ©À©ÎÍl³| ﬁle for numeric limit declarations, 103–104, 113 line feed (LF) characters, 928 line matching direct-mapped caches, 654 fully associative caches, 662 set associative caches, 661–662 line replacement direct-mapped caches, 655 set associative caches, 662 lÄ©Å− section, 711 linear address spaces, 840 link-time errors, 43 link-time interpositioning, 744, 746 linkers and linking, 41, 200, 206 compiler drivers, 707–708 dynamic, 735, 735–737 library interpositioning, 743, 743– 748 object ﬁles, 709, 709–710 executable, 731–734 loading, 733–734 relocatable, 710–711 tools for, 749 overview, 706–707 position-independent code, 740– 743 relocation, 725–731 shared libraries from applications, 737–739 static, 708 summary, 749–750 symbol resolution, 715–725 symbol tables, 711–715 virtual memory for, 847–848 linking phase, 42 links in directories, 927 Linux operating system, 56, 81 code segments, 733–734 dynamic linker interfaces, 738 and ELF, 709 exceptions, 765–767 ﬁles, 927–929 signals, 792 static libraries, 721–722 virtual memory, 866–869 Lisp language, 121 Ä©ÍÎ−Å [Unix] convert active socket to listening socket, 971 listening descriptors, 972–973 listening sockets, 971 little-endian ordering convention, 78, 78–80 load effective address instruction, 227–229, 313 load forwarding in PIPE, 513 load instructions, 46 load interlocks, 477 load operations example, 624 process, 555–556 load penalty in CPI, 503 load performance of memory, 590–591 load program function, 786 load-store architecture in CISC vs. RISC, 398 load time for code, 706 load/use data hazards, 475, 475–477 loaders, 708, 733 loading concepts, 735 executable object ﬁles, 733–734 process, 733 programs, 786–788 shared libraries from applications, 737–739 virtual memory for, 848 local area networks (LANs), 956, 956–958 local automatic variables, 1030 local registers, 563 local static variables, 1030, 1030–1031 local storage registers, 287–289 stack, 284–287 local symbols, 712 locality, 49, 616, 640–641 blocking for, 683 caches, 679–683, 846 exploiting, 683 forms, 640, 650 instruction fetches, 643–644 program data references, 642–643 summary, 644–645 ÄÇ²þÄÎ©À− function, 1060 lock-and-copy technique, 1058, 1058 locking mutexes lock ordering rule, 1065 for semaphores, 1039 logic design, 408 combinational circuits, 410–416, 449 logic gates, 409, 409 memory and clocking, 417–420 set membership, 416–417 logic gates, 409 logic synthesis, 391, 409, 503 logical blocks disks, 631, 631–632 SSDs, 637 logical control ﬂow, 768, 768–769 logical operations, 92–93, 227 discussion, 232–233 load effective address, 227–229 shift, 94, 140, 228, 230–232 special, 233–236 unary and binary, 230 ÄÇÅ× [C] integer data type, 76–77, 97–98 ÄÇÅ× double [C] extended-precision ﬂoating point, 161, 173 ÄÇÅ× ®ÇÏ¾Ä− ﬂoating-point declaration, 214 long words in machine-level data, 215 ÄÇÅ×ÁÀÉ [C Stdlib] nonlocal jump, 759, 819, 819 loop registers, 563 loop unrolling, 538, 540, 567 Core i7, 608 k × 1, 567 k × 1a, 580 k × k, 575–576 overview, 567–571 with reassociation transformations, 577–579 loopback addresses, 964 loops, 256 ®ÇkÑ³©Ä−, 256–259 ðÇË, 264–268 inefﬁciencies, 544–548 reverse engineering, 258 segments, 562–563 for spatial locality, 679–683 Ñ³©Ä−, 259–264 low-level instructions. See machine- level programming low-level optimizations, 598 Index 1105 lowercase conversions, 545–547 LRU (least-recently-used) replace- ment policies, 648, 662 ÄÍ command, 928 ÄÍ−−Â [Unix] function, 932–933 lvalue (C) assignable value for pointers, 313 Mac OS X (Apple Macintosh) operating system, 63 machine checks, 765 machine code, 200 machine-level programming arithmetic. See arithmetic arrays. See arrays buffer overﬂow. See buffer overﬂow control. See control structures data-ﬂow graphs from, 561–565 data formats, 213–215 data movement instructions, 218– 225 encodings, 205–213 ﬂoating point. See ﬂoating-point code gdb debugger, 315–316 heterogeneous data structures. See heterogeneous data structures historical perspective, 202–205 information access, 215–216 instructions, 40 operand speciﬁers, 216–218 overview, 200–202 pointer principles, 314 procedures. See procedures x86-64. See x86-64 microprocessors macros for storage allocators, 892– 893 main memory, 45 accessing, 623–625 memory modules, 620 main threads, 1022 ÀþÄÄÇ² [C Stdlib] allocate heap storage, 71, 360, 733, 875–876, 876 alignment with, 312 declaration, 170–171 dynamic memory allocation, 876– 879 interpositioning libraries, 744 wrappers for, 747 ÀþÅ þÍ²©© command, 84 mandatory alignment, 312 mangling process (C++ and Java), 716 many-core processors, 507 map disk object into memory function, 873 mapping memory. See memory mapping variables, 1030–1031 mark phase in Mark&Sweep, 903 Mark&Sweep algorithm, 902 Mark&Sweep garbage collectors, 903, 903–906 masking operations, 91 matrices adjacency, 696 multiplying, 679–683 maximum ﬂoating-point instructions, 338 maximum two’s complement number, 102 maximum unsigned number function, 99 maximum values, constants for, 104 McCarthy, John, 902 McIlroy, Doug, 52 media instructions, 330 À−Àˆ©Å©Î [CS:APP] heap model, 891 À−ÀˆÍ¾ËÂ [CS:APP] Í¾ËÂ emulator, 891 membership, set, 416–417 À−À²ÉÔ [Unix] copy bytes from one region of memory to another, 169 memory, 616 accessing, 623–625 aliasing, 535, 536 associative, 661 caches. See caches and cache memory copying bytes in, 169 data alignment in, 309–312 data hazards, 471 design, 420 dynamic. See dynamic memory allocation hazards, 471 hierarchy, 50, 50, 645–650 leaks, 911, 1028 load performance, 590–591 in logic design, 397–400 machine-language procedures, 275 machine-level programming, 206 main, 45, 620, 623–625 mapping. See memory mapping nonvolatile, 623 performance, 589–597 pipelining, 505–506 protecting, 325, 848–849 RAM. See random access memory (RAM) ROM, 623 threads, 1029–1030 trends, 638–640 virtual. See virtual memory (VM) Y86, 392 memory buses, 623 memory controllers, 619, 620 memory management units (MMUs), 840, 843 memory-mapped I/O, 634 memory mapping, 848 areas, 869, 869 −Ó−²Ì− function, 872–873 ðÇËÂ function, 872 in loading, 735 objects, 869–872 user-level, 873–875 memory mountains, 675 Core i7 microprocessors, 677 overview, 675–679 memory references operands, 217 out of bounds. See buffer overﬂow in performance, 550–553 memory stage instruction processing, 421, 423–433 PIPE processor, 490–491 sequential processing, 436 sequential Y86-64 implementation, 445–447 Y86-64 pipelining, 459 memory system, 616 memory utilization, 881, 881 À−ÀÍ−Î function, declaration, 170–171 metadata, 939, 939–940 metastable states, 617 methods hypertext transfer protocol, 987– 988 objects, 303 micro-operations, 555 microarchitecture, 46, 553 microprocessors. See central processing units (CPUs) Microsoft Windows operating system, 81 MIME (multipurpose internet mail extensions) types, 985 minimum block size, 884 minimum ﬂoating-point instructions, 338 minimum two’s complement number, 102 minimum values constants, 104 two’s complement representation, 102 1106 Index mispredicted branches handling, 479–480 performance penalties, 503, 556, 585–589 miss rates, 667 misses, caches, 506, 648 kinds, 648–649 penalties, 668, 842 rates, 667 ÀÂ®©Ë command, 928 ÀÀˆ²ÇþÄ−Í²− [CS:APP] allocator: boundary tag coalescing, 896 ÀÀˆðË−− [CS:APP] allocator: free heap block, 896 ÀÀk©ÁÂ [CS:APP] matrix multiply ij k, 681 ÀÀk©ÂÁ [CS:APP] matrix multiply ikj , 681 ÀÀˆ©Å©Î [CS:APP] allocator: initialize heap, 894 ÀÀkÁ©Â [CS:APP] matrix multiply jik, 681 ÀÀkÁÂ© [CS:APP] matrix multiply jki, 681 ÀÀkÂ©Á [CS:APP] matrix multiply kij , 681 ÀÀkÂÁ© [CS:APP] matrix multiply kj i, 681 ÀÀˆÀþÄÄÇ² [CS:APP] allocator: allocate heap block, 896, 897 ÀÀþÉ [Unix] map disk object into memory, 873, 873–875 MMUs (memory management units), 840, 843 MMX media instructions, 203, 330 Mockapetris, Paul, 967 mode bits, 771 modern processor performance, 554–567 modes kernel, 762, 764 processes, 770–772, 771 user, 762, 764 modiﬁed sequential processor implementation, 457–458 modular arithmetic, 121–122, 125 modules DRAM, 620, 621 object, 709 monitors, Java, 1046 monotonicity assumption, 882 monotonicity property, 160 Moore, Gordon, 205 Moore’s Law, 205, 205 mosaic browser, 985 motherboards, 45 Motorola RISC processors, 399 mov [instruction class] move data, 218, 218–219 ÀÇÌþ¾ÍÊ [x86-64] move absolute quad word, 219, 219 ÀÇÌ¾ [x86-64] move byte, 219 move absolute quad word instruction, 219, 219 move aligned, packed double precision instruction, 332 move aligned, packed single precision instruction, 332 move and sign-extend instruction, 220, 221 move byte instruction, 219 move data instructions, 218–225 move double precision instruction, 332 move double word instruction, 219 move if even parity instruction, 360 move if greater instruction, 253, 393 move if greater or equal instruction, 253, 393 move if less instruction, 253, 393 move if less or equal instruction, 253, 393 move if negative instruction, 253 move if nonnegative instruction, 253 move if not equal instruction, 253, 393 move if not greater instruction, 253 move if not greater or equal instruction, 253 move if not less instruction, 253 move if not less or equal instruction, 253 move if not unsigned greater instruction, 253 move if not unsigned less instruction, 253 move if not unsigned less or equal instruction, 253 move if not zero instruction, 253 move if unsigned greater instruction, 253 move if unsigned greater or equal instruction, 253 move if unsigned less instruction, 253 move if unsigned less or equal instruction, 253 move if zero instruction, 253 move instructions, conditional, 250– 256, 586–589 move quad word instruction, 219 move sign-extended byte to double word instruction, 221 move sign-extended byte to quad word instruction, 221 move sign-extended byte to word instruction, 221 move sign-extended double word to quad word instruction, 221 move sign-extended word to double word instruction, 221 move sign-extended word to quad word instruction, 221 move single precision instruction, 332 move when equal instruction, 393 move with zero extension instruction, 220, 220 move word instruction, 219 move zero-extended byte to double word instruction, 220 move zero-extended byte to quad word instruction, 220 move zero-extended byte to word instruction, 220 move zero-extended word to double word instruction, 220 move zero-extended word to quad word instruction, 220 movement operations, ﬂoating-point code, 332–337 ÀÇÌÄ [x86-64] move double word, 219 ÀÇÌÊ [x86-64] move quad word, 219 movs [instruction class] move and sign-extend, 220, 221 ÀÇÌÍ¾Ä [x86-64] move sign-extended byte to double word, 221 ÀÇÌÍ¾Ê [x86-64] move sign-extended byte to quad word, 221 ÀÇÌÍ¾Ñ [x86-64] move sign-extended byte to word, 221 ÀÇÌÍÄÊ [x86-64] move sign-extended double word to quad word, 221 ÀÇÌÍÑÄ [x86-64] move sign-extended word to double word, 221 ÀÇÌÍÑÊ [x86-64] move sign-extended word to quad word, 221 ÀÇÌÑ [x86-64] move word, 219 movz [instruction class] move with zero extension, 220, 220 ÀÇÌÖ¾Ä [x86-64] move zero-extended byte to double word, 220 ÀÇÌÖ¾Ê [x86-64] move zero-extended byte to quad word, 220 ÀÇÌÖ¾Ñ [x86-64] move zero-extended byte to word, 220 ÀÇÌÖÑÄ [x86-64] move zero-extended word to double word, 220 ÀÇÌÖÑÊ [x86-64] move zero-extended word to quad word, 220 Index 1107 ÀËÀÇÌÊ instruction, 440 ÀÏÄÊ [x86-64] unsigned multiply, 234, 234 multi-core processors, 52, 60–61, 204, 641, 1008 multi-level page tables, 855–857 multi-threading, 53–54, 61 Multics, 52 multicycle instructions, 504–505 multidimensional arrays, 294–296 multiple accumulators in parallelism, 572–577 multiple zone recording, 628 multiplexing, I/O, 1009 concurrent programming with, 1014–1021 event-driven servers based on, 1016–1021 pros and cons, 1021 multiplexors, 410, 410–411 HCL with case expression, 414 word-level, 414–416 multiplication constants, 137–139 ﬂoating point, 160, 338 instructions, 234 matrices, 679–683 two’s complement, 133–137 unsigned, 132–133, 234, 234 multiply instruction, 228 multiported random access memory, 418 multiprocessor systems, 60 multipurpose internet mail extensions (MIME) types, 985 multitasking, 769 multiway branch statements, 268–274 ÀÏÅÀþÉ [Unix] unmap disk object, 875 mutexes lock ordering rule, 1065 Pthreads, 1046 for semaphores, 1039 mutual exclusion progress graphs, 1036 semaphores for, 1038–1040 mutually exclusive access, 1036 ¿Å (newline character), 39, 927 n-gram statistics, 601 named pipes, 928 names domain, 961, 963–965 mangling and demangling processes (C++ and Java), 716, 716 protocols, 958 types, 83 Y86-64 pipelines, 463 NaN (not a number) constants, 160 ﬂoating point, 342 representation, 150, 151 nanoseconds (ns), 538 National Science Foundation (NSF), 967 Å−−®ˆË−×©®Í signal, 441 Å−−®ˆÌþÄ£ signal, 441 neg [instruction class] negate, 228 negate instruction, 228 negation, two’s complement, 131 negative overﬂow, 126, 126–127 nested arrays, 294–296 nested structures, 304 network adapters, 633 network byte order, 961 network clients, 57, 954 Network File System (NFS), 646 network programming, 954 client-server model, 954–955 Internet. See Internet networks, 955–959 sockets interface. See sockets interface summary, 1000–1001 Tiny Web server, 992–1000 Web servers, 984–992 network servers, 57, 954 networks, 56–57 acyclic, 410 LANs, 956, 956–958 WANs, 957, 957–958 never taken (NT) branch prediction strategy, 464 newline character (¿Å), 39, 927 next-ﬁt block placement policy, 885, 885 Å−ÓÎ© command, 316 NFS (Network File System), 646 nm tool, 749 no-execute (NX) memory protection, 325 no operation ÅÇÉ instruction, 322, 440 instruction code for, 441 pipelining, 466–467 in stack randomization, 322 no-write-allocate approach, 666 nodes, root, 902 nondeterminism, 784 nondeterministic behavior, 784 nonexistent variables, referencing, 910 nonlocal jumps, 759, 817, 817–822 nonuniform partitioning, 452–454 nonvolatile memory, 622 ÅÇÉ [x86-64] no operation instruction, 322, 440 instruction code for, 441 pipelining, 466–467 in stack randomization, 322 nop sleds, 322 ÅÇËþ²−l² [CS:APP] Pthreads program without a race, 1063 normal operation status code, 400, 440 normalized values, ﬂoating-point, 149, 149–150 northbridge chipsets, 624 not a number (NaN) constants, 160 ﬂoating point, 342 representation, 150, 151 not [instruction class] complement, 228 not operation Boolean, 87–88 C operators, 92–93 logic gates, 409 ns (nanoseconds), 538 NSF (National Science Foundation), 967 NSFNET, 967 nslookup program, 964 ÅÎÇ³Ä [Unix] convert network-to-host long, 961 ÅÎÇ³Í [Unix] convert network-to-host short, 961 number systems conversions. See conversions numeric limit declarations, 113 numeric ranges C standards, 97 integral types, 96–98 Java standard, 104 NX (no-execute) memory protection, 325 lÇ ﬁles, 209, 708 kﬂo optimization ﬂag, 206 kﬂp optimization ﬂag, 206 objdump gnu machine-code ﬁle reader, 209, 315, 728, 749 object code, 206, 209 object ﬁles, 209 executable. See executable object ﬁles formats, 709 forms, 709 relocatable, 41, 708, 709–711 shared, 709 tools, 749 object modules, 709 1108 Index objects C++ and Java, 302 memory-mapped, 869–872 private, 870, 870 program, 70 shared, 735, 869–872 as ÍÎËÏ²Î, 302–303 oct word, 233, 233–234 ﬂƒ [x86-64] overﬂow ﬂag condition code, 237, 391 off-by-one errors, 908 offsets GOTs, 741, 741–743 memory references, 217 PPOs, 850 unions, 306 VPOs, 850 kﬂ× optimization ﬂag, 206, 599 one-operand multiply instructions, 234 ones’-complement representation, 104 ÇÉ−Å [Unix] open ﬁle, 927, 929–931 ÇÉ−Åˆ²Ä©−ÅÎð® [CS:APP] establish connection with server, 978, 978–980 ÇÉ−ÅˆÄ©ÍÎ−Åð® [CS:APP] establish a listening socket, 980, 980 open operations for ﬁles, 927, 929–931 open shared library function, 737 open-source operating systems, 122–123 ÇÉ−Å®©Ë functions, 941 operand speciﬁers, 216–218 operate instruction, 46 operating systems (OS), 51 ﬁles, 55 hardware management, 50–51 kernels, 55 Linux, 56, 81 processes, 51–53 threads, 53–54 Unix, 71 virtual memory, 54–55 Windows, 81 operations bit-level, 90–92 logical, 92–93 shift, 93–95 optest script, 501 optimization address translation, 866 compiler, 206 levels, 534 program performance. See performance optimization blockers, 532–533, 536 OPTIONS method, 987 or [instruction class] or, 228 or operation Boolean, 87–88 C operators, 92–93 HCL expressions, 410–411 logic gates, 409 order, bytes, 78–85 disassembled code, 246 network, 961 unions, 308 origin servers, 988 OS. See operating systems (OS) Ossanna, Joe, 52 out-of-bounds memory references. See buffer overﬂow out-of-order execution, 554 ﬁve-stage pipelines, 507 history, 558 overﬂow arithmetic, 123, 123–125, 170 buffer. See buffer overﬂow ﬂoating-point values, 163 identifying, 128–129 inﬁnity representation, 151 multiplication, 138 negative, 126, 126–127 operations, 68 positive, 126, 126–127 overﬂow ﬂag condition code, 237, 391 overloaded functions (C++ and Java), 716 P semaphore operation, 1037, 1037– 1038 – [CS:APP] wrapper function for Posix sem_wait, 1038 P6 microarchitecture, 203 PA (physical addresses), 839 vs. virtual, 839–840 Y86-64, 392 packages, processor, 861 packet headers, 958 packets, 958 padding alignment, 310–311 blocks, 883 page faults DRAM caches, 844, 844–845 Linux/x86-64 systems, 765, 869– 869 memory caches, 506 pipelining caches, 844 page frames, 841 page hits in caches, 844 page table base registers (PTBRs), 850 page table entries (PTEs), 843, 843–844 Core i7, 862–864 TLBs for, 853–857, 859 page table entry addresses (PTEAs), 853 page tables, 772, 859 caches, 842–844, 843 multi-level, 855–857 paged-in pages, 845 paged-out pages, 845 pages allocation, 846 demand zero, 869 dirty, 863 physical, 841, 841–842 SSDs, 637 virtual, 325, 841, 841–842 paging demand, 846 description, 845 parallel execution, 770 parallel ﬂows, 770, 770 parallel programs, 1049 parallelism, 60, 572 instruction-level, 62, 533, 554, 598 multiple accumulators, 572–577 reassociation transformations, 577–582 SIMD, 62, 582–583 thread-level, 62 threads for, 1049–1054 parent directories, 928 parent processes, 775, 775–776 parity ﬂag condition code, 214, 342 ÉþËÍ−ˆÏË© [CS:APP] Tiny helper function, 996 ÉþËÍ−Ä©Å− [CS:APP] shell helper routine, 792 partitioning addresses, 651–652 nonuniform in pipelining, 452–454 passing data machine-language procedures, 275 pointers to structures, 302 pathnames, 929 Patterson, David, 397, 507 ÉþÏÍ− [Unix] suspend until signal arrives, 786 payloads aggregate, 881 Ethernet, 956 protocol, 958 PC. See program counters (PCs) PC-relative addressing jumps, 243, 243–245 Index 1109 symbol references, 726, 728–729 Y86-64, 395 PC selection stage in PIPE processor, 483–485 PC update stage instruction processing, 421, 423–431 sequential processing, 436 sequential Y86-64 implementation, 447 PCI (peripheral component interconnect), 634 PCIe (PCI express), 634 PE (Portable Executable) format, 709 peak utilization metric, 880–881, 881 peer threads, 1022 É−Å®©Å× bit vectors, 795 pending signals, 794 Pentium II microprocessor, 203 Pentium III microprocessor, 203–204 Pentium 4 microprocessor, 204 Pentium 4E microprocessor, 204 Pentium microprocessor, 203 PentiumPro microprocessor, 203, 558 performance, 42 Amdahl’s law, 58–60 basic strategies, 597–598 bottlenecks, 598–604 branch prediction and mispredic- tion penalties, 585–589 caches, 589, 667–669, 675–683 compiler capabilities and limitations, 534–538 expressing, 538–540 limiting factors, 584–589 loop inefﬁciencies, 544–548 loop unrolling, 567, 567–571 memory, 589–597 memory references, 550–553 modern processors, 554–567 overview, 532–534 parallelism. See parallelism procedure calls, 548–549 program example, 540–544 program proﬁling, 598–600 register spilling, 584–585 results summary, 583–584 sequential Y86-64 implementation, 448 summary, 604–605 Y86-64 pipelining, 500–504 periods (.) in dotted-decimal notation, 962 persistent connections in HTTP, 988 –ƒ [x86-64] parity ﬂag condition code, 214, 342 physical address spaces, 840 physical addresses (PA), 839 vs. virtual, 839–840 Y86-64, 392 physical page numbers (PPNs), 850 physical page offset (PPO), 850 physical pages (PPs), 841, 841–842 pi in ﬂoating-point representation, 176 PIC (position-independent code), 740 data references, 740–741 function calls, 741–743 picoseconds (ps), 449, 538 PIDs (process IDs), 775 pins, DRAM, 618–619 PIPE− processor, 457, 458, 462–466 PIPE processor stages, 475–476, 483 decode and write-back, 485–489 execute, 489–490 memory, 490–491 PC selection and fetch, 483–485 pipelining, 62, 251, 448 bubble, 470 combinational, 448–450 deep, 454–455 diagram, 449 ﬁve-stage, 507 functional units, 559–560 instruction, 585 limitations, 452–454 nonuniform partitioning, 452–454 operation, 450–452 registers, 449, 463 store operation, 591–592 systems with feedback, 455–457 Y86-64. See Y86-64 pipelined implementations pipes, 1013 Pisano, Leonardo (Fibonacci), 68 placement memory blocks, 883, 885 policies, 648, 885 platters, disk, 626, 627 PLT (procedure linkage table), 742, 742–743 pmap tool, 822 point-to-point connections, 965 pointers, 70 arithmetic, 293–294, 909 arrays relationship to, 84, 313 block, 892 creating, 84, 224 declaring, 77 dereferencing, 84, 224, 293, 313, 906–907 examples, 224 to functions, 314 machine-level data, 213 principles, 314 role, 72 stack, 275 to structures, 302 virtual memory, 906–909 ÌÇ©®h,84 polynomial evaluation, 566, 566, 608–609 pools of peer threads, 1023 pop instructions in x86-64 models, 408 pop operations on stack, 225, 225–227 ÉÇÉÊ [Y86-64] pop instruction, 226, 226, 393 behavior of, 407 code for, 440 run-time stack, 275 portability and data type size, 77 Portable Executable (PE) format, 709 portable signal handling, 810–811 ports Ethernet, 956 Internet, 966 I/O, 634 register ﬁles, 418 lÉÇÍ [Y86-64] directive, 402 position-independent code (PIC), 740 data references, 740–741 function calls, 741–743 positive overﬂow, 126, 126–127 ÉÇÍ©Óˆ−ËËÇË [CS:APP] reports Posix-style errors, 1079 Posix standards, 52 Posix-style error handling, 1078, 1079 Posix threads, 1023, 1023–1024 POST method, 987–989 PowerPC processor family, 388, 397 RISC design, 397–399 powers of 2, division by, 139–143 PPNs (physical page numbers), 850 PPO (physical page offset), 850 PPs (physical pages), 841, 841–842 precedence of shift operations, 95 precision, ﬂoating-point, 149, 173 prediction branch, 251 misprediction penalties, 585–589 Y86-64 pipelining, 458, 463–465 preempted processes, 769 prefetching mechanism, 677–678 preﬁx sums, 538, 539, 597, 609 prepare stack for return instruction, 328 preprocessors, 41, 206 prethreading, 1041–1049, 1044 primary inputs in logic gates, 410 1110 Index principle of locality, 640, 640 ÉË©ÅÎ command, 316 print getaddrinfo error message function, 974 ÉË©ÅÎð [C Stdlib] formatted printing function formatted printing, 83 numeric values with, 111 printing, formatted, 83 priorities PIPE processor forwarding sources, 487–488 write ports, 444 private address space, 770 private areas, 870 private copy-on-write structures, 872 private declarations (C++ and Java), 713 private objects, 870, 870 privileged instructions, 771 mÉËÇ² ﬁlesystem, 771, 771–772, 822 procedure linkage table (PLT), 742, 742–743 procedure return instruction, 393 procedures, 274–275 call performance, 548–549 control transfer, 277–281 data transfer, 281–284 ﬂoating-point code in, 337–338 recursive, 289–291 register usage conventions, 287–289 run-time stack, 275–277 process contexts, 52, 772 process graphs, 777, 778 process groups, 795 process IDs, 775 process tables, 772 processes, 51, 768, 774 background, 789 child, 776 concurrent ﬂow, 768–770, 769 concurrent programming with, 1009–1013 concurrent servers based on, 1010–1011 context switches, 772–773 creating and terminating, 775–779 default behavior, 780 error conditions, 781–782 exit status, 781 foreground, 789 group, 795 IDs, 775–776 loading programs, 735, 786–788 overview, 51–53 parent, 775, 776 preempted, 769 private address space, 770 vs. programs, 789 pros and cons, 1011 reaping, 779, 779–785 running programs, 786–792 sleeping, 785–786 tools, 822–823 user and kernel modes, 770–771 Ñþ©ÎÉ©® function, 782–785 zombie, 779 processor-memory gap, 49, 640 processor packages, 861 processor states, 759 processors. See central processing units (CPUs) producer-consumer problem, 1040, 1041–1042 proﬁlers code, 533 proﬁling, program, 598–600 program counters (PCs), 45,80 in fetch stage, 420 hazards, 471 machine-language procedures, 275 cË©É, 207 SEQ timing, 437 Y86-64 instruction set architecture, 392 Y86-64 pipelining, 459, 463–465 program data references locality, 642–643 program header tables, 732, 732 program registers clocked, 417–420 data hazards, 471 Y86-64, 391–392 programmable ROMs (PROMs), 623 programmer-visible state, 391, 391– 392 programs code and data, 54 concurrent. See concurrent programming forms, 40–41 loading and running, 786–788 machine-level. See machine-level programming objects, 70 vs. processes, 789 proﬁling, 598–600 running, 46–48, 789–792 Y86-64, 400–406 progress graphs, 1035, 1035–1037 deadlock regions, 1063–1064, 1064 forbidden regions, 1039 limitations, 1040 prologue blocks, 891 PROMs (programmable ROMs), 623 protection, memory, 848–849 protocol software, 958 protocols, 958 proxy caches, 988 proxy chains, 988 ps (picoseconds), 449, 538 ps tool, 822 pseudorandom number generator functions, 1057 ÉÍÏÀkþËËþÔl² [CS:APP] parallel sum program using array, 1052 ÉÍÏÀkÄÇ²þÄl² [CS:APP] parallel sum program using local variables, 1053 ÉÍÏÀkÀÏÎ−Ól² [CS:APP] parallel sum program using mutex, 1051 PTBRs (page table base registers), 850 PTEAs (page table entry addresses), 853 PTEs (page table entries), 843, 843–844 Core i7, 862–864 TLBs for, 853–857, 859 ÉÎ³Ë−þ®ˆ²þÅ²−Ä [Unix] terminate another thread, 1025 ÉÎ³Ë−þ®ˆ²Ë−þÎ− [Unix] create a thread, 1024 ÉÎ³Ë−þ®ˆ®−Îþ²³ [Unix] detach thread, 1026, 1026 ÉÎ³Ë−þ®ˆ−Ó©Î [Unix] terminate current thread, 1025 ÉÎ³Ë−þ®ˆÁÇ©Å [Unix] reap a thread, 1025 ÉÎ³Ë−þ®ˆÇÅ²− [Unix] initialize a thread, 1026, 1048 ÉÎ³Ë−þ®ˆÍ−Äð [Unix] get thread ID, 1024 Pthreads, 1023, 1023–1024, 1046 public declarations (C++ and Java), 713 push instructions in x86-64 models, 408 push operations on stack, 225, 225–227 ÉÏÍ³Ê [x86-64] push quad word, 209, 226, 226, 393 code for, 440 processing steps, 406–407, 428 run-time stack, 275 PUT method in HTTP, 987 “put to” operator (C++), 926 Index 1111 ÊÍÇËÎ function, 602 quad words, 213 QuickPath interconnect, 624, 862 ÊÏ©Î command, 316 ‡ˆ”vtˆtrˆqp (absolute addressing), 727 ‡ˆ”vtˆtrˆ–£qp (PC-relative addressing), 726 symbol table entry, 713 and Unix, 709 cËv [Y86-64] program register, 216, 391 cËv® [x86-64] low order 32 bits of register cËv, 216 cËvÑ [x86-64] low order 16 bits of register cËv, 216 cËw [Y86-64] program register, 216, 391 cËw® [x86-64] low order 32 bits of register cËw, 216 cËwÑ [x86-64] low order 16 bits of register cËw, 216 cËon [Y86-64] program register, 216, 391 cËon® [x86-64] low order 32 bits of register cËon, 216 cËonÑ [x86-64] low order 16 bits of register cËon, 216 cËoo [Y86-64] program register, 216, 391 cËoo® [x86-64] low order 32 bits of register cËoo, 216 cËooÑ [x86-64] low order 16 bits of register cËoo, 216 cËop [Y86-64] program register, 216, 391 cËop® [x86-64] low order 32 bits of register cËop, 216 cËopÑ [x86-64] low order 16 bits of register cËop, 216 cËoq [Y86-64] program register, 216, 391 cËoq® [x86-64] low order 32 bits of register cËoq, 216 cËoqÑ [x86-64] low order 16 bits of register cËoq, 216 cËor [Y86-64] program register, 216, 391 cËor® [x86-64] low order 32 bits of register cËor, 216 cËorÑ [x86-64] low order 16 bits of register cËor, 216 cËos [x86-64] program register, 216, 391 cËos® [x86-64] low order 32 bits of register cËos, 216 cËosÑ [x86-64] low order 16 bits of register cËos, 216 Ëþ²−l² [CS:APP] program with a race, 1061 race conditions, 812, 1028 concurrent programming, 1061, 1061–1063 signals, 812–814 RAM. See random access memory (RAM) ËþÅ® [CS:APP] pseudorandom number generator, 1057, 1060 ËþÅ®ˆË function, 1060 random access memory (RAM), 417, 617 dynamic. See dynamic RAM (DRAM) multiported, 418 processors, 420 SEQ timing, 437 static. See static RAM (SRAM) random operations in SSDs, 636 random replacement policies, 648 ranges asymmetric, 102, 113 bytes, 72 constants for, 103–104 data types, 76 integral types, 96–98 Java standard, 104 RAS (row access strobe) requests, 619 cËþÓ [Y86-64] program register, 216, 391 cË¾É [Y86-64] program register, 216, 391 cË¾Ó [Y86-64] program register, 216, 391 cË²Ó [Y86-64] program register, 216, 391 cË®© [Y86-64] program register, 216, 391 cË®Ó [Y86-64] program register, 216, 391 reachability graphs, 902 reachable nodes, 902 read access, 325 read and echo input lines function, 983 read bandwidth, 675 read environment variable function, 787 read/evaluate steps, 789 Ë−þ® [Unix] read ﬁle, 931, 931–933 read-only memory (ROM), 622 read-only register, 563 read operations buffered, 934, 936–937 disk sectors, 633–635 ﬁle metadata, 939–940 ﬁles, 927, 931–933 SSDs, 637 unbuffered, 933–934 uninitialized memory, 907 read ports, 418 Ë−þ®ˆË−ÊÏ−ÍÎ³®ËÍ [CS:APP] Tiny helper function, 996 read sets, 1014 read throughput, 675 read transactions descriptions, 623 example of, 624–625 read/write heads, 628 Ë−þ®®©Ë functions, 941 readelf GNU object ﬁle reader, 714, 749 readers-writers problem, 1042, 1044 reading directory contents, 941–942 disk sectors, 633 Ë−þ®Ä©Å− function, 939 Ë−þ®Å function, 939 ready read descriptors, 1014 ready sets, 1014 Ë−þÄÄÇ² function, 877 reap thread function, 1025 reaping child processes, 779, 779–785 threads, 1025 rearranging signals in pipelining, 462–463 reassociation transformations, 577, 577–582, 606 receiving signals, 794, 798–800 recording density, 627 recording zones, 628 recursive procedures, 289–291 redirection of I/O, 945, 945–946 reduced instruction set computers (RISC), 397 vs. CISC, 397–399 SPARC processors, 507 reentrancy issues, 1059–1060 reentrant functions, 802, 1059 reference bits, 863 reference counts, 942 reference machines, 543 referencing data in free heap blocks, 910–911 1112 Index referencing (continued) nonexistent variables, 910 refresh, DRAM, 618 regions, deadlock, 1063–1064, 1064 register ﬁles, 46, 394 contents, 418–419, 557 purpose, 394–395 SEQ timing, 437 register identiﬁer (ID), 394–395 register operands, 217 register speciﬁer bytes in Y86-64 instruction, 394 register to memory move instruction, 392 register to register move instruction, 392 registers, 45 clocked, 417 data hazards, 471 data transfer, 281–284 hardware, 417–420 local, 563 local storage, 287–289 loop, 563 pipeline, 449, 463 program, 391–392, 417–420, 471 read-only, 563 register ﬁles, 207 renaming, 558 spilling, 584–585 updating conventions, 215 write-only, 563 x86-64 integer, 215, 215–216 Y86-64, 395, 458–462 regular ﬁles, 869, 927 lË−Äl®þÎþ section, 711 lË−ÄlÎ−ÓÎ section, 711 relabeling signals, 462–463 relative pathnames, 929 relative speedup in parallel programs, 1055 reliable connections, 966 relocatable object ﬁles, 41, 708, 709– 711 relocation, 709, 725–726 algorithm, 727 entries, 726, 726–727 PC-relative references, 728–729 practice problems, 730–731 remove item from bounded buffer function, 1043 renaming registers, 558 Ë−É [x86-64] string repeat instruction used as no-op, 244 replacement policies, 649 replacing blocks, 648 report shared library error function, 738 reporting errors, 1079 request headers in HTTP, 987 request lines in HTTP, 987 requests client-server model, 954 HTTP, 987, 987–988 requests for comments (RFCs), 1001 reset conﬁguration in pipelining, 496 resident sets, 846 resources client-server model, 954 shared, 1040–1044 ‡¥·– [Y86-64] register ID for cËÍÉ, 440 response bodies in HTTP, 988 response headers in HTTP, 988 response lines in HTTP, 988 responses client-server model, 954 HTTP, 988, 988–989 Ë−ÍÎþËÎl² [CS:APP] nonlocal jump example, 821 restrictions, alignment, 309–312 Ë−Î [Y86-64] procedure return, 393 Ë−Î [x86-64] return from procedure call, 244, 277–278 Ë−Î instruction, 440 processing steps, 431 Y86-64 pipelining, 464–465, 491– 493, 497–498 retiming circuits, 457 retirement units, 557 Ë−ÎÊ [x86-64] return from procedure, 277 return addresses, 277 predicting, 465 procedures, 276 return penalty in CPI, 503 reverse engineering loops, 258 machine code, 201 revolutions per minute (RPM), 626 RFCs (requests for comments), 1001 ridges in memory mountains, 677 right hoinkies (|), 946 right shift operations, 93–94, 228 rings, Boolean, 88 rio [CS:APP] Robust I/O package, 933 buffered functions, 934–938 origins, 939 unbuffered functions, 933–934 Ë©ÇˆË−þ® [CS:APP] internal read function, 937 Ë©ÇˆË−þ®©Å©Î¾ [CS:APP] init read buffer, 934, 936 Ë©ÇˆË−þ®Ä©Å−¾ [CS:APP] robust buffered read, 934, 938 Ë©ÇˆË−þ®Å [CS:APP] robust unbuffered read, 933, 933–935, 937, 939 Ë©ÇˆË−þ®Å¾ [CS:APP] robust buffered read, 934, 938 Ë©ÇˆÎ [CS:APP] read buffer, 936 Ë©ÇˆÑË©Î−Å [CS:APP] robust unbuffered write, 933, 933–935, 939 Ë©É [x86-64] program counter, 207 cË©É program counter, 207 RISC (reduced instruction set computers), 397 vs. CISC, 397–399 SPARC processors, 507 Ritchie, Dennis, 38, 40, 52, 71, 950 ËÀ®©Ë command, 928 ËÀÀÇÌÊ [Y86-64] register to memory move, 392, 426, 440 ‡ﬁﬂﬁ¥ [Y86-64] ID for indicating no register, 440 Roberts, Lawrence, 967 robust buffered read functions, 934, 938 Robust I/O (rio) package, 933 buffered functions, 934–938 origins, 939 unbuffered functions, 933–934 robust unbuffered read function, 933, 933–935 robust unbuffered write function, 933, 933–935 lËÇ®þÎþ section, 710 ROM (read-only memory), 622 root directory, 928 root nodes, 902 rotating disks term, 627 rotational latency of disks, 630 rotational rate of disks, 626 round-down mode, 157, 157 round-to-even mode, 156, 156–157, 160 round-to-nearest mode, 156, 156 round-toward-zero mode, 156, 156– 157 round-up mode, 157, 157 rounding in division, 141–142 ﬂoating-point representation, 156–158 rounding modes, 156, 156–158 routers, Ethernet, 957 Index 1113 routines, thread, 1023 row access strobe (RAS) requests, 619 row-major array order, 294, 642 row-major sum function, 671, 671 RPM (revolutions per minute), 626 ËËÀÇÌÊ [Y86-64] register to register move, 392, 440 cËÍ© [x86-64] program register, 216 cËÍÉ [Y86-64] stack pointer program register215–216, 391 ËÏÅ command, 316 run concurrency, 769 run time interpositioning, 746–748 linking, 706 shared libraries, 735 stacks, 207, 275–277 running in parallel, 770 processes, 775 programs, 46–48, 786–792 lÍ assembly language ﬁles, 708 ·¡ [CS:APP] shorthand for ÍÎËÏ²Î sockaddr, 969 ·¡⁄‡ [Y86-64] status code for address exception, 440 safe optimization, 534, 534–535 safe signal handling, 802–806 safe trajectories in progress graphs, 1036 safely emit error message and terminate instruction, 802, 804 safely emit long int instruction, 802, 804 safely emit string instruction, 802, 804 sal [instruction class] shift left, 228 ÍþÄ¾ [x86-64] shift left, 231 ÍþÄÊ [x86-64] shift left, 231 ÍþÄÑ [x86-64] shift left, 231 Sandy Bridge microprocessor, 204 ·¡ﬂ« [Y86-64] status code for normal operation, 440 sar [instruction class] shift arithmetic right, 228, 231 SATA interfaces, 633 saturating arithmetic, 170 Í¾ËÂ [C Stdlib] extend the heap, 877, 877 emulator, 891 heap memory, 886 ·¾Ïð [CS:APP] shared bounded buffer package, 1041, 1042 Í¾Ïðˆ®−©Å©Î [CS:APP] free bounded buffer, 1043 Í¾Ïðˆ©Å©Î [CS:APP] allocate and init bounded buffer, 1043 Í¾Ïðˆ©ÅÍ−ËÎ [CS:APP] insert item in a bounded buffer, 1043 Í¾ÏðˆË−ÀÇÌ− [CS:APP] remove item from bounded buffer, 1043 Í¾ÏðˆÎ [CS:APP] bounded buffer used by Sbuf package, 1042 scalar code performance summary, 583–584 scalar format data, 330 scalar instructions, 332 scale factor in memory references, 217 scaling parallel programs, 1055, 1055–1056 Í²þÅð function, 906–907 schedule alarm to self function, 798 schedulers, 772 scheduling, 772 events, 799 shared resources, 1040–1044 SCSI interfaces, 633 SDRAM (synchronous DRAM), 622 second-level domain names, 964 second readers-writers problem, 1044 sectors, disk, 626, 626–628 access time, 629–631 gaps, 632 reading, 633–635 security monoculture, 321 security vulnerabilities, 43 ×−ÎÉ−−ËÅþÀ− function, 122–123 XDR library, 136 seeds for pseudorandom number generators, 1057 seek operations, 629, 927 seek time for disks, 629, 629 segmentation faults, 765 segmented addressing, 323–324 segments code, 732, 733–734 data, 732 Ethernet, 956, 956 loops, 562–563 virtual memory, 866 segregated ﬁts, 899, 900–901 segregated free lists, 899–901 segregated storage, 899 Í−Ä−²Î [Unix] wait for I/O events, 1013 self-loops, 1016 self-modifying code, 471 Í−Àˆ©Å©Î [Unix] initialize semaphore, 1038 Í−ÀˆÉÇÍÎ [Unix] V operation, 1038 Í−ÀˆÑþ©Î [Unix] P operation, 1038 semaphores, 1037, 1037–1038 concurrent server example, 1041– 1049 for mutual exclusion, 1038–1040 for scheduling shared resources, 1040–1044 sending signals, 771, 795–798 separate compilation, 706 SEQ+ pipelined implementations, 457, 457–458 SEQ Y86-64 processor design. See sequential Y86-64 implementation sequential circuits, 417 sequential execution, 236–237 sequential operations in SSDs, 636 sequential reference patterns, 642 sequential Y86-64 implementation, 420, 457 decode and write-back stage, 442–444 execute stage, 444–445 fetch stage, 440–442 hardware structure, 432–436 instruction processing stages, 420–431 memory stage, 445–447 PC update stage, 447 performance, 448 SEQ+ implementations, 457, 457–458 timing, 436–439 Í−ËÌ−ˆ®ÔÅþÀ©² [CS:APP] Tiny helper function, 999–1000 Í−ËÌ−ˆÍÎþÎ©² [CS:APP] Tiny helper function, 997–999 servers, 57 client-server model, 954 concurrent. See concurrent servers network, 57 Web. See Web servers service conversions in sockets interface, 973–978 services in client-server model, 954 serving dynamic content, 989–990 Web content, 985 set associative caches, 660 line matching and word selection, 661–662 line replacement, 661 set selection, 661, 661 set bit in descriptor set macro, 1014 set index bits, 651, 651–652 set on equal instruction, 239 set on greater instruction, 239 1114 Index set on greater or equal instruction, 239 set on less instruction, 239 set on less or equal instruction, 239 set on negative instruction, 239 set on nonnegative instruction, 239 set on not equal instruction, 239 set on not greater instruction, 239 set on not greater or equal instruction, 239 set on not less instruction, 239 set on not less or equal instruction, 239 set on not zero instruction, 239 set on unsigned greater instruction, 239 set on unsigned greater or equal instruction, 239 set on unsigned less instruction, 239 set on unsigned less or equal instruction, 239 set on unsigned not greater instruction, 239 set on unsigned not less instruction, 239 set on unsigned not less or equal instruction, 239 set on zero instruction, 239 set process group ID function, 795 set selection direct-mapped caches, 654 fully associative caches, 661 set associative caches, 661 Í−Îþ [x86-64] set on unsigned greater, 239 Í−Îþ− [x86-64] set on unsigned greater or equal, 239 Í−Î¾ [x86-64] set on unsigned less, 239 Í−Î¾− [x86-64] set on unsigned less or equal, 239 Í−Î− [x86-64] set on equal, 239 Í−Î−ÅÌ [Unix] create/change environment variable, 788 Í−Î× [x86-64] set on greater, 239 Í−Î×− [x86-64] set on greater or equal, 239 Í−ÎÁÀÉ [C Stdlib] init nonlocal jump, 759, 817, 819 Í−ÎÁÀÉl² [CS:APP] nonlocal jump example, 820 Í−ÎÄ [x86-64] set on less, 239 Í−ÎÄ− [x86-64] set on less or equal, 239 Í−ÎÅþ [x86-64] set on unsigned not greater, 239 Í−ÎÅþ− [x86-64] set on unsigned not less or equal, 239 Í−ÎÅ¾ [x86-64] set on unsigned not less, 239 Í−ÎÅ¾− [x86-64] set on unsigned not less or equal, 239 Í−ÎÅ− [x86-64] set on not equal, 239 Í−ÎÅ× [x86-64] set on not greater, 239 Í−ÎÅ×− [x86-64] set on not greater or equal, 239 Í−ÎÅÄ [x86-64] set on not less, 239 Í−ÎÅÄ− [x86-64] set on not less or equal, 239 Í−ÎÅÍ [x86-64] set on nonnegative, 239 Í−ÎÅÖ [x86-64] set on not zero, 239 Í−ÎÉ×©® [Unix] set process group ID, 795 sets vs. cache lines, 670 membership, 416–417 Í−ÎÍ [x86-64] set on negative, 239 Í−ÎÖ [x86-64] set on zero, 239 ·ƒ [x86-64] sign ﬂag condition code, 237, 391 Í³ [Unix] Unix shell program, 789 Shannon, Claude, 87 shared areas, 870 shared libraries, 55, 735 dynamic linking with, 735–737 loading and linking from applications, 737–739 shared object ﬁles, 709 shared objects, 735, 869–872, 870 shared resources, scheduling, 1040– 1044 shared variables, 1028–1031, 1029 sharing ﬁles, 942–944 virtual memory for, 848 Í³þË©Å×l² [CS:APP] sharing in Pthreads programs, 1029 Í³−ÄÄ−Ól² [CS:APP] shell main routine, 790 shells, 43, 789 shift arithmetic right instruction, 228 shift left instruction, 228 shift logical right instruction, 228 shift operations, 93, 93–95 for division, 139–143 machine language, 230–232 for multiplication, 137–139 shift arithmetic right instruction, 228 shift left instruction, 228 shift logical right instruction, 228 shl [instruction class] shift left, 228, 231 ·¤‹¶ [Y86-64] status code for ³þÄÎ, 440 short counts, 931 Í³ÇËÎ [C] integer data type, 76, 97 shr [instruction class] shift logical right, 228, 231 cÍ© [x86-64] low order 16 bits of register cËÍ©, 216 side effects, 536 Í©×ˆþÎÇÀ©²ˆÎ type, 806 Í©×þ²Î©ÇÅ [Unix] install portable handler, 811 Í©×þ®®Í−Î [Unix] add signal to signal set, 801 Í©×®−ÄÍ−Î [Unix] delete signal from signal set, 801 Í©×−ÀÉÎÔÍ−Î [Unix] clear a signal set, 801 Í©×ð©ÄÄÍ−Î [Unix] add every signal to signal set, 801 Í©×©ÅÎl² [CS:APP] catches SIGINT signal, 799 Í©×©ÍÀ−À¾−Ë [Unix] test signal set membership, 801 Í©×ÄÇÅ×ÁÀÉ [Unix] init nonlocal jump, 819, 821 sign bits ﬂoating-point representation, 173 two’s complement representation, 100 sign extension, 113, 113, 219–220 sign ﬂag condition code, 237, 391 sign-magnitude representation, 104 ·©×ÅþÄ [CS:APP] portable version of Í©×ÅþÄ, 811 signal handlers, 794 installing, 799 writing, 802–811 Y86-64, 400 Í©×ÅþÄol² [CS:APP] ﬂawed signal handler, 807 Í©×ÅþÄpl² [CS:APP] ﬂawed signal handler, 808 signals, 758, 792–794 blocking and unblocking, 800–801 correct handling, 806–810 enabling and disabling, 88 ﬂow synchronizing, 812–814 portable handling, 810–811 processes, 775 receiving, 798, 798–800 safe handling, 802–806 sending, 794, 795–798 terminology, 794–795 waiting for, 814–817 Index 1115 Y86-64 pipelined implementations, 462–463 Í©×Å−® [C] integer data type, 77 signed divide instruction, 234, 235 signed integers, 68, 76, 97–98, 103 alternate representations, 104 shift operations, 94 two’s complement encoding, 100–106 unsigned conversions, 106–112 signed multiply instruction, 234, 234 signed number representation guidelines, 119–120 ones’ complement, 104 sign magnitude, 104 signed size type, 932 signiﬁcands in ﬂoating-point representation, 148 signs for ﬂoating-point representation, 148, 148–149 SIGPIPE signal, 1000 Í©×ÉËÇ²ÀþÍÂ [Unix] block and unblock signals, 801, 817 Í©×Í−ÎÁÀÉ [Unix] init nonlocal handler jump, 817, 821 Í©×ÍÏÍÉ−Å® [Unix] wait for a signal, 817 cÍ©Ä [x86-64] low order 8 of register cËÍ©, 216 SimAquarium game, 673–674 SIMD (single-instruction, multiple- data) parallelism, 62, 330, 582, 583 SIMD streaming extensions (SSE) instructions, 312 simple segregated storage, 899, 899–900 simplicity in instruction processing, 421 simulated concurrency, 60 simultaneous multi-threading, 61 single-bit data connections, 434 single-instruction, multiple-data (SIMD) parallelism, 62, 330, 582–583 single-precision ﬂoating-point representation IEEE, 149, 149 machine-level data, 214 support for, 77 ·'ﬁ· [Y86-64] status code for illegal instruction exception, 440 Í©Çˆ−ËËÇË [CS:APP] safely emit error message and terminate, 802, 804 Í©ÇˆÄÎÇþ [CS:APP] safely emit string, 804 Í©ÇˆÉÏÎÄ [CS:APP] safely emit long int, 802, 804 Í©ÇˆÉÏÎÍ [CS:APP] safely emit string, 802, 804 Í©ÇˆÍÎËÄ−Å [CS:APP] safely emit string, 804 size blocks, 884 caches, 668–669 data, 75–78 word, 44, 75 size classes, 899 Í©Ö−ˆÎ [Unix] unsigned size type for designating sizes, 80, 119–120, 122, 135, 932 size tool, 749 Í©Ö−Çð [C] compute size of object, 81, 165–167, 169 slashes (m) for root directory, 928 ÍÄ−−É [Unix] suspend process, 785 slow system calls, 810 lÍÇ shared object ﬁle, 735 ÍÇ²Âþ®®Ë [Unix] generic socket address structure, 969 ÍÇ²Âþ®®Ëˆ©Å [Unix] Internet-style socket address structure, 969 socket addresses, 966 socket descriptors, 948, 970 ÍÇ²Â−Î function, 970 socket pairs, 966 sockets, 928, 966 sockets interface, 968, 968–969 þ²²−ÉÎ function, 972–973 address structures, 969–970 ¾©Å® function, 971 ²ÇÅÅ−²Î function, 970–971 example, 980–983 helper functions, 978–980 host and service conversions, 973–978 Ä©ÍÎ−Å function, 971 ÇÉ−Åˆ²Ä©−ÅÎð® function, 970–971 ÍÇ²Â−Î function, 970 Software Engineering Institute, 136 software exceptions C++ and Java, 822 ECF for, 759–760 vs. hardware, 760 Solaris Sun Microsystems operating system, 52,81 solid state disks (SSDs), 627, 636 beneﬁts, 623 operation, 636–638 sorting performance, 602–603 source ﬁles, 39 source hosts, 958 source programs, 39 southbridge chipsets, 624 Soviet Union, 967 cÍÉ [x86-64] low order 16 bits of stack pointer register cËÍÉ, 216 SPARC ﬁve-stage pipelines, 507 RISC processors, 399 Sun Microsystems processor, 81 spare cylinders, 632 spatial locality, 640 caches, 679–683 exploiting, 650 special arithmetic operations, 233–236 special control conditions in Y86-64 pipelining detecting, 493–495 handling, 491–493 speciﬁers, operand, 216–218 speculative execution, 555, 555, 585–586 speedup of parallel programs, 1054, 1054–1055 spilling, register, 584–585 spin loops, 814 spindles, disks, 626 cÍÉÄ [x86-64] low order 8 of stack pointer register cËÍÉ, 216 splitting free blocks, 885–886 memory blocks, 883 ÍÉË©ÅÎð [C Stdlib] function, 83, 318 Sputnik, 967 ÍÊËÎÍ® [x86-64] double-precision square root, 338 ÍÊËÎÍÍ [x86-64] single-precision square root, 338 square root ﬂoating-point instructions, 338 squashing mispredicted branch handling, 480 SRAM (static RAM), 49, 617, 617–618 cache. See caches and cache memory vs. DRAM, 618 trends, 638–639 SRAM cells, 617 ÍËþÅ® [CS:APP] pseudorandom number generator seed, 1057 SSDs (solid state disks), 627, 636 beneﬁts, 623 operation, 636–638 SSE (streaming SIMD extensions) instructions, 203–204, 330 alignment exceptions, 312 parallelism, 582–583 ÍÍ©Ö−ˆÎ [Unix] signed size type, 932 1116 Index stack corruption detection, 322–325 stack frames, 276, 276–277 alignment on, 312 variable-size, 326–329 stack pointers, 275 stack protectors, 322–323 stack randomization, 320–322 stack storage allocation function, 326, 360 stacks, 55, 225, 225–227 bottom, 226 buffer overﬂow, 907 with −Ó−²Ì− function, 787–788 local storage, 284–287 machine-level programming, 207 overﬂow. See buffer overﬂow recursive procedures, 289–291 run time, 275–277 top, 226 Y86-64 pipelining, 465 stages, SEQ, 420–431 decode and write-back, 442–444 execute, 444–445 fetch, 440–442 memory stage, 445–447 PC update, 447 stalling for data hazards, 478 pipeline, 469–472, 495–496 Stallman, Richard, 42, 52 standard C library, 40, 40–41 standard error ﬁles, 927 standard I/O library, 947, 947 standard input ﬁles, 927 standard output ﬁles, 927 Standard Unix Speciﬁcation, 52 ˆÍÎþËÎ, 734 starvation in readers-writers problem, 1044 ÍÎþÎ [Unix] fetch ﬁle metadata, 939–940 state machines, 1016 states bistable memory, 617 deadlock, 1063 processor, 759 programmer-visible, 391, 391–392 progress graphs, 1035 state machines, 1016 static libraries, 720, 720–724 static linkers, 708 static linking, 708 static RAM (SRAM), 49, 617–618 cache. See caches and cache memory vs. DRAM, 618 trends, 638–639 ÍÎþÎ©² [C] variable and function attribute, 712, 713, 1030 static variables, 1030, 1030–1031 static Web content, 985 status code registers, 471 status codes HTTP, 989 Y86-64, 399–400, 400 status messages in HTTP, 989 status register hazards, 471 ·¶⁄¥‡‡ˆƒ'‹¥ﬁﬂ [Unix] constant for standard error descriptor, 927 ÍÎ®−ËË stream, 947 ·¶⁄'ﬁˆƒ'‹¥ﬁﬂ [Unix] constant for standard input descriptor, 927 ÍÎ®©Å stream, 947 ÍÎ®©ÅÎl³ ﬁle, 103 zÍÎ®©Çl³| [Unix] standard I/O library header ﬁle, 120, 122 ÍÎ®Ä©¾, 40, 40–41 ·¶⁄ﬂ•¶ˆƒ'‹¥ﬁﬂ [Unix] constant for standard output descriptor, 927 ÍÎ®ÇÏÎ stream, 947 ÍÎ−É© command, 316 ÍÎ−É©r command, 316 Stevens, W. Richard, 939, 950, 1001, 1077 stopped processes, 775 storage. See also information storage device hierarchy, 50 registers, 287–289 stack, 284–287 storage classes for variables, 1030–1031 store buffers, 593–594 store instructions, 46 store operations example, 624 processors, 557 store performance of memory, 591– 597 strace tool, 822 straight-line code, 236–237 ÍÎË²þÎ [C Stdlib] string concatenation function, 318 ÍÎË²ÉÔ [C Stdlib] string copy function, 318 streaming SIMD extensions (SSE) instructions, 203–204, 330 alignment exceptions, 312 parallelism, 582–583 streams, 947 buffers, 947 directory, 941 full duplex, 948 ÍÎË−ËËÇË function, 774 stride-1 reference patterns, 642 stride-k reference patterns, 642 string concatenation function, 318 string copy function, 318 string generation function, 318 strings in buffer overﬂow, 315, 317 length, 119 lowercase conversions, 545–547 representing, 85 strings tool, 749 strip tool, 749 ÍÎËÄ−Å [C Stdlib] string length function, 119, 545–547 strong scaling, 1055 strong symbols, 716 lÍÎËÎþ¾ section, 711 ÍÎËÎÇÂ [C Stdlib] string function, 1060 ÍÎËÏ²Î [C] structure data type, 301 structures address, 969–970 heterogeneous. See heterogeneous data structures machine-level programming, 207 sub [instruction class] subtract, 228 subdomains, 963 ÍÏ¾Ê [Y86-64] subtract, 392, 424 substitution, inline, 537 subtract instruction, 228 subtract operation in execute stage, 444 subtraction, ﬂoating-point, 338 ÍÏÀþËËþÔ²ÇÄÍ [CS:APP] column- major sum, 672 ÍÏÀþËËþÔËÇÑÍ [CS:APP] row-major sum, 671, 671 ÍÏÀÌ−² [CS:APP] vector sum, 670, 671–672 Sun Microsystems, 81 ﬁve-stage pipelines, 507 RISC processors, 399 security vulnerability, 136 supercells, 618, 618–619 superscalar processors, 62, 507, 554 supervisor mode, 771 surfaces, disks, 626, 631 suspend process function, 785 suspend until signal arrives function, 786 suspended processes, 775 swap areas, 869 swap ﬁles, 869 swap space, 869 swapped-in pages, 845 Index 1117 swapped-out pages, 845 swapping pages, 845 sweep phase in Mark&Sweep garbage collectors, 903 Swift, Jonathan, 79 ÍÑ©Î²³ [C] multiway branch statement, 268–274 switches, context, 772–773 symbol resolution, 709, 715 duplicate symbol names, 716–720 static libraries, 720–724 symbol tables, 711, 711–715 symbolic links, 928 symbolic methods, 502 symbols address translation, 850 caches, 653 global, 711 local, 712 relocation, 725–731 strong and weak, 716 lÍÔÀÎþ¾ section, 711 synchronization ﬂow, 812–814 Java threads, 1046 progress graphs, 1036 threads, 1031–1035 progress graphs, 1035–1037 with semaphores. See sema- phores synchronization errors, 1031 synchronous DRAM (SDRAM), 622 synchronous exceptions, 763 mÍÔÍ ﬁlesystem, 772 ÍÔÍ²þÄÄ function, 766 system bus, 623 system calls, 53, 763, 763–764 error handling, 773–774 Linux/x86-64 systems, 766–767 slow, 810 system-level functions, 766 system-level I/O closing ﬁles, 930–931 ﬁle metadata, 939–940 I/O redirection, 945–946 opening ﬁles, 929–931 packages summary, 947–949 reading ﬁles, 931–933 rio package, 933–939 sharing ﬁles, 942–944 standard, 947 summary, 949–950 Unix I/O, 926–927 writing ﬁles, 932–933 system startup function, 734 System V Unix, 52 semaphores, 1013 shared memory, 1013 T2B (two’s complement to binary conversion), 96, 101, 107 T2U (two’s complement to unsigned conversion), 96, 107, 107–109 tables descriptor, 943, 945 exception, 761, 761 GOTs, 741, 741–743 hash, 603–604 header, 710, 732 jump, 269, 270–271, 761 page, 772, 842–844, 843, 855–857, 859 program header, 732, 732 symbol, 711, 711–715 tag bits, 651, 652 tags, boundary, 887, 887–890, 895 Tanenbaum, Andrew S., 56 target functions in interpositioning libraries, 744 targets, jump, 242, 242–245 TCP (Transmission Control Protocol), 960 TCP/IP (Transmission Control Protocol/Internet Protocol), 960 Î²Í³ [Unix] Unix shell program, 789 telnet remote login program, 986, 986–987 temporal locality, 640 blocking for, 683 exploiting, 650 terminate another thread function, 1025 terminate current thread function, 1025 terminate process function, 775 terminated processes, 775 terminating processes, 775–779 threads, 1024–1025 test [instruction class] Test, 238 test byte instruction, 238 test double word instruction, 238 test instructions, 238 test quad word instruction, 238 test signal set membership instruction, 801 test word instruction, 238 Î−ÍÎ¾ [x86-64] test byte, 238 testing Y86-64 pipeline design, 501 Î−ÍÎÄ [x86-64] test double word, 238 Î−ÍÎÊ [x86-64] test quad word, 238 Î−ÍÎÑ [x86-64] test word, 238 text ﬁles, 39, 927, 928, 936 text lines, 927, 934 text representation ASCII, 85 Unicode, 86 lÎ−ÓÎ section, 710 Thompson, Ken, 52 thrashing direct-mapped caches, 658, 658–659 pages, 846 thread contexts, 1022, 1029 thread IDs (TIDs), 1022 thread-level concurrency, 60–62 thread-level parallelism, 62 thread routines, 1023, 1024 thread-safe functions, 1056, 1056–1058 thread-unsafe functions, 1056, 1056– 1058 threads, 53, 54, 1009, 1021–1022 concurrent server based on, 1027– 1028 creating, 1024 detaching, 1025–1026 execution model, 1022–1023 initializing, 1026 library functions for, 1060–1061 mapping variables in, 1030–1031 memory models, 1029–1030 for parallelism, 1049–1054 Posix, 1023–1024 races, 1061–1063 reaping, 1025 safety issues, 1056–1058 shared variables with, 1028–1031, 1029 synchronizing, 1031–1035 progress graphs, 1035–1037 with semaphores. See sema- phores terminating, 1024–1025 three-stage pipelines, 450–452 throughput, 560 dynamic memory allocators, 881 pipelining for. See pipelining read, 675 throughput bounds, 554, 560 TIDs (thread IDs), 1022 time slicing, 769 timing, SEQ, 436–439 Tiny [CS:APP] Web server, 992, 992–1000 TLB index (TLBI), 853 TLB tags (TLBT), 853, 859 TLBI (TLB index), 853 1118 Index TLBs (translation lookaside buffers), 506, 853, 853–861 TLBT (TLB tags), 853, 859 TMax (maximum two’s complement number), 96, 101, 102 TMin (minimum two’s complement number), 96, 101, 102, 113 top of stack, 226, 226 top tool, 822 topological sorts of vertices, 778 Torvalds, Linus, 56 touching pages, 869 TRACE method, 987 tracing execution, 423, 430–431, 439 track density of disks, 627 tracks, disk, 626, 631 trajectories in progress graphs, 1036, 1036 transactions bus, 623, 624–625 client-server model, 954 client-server vs. database, 955 HTTP, 986–989 transfer time for disks, 630 transfer units, 648 transferring control, 277–281 transformations, reassociation, 577, 577–582, 606 transistors in Moore’s Law, 205 transitions progress graphs, 1035 state machines, 1016 translating programs, 40–41 translation address. See address translation ÍÑ©Î²³ statements, 269 translation lookaside buffers (TLBs), 506, 853, 853–861 Transmission Control Protocol (TCP), 960 Transmission Control Protocol/ Internet Protocol (TCP/IP), 960 trap exception class, 763 traps, 763, 763–764 tree height reduction, 606 tree structure, 306–307 truncating numbers, 117–118 two-operand multiply instructions, 234 two-way parallelism, 572–573 two’s-complement representation addition, 126–131 asymmetric range, 102, 113 bit-level representation, 132 encodings, 68 minimum value, 101 multiplication, 133–137 negation, 131 signed and unsigned conversions, 106–110 signed numbers, 100, 100–106 ÎÔÉ−®−ð [C] type deﬁnition, 80,83 types conversions. See conversions ﬂoating point, 160–162 integral, 96, 96–98 machine-level, 207, 213–214 MIME, 985 naming, 83 pointers, 72, 313 pointers associated with, 70 U2B (unsigned to binary conversion), 96, 100, 107, 110 U2T (unsigned to two’s-complement conversion), 96, 107, 109, 118 Ï²ÇÀ©Í® [x86-64] compare double precision, 342 Ï²ÇÀ©ÍÍ [x86-64] compare single precision, 342 UDP (Unreliable Datagram Protocol), 960 •'ﬁ¶ˆ›¡” constant, maximum unsigned integer, 104 •'ﬁ¶Nˆ›¡” [C] maximum value of N -bit unsigned data type, 103 Ï©ÅÎNˆÎ [C] N -bit unsigned integer data type, 103 ÏÀþÍÂ function, 930–931 UMax (maximum unsigned number), 99, 102–103 unallocated pages, 841 unary operations, 230 unblocking signals, 800–801 unbuffered input and output, 933–934 uncached pages, 842 unconditional jump instruction, 393 underﬂow, gradual, 151 Unicode characters, 86 uniﬁed caches, 667 uniform resource identiﬁers (URIs), 987 uninitialized memory, reading, 907 unions, 80, 305–309 uniprocessor systems, 52, 60 United States, ARPA creation in, 967 universal resource locators (URLs), 985 Universal Serial Bus (USB), 632 Unix 4.xBSD, 52, 968 ÏÅ©Óˆ−ËËÇË [CS:APP] reports Unix- style errors, 774, 774, 1079 Unix IPC, 1013 Unix operating systems, 52, 52, 71 constants, 782 error handling, 1079, 1079 I/O, 55, 926, 926–927 Unix signals, 795 unlocking mutexes, 1039 unmap disk object function, 875 unordered, ﬂoating-point comparison outcome, 342 unpack and interleave low packed double precision instruction, 334 unpack and interleave low packed single precision instruction, 334 Unreliable Datagram Protocol (UDP), 960 unrolling k × 1, 567 k × 1a, 580 k × k, 575–576 loops, 538, 540, 567, 567–571, 608 unsafe regions in progress graphs, 1036 unsafe trajectories in progress graphs, 1036 ÏÅÍ−Î−ÅÌ [Unix] delete environment variable, 788 ÏÅÍ©×Å−® [C] integer data type, 77, 97 unsigned representations, 119–120 addition, 120–126 conversions, 106–112 division, 234, 235 encodings, 68, 98–100 integers, 76 maximum value, 99 multiplication, 132–133, 234, 234 ÏÅÍ©×Å−® size type, 932 update instructions, 45–46 URIs (uniform resource identiﬁers), 987 URLs (universal resource locators), 985 USB (Universal Serial Bus), 632 user-level memory mapping, 873–875 user mode, 762 processes, 770–772, 771 regular functions in, 764 user stack, 55 UTF-8 characters, 86 ‚ [CS:APP] wrapper function for Posix sem_post, 1038 Index 1119 v-node tables, 942 V semaphore operation, 1037, 1037– 1038 VA. See virtual addresses (VA) Ìþ®®Í® [x86-64] double-precision addition, 338 Ìþ®®ÍÍ [x86-64] single-precision addition, 338 valgrind program, 605 valid bit cache lines, 651 page tables, 843 values, pointers, 72, 313 ÌþÅ®É® [x86-64] and packed double precision, 341 ÌþÅ®ÉÍ [x86-64] and packed single precision, 341 variable-size stack frames, 326–329 variable-size arrays, 298–301 variables mapping, 1030–1031 nonexistent, 910 shared, 1028–1031, 1029 storage classes, 1030–1031 VAX computers (Digital Equipment Corporation), Boolean operations, 92 Ì²ÌÎÉÍpÉ® [x86-64] convert packed single to packed double precision, 334 Ì²ÌÎÍ©pÍ® [x86-64] convert integer to double precision, 333 Ì²ÌÎÍ©pÍ®Ê [x86-64] convert quad- word integer to double precision, 333 Ì²ÌÎÍ©pÍÍ [x86-64] convert integer to single precision, 333 Ì²ÌÎÍ©pÍÍÊ [x86-64] convert quad- word integer to single precision, 333 Ì²ÌÎÎÍ®pÍ© [x86-64] convert double precision to integer, 333 Ì²ÌÎÎÍ®pÍ©Ê [x86-64] convert double precision to quad-word integer, 333 Ì²ÌÎÎÍÍpÍ© [x86-64] convert single precision to integer, 333 Ì²ÌÎÎÍÍpÍ©Ê [x86-64] convert single precision to quad-word integer, 333 Ì®©ÌÍ® [x86-64] double-precision division, 338 Ì®©ÌÍÍ [x86-64] single-precision division, 338 vector data types, 62, 540–543 vector dot product function, 658 vector registers, 207, 582 vector sum function, 670, 671–672 vectors, bit, 87, 87–88 veriﬁcation in pipelining, 502 Verilog hardware description language for logic design, 409 Y86-64 pipelining implementation, 503 vertical bars ÚÚ for or operation, 409 VHDL hardware description language, 409 victim blocks, 648 Video RAM (VRAM), 622 virtual address spaces, 54, 70, 840 virtual addresses (VA) machine-level programming, 206– 207 vs. physical, 839–840 Y86-64, 392 virtual machines as abstraction, 63 Java byte code, 346 virtual memory (VM), 51, 54, 70, 838 as abstraction, 63 address spaces, 840–841 address translation. See address translation bugs, 906–911 for caching, 841–847 characteristics, 838–839 Core i7, 861–864 dynamic memory allocation. See dynamic memory allocation garbage collection, 901–906 Linux, 866–869 in loading, 735 managing, 875 mapping. See memory mapping for memory management, 847–848 for memory protection, 848–849 overview, 54–55 physical vs. virtual addresses, 839–840 summary, 911–912 virtual page numbers (VPNs), 850 virtual page offset (VPO), 850 virtual pages (VPs), 325, 841, 841–842 viruses, 321–322 VLOG implementation of Y86-64 pipelining, 503 VM. See virtual memory (VM) ÌÀþÓÍ® [x86-64] double-precision maximum, 338 ÌÀþÓÍÍ [x86-64] single-precision maximum, 338 ÌÀ©ÅÍ® [x86-64] double-precision minimum, 338 ÌÀ©ÅÍÍ [x86-64] single-precision minimum, 338 ÌÀÇÌþÉ® [x86-64] move aligned, packed double precision, 332 ÌÀÇÌþÉÍ [x86-64] move aligned, packed single precision, 332 ÌÀÇÌÍ® [x86-64] move double precision, 332 ÌÀÇÌÍÍ [x86-64] move single precision, 332 ÌÀÏÄÍ® [x86-64] double-precision multiplication, 338 ÌÀÏÄÍÍ [x86-64] single-precision multiplication, 338 ÌÇ©®h [C] untyped pointers, 84 ÌÇÄþÎ©Ä− [C] volatile type qualiﬁer, 805–806 VP (virtual pages), 325, 841, 841–842 VPNs (virtual page numbers), 850 VPO (virtual page offset), 850 VRAM (video RAM), 622 ÌÍÏ¾Í® [x86-64] double-precision subtraction, 338 ÌÍÏ¾ÍÍ [x86-64] single-precision subtraction, 338 vtune program, 605 vulnerabilities, security, 122–123 ÌÏÅÉ²ÂÄÉ® [x86-64] unpack and interleave low packed double precision, 334 ÌÏÅÉ²ÂÄÉÍ [x86-64] unpack and interleave low packed single precision, 334 ÌÓÇËÉ® [x86-64] exclusive-or packed double precision, 341 ÌÓÇËÉÍ [x86-64] exclusive-or packed single precision, 341 Ñþ©Î [Unix] wait for child process, 782 wait for child process functions, 780, 782–785 wait for client connection request function, 972, 972–973 wait for signal instruction, 817 Ñþ©Îl³ ﬁle, 782 wait sets, 780, 780 waiting for signals, 814–817 Ñþ©ÎÉ©® [Unix] wait for child process, 779, 782–785 Ñþ©ÎÉ©®o [CS:APP] Ñþ©ÎÉ©® example, 783 1120 Index Ñþ©ÎÉ©®p [CS:APP] Ñþ©ÎÉ©® example, 785 WANs (wide area networks), 957, 957–958 warming up caches, 648 WCONTINUED constant, 780 weak scaling, 1055, 1056 weak symbols, 716 wear leveling logic, 637 Web clients, 984, 984 Web servers, 737, 984 basics, 984–985 dynamic content, 989–990 HTTP transactions, 986–989 Tiny example, 992–1000 Web content, 985–986 well-known ports, 966 well-known service names, 966 Ñ³©Ä− [C] loop statement, 259–264 wide area networks (WANs), 957, 957–958 WIFEXITED constant, 781 WIFEXITSTATUS constant, 781 WIFSIGNALED constant, 781 WIFSTOPPED constant, 781 Windows Microsoft operating system, 63, 81 wire names in hardware diagrams, 434 WNOHANG constant, 780–781 word-level combinational circuits, 412–416 word selection direct-mapped caches, 655 fully associative caches, 663–664 set associative caches, 661–662 word size, 44, 75 words, 44, 213 working sets, 649, 846 world-wide data connections in hardware diagrams, 434 World Wide Web, 985 worm programs, 320–322 wrapper functions, 747 error handling, 774, 1077, 1079– 1081 interpositioning libraries, 744 write access, 325 write-allocate approach, 666 write-back approach, 666 write-back stage instruction processing, 421, 423–433 PIPE processor, 485–489 sequential processing, 436 sequential Y86-64 implementation, 442–444 ÑË©Î− [Unix] write ﬁle, 931, 932–933 write hits, 666 write issues for caches, 666–667 write-only register, 563 write operations for ﬁles, 927, 932– 933 write ports priorities, 444 register ﬁles, 418 write/read dependencies, 593–595 write strategies for caches, 669 write-through approach, 666 write transactions, 623, 624–625 ÑË©Î−Å function, 939 writers in readers-writers problem, 1042, 1044 writing signal handlers, 802–811 SSD oprations, 636 WSTOPSIG constant, 781 WTERMSIG constant, 781 WUNTRACED constant, 780–781 x86 Intel microprocessor line, 202 x86-64 instruction set architecture vs. Y86-64, 396 x86-64 microprocessors, 204 array access, 292 conditional move instructions, 250–256 data alignment, 312 exceptions, 765–767 Intel-compatible 64-bit micropro- cessors, 81 machine language, 201–202 registers data movement, 218–225 operand speciﬁers, 216–218 vs. Y86-64, 401–402 x87 microprocessors, 203 XDR library security vulnerability, 136 cÓÀÀ [x86-64] 16-byte media register. Subregion of YMM, 331 cÓÀÀn, return ﬂoating-point value register, 335, 337 XMM, SSE vector registers, 330–332 xor [instruction class] exclusive-or, 228 ÓÇËÊ [Y86-64] exclusive-or, 392 Y86-64 instruction set architecture, 389–390 details, 406–408 exception handling, 399–400 hazards, 471 instruction encoding, 394–396 instruction set, 392–394 programmer-visible state, 391– 392 programs, 400–406 sequential implementation. See sequential Y86-64 implementation vs. x86-64, 396 Y86-64 pipelined implementations, 457 computation stages, 457–458 control logic. See control logic in pipelining exception handling, 480–483 hazards. See hazards in pipelining memory system interfacing, 505– 506 multicycle instructions, 504–505 performance analysis, 500–504 predicted values, 463–465 register insertions, 458–462 signals, 462–463 stages. See PIPE processor stages testing, 501 veriﬁcation, 502 Verilog, 503 yas Y86-64 assembler, 402 yis Y86-64 instruction set simulator, 402 cÔÀÀ [x86-64] 32-byte media register, 331 YMM, AVX vector registers, 330–332 zero extension, 113 zero ﬂag condition code, 237, 342, 391 …ƒ [x86-64] zero ﬂag condition code, 237, 342, 391 zombie processes, 779, 779–780, 806 zones, recording, 628 For these Global Editions, the editorial team at Pearson has collaborated with educators across the world to address a wide range of subjects and requirements, equipping students with the best possible learning tools. This Global Edition preserves the cutting-edge approach and pedagogy of the original, but also features alterations, customization, and adaptation from the North American version.Computer Systems A Programmer’s PerspectiveBryant • O’HallaronTHird EdiTiON GlOBAl EdiTiON This is a special edition of an established title widely used by colleges and universities throughout the world. Pearson published this exclusive edition for the benefit of students outside the United States and Canada. if you purchased this book within the United States or Canada, you should be aware that it has been imported without the approval of the Publisher or Author. Pearson Global Edition GlOBAl EdiTiON Computer Systems A Programmer’s Perspective THird EdiTiON Randal E. Bryant • David R. O’HallaronGlOBAl EdiTiON Bryant_1292101768_mech.indd 1 07/05/15 3:22 PM","libVersion":"0.5.0","langs":""}